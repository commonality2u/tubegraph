---
title: Nvidias market vulnerability to competition
videoId: OC61Vo4tAaE
---

From: [[bankless]] <br/> 

Jeffrey Emanuel, an investor and technologist with deep insights into AI research advances, authored an article titled "The Short Case for Nvidia Stock," which gained significant traction after the recent market fluctuations. Emanuel argues that while the media attributes recent market crashes, including the wiping out of $2 trillion from global equity markets, primarily to the announcement of the [[DeepSeek AI model and its impact on Nvidia | DeepSeek AI model]], the underlying factors contributing to Nvidia's market share unbundling are more complex and pre-existing <a class="yt-timestamp" data-t="00:00:07">[00:00:07]</a>, <a class="yt-timestamp" data-t="02:07:00">[02:07:00]</a>. His analysis suggests that Nvidia's dominant position is vulnerable due to a confluence of factors, including hardware competition, software efficiencies, and unsustainable business margins <a class="yt-timestamp" data-t="02:09:00">[02:09:00]</a>.

## Nvidia's Moat Under Threat
Emanuel contends that the "short thesis" for Nvidia's stock remains strong, even without considering the specific impact of the [[DeepSeek AI model and its impact on Nvidia | DeepSeek AI model]] <a class="yt-timestamp" data-t="07:34:00">[07:34:00]</a>. He emphasizes that no company, save for a regulatory-enforced monopoly, can indefinitely print infinite profits with triple-digit revenue growth and 90% gross margins without attracting intense competition <a class="yt-timestamp" data-t="11:11:00">[11:11:00]</a>. Nvidia's high margins on data center revenue are particularly susceptible, as this revenue largely originates from a small number of hyperscalers <a class="yt-timestamp" data-t="12:06:00">[12:06:00]</a>.

### Hardware Competition
A significant threat to Nvidia's market position comes from its own customers, who are increasingly developing custom silicon to reduce reliance on Nvidia's expensive GPUs <a class="yt-timestamp" data-t="12:44:00">[12:44:00]</a>.

*   **Custom Silicon Development:** Major hyperscalers like Amazon, Microsoft, OpenAI, and Meta are actively designing their own custom chips for both training and inference <a class="yt-timestamp" data-t="12:51:00">[12:51:00]</a>. These custom chips don't need to outperform Nvidia's; they only need to be cost-effective. Given that Nvidia charges approximately 10 times its manufacturing cost for GPUs, a self-produced chip could be five times less powerful and still represent significant savings for hyperscalers <a class="yt-timestamp" data-t="13:13:00">[13:13:00]</a>, <a class="yt-timestamp" data-t="24:41:00">[24:41:00]</a>. This strategy allows hyperscalers to reduce costs and gain more control over their infrastructure <a class="yt-timestamp" data-t="13:31:00">[13:31:00]</a>.
*   **Specialized Chip Companies:**
    *   **Cerebras:** This company manufactures wafer-scale chips, which are essentially entire silicon wafers transformed into a single, enormous chip <a class="yt-timestamp" data-t="14:46:00">[14:46:00]</a>. This design reduces the need for complex GPU interconnects, a key aspect of Nvidia's moat, by integrating everything onto one large unit <a class="yt-timestamp" data-t="15:00:00">[15:00:00]</a>.
    *   **Groq:** Groq (not to be confused with the Twitter-linked "Grok") has developed specialized hardware optimized solely for AI inference, not training <a class="yt-timestamp" data-t="21:10:00">[21:10:00]</a>. Their chips offer significantly higher inference speeds (e.g., 1500 tokens per second compared to 40-50 on a high-end consumer GPU) <a class="yt-timestamp" data-t="22:19:00">[22:19:00]</a>. While Groq's servers are expensive, their efficiency makes them cheaper to operate for high-demand inference tasks, diverting revenue from Nvidia <a class="yt-timestamp" data-t="22:30:00">[22:30:00]</a>.
*   **The "Cuda" Software Moat:** Nvidia's proprietary software platform, Cuda, has been a significant barrier to competition, making it easy for developers to utilize Nvidia's GPUs efficiently <a class="yt-timestamp" data-t="30:28:00">[30:28:00]</a>. However, this moat is also being eroded:
    *   **Higher-Level Frameworks:** The emergence of higher-level frameworks like MLX and Triton allows developers to express parallelized programming in a way that can be compiled and run on various hardware, not just Nvidia's <a class="yt-timestamp" data-t="33:06:00">[33:06:00]</a>.
    *   **LLM-Assisted Code Porting:** Large Language Models (LLMs) are exceptionally proficient at porting code between different programming languages and frameworks <a class="yt-timestamp" data-t="34:37:00">[34:37:00]</a>. This capability could allow developers to write algorithms in Cuda (as a "specification language") and then use LLMs to translate them for use on AMD GPUs or other custom silicon, effectively bypassing Nvidia's hardware lock-in <a class="yt-timestamp" data-t="35:14:00">[35:14:00]</a>.

### Algorithmic Efficiency and the [[DeepSeek AI model and its impact on Nvidia | DeepSeek AI Model]]
The [[DeepSeek AI model and its impact on Nvidia | DeepSeek AI model]], developed in China, demonstrated a remarkable 45x efficiency improvement over US-based AI models and offered API calls at 95% less cost than ChatGPT <a class="yt-timestamp" data-t="01:32:00">[01:32:00]</a>, <a class="yt-timestamp" data-t="01:37:00">[01:37:00]</a>. This breakthrough, released in December, but gaining market attention in January, sparked a 20% drop in Nvidia's stock, wiping out $600 billion in market value <a class="yt-timestamp" data-t="01:43:00">[01:43:00]</a>.

*   **Necessity as the Mother of Invention:** Emanuel suggests that Chinese AI labs, operating with fewer resources and facing export controls on advanced chips, have been compelled to innovate in terms of software and algorithmic efficiency <a class="yt-timestamp" data-t="45:06:00">[45:06:00]</a>. Unlike Western companies that can "throw money at a problem," Chinese developers have focused on maximizing performance from limited hardware <a class="yt-timestamp" data-t="45:14:00">[45:14:00]</a>.
*   **Integrated Research and Engineering:** DeepSeek's success is partly attributed to a more integrated approach where researchers and engineers collaborate closely, optimizing algorithms for hardware performance from the outset <a class="yt-timestamp" data-t="49:46:00">[49:49:00]</a>.
*   **Key Efficiency Innovations:**
    *   **Efficient KV Caches:** DeepSeek implemented a smarter way to store key-value caches and indices in memory, which are crucial for Transformer models <a class="yt-timestamp" data-t="52:45:00">[52:45:00]</a>. By storing only the meaningful subset of data in a compressed form, they significantly reduced memory usage and data transfer, leading to faster calculations <a class="yt-timestamp" data-t="53:14:00">[53:14:00]</a>, <a class="yt-timestamp" data-t="57:41:00">[57:41:00]</a>.
    *   **Multi-Token Predictions (Speculative Decoding):** Instead of predicting one token at a time, DeepSeek's models accurately predict multiple tokens simultaneously (e.g., two or three) <a class="yt-timestamp" data-t="55:13:00">[55:13:00]</a>. By achieving a 95% accuracy rate in "speculative decoding," they effectively doubled inference throughput without additional computational cost <a class="yt-timestamp" data-t="55:50:00">[55:50:00]</a>.
    *   **Direct Low-Precision Training:** Traditional models are trained at higher precision and then "quantized" (rounded off) for cheaper GPUs, which can hurt accuracy <a class="yt-timestamp" data-t="56:46:00">[56:46:00]</a>. DeepSeek found a way to train the entire model using a smaller, compressed representation from the start, improving efficiency without sacrificing quality <a class="yt-timestamp" data-t="57:26:00">[57:26:00]</a>.
*   **Market Impact:** The 45x efficiency gain, whether entirely accurate or not, points to a massive over-provisioning of compute resources across the industry <a class="yt-timestamp" data-t="06:36:00">[06:36:00]</a>. This means future demand for hardware might be significantly lower than current projections. While the [[AI efficiency improvements and market implications | Jevons Paradox]] suggests efficiency can increase overall demand, there will likely be temporary dislocations and strategic re-evaluations of capital expenditure by major players, potentially reducing Nvidia's sales <a class="yt-timestamp" data-t="08:01:00">[08:01:00]</a>, <a class="yt-timestamp" data-t="09:05:00">[09:05:00]</a>. The 95% lower inference cost from DeepSeek also puts immense pressure on companies like OpenAI and Anthropic to lower their API prices, directly impacting their revenue streams <a class="yt-timestamp" data-t="59:50:00">[59:50:00]</a>.

## Market Dynamics and Valuation
Emanuel argues that Nvidia's stock was "priced to perfection," with investment banks caught off guard by the sudden re-evaluation <a class="yt-timestamp" data-t="01:07:01">[01:07:01]</a>. He compares Nvidia's situation to a historical example of a company with unsustainably high margins that faced a predictable decline once competition emerged <a class="yt-timestamp" data-t="41:11:00">[41:11:00]</a>. The entry of even one or two competitors can cause margins to fall dramatically due to market forces, even if the overall AI "pie" continues to grow <a class="yt-timestamp" data-t="39:50:00">[39:50:00]</a>, <a class="yt-timestamp" data-t="43:43:00">[43:43:00]</a>.

The market's surprise at the [[AI efficiency improvements and market implications | step-function changes]] in AI efficiency, rather than predictable Moore's Law progression, highlights the fragility of valuations based on unsustainable growth rates and margins <a class="yt-timestamp" data-t="01:06:09">[01:06:09]</a>, <a class="yt-timestamp" data-t="01:19:00">[01:19:00]</a>. This shift benefits AI consumers, as products become more powerful and accessible, even on personal devices <a class="yt-timestamp" data-t="01:09:05">[01:09:05]</a>, <a class="yt-timestamp" data-t="01:29:25">[01:29:25]</a>.
