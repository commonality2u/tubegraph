---
title: Decentralized and opensource AI model training
videoId: UnMYLttFaUg
---

From: [[bankless]] <br/> 

The field of AI model training is witnessing a **[[emerging_trends_in_ai_development | growing trend]]** toward distributed and [[innovations_and_challenges_in_decentralized_ai_models and platforms | decentralized training]] [01:20:08]. This approach offers an alternative to the traditional, highly centralized training setups often housed in single data centers [01:17:41].

## The Challenge and Opportunity of Decentralized Training

Historically, training large AI models has been dominated by major corporations due to the immense computational resources and billions of dollars required [01:17:38]. These centralized setups, while powerful, lead to an over-reliance on single entities, raising concerns about control and ethical alignment [01:17:56].

[[innovations_and_challenges_in_decentralized_ai_models and platforms | Decentralized training]], where computing power is sourced from computers globally, has faced significant scientific and physics challenges, limiting its widespread adoption [01:18:10]. However, recent breakthroughs by various groups, including crypto companies like [[decentralized_ai_model_training_and_innovations | Prime Intellect]], are overcoming these hurdles [01:18:34].

## Prime Intellect's Breakthrough

[[decentralized_ai_model_training_and_innovations | Prime Intellect]], a crypto-native company, recently announced a significant advancement in [[innovations_and_challenges_in_decentralized_ai_models and platforms | decentralized AI model training]] by successfully training a 32 billion parameter model [03:13:00]. This achievement is particularly noteworthy because such models, even at this scale, can perform a wide range of complex tasks [01:19:22]. The training run concluded successfully, officially yielding a 32 billion parameter model through [[innovations_and_challenges_in_decentralized_ai_models and platforms | decentralized means]] [01:19:47].

This milestone highlights a rapidly accelerating field:
*   Just two years prior, in late 2023, Google's DeepMind could only achieve [[innovations_and_challenges_in_decentralized_ai_models and platforms | decentralized training]] for a 400 million parameter model [01:22:46].
*   Three months later, a group of [[developments_and_challenges_in_opensource_ai_models | open-source]] AI researchers, unrelated to crypto, successfully trained a 1.5 billion parameter model [01:23:05].
*   A month after that, [[decentralized_ai_model_training_and_innovations | Prime Intellect]] trained a two billion parameter model, rapidly progressing to 32 billion parameters [01:23:19].

[!INFO] **Key Insight:** While current centralized [[frontier_ai_models_and_advancements_by_openai | frontier models]] are approaching trillions of parameters, a 32 billion parameter model is still functionally useful and capable of amazing things, akin to earlier models that could run locally on personal devices [01:19:22].

## Centralized vs. Decentralized vs. Open-Source

The discussion around AI models often involves terms like centralized, decentralized, and [[open_source_ai_and_its_implications | open-source]].
*   **Centralized models:** Are trained and controlled by a single entity, often requiring massive investment in hardware and pre-training [01:07:07].
*   **[[innovations_and_challenges_in_decentralized_ai_models and platforms | Decentralized training]]:** Involves distributing the training process across multiple computers or data centers globally, reducing reliance on a single point of control [01:18:10]. This method leverages a network of compute providers who might offer resources in exchange for model ownership or other incentives [01:26:53].
*   **[[developments_and_challenges_in_opensource_ai_models | Open-source models]]:** Are models where the weights, designs, and blueprints are publicly available, allowing anyone to replicate, train, or fine-tune them [01:25:46]. An [[developments_and_challenges_in_opensource_ai_models | open-source model]] can be created by a centralized entity but then released to the public [01:25:41].

[!NOTE] **Significance:** The development of [[innovations_and_challenges_in_decentralized_ai_models and platforms | decentralized training]] is a crucial check on the power of large, centralized AI entities like OpenAI, Google, and Meta [01:24:27]. It provides a backup plan if centralized models become misaligned with public interest [01:24:57].

## Implications for the Future of AI

The rise of [[innovations_and_challenges_in_decentralized_ai_models and platforms | decentralized training]] and [[developments_and_challenges_in_opensource_ai_models | open-source]] models suggests several future possibilities:
*   **Increased [[developments_in_ai_model_competition | competition]]:** Decentralized efforts could lead to a more diverse and [[developments_in_ai_model_competition | competitive]] landscape, potentially challenging the centralized hold on AI models [01:21:15].
*   **Personalized AI:** If hardware advancements continue, it might become feasible for individuals to own and run powerful AI models locally on their devices, raising questions about whether these models should be controlled by corporate entities or individuals [01:23:46].
*   **Product Experience:** Ultimately, the success of [[innovations_and_challenges_in_decentralized_ai_models and platforms | decentralized models]] will depend on their product experience. While privacy-first is a strong value, centralized systems often offer superior product polish and virality [01:24:08].

[!WARNING] **Potential Risk:** While a significant step, the long-term viability of [[innovations_and_challenges_in_decentralized_ai_models and platforms | decentralized training]] in outright competing with trillion-parameter centralized models is still an open question [01:22:23]. However, the rapid progress in this cutting-edge field is undeniable [01:23:25].