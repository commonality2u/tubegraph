---
title: Chinas DeepSeek AI model
videoId: 3ezFd3x718A
---

From: [[bankless]] <br/> 

The [[deepseek_ai_model_and_its_impact_on_nvidia | DeepSeek AI model]], an [[open_source_ai_models | open-source AI model]] developed by a team in [[chinas_ai_innovation_and_ai_agents | China]], has recently gained significant attention in the AI space <a class="yt-timestamp" data-t="00:00:35">[00:00:35]</a>. It has been noted for its ability to match or even surpass OpenAI's top models in performance benchmarks while costing significantly less to train and operate <a class="yt-timestamp" data-t="00:00:54">[00:00:54]</a>.

## Groundbreaking Innovations

[[deepseek_ai_model_and_its_impact_on_nvidia | DeepSeek AI]] has pioneered two "groundbreaking research techniques" that have surprised those who have invested billions into AI development <a class="yt-timestamp" data-t="00:01:13">[00:01:13]</a>.

### Efficient Data Usage
One key innovation is how [[deepseek_ai_model_and_its_impact_on_nvidia | DeepSeek]] uses training data more effectively <a class="yt-timestamp" data-t="00:04:52">[00:04:52]</a>. Traditional models require massive amounts of compute power combined with raw data for training, costing billions of dollars <a class="yt-timestamp" data-t="00:05:20">[00:05:20]</a>. [[chinas_ai_innovation_and_ai_agents | China]] is "compute constrained" due to chip bands, necessitating smarter ways to train models <a class="yt-timestamp" data-t="00:06:00">[00:06:00]</a>.

[[deepseek_ai_model_and_its_impact_on_nvidia | DeepSeek]] addresses this by designing the model to feed itself a bit of compute, produce an output, and then "review the output and figure out why it's wrong or why it's correct" <a class="yt-timestamp" data-t="00:06:29">[00:06:29]</a>. This allows the model to learn from mistakes and require fewer steps, less compute, and less data to achieve desired outcomes <a class="yt-timestamp" data-t="00:07:24">[00:07:24]</a>. This contrasts with earlier LLM models that were prone to "hallucinations" and were poor at backtracking or correcting previous responses <a class="yt-timestamp" data-t="00:07:48">[00:08:00]</a>.

While this self-correction process might involve more "inference" (making calls to the model) <a class="yt-timestamp" data-t="00:08:46">[00:08:46]</a>, inference is generally cheaper than full training <a class="yt-timestamp" data-t="00:09:40">[00:09:40]</a>. This method allows the model to become smarter using less overall compute <a class="yt-timestamp" data-t="00:09:51">[00:09:51]</a>.

### Mixture of Experts (MoE)
The second breakthrough is the use of a "Mixture of Experts" (MoE) architecture <a class="yt-timestamp" data-t="00:10:54">[00:10:54]</a>. Unlike traditional models (like OpenAI's GPT) where every neuron in the entire model's "brain" is stimulated for each query, [[deepseek_ai_model_and_its_impact_on_nvidia | DeepSeek]] only routes a request to the specific section of the model's "brain" that needs to be engaged <a class="yt-timestamp" data-t="00:10:23">[00:10:23]</a>. This means if a model has 27 experts, only a few might run inference, while others shut down, producing 97% of the quality output by saving 97% of the energy <a class="yt-timestamp" data-t="00:11:08">[00:11:08]</a>.

## Performance and Cost Comparison

According to ArtificialAnalysis.ai, a website that tracks and ranks AI models <a class="yt-timestamp" data-t="00:11:37">[00:11:37]</a>:
*   **Overall Quality**: [[deepseek_ai_model_and_its_impact_on_nvidia | DeepSeek R1]] ranks very close to the top model (01), often just a single point below <a class="yt-timestamp" data-t="00:11:47">[00:11:47]</a>.
*   **Speed**: While it is noted as "the slowest" in comparison, it is not significantly different from the 01 model in terms of reasoning and delivery <a class="yt-timestamp" data-t="00:12:07">[00:12:07]</a>.
*   **Price**: [[deepseek_ai_model_and_its_impact_on_nvidia | DeepSeek]] is significantly cheaper, around six times less expensive than 01 <a class="yt-timestamp" data-t="00:12:23">[00:12:23]</a>.

Overall, the [[deepseek_ai_model_and_its_impact_on_nvidia | DeepSeek R1 model]] is measured to be 45 times more efficient than its United States-based competitors, using 45 times less compute resources for similar response quality and time <a class="yt-timestamp" data-t="00:12:56">[00:12:56]</a>. This allows [[deepseek_ai_model_and_its_impact_on_nvidia | DeepSeek]] to charge 95% less for its API calls compared to OpenAI <a class="yt-timestamp" data-t="00:03:04">[00:03:04]</a>.

## Market Impact and Jeevan's Paradox

The emergence of [[deepseek_ai_model_and_its_impact_on_nvidia | DeepSeek AI]] has had a notable impact on the market, contributing to concerns that tanked [[nvidia_market_fluctuations_and_deep_seek_ai_model_impact | Nvidia's]] stock <a class="yt-timestamp" data-t="00:01:49">[00:01:49]</a>. This is because it represents a significant victory for software efficiency over brute-force hardware <a class="yt-timestamp" data-t="00:13:17">[00:13:17]</a>. The realization is that more can be "eeked out of our models" through better software, rather than just throwing more resources at them <a class="yt-timestamp" data-t="00:13:29">[00:13:29]</a>.

This situation can be understood through **Jeevan's Paradox**:
> "As technological advancements improve the efficiency of a resource, its overall consumption can paradoxically increase rather than decrease. This occurs because increased efficiency lowers cost and expands potential use cases, driving greater demand" <a class="yt-timestamp" data-t="00:15:19">[00:15:19]</a>.

In the context of AI, as compute becomes cheaper due to models like [[deepseek_ai_model_and_its_impact_on_nvidia | DeepSeek]], [[emerging_ai_tools_and_projects | AI adoption]] will spread faster, expanding demand rather than reducing it <a class="yt-timestamp" data-t="00:15:41">[00:15:41]</a>. Therefore, innovations like [[deepseek_ai_model_and_its_impact_on_nvidia | DeepSeek]] do not reduce overall compute consumption but make AI more accessible, leading to greater global demand for GPUs, energy, and data infrastructure <a class="yt-timestamp" data-t="00:15:50">[00:15:50]</a>. This creates an "induced demand" <a class="yt-timestamp" data-t="00:16:37">[00:16:37]</a>, meaning more models and applications will be built, ultimately consuming the "surplus" compute <a class="yt-timestamp" data-t="00:16:48">[00:16:48]</a>.

## Geopolitical Context: The AI Arms Race

[[deepseek_ai_model_and_its_impact_on_nvidia | DeepSeek's]] emergence highlights that the "arms race" between the United States and [[chinas_ai_innovation_and_ai_agents | China]] in AI is "fully on" <a class="yt-timestamp" data-t="00:30:28">[00:30:28]</a>. Any innovation made in the US is likely to be copied and improved upon by [[chinas_ai_innovation_and_ai_agents | China]] <a class="yt-timestamp" data-t="00:03:45">[00:03:45]</a>.

There is speculation that [[chinas_ai_innovation_and_ai_agents | China's]] announcement of [[deepseek_ai_model_and_its_impact_on_nvidia | DeepSeek's]] low training cost ($6 million) was a deliberate strategy to "knock down the valuations of US-based AI companies" and slow down investment in them <a class="yt-timestamp" data-t="00:30:05">[00:30:05]</a>. [[chinas_ai_innovation_and_ai_agents | China]] has a history of heavily subsidizing its tech industry to commoditize US innovations, copy them, and then produce them at scale to be highly competitive <a class="yt-timestamp" data-t="00:30:19">[00:30:19]</a>.

The rise of [[deepseek_ai_model_and_its_impact_on_nvidia | DeepSeek]] serves as a "splash of very very cold water" on America's AI Darlings, prompting them to assess their research direction and product development <a class="yt-timestamp" data-t="00:31:06">[00:31:06]</a>.

## [[open_source_ai_models | Open Source]] Nature

A crucial aspect of [[deepseek_ai_model_and_its_impact_on_nvidia | DeepSeek]] is that the entire model has been [[open_source_ai_models | open-sourced]] <a class="yt-timestamp" data-t="00:04:00">[00:04:00]</a>. This is considered a "crazy move," akin to revealing a secret formula <a class="yt-timestamp" data-t="00:04:05">[00:04:05]</a>. The irony was noted that a $200/month AI comes from a non-profit (OpenAI), while an [[open_source_ai_models | open-source AI]] comes from a [[chinas_ai_innovation_and_ai_agents | Chinese hedge fund]] <a class="yt-timestamp" data-t="00:04:23">[00:04:23]</a>.

This [[open_source_ai_development | open-source]] approach means that even if Open AI or Meta build advanced agent products, the technology can eventually "show up in the [[open_source_ai_models | open source]] world," allowing it to be reflected on open blockchains in the form of tokens <a class="yt-timestamp" data-t="00:32:00">[00:32:00]</a>. This is considered fundamentally bullish for the consumer and the AI sector <a class="yt-timestamp" data-t="00:17:18">[00:17:18]</a>.

## Integration into [[major_players_and_projects_in_crypto_ai | Crypto AI]]

The [[open_source_ai_models | open-source]] nature of [[deepseek_ai_model_and_its_impact_on_nvidia | DeepSeek's]] innovations allows for rapid integration into [[major_players_and_projects_in_crypto_ai | crypto AI]] projects <a class="yt-timestamp" data-t="00:33:04">[00:33:04]</a>. Many teams and protocols in the [[major_players_and_projects_in_crypto_ai | crypto AI]] space have already integrated [[deepseek_ai_model_and_its_impact_on_nvidia | DeepSeek]] into their frameworks, making it accessible for their agents <a class="yt-timestamp" data-t="00:36:55">[00:36:55]</a>. This benefits all agents built on [[major_players_and_projects_in_crypto_ai | crypto rails]], enhancing their capabilities <a class="yt-timestamp" data-t="00:37:47">[00:37:47]</a>.

Examples of [[major_players_and_projects_in_crypto_ai | crypto AI]] projects that have integrated [[deepseek_ai_model_and_its_impact_on_nvidia | DeepSeek]] include:
*   Eliza framework <a class="yt-timestamp" data-t="00:37:12">[00:37:12]</a>
*   Arc framework <a class="yt-timestamp" data-t="00:37:16">[00:37:16]</a>
*   Virtuals <a class="yt-timestamp" data-t="00:37:16">[00:37:16]</a>
*   Venice AI <a class="yt-timestamp" data-t="00:37:19">[00:37:19]</a>

This integration process took as little as 5 minutes for some protocols <a class="yt-timestamp" data-t="00:37:00">[00:37:00]</a>, showcasing the adaptability and resilience of the [[major_players_and_projects_in_crypto_ai | crypto AI]] space <a class="yt-timestamp" data-t="00:37:38">[00:37:38]</a>.