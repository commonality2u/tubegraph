---
title: The rise of custom silicon in AI hardware
videoId: OC61Vo4tAaE
---

From: [[bankless]] <br/> 

The landscape of [[the_role_of_ai_and_hardware_in_technological_evolution | AI]] hardware is undergoing a significant shift, with a notable increase in the development and deployment of custom silicon. This trend poses a direct challenge to the market dominance of companies like Nvidia, signaling a rebalancing of value from expensive, generalized hardware to more efficient, specialized, and often custom-built solutions <a class="yt-timestamp" data-t="07:12:00">[07:12:00]</a>.

## Nvidia's Dominance and Its Moat
Nvidia has long held a dominant position in the AI hardware market, largely due to its powerful Graphics Processing Units (GPUs) and its proprietary software platform, CUDA <a class="yt-timestamp" data-t="05:43:00">[05:43:00]</a>, <a class="yt-timestamp" data-t="30:00:00">[30:00:00]</a>. CUDA is a critical component, making it easy for developers to utilize Nvidia's thousands of GPU cores efficiently <a class="yt-timestamp" data-t="30:44:00">[30:44:00]</a>. This system allows developers to describe complex problems using high-level concepts, which CUDA then optimizes for Nvidia GPUs <a class="yt-timestamp" data-t="31:01:00">[31:01:00]</a>. This software lock-in, combined with Nvidia's fast GPU interconnect technology (acquired through the purchase of Mellanox <a class="yt-timestamp" data-t="15:08:00">[15:08:00]</a>), has traditionally formed a strong "moat" around its business <a class="yt-timestamp" data-t="06:25:00">[06:25:00]</a>, <a class="yt-timestamp" data-t="14:12:00">[14:12:00]</a>.

However, this dominance, characterized by "triple-digit revenue growth with 90% gross margins" on data center revenue <a class="yt-timestamp" data-t="11:21:00">[11:21:00]</a>, <a class="yt-timestamp" data-t="12:06:00">[12:06:00]</a>, is now attracting significant competition. The immense profit potential means that "everyone in their brother" is trying to find a way to compete <a class="yt-timestamp" data-t="11:26:00">[11:26:00]</a>.

## The Challenge of Custom Silicon
Major players, particularly hyperscalers (large cloud providers), are investing heavily in designing their own custom silicon <a class="yt-timestamp" data-t="12:44:00">[12:44:00]</a>. Companies like Amazon, Microsoft, OpenAI, and Meta are all developing their own chips for both training and inference tasks <a class="yt-timestamp" data-t="12:56:00">[12:56:00]</a>.

The primary motivations for this shift are:
*   **Cost-Effectiveness**: Nvidia's GPUs are expensive, with a H100 GPU costing around $40,000 to customers, while costing Nvidia perhaps $3,500 to produce <a class="yt-timestamp" data-t="09:19:00">[09:19:00]</a>, <a class="yt-timestamp" data-t="21:02:00">[21:02:00]</a>. If hyperscalers can produce their own chips for a fraction of that cost, even if they are not superior in raw performance, it becomes economically compelling <a class="yt-timestamp" data-t="13:13:00">[13:13:00]</a>. The focus shifts from raw performance to efficiency per dollar <a class="yt-timestamp" data-t="13:32:00">[13:32:00]</a>.
*   **Reduced Reliance on Monopolies**: Developing in-house solutions reduces dependence on a single supplier like Nvidia, mitigating the risk of inflated prices or supply chain constraints <a class="yt-timestamp" data-t="13:39:00">[13:39:39]</a>.

### Key Players and Examples
*   **Cerebras**: This company offers wafer-scale chips, which are essentially an "enormous chip" made from an entire 300mm wafer <a class="yt-timestamp" data-t="14:46:00">[14:46:00]</a>. This design bypasses the need for complex interconnects between multiple smaller GPUs <a class="yt-timestamp" data-t="15:00:00">[15:00:00]</a>.
*   **Groq (Groq with a Q)**: Specializing in inference-optimized hardware, Groq offers incredibly fast token generation rates for large language models (LLMs) <a class="yt-timestamp" data-t="21:07:00">[21:07:00]</a>. While their servers are expensive, their efficiency can make them cheaper to use if demand is high <a class="yt-timestamp" data-t="22:30:00">[22:30:00]</a>.
*   **Hyperscalers' Internal Efforts**: Amazon, for instance, developed its own Graviton CPUs to compete with Intel and AMD, offering cost savings to customers who switch <a class="yt-timestamp" data-t="27:29:00">[27:29:00]</a>. They are applying similar strategies to AI chips like their Trinium <a class="yt-timestamp" data-t="34:48:00">[34:48:00]</a>. Apple, known for its secrecy, also has one of the world's best silicon teams and is likely developing advanced internal chips for AI applications, which could eventually run powerful AI models locally on user devices <a class="yt-timestamp" data-t="10:04:00">[10:04:00]</a>, <a class="yt-timestamp" data-t="40:57:00">[40:57:00]</a>.
*   **AMD**: While largely absent from the data center [[ai_innovation_and_emerging_technologies | AI]] space, AMD has the potential to become a significant competitor if they can improve their software ecosystem and drivers to be more competitive with CUDA <a class="yt-timestamp" data-t="28:06:00">[28:06:00]</a>.

## Software Layer Challenges to Nvidia's Moat
Beyond hardware, Nvidia's software moat, CUDA, is also being challenged.
*   **High-Level Frameworks**: The rise of frameworks like MLX and Triton allows developers to express highly parallelized programming at a higher level of abstraction <a class="yt-timestamp" data-t="33:12:00">[33:12:00]</a>. This means code can be written that targets these frameworks, which can then be compiled to run on various hardware platformsâ€”including Nvidia's GPUs via CUDA, or completely different chips <a class="yt-timestamp" data-t="33:30:00">[33:30:00]</a>. This reduces the vendor lock-in to Nvidia's expensive hardware <a class="yt-timestamp" data-t="34:06:00">[34:06:00]</a>.
*   **LLM Porting Capabilities**: Large Language Models (LLMs) are becoming incredibly adept at porting code from one language to another <a class="yt-timestamp" data-t="34:36:00">[34:36:00]</a>. This capability suggests that even if engineers continue to think and write algorithms in Cuda-like concepts, LLMs could potentially translate that code to work efficiently on non-Nvidia GPUs or other custom hardware <a class="yt-timestamp" data-t="35:14:00">[35:14:00]</a>. This could break down the "monopoly" around Cuda-specific engineering talent <a class="yt-timestamp" data-t="36:00:00">[36:00:00]</a>.

## Impact on the Market
The emergence of custom silicon and efficient software solutions like DeepSeek (which is reportedly 45 times more cost-efficient and charges 95% less for API calls than models like ChatGPT <a class="yt-timestamp" data-t="01:32:00">[01:32:00]</a>, <a class="yt-timestamp" data-t="06:50:00">[06:50:00]</a>) puts significant pressure on Nvidia's market share and margins. The efficiency gains imply that the industry may have been "massively over-provisioning compute resources" <a class="yt-timestamp" data-t="06:35:00">[06:35:00]</a>.

While some argue for a "Jevons Paradox" where increased efficiency leads to increased demand, thereby still benefiting hardware providers <a class="yt-timestamp" data-t="07:47:00">[07:47:00]</a>, others contend that the immediate impact will be a "temporary dislocation" and a push by large customers like Meta to reduce their spending on expensive Nvidia chips <a class="yt-timestamp" data-t="08:32:00">[08:32:00]</a>.

The process of developing and manufacturing custom silicon is lengthy, often taking years to design and secure fabrication capacity from companies like TSMC <a class="yt-timestamp" data-t="23:32:00">[23:32:00]</a>. However, once this additional capacity comes online, it is expected to create a "massive flood of alternative supply" that will intensely pressure Nvidia's market share and margins <a class="yt-timestamp" data-t="39:17:00">[39:17:00]</a>. The shift from a virtual monopoly to even a few competitors can cause margins to fall "very, very quickly" <a class="yt-timestamp" data-t="39:50:00">[39:50:00]</a>.

This trend underscores a broader [[commoditization_of_hardware_technologies | commoditization of hardware technologies]] in the [[advancements_and_challenges_in_ai_hardware_and_software_integration | AI]] sector. The market's previous valuation of Nvidia's stock was based on an assumption of sustained, high-margin revenue growth, akin to being "priced to perfection" <a class="yt-timestamp" data-t="01:06:27:00">[01:06:27:00]</a>. The emergence of highly efficient [[open_source_ai_development | open-source AI]] models and the strategic move towards custom silicon highlight that such high margins are unsustainable in a competitive market, leading to a significant market recalibration <a class="yt-timestamp" data-t="43:07:00">[43:07:00]</a>.