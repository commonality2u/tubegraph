---
title: AI industry competition dynamics
videoId: OC61Vo4tAaE
---

From: [[bankless]] <br/> 

The [[developments_in_ai_model_competition | AI industry]] is characterized by intense competition and rapid advancements, leading to significant shifts in market dynamics, particularly concerning hardware and software providers. Recent events have highlighted how algorithmic breakthroughs can profoundly impact market valuations, challenging established monopolies and fostering new forms of competition.

## The DeepSeek Catalyst and Market Reaction

A recent market event saw approximately $2 trillion wiped off global equity markets, with headlines attributing the crash to the Chinese [[developments_in_ai_model_competition | AI model]] DeepSeek <a class="yt-timestamp" data-t="00:00:08">[00:00:08]</a>. The DeepSeek V3 technical paper, which discusses its efficiency gains, was released on December 27th, a month before the market crash <a class="yt-timestamp" data-t="00:00:22">[00:00:22]</a>. A newer R1 model paper, featuring Chain of Thought, came out a week prior to the crash <a class="yt-timestamp" data-t="00:00:31">[00:00:31]</a>.

Jeffrey Emanuel suggests that the sudden market impact on a Monday was due to his 12,000-word article, "The Short Case for Nvidia Stock," which was published on a Friday night and quickly went viral <a class="yt-timestamp" data-t="00:00:41">[00:00:41]</a>, <a class="yt-timestamp" data-t="00:02:17">[00:02:17]</a>. The article was shared by influential figures like Chamath Palihapitiya, Naval Ravikant, and Y Combinator's Gary Tan, collectively reaching millions of followers <a class="yt-timestamp" data-t="00:00:58">[00:00:58]</a>, <a class="yt-timestamp" data-t="00:01:06">[00:01:06]</a>, <a class="yt-timestamp" data-t="00:01:10">[00:01:10]</a>. The article was viewed over two million times <a class="yt-timestamp" data-t="00:01:06">[00:01:06]</a>.

DeepSeek's reported capabilities include being 45 times more cost-efficient than US-based [[developments_in_ai_model_competition | AI models]] and charging 95% less to use than ChatGPT <a class="yt-timestamp" data-t="00:01:32">[00:01:32]</a>, <a class="yt-timestamp" data-t="00:01:37">[00:01:37]</a>. This led to Nvidia's stock dropping 20%, wiping out $600 billion in market value, and prompting companies like OpenAI and Meta's [[developments_in_ai_model_competition | AI labs]] to investigate how a less-known Chinese lab achieved such efficiency with a $6 million training cost <a class="yt-timestamp" data-t="00:01:40">[00:01:40]</a>, <a class="yt-timestamp" data-t="00:01:46">[00:01:46]</a>, <a class="yt-timestamp" data-t="00:01:51">[00:01:51]</a>, <a class="yt-timestamp" data-t="00:01:58">[00:01:58]</a>.

### DeepSeek's Efficiency Breakthroughs

DeepSeek's 45x efficiency improvement is a result of several innovations:
*   **Engineers' Approach**: Unlike the Western approach where researchers design and engineers optimize, DeepSeek's engineers are adept at both research and high-performance optimization <a class="yt-timestamp" data-t="00:49:28">[00:49:28]</a>, <a class="yt-timestamp" data-t="00:50:11">[00:50:11]</a>. They focused on maximizing GPU saturation and minimizing communication overhead <a class="yt-timestamp" data-t="00:50:35">[00:50:35]</a>.
*   **Memory Efficiency**: DeepSeek uses an "incredibly smart" method for storing KV (key-value) caches and indices, which are crucial for Transformer models. This method stores only the meaningful subset of data in a compressed form, significantly reducing memory usage <a class="yt-timestamp" data-t="00:52:51">[00:52:51]</a>, <a class="yt-timestamp" data-t="00:53:57">[00:53:57]</a>. Less memory usage means fewer GPUs are needed and data transfer is faster <a class="yt-timestamp" data-t="00:52:39">[00:52:39]</a>.
*   **Multi-Token Predictions**: Instead of predicting one token (word) at a time, DeepSeek developed a "speculative decoding" method to predict multiple tokens simultaneously, achieving a 95% accuracy rate <a class="yt-timestamp" data-t="00:55:13">[00:55:13]</a>, <a class="yt-timestamp" data-t="00:55:47">[00:55:47]</a>, <a class="yt-timestamp" data-t="00:56:02">[00:56:02]</a>. This technique can nearly double inference throughput with no additional cost <a class="yt-timestamp" data-t="00:56:07">[00:56:07]</a>.
*   **Compressed Parameter Storage**: The model parameters (gigantic list of numbers) are stored in a more compressed form, and the entire training process is performed at lower precision without compromising quality, further enhancing efficiency <a class="yt-timestamp" data-t="00:56:37">[00:56:37]</a>, <a class="yt-timestamp" data-t="00:57:26">[00:57:26]</a>.

These efficiency gains compound multiplicatively, leading to the reported 45x improvement <a class="yt-timestamp" data-t="00:58:17">[00:58:17]</a>, <a class="yt-timestamp" data-t="00:58:28">[00:58:28]</a>. While the exact GPU hours used by DeepSeek are unverified, their 95% lower inference API cost suggests a significant efficiency advantage <a class="yt-timestamp" data-t="00:58:35">[00:58:35]</a>, <a class="yt-timestamp" data-t="00:59:01">[00:59:01]</a>.

## Unbundling Nvidia's Moat

Nvidia has enjoyed a dominant position in the [[the_ai_arms_race_and_global_competition | AI hardware]] market, particularly due to its high-bandwidth GPU interconnect technology and its CUDA software platform <a class="yt-timestamp" data-t="00:14:12">[00:14:12]</a>, <a class="yt-timestamp" data-t="00:30:00">[00:30:00]</a>. However, this moat is facing multiple challenges:

### Hardware Competition
*   **Custom Silicon**: Hyperscalers like Amazon, Microsoft, OpenAI, and Meta are developing their own custom silicon for both training and inference <a class="yt-timestamp" data-t="00:12:44">[00:12:44]</a>, <a class="yt-timestamp" data-t="00:12:51">[00:12:51]</a>. These custom chips do not need to outperform Nvidia's; they only need to be significantly cheaper. If they can produce chips for 1X the cost of Nvidia's 10X markup, they can still make substantial profits <a class="yt-timestamp" data-t="00:13:13">[00:13:13]</a>.
*   **Alternative Chip Companies**: Companies like Cerebras and Groq are offering compelling hardware solutions <a class="yt-timestamp" data-t="00:11:34">[00:11:34]</a>, <a class="yt-timestamp" data-t="00:11:50">[00:11:50]</a>. Cerebras has developed a wafer-scale chip that integrates an entire 300mm wafer into a single enormous chip, eliminating the need for external wiring <a class="yt-timestamp" data-t="00:14:46">[00:14:46]</a>, <a class="yt-timestamp" data-t="00:15:00">[00:15:00]</a>. Groq, on the other hand, specializes in inference-only optimization, achieving 1500 tokens per second for models like Llama 3.37 billion, compared to 40 tokens per second on a high-end consumer Nvidia GPU <a class="yt-timestamp" data-t="00:21:20">[00:21:20]</a>, <a class="yt-timestamp" data-t="00:22:19">[00:22:19]</a>. While Groq hardware is expensive, it becomes much cheaper per unit of work if consistently utilized <a class="yt-timestamp" data-t="00:22:30">[00:22:30]</a>.
*   **AMD's Potential**: While AMD has been largely absent from the [[the_ai_arms_race_and_global_competition | data center AI]] market, they compete effectively in consumer GPUs <a class="yt-timestamp" data-t="00:28:06">[00:28:06]</a>. There's potential for them to become a real competitor, especially with community efforts to create software stacks for their GPUs <a class="yt-timestamp" data-t="00:28:48">[00:28:48]</a>, <a class="yt-timestamp" data-t="00:29:07">[00:29:07]</a>.

### Software and Algorithmic Competition
*   **CUDA Alternatives**: Nvidia's CUDA software framework has been a significant barrier to entry, as it optimizes code for Nvidia GPUs <a class="yt-timestamp" data-t="00:30:28">[00:30:28]</a>, <a class="yt-timestamp" data-t="00:31:10">[00:31:10]</a>. However, higher-level frameworks like MLX and Triton are gaining momentum, allowing developers to write code that can be compiled to run on various chips, not just Nvidia's <a class="yt-timestamp" data-t="00:33:06">[00:33:06]</a>, <a class="yt-timestamp" data-t="00:33:56">[00:33:56]</a>.
*   **LLMs for Code Porting**: Large Language Models (LLMs) are becoming incredibly proficient at porting code between different languages and frameworks <a class="yt-timestamp" data-t="00:34:37">[00:34:37]</a>, <a class="yt-timestamp" data-t="00:34:40">[00:34:40]</a>. This means developers could write their algorithms in CUDA as a specification language, then use an LLM to port it to a framework compatible with AMD GPUs or other custom chips, effectively bypassing Nvidia's software moat <a class="yt-timestamp" data-t="00:35:16">[00:35:16]</a>, <a class="yt-timestamp" data-t="00:35:33">[00:35:33]</a>.
*   **Chain of Thought Models**: The emergence of models like OpenAI's O1 (now ChatGPT Plus/Pro) that use "Chain of Thought" significantly increases inference-time compute <a class="yt-timestamp" data-t="01:16:03">[01:16:03]</a>. While this still uses Nvidia GPUs, it highlights a shift in compute demand towards inference, which is a different computational problem that can be optimized by specialized hardware like Groq's <a class="yt-timestamp" data-t="00:16:36">[00:16:36]</a>, <a class="yt-timestamp" data-t="00:16:39">[00:16:39]</a>, <a class="yt-timestamp" data-t="00:21:20">[00:21:20]</a>.

### Supply Chain and Manufacturing
TSMC (Taiwan Semiconductor) is the dominant chip manufacturer, producing chips for Nvidia and Apple <a class="yt-timestamp" data-t="00:36:56">[00:36:56]</a>, <a class="yt-timestamp" data-t="00:37:07">[00:37:07]</a>. While TSMC's fabs are currently "book solid," the ability to build new fabs (like the one in Arizona) means that additional capacity will eventually come online, increasing the supply of alternative chips <a class="yt-timestamp" data-t="00:37:43">[00:37:43]</a>, <a class="yt-timestamp" data-t="00:38:05">[00:38:05]</a>, <a class="yt-timestamp" data-t="00:38:32">[00:38:32]</a>. This influx of alternative supply will likely pressure Nvidia's market share and, critically, its high profit margins <a class="yt-timestamp" data-t="00:39:23">[00:39:23]</a>, <a class="yt-timestamp" data-t="00:39:34">[00:39:34]</a>.

## Economic and Strategic Implications

Nvidia's gross margins of over 90% on data center revenue are exceptionally high, leading to an unsustainable market position <a class="yt-timestamp" data-t="00:11:19">[00:11:19]</a>, <a class="yt-timestamp" data-t="00:24:51">[00:24:51]</a>, <a class="yt-timestamp" data-t="00:26:17">[00:26:17]</a>. This makes them a prime target for competitors, as even a product that is 40% as good can be a "no-brainer" for customers like Amazon if it significantly cuts costs <a class="yt-timestamp" data-t="00:24:40">[00:24:40]</a>, <a class="yt-timestamp" data-t="00:25:06">[00:25:06]</a>. The market was "priced to perfection," assuming continued triple-digit revenue growth and maintenance of high margins <a class="yt-timestamp" data-t="01:06:25">[01:06:25]</a>, <a class="yt-timestamp" data-t="01:06:45">[01:06:45]</a>.

The rise of efficient [[developments_in_ai_model_competition | AI models]] means that companies like Meta can potentially cut their [[ai_in_financial_and_commercial_applications | AI service]] costs by 95%, which is beneficial for them despite their significant investments in GPUs <a class="yt-timestamp" data-t="00:59:42">[00:59:42]</a>. However, this creates pressure on model providers like OpenAI and Anthropic, who might be forced to cut their API prices, impacting their profitability <a class="yt-timestamp" data-t="00:59:50">[00:59:50]</a>, <a class="yt-timestamp" data-t="01:00:07">[01:00:07]</a>, <a class="yt-timestamp" data-t="01:00:14">[01:00:14]</a>.

### Synthetic Data and Future Model Development
The quality and quantity of training data are crucial for [[developments_in_ai_model_competition | AI model]] improvement <a class="yt-timestamp" data-t="01:13:03">[01:13:03]</a>. High-quality human-generated data is becoming scarce, posing a "big wall" for future model scaling <a class="yt-timestamp" data-t="01:13:17">[01:13:17]</a>, <a class="yt-timestamp" data-t="01:14:06">[01:14:06]</a>.

Synthetic data, generated by LLMs themselves, offers a solution <a class="yt-timestamp" data-t="01:14:32">[01:14:32]</a>. While seemingly circular for general knowledge, it is highly effective for domains like logic, math, and computer programming because the generated output can be verified for correctness <a class="yt-timestamp" data-t="01:15:13">[01:15:13]</a>, <a class="yt-timestamp" data-t="01:15:16">[01:15:16]</a>, <a class="yt-timestamp" data-t="01:15:20">[01:15:20]</a>. This allows for the continuous generation of high-quality training data, enabling models to improve rapidly in these specific areas <a class="yt-timestamp" data-t="01:16:01">[01:16:01]</a>, <a class="yt-timestamp" data-t="01:16:16">[01:16:16]</a>. This trend suggests that quantitative jobs may be more susceptible to [[ai_in_robotics_and_workforce_implications | AI automation]] than initially perceived, as AI becomes superhuman in these domains <a class="yt-timestamp" data-t="01:16:44">[01:16:44]</a>, <a class="yt-timestamp" data-t="01:16:57">[01:16:57]</a>.

## The Future of AI Competition
The [[the_ai_arms_race_and_global_competition | AI market]] is not just progressing along a predictable Moore's Law curve but is experiencing "step function changes" in efficiency and capabilities <a class="yt-timestamp" data-t="01:03:32">[01:03:32]</a>, <a class="yt-timestamp" data-t="01:03:34">[01:03:34]</a>. These sudden, non-linear improvements, such as a 45x efficiency gain, catch markets off guard and redefine competitive landscapes <a class="yt-timestamp" data-t="01:06:09">[01:06:09]</a>, <a class="yt-timestamp" data-t="01:06:15">[01:06:15]</a>.

For consumers, these advancements mean that [[ai_in_financial_and_commercial_applications | AI products]] will become more powerful and accessible, even enabling advanced [[ai_in_financial_and_commercial_applications | AI models]] to run privately on personal devices like laptops and phones <a class="yt-timestamp" data-t="01:09:05">[01:09:05]</a>, <a class="yt-timestamp" data-t="01:09:25">[01:09:25]</a>. This shift benefits companies like Apple, which have strong internal silicon teams and can leverage these new efficiencies <a class="yt-timestamp" data-t="01:10:01">[01:10:01]</a>.

The dynamic nature of [[the_ai_arms_race_and_global_competition | AI competition]] necessitates continuous innovation and adaptation. Companies that can find "creative ways to get around" existing monopolies, such as developing specialized hardware or novel algorithmic approaches, are poised to disrupt the market <a class="yt-timestamp" data-t="00:23:01">[00:23:01]</a>.