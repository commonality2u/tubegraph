---
title: DeepSeek AI model and its impact on Nvidia
videoId: OC61Vo4tAaE
---

From: [[bankless]] <br/> 

The emergence of the [[chinas_deepseek_ai_model | DeepSeek AI model]] from China has sent ripples through the global equity markets, particularly impacting [[NVIDIA market fluctuations and deep seek AI model impact – Link name: nvidia_market_fluctuations_and_deep_seek_ai_model_impact | Nvidia's stock]]. Reports indicate that the [[chinas_deepseek_ai_model | DeepSeek]] model is significantly more cost-efficient than its US-based counterparts, leading to a re-evaluation of hardware demand and the value proposition of leading AI companies <a class="yt-timestamp" data-t="01:32:00">[01:32:00]</a>.

## The DeepSeek Breakthrough

The [[chinas_deepseek_ai_model | DeepSeek AI model]], developed by a Chinese AI lab, has gained notoriety for its reported efficiency. It is claimed to be 45 times more cost-efficient than US-based AI models and charges 95% less for API calls compared to Chat GPT <a class="yt-timestamp" data-t="01:32:00">[01:32:00]</a>. Furthermore, it was reportedly trained for a mere $6 million, a fraction of the cost associated with developing models by major US labs <a class="yt-timestamp" data-t="01:58:00">[01:58:00]</a>.

While the technical paper for [[chinas_deepseek_ai_model | DeepSeek V3]] was released on December 27th and the R1 model paper a week prior, the market's reaction, specifically a significant stock market crash on a Monday, was attributed to the viral spread of an article written by investor and technologist Jeffrey Emanuel <a class="yt-timestamp" data-t="00:20:00">[00:20:00]</a>, <a class="yt-timestamp" data-t="00:26:00">[00:26:00]</a>, <a class="yt-timestamp" data-t="00:39:00">[00:39:00]</a>. This article, written to be understandable by hedge fund managers, was published late on a Friday and quickly gained traction, being shared by prominent figures like Chamath Palihapitiya and Naval Ravikant, accumulating over two million views <a class="yt-timestamp" data-t="00:41:00">[00:41:00]</a>, <a class="yt-timestamp" data-t="00:54:00">[00:54:00]</a>, <a class="yt-timestamp" data-t="01:06:00">[01:06:00]</a>.

## Impact on Nvidia and the Market

The immediate consequence of the [[chinas_deepseek_ai_model | DeepSeek]] news was a dramatic 20% drop in [[NVIDIA market fluctuations and deep seek AI model impact – Link name: nvidia_market_fluctuations_and_deep_seek_ai_model_impact | Nvidia's stock price]], wiping out $600 billion in market value <a class="yt-timestamp" data-t="01:40:00">[01:40:00]</a>. This market reaction stemmed from the perception that such significant efficiency gains in [[innovative_trends_in_ai_and_machine_learning_models | AI models]] would drastically reduce the overall demand for high-end [[role_of_gpus_in_deep_learning | GPUs]], thereby impacting [[NVIDIA market fluctuations and deep seek AI model impact – Link name: nvidia_market_fluctuations_and_deep_seek_ai_model_impact | Nvidia's revenue projections]] <a class="yt-timestamp" data-t="01:35:00">[01:35:00]</a>, <a class="yt-timestamp" data-t="01:45:00">[01:45:00]</a>.

Jeffrey Emanuel, the author of the viral article, posits that while [[chinas_deepseek_ai_model | DeepSeek]]'s impact is significant, it's "over-indexed" as the sole cause <a class="yt-timestamp" data-t="02:07:00">[02:07:00]</a>. He argues that a confluence of factors, including the emergence of other chip suppliers and the inherent challenges to [[NVIDIA market fluctuations and deep seek AI model impact – Link name: nvidia_market_fluctuations_and_deep_seek_ai_model_impact | Nvidia's market dominance]], are contributing to the "unbundling" of [[NVIDIA market fluctuations and deep seek AI model impact – Link name: nvidia_market_fluctuations_and_deep_seek_ai_model_impact | Nvidia's market share]] <a class="yt-timestamp" data-t="02:09:00">[02:09:00]</a>, <a class="yt-timestamp" data-t="02:11:00">[02:11:00]</a>.

The efficiency of [[chinas_deepseek_ai_model | DeepSeek]] also puts pressure on US-based AI companies like [[OpenAI updates and their effects | OpenAI]] and Meta's AI labs, forcing them to understand how a relatively unknown Chinese lab could outperform their expensive models <a class="yt-timestamp" data-t="01:48:00">[01:48:00]</a>.

## Hardware vs. Software Dynamics

The market reaction highlights a "tug-of-war" between hardware and software in the AI industry <a class="yt-timestamp" data-t="07:12:00">[07:12:00]</a>. [[chinas_deepseek_ai_model | DeepSeek]]'s advancements represent a significant win for the software side, as algorithmic and architectural improvements can dramatically reduce the need for extensive, costly hardware <a class="yt-timestamp" data-t="07:17:00">[07:17:00]</a>.

Jeffrey notes the "Jevons Paradox," where increased efficiency might lead to increased demand, but he believes this effect isn't immediate and doesn't negate the short-term impact on hardware demand <a class="yt-timestamp" data-t="07:47:00">[07:47:00]</a>, <a class="yt-timestamp" data-t="08:29:00">[08:29:00]</a>. Large players like Meta, facing high costs for [[role_of_gpus_in_deep_learning | Nvidia chips]] (e.g., $40,000 for a [[role_of_gpus_in_deep_learning | GPU]] that costs [[NVIDIA market fluctuations and deep seek AI model impact – Link name: nvidia_market_fluctuations_and_deep_seek_ai_model_impact | Nvidia]] $3,500 to make), might now "pump the brakes" on hardware investments if software efficiencies can meet their forecasted demand <a class="yt-timestamp" data-t="09:18:00">[09:18:00]</a>, <a class="yt-timestamp" data-t="09:32:00">[09:32:00]</a>.

## DeepSeek's Efficiency Innovations

[[chinas_deepseek_ai_model | DeepSeek]]'s breakthrough is partly attributed to China's different approach to AI development, driven by necessity rather than an abundance of capital, in contrast to Western labs that "throw money at a problem" <a class="yt-timestamp" data-t="45:06:00">[45:06:00]</a>, <a class="yt-timestamp" data-t="46:06:00">[46:06:00]</a>. The [[chinas_deepseek_ai_model | DeepSeek]] team integrated both advanced research and high-performance engineering, inverting the typical two-step process seen in the West <a class="yt-timestamp" data-t="48:54:00">[48:54:00]</a>, <a class="yt-timestamp" data-t="49:51:00">[49:51:00]</a>.

Key innovations contributing to [[chinas_deepseek_ai_model | DeepSeek]]'s efficiency include:

*   **Optimized Communication Overhead**: They focused on saturating [[role_of_gpus_in_deep_learning | GPU]] performance by minimizing communication overhead, ensuring that [[role_of_gpus_in_deep_learning | GPUs]] are not idle waiting for data <a class="yt-timestamp" data-t="49:56:00">[49:56:00]</a>.
*   **Efficient KV Cache**: For Transformer models, [[chinas_deepseek_ai_model | DeepSeek]] developed a smarter way to store Key-Value (KV) caches, which are crucial for the attention mechanism <a class="yt-timestamp" data-t="52:45:00">[52:45:00]</a>. By storing only the necessary, compressed subset of data, they significantly reduced memory usage, allowing more to be done with fewer [[role_of_gpus_in_deep_learning | GPUs]] <a class="yt-timestamp" data-t="52:56:00">[52:56:00]</a>, <a class="yt-timestamp" data-t="54:57:00">[54:57:00]</a>.
*   **Multi-Token Predictions (Speculative Decoding)**: Instead of predicting one token (word) at a time, [[chinas_deepseek_ai_model | DeepSeek]] implemented speculative decoding, predicting multiple tokens simultaneously. They achieved a 95% accuracy rate in these predictions, effectively doubling inference throughput with no additional cost <a class="yt-timestamp" data-t="55:13:00">[55:13:00]</a>, <a class="yt-timestamp" data-t="56:19:00">[56:19:00]</a>.
*   **Compressed Parameter Storage**: They found a way to store model parameters in a more compressed form, enabling the entire training process to largely use a smaller representation without sacrificing model intelligence. This leads to faster calculations and less inter-CPU communication <a class="yt-timestamp" data-t="56:37:00">[56:37:00]</a>, <a class="yt-timestamp" data-t="57:39:00">[57:39:00]</a>.

These optimization tricks, when multiplied, result in the reported 45x efficiency gains, although the precise figures can be difficult to verify <a class="yt-timestamp" data-t="58:14:00">[58:14:00]</a>, <a class="yt-timestamp" data-t="58:30:00">[58:30:00]</a>. The most tangible impact is the 95% lower cost for inference, which directly threatens the profitability of API-based AI services offered by companies like [[OpenAI updates and their effects | OpenAI]] <a class="yt-timestamp" data-t="58:46:00">[58:46:00]</a>.

## Broader Implications

The [[chinas_deepseek_ai_model | DeepSeek]] efficiency gains create significant pressure on companies like [[OpenAI updates and their effects | OpenAI]] and Anthropic, who currently charge high prices for their API models. They may be forced to cut prices dramatically, impacting their revenue streams and profitability <a class="yt-timestamp" data-t="59:50:00">[59:50:00]</a>, <a class="yt-timestamp" data-t="01:00:07:00">[01:00:07:00]</a>.

This development was unexpected by many market analysts, who anticipated a more linear, "Moore's Law type progression" in AI advancements rather than "step function changes" of 45x efficiency <a class="yt-timestamp" data-t="01:05:30:00">[01:05:30:00]</a>, <a class="yt-timestamp" data-t="01:06:09:00">[01:06:09:00]</a>. [[NVIDIA market fluctuations and deep seek AI model impact – Link name: nvidia_market_fluctuations_and_deep_seek_ai_model_impact | Nvidia's stock]], which was "priced to perfection," failed to account for such rapid, non-linear algorithmic improvements and the potential for competitive disruption <a class="yt-timestamp" data-t="01:06:25:00">[01:06:25:00]</a>, <a class="yt-timestamp" data-t="01:06:51:00">[01:06:51:00]</a>.

For consumers of AI products, these efficiency gains are largely positive. They promise more powerful models at potentially lower costs, even enabling advanced [[open_source_ai_models | AI models]] to run efficiently on personal devices like laptops and phones <a class="yt-timestamp" data-t="01:09:14:00">[01:09:14:00]</a>, <a class="yt-timestamp" data-t="01:28:10:00">[01:28:10:00]</a>. This means greater accessibility to advanced AI capabilities, irrespective of constant internet connectivity <a class="yt-timestamp" data-t="01:10:50:00">[01:10:50:00]</a>.