---
title: Decentralized Training of AI Models
videoId: UnMYLttFaUg
---

From: [[bankless]] <br/> 

Traditionally, training advanced AI models has been a highly centralized process, requiring billions of dollars and housing all operations under single data centers <a class="yt-timestamp" data-t="01:17:35">[01:17:35]</a>. This approach leads to issues of centralization and over-reliance on a single entity, which can dictate how models are designed and utilized <a class="yt-timestamp" data-t="01:17:56">[01:17:56]</a>.

## What is Decentralized Training?
[[decentralized_ai_model_training | Decentralized training]] offers an alternative by allowing compute to be provided from computers and data centers globally <a class="yt-timestamp" data-t="01:18:10">[01:18:10]</a>. Historically, this has been an "incredibly hard scientific and research problem" due to physics challenges <a class="yt-timestamp" data-t="01:18:27">[01:18:27]</a>.

## Prime Intellect's Breakthrough
Recent breakthroughs by crypto companies, including Prime Intellect, have engineered solutions for decentralized model training <a class="yt-timestamp" data-t="01:18:34">[01:18:34]</a>. Prime Intellect notably announced they have successfully trained a 32 billion parameter model using decentralized means <a class="yt-timestamp" data-t="01:19:00">[01:19:00]</a>. While larger models with trillions of parameters exist in centralized training <a class="yt-timestamp" data-t="01:19:11">[01:19:11]</a>, a 32 billion parameter model is still capable of many "really cool things" and can run locally on devices like laptops or phones <a class="yt-timestamp" data-t="01:19:22">[01:19:22]</a>.

The completion of this training run signifies "real fundamental traction" for the crypto and Web3 space in AI <a class="yt-timestamp" data-t="01:20:34">[01:20:34]</a>.

## Historical Context and Accelerating Progress
The field of decentralized AI model training is new and rapidly advancing <a class="yt-timestamp" data-t="01:23:25">[01:23:25]</a>:
*   **Two years ago:** Google's DeepMind, a leading AI research entity, could only achieve decentralized training for a 400 million parameter model <a class="yt-timestamp" data-t="01:22:40">[01:22:40]</a>.
*   **Three months later:** A group of open-source AI researchers successfully trained a 1.5 billion parameter model <a class="yt-timestamp" data-t="01:23:05">[01:23:05]</a>.
*   **One month after that:** Prime Intellect trained a 2 billion parameter model <a class="yt-timestamp" data-t="01:23:19">[01:23:19]</a>.
*   **Present:** Prime Intellect's 32 billion parameter model <a class="yt-timestamp" data-t="01:19:00">[01:19:00]</a>.

This demonstrates exponential improvement, with the potential to reach 100 billion parameter models that could compete with large centralized models by the end of the year <a class="yt-timestamp" data-t="01:21:04">[01:21:04]</a>.

## Significance and Future Outlook
The trend towards more distributed and decentralized AI model training is gaining recognition from traditional AI experts, including a co-founder of Anthropic <a class="yt-timestamp" data-t="01:20:03">[01:20:03]</a>. This shift is driven by constraints within centralized systems <a class="yt-timestamp" data-t="01:20:12">[01:20:12]</a>.

### Decentralized vs. Centralized Models
While [[ai_model_differences_and_enhancements | decentralized models]] are exciting, their ability to "outcompete or beat the centralized competitors" is still unproven <a class="yt-timestamp" data-t="01:22:27">[01:22:27]</a>. In the context of crypto, decentralization often implies security, but this argument doesn't always apply the same way to [[innovative_trends_in_ai_and_machine_learning_models | AI model training]] <a class="yt-timestamp" data-t="01:21:34">[01:21:34]</a>. Ultimately, the "product experience" will be a deciding factor for user adoption <a class="yt-timestamp" data-t="01:24:08">[01:24:08]</a>. Decentralized systems could serve as a valuable "check on the power" of major centralized players like OpenAI, Google, and Facebook, acting as a "fantastic plan B" if centralized entities act nefariously <a class="yt-timestamp" data-t="01:24:27">[01:24:27]</a>.

### Decentralized vs. [[open_source_ai_models | Open Source AI Models]]
A key distinction exists between decentralized training and [[open_source_ai_models | open source models]] <a class="yt-timestamp" data-t="01:25:20">[01:25:20]</a>. [[open_source_ai_models | Open source models]] involve a centralized entity creating a model but making its weights, designs, and blueprints public for replication and fine-tuning <a class="yt-timestamp" data-t="01:25:36">[01:25:36]</a>. In contrast, [[decentralized_ai_model_training | decentralized trained models]] do not rely on a centralized entity or massive funding for their training; instead, they tap into a distributed network of compute providers <a class="yt-timestamp" data-t="01:26:01">[01:26:01]</a>. This allows individuals, such as a "budding college AI computer nerd," to train models by leveraging distributed compute in exchange for potential ownership <a class="yt-timestamp" data-t="01:26:18">[01:26:18]</a>.