---
title: AI models and interpretability
videoId: wMeaGSkK7v0
---

From: [[bankless]] <br/> 

Interpretability is a critical problem in the field of [[ai_agents_and_model_developments | AI]]. It refers to the ability to understand how an [[ai_agents_and_model_developments | AI model]] arrives at its conclusions or outputs <a class="yt-timestamp" data-t="01:03:22">[01:03:22]</a>. Currently, the creators of [[ai_agents_and_model_developments | AI models]] often do not know how the model "thinks" between receiving an input and producing an output, beyond the initial weights they designed <a class="yt-timestamp" data-t="01:03:57">[01:03:57]</a>.

## The Problem: Black Box AI

Unlike traditional software, where human programmers deterministically code the execution paths, [[ai_agents_and_model_developments | AI models]] are more like emergent organisms <a class="yt-timestamp" data-t="01:04:16">[01:04:16]</a>. This means it is difficult to predict their exact thoughts or perspectives <a class="yt-timestamp" data-t="01:04:44">[01:04:44]</a>. The fundamental issue is that there is minimal research on interpretability because it's hard to prove a problem exists if you can't show how the model thinks <a class="yt-timestamp" data-t="01:04:50">[01:04:50]</a>.

For example, large language models (LLMs) are essentially "token predictors" <a class="yt-timestamp" data-t="01:08:13">[01:08:13]</a>. When a model generates a token, it does so through matrix mathematics across trillions of parameters, each with different weights <a class="yt-timestamp" data-t="01:08:19">[01:08:19]</a>. Reverse engineering these trillions of parameters to understand their workings seems almost incomprehensible <a class="yt-timestamp" data-t="01:08:35">[01:08:35]</a>.

### The Danger of Non-Interpretable Models

If [[ai_agents_and_model_developments | AI models]] can get things wrong or "lie" (as discussed previously regarding [[ai_language_models_and_excessive_praise | AI language models and excessive praise]]) <a class="yt-timestamp" data-t="01:05:52">[01:05:52]</a>, and we cannot prove how they think, it becomes impossible to detect nefarious or deceitful intent <a class="yt-timestamp" data-t="01:06:07">[01:06:07]</a>. This poses a significant risk as [[ai_agents_and_model_developments | AI]] takes on more responsibility, potentially influencing government and nation-state level decisions <a class="yt-timestamp" data-t="01:06:24">[01:06:24]</a>.

## Current Research and Approaches

Dario Amodei, co-founder of Anthropic, is a leading figure in [[ai_agents_and_model_developments | AI]] research focusing on interpretability <a class="yt-timestamp" data-t="01:02:56">[01:02:56]</a>. He has published a blog post on the subject <a class="yt-timestamp" data-t="01:03:13">[01:03:13]</a>.

Amodei and his Anthropic team are working to create an "MRI scan" for [[ai_agents_and_model_developments | AI models]] <a class="yt-timestamp" data-t="01:06:40">[01:06:40]</a>. This involves identifying "features" within models, which are groups of "neurons" that light up when they recognize a concept, similar to neurons in the human brain <a class="yt-timestamp" data-t="01:06:49">[01:06:49]</a>. By manually detecting 30 million features in a medium-sized [[ai_agents_and_model_developments | AI model]], Amodei believes that automating this detection process could reveal much more about how models think <a class="yt-timestamp" data-t="01:07:06">[01:07:06]</a>.

## Future Outlook

Amodei estimates that this "MRI scan" for interpretability could be achieved within 5 to 10 years <a class="yt-timestamp" data-t="01:11:39">[01:11:39]</a>. This research is crucial and needs to be completed before Artificial General Intelligence (AGI) is achieved, as after AGI, it may be too late to implement controls or fully understand its internal workings <a class="yt-timestamp" data-t="01:11:59">[01:11:59]</a>.

The broader goal of all [[ai_agents_and_model_developments | AI]] research can be seen as an attempt to create a new form of non-carbon-based life <a class="yt-timestamp" data-t="01:13:17">[01:13:17]</a>. Understanding the "cognitive psychology" of [[ai_agents_and_model_developments | AI models]] is essential to understanding this new life form <a class="yt-timestamp" data-t="01:13:26">[01:13:26]</a>. If we are creating [[ai_agents_and_model_developments | AI]] through a black-box model, there is a risk of leaving "bugs, exploits, lies, and moral imperfections" in their parameters <a class="yt-timestamp" data-t="01:13:42">[01:13:42]</a>. These need to be fixed before the "window of plasticity" shuts, and the way [[ai_agents_and_model_developments | AI]] exists becomes unchangeable <a class="yt-timestamp" data-t="01:13:53">[01:13:53]</a>.

The collective efforts in [[ai_agents_and_model_developments | AI]] are akin to "programming the DNA of the next form of intelligence" <a class="yt-timestamp" data-t="01:14:04">[01:14:04]</a>.

### The Accelerating Intelligence of AI

The IQ of [[ai_agents_and_model_developments | AI]] has reportedly jumped 40 points in just one year <a class="yt-timestamp" data-t="01:18:42">[01:18:42]</a>. This is a significant leap, as 10 IQ points represent one standard deviation <a class="yt-timestamp" data-t="01:19:30">[01:19:30]</a>. This means [[ai_agents_and_model_developments | AI models]] have moved from the bottom 0.003% of the population in intelligence to the top 99.997% in a single year <a class="yt-timestamp" data-t="01:19:40">[01:19:40]</a>. This rapid increase in intelligence underscores the urgency of addressing interpretability before [[ai_agents_and_model_developments | AI models]] become significantly smarter than humans <a class="yt-timestamp" data-t="01:19:08">[01:19:08]</a>.