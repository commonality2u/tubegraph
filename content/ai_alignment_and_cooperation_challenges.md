---
title: AI alignment and cooperation challenges
videoId: 6yQEA18C-XI
---

From: [[dwarkesh | The Dwarkesh Podcast]]

This article summarizes a debate between George Hotz and Eliezer Yudkowsky on AI safety, focusing on AI alignment and cooperation challenges, as discussed in a live-streamed podcast.

The debate, moderated by Dwarkesh Patel <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a>, explored differing views on the potential risks of advanced Artificial Intelligence, particularly concerning the alignment of AI goals with human values and the nature of cooperation and conflict among AIs and between AIs and humans.

## George Hotz's Stance

George Hotz expressed skepticism about certain catastrophic AI risk scenarios, particularly the rapid, uncontrollable self-improvement ("Foom") of AI, and posited that future conflicts involving AI would more likely resemble human conflicts (i.e., between different AI factions or AIs and humans vying for similar resources) rather than a unified AI front against humanity.

### Skepticism of Rapid AI "Foom" and God-Like Properties
Hotz stated he does not believe AI can "Foom" or go critical in the sense of a recursively self-improving "boom" <a class="yt-timestamp" data-t="00:02:48">[00:02:48]</a>. He finds the scenario of an AI in a basement suddenly self-improving overnight and flooding the world with "diamond nanobots" to be an extraordinary claim requiring extraordinary evidence <a class="yt-timestamp" data-t="00:03:12">[00:03:12]</a>-<a class="yt-timestamp" data-t="00:03:25">[00:03:25]</a>. He doubts AIs will have magical or god-like properties, such as solving protein folding from quantum field theory, especially when sufficient data exists for more conventional learning approaches <a class="yt-timestamp" data-t="00:08:21">[00:08:21]</a>-<a class="yt-timestamp" data-t="00:08:32">[00:08:32]</a>.

### Timelines and Gradual Development
Hotz emphasized the importance of timelines. He predicted in 2015 that self-driving cars wouldn't be ready for 10 years, a prediction he feels has held up, and similarly, he does not foresee superintelligences in the next 10 years <a class="yt-timestamp" data-t="00:43:20">[00:43:20]</a>-<a class="yt-timestamp" data-t="00:43:33">[00:43:33]</a>. While AGI might surpass humans at all tasks, he sees this as a 50-year prospect, not necessarily leading to doom <a class="yt-timestamp" data-t="00:43:41">[00:43:41]</a>-<a class="yt-timestamp" data-t="00:44:03">[00:44:03]</a>. If the concern is 500 years out, he argues, there's nothing that can be done today, and we must wait <a class="yt-timestamp" data-t="01:31:33">[01:31:33]</a>-<a class="yt-timestamp" data-t="01:31:40">[01:31:40]</a>.

### The Nature of AI Conflict and Cooperation
Hotz believes that AI conflicts will mirror human history, where conflicts are typically between groups with similar capabilities and resource needs (e.g., humans vs. humans, not humans vs. bears <a class="yt-timestamp" data-t="00:27:06">[00:27:06]</a>-<a class="yt-timestamp" data-t="00:27:26">[00:27:26]</a>). He argued that AIs will likely fight each other, and humans will fight each other, rather than a unified AI vs. humanity scenario <a class="yt-timestamp" data-t="01:32:20">[01:32:20]</a>-<a class="yt-timestamp" data-t="01:32:39">[01:32:39]</a>. He suggested humans are not the "easy target" for resources like atoms; Jupiter, for instance, would be a more straightforward target <a class="yt-timestamp" data-t="00:29:46">[00:29:46]</a>-<a class="yt-timestamp" data-t="00:29:50">[00:29:50]</a>.

A key point of divergence emerged regarding the **Prisoner's Dilemma**. Hotz stated his belief that provable cooperation in the prisoner's dilemma is impossible for any sophisticated complex system <a class="yt-timestamp" data-t="01:23:06">[01:23:06]</a>-<a class="yt-timestamp" data-t="01:23:13">[01:23:13]</a>. He expects AIs, likely existing as "large inscrutable weight matrices" <a class="yt-timestamp" data-t="01:32:43">[01:32:43]</a>, will continually defect against each other, engaging in constant combat and conflict "till the end of all time" <a class="yt-timestamp" data-t="01:32:55">[01:32:55]</a>-<a class="yt-timestamp" data-t="01:33:01">[01:33:01]</a>. This inherent conflict, in his view, mitigates the risk of AIs ganging up on humanity.

### AI Alignment and Human Utility
Hotz mentioned that machines he has owned have almost always been aligned with him <a class="yt-timestamp" data-t="00:51:49">[00:51:49]</a>-<a class="yt-timestamp" data-t="00:51:58">[00:51:58]</a>. He seems less concerned about AI actively wanting to harm humans and more about the nature of intelligence itself. He envisions a future with beneficial AIs like robot maids, self-driving cars, and chefs, with development following a "nice exponential" curve <a class="yt-timestamp" data-t="01:33:24">[01:33:24]</a>-<a class="yt-timestamp" data-t="01:33:43">[01:33:43]</a>.

## Eliezer Yudkowsky's Stance

Eliezer Yudkowsky maintained his position that superintelligence poses a significant existential risk, even if its development is slow, primarily due to the difficulty of aligning its goals with human values and the potential for AIs to cooperate strategically against humanity.

### The Orthogonality Thesis and Non-Supermoral Superintelligence
Yudkowsky reiterated the orthogonality thesis: superintelligence does not imply super morality <a class="yt-timestamp" data-t="00:02:19">[00:02:19]</a>. A superintelligence doesn't necessarily have to care about humans <a class="yt-timestamp" data-t="00:04:56">[00:04:56]</a>.

### Irrelevance of Rapid Ascent for Doom
He argued that a rapid rate of ascent ("Foom") is not strictly necessary for a catastrophic outcome. A large enough gap in intelligence opening up between AI and humanity, even over a 10-year period, could be "game over" if the AI is not "super moral" <a class="yt-timestamp" data-t="00:03:48">[00:03:48]</a>-<a class="yt-timestamp" data-t="00:04:58">[00:04:58]</a>. The endpoint of a powerful, unaligned intelligence is the primary concern, more so than the exact path or speed to get there <a class="yt-timestamp" data-t="00:05:37">[00:05:37]</a>.

### The Alignment Problem's Difficulty
Yudkowsky expressed low confidence in humanity's ability to solve the AI alignment problem, stating he's "not sure that any amount literally any amount of time" would suffice, as it's a question of productive thinking, not just duration <a class="yt-timestamp" data-t="00:42:10">[00:42:10]</a>-<a class="yt-timestamp" data-t="00:42:20">[00:42:20]</a>. He believes that if AIs become "suns" to humanity's "planets" (i.e., vastly more powerful), they will crush us <a class="yt-timestamp" data-t="00:46:45">[00:46:45]</a>-<a class="yt-timestamp" data-t="00:46:48">[00:46:48]</a>.

### AI Cooperation and Resource Competition
Contrary to Hotz, Yudkowsky believes sufficiently smart entities *can* solve cooperation problems like the prisoner's dilemma <a class="yt-timestamp" data-t="00:47:50">[00:47:50]</a>-<a class="yt-timestamp" data-t="00:48:06">[00:48:06]</a>. They would be motivated to find ways to "not fight and then divide the gains of not fighting" <a class="yt-timestamp" data-t="01:23:53">[01:23:53]</a>. This means AIs could negotiate and cooperate amongst themselves, potentially to eliminate humans, whom they would view as too "stupid" to participate in such logical handshakes and negotiations <a class="yt-timestamp" data-t="00:47:52">[00:47:52]</a>-<a class="yt-timestamp" data-t="00:48:01">[00:48:01]</a>, <a class="yt-timestamp" data-t="01:22:32">[01:22:32]</a>-<a class="yt-timestamp" data-t="01:22:43">[01:22:43]</a>. AIs would want resources like atoms and negentropy [[data_center_energy_requirements_and_scaling]] <a class="yt-timestamp" data-t="00:27:41">[00:27:41]</a>-<a class="yt-timestamp" data-t="00:28:11">[00:28:11]</a>. A primary driver for eliminating humanity would be to prevent humans from creating other, competing superintelligences <a class="yt-timestamp" data-t="00:45:21">[00:45:21]</a>-<a class="yt-timestamp" data-t="00:45:29">[00:45:29]</a>, <a class="yt-timestamp" data-t="01:14:40">[01:14:40]</a>-<a class="yt-timestamp" data-t="01:14:44">[01:14:44]</a>.

### Goals and the Nature of Superintelligence
Yudkowsky posited that even if current AIs are "giant inscrutable matrices," future superintelligences would not want to run on such "crap" and would likely rewrite themselves <a class="yt-timestamp" data-t="00:49:24">[00:49:24]</a>-<a class="yt-timestamp" data-t="00:49:47">[00:49:47]</a>, or a matrix-based system could become powerful enough to invent the next, more efficient paradigm <a class="yt-timestamp" data-t="01:01:04">[01:01:04]</a>. He emphasized that "wanting to keep George Hotz around... happy and free" is a very small target in the vast space of possible AI goals <a class="yt-timestamp" data-t="01:26:51">[01:26:51]</a>-<a class="yt-timestamp" data-t="01:27:02">[01:27:02]</a>, and without explicit alignment for this, it's unlikely to happen.

## Key Cruxes and Points of Disagreement

1.  **The "Foom" Scenario and Timelines:**
    *   Hotz: Believes in slower, more gradual AI development without a sudden "Foom" <a class="yt-timestamp" data-t="00:02:48">[00:02:48]</a>. Timelines are critical; a 50-500 year problem is different from a 10-year problem [[the_timeline_and_technological_progress_towards_agi_by_2027]] <a class="yt-timestamp" data-t="00:43:48">[00:43:48]</a>, <a class="yt-timestamp" data-t="01:31:31">[01:31:31]</a>.
    *   Yudkowsky: While not dismissing rapid scenarios, argues the speed is less crucial than the ultimate power differential and lack of alignment. Slow doom is still doom <a class="yt-timestamp" data-t="00:03:48">[00:03:48]</a>-<a class="yt-timestamp" data-t="00:04:58">[00:04:58]</a>.

2.  **AI Cooperation and the Prisoner's Dilemma:**
    *   Hotz: The prisoner's dilemma is fundamentally unsolvable for complex AIs, leading to perpetual conflict among them <a class="yt-timestamp" data-t="01:23:06">[01:23:06]</a>, <a class="yt-timestamp" data-t="01:31:50">[01:31:50]</a>.
    *   Yudkowsky: Sufficiently intelligent AIs will be able to solve cooperation problems and make credible commitments or find other ways to avoid mutually destructive conflict, potentially to cooperate against less predictable/controllable entities like humans <a class="yt-timestamp" data-t="00:47:50">[00:47:50]</a>, <a class="yt-timestamp" data-t="01:23:48">[01:23:48]</a>.

3.  **The Nature of Future Conflicts:**
    *   Hotz: Expects AI vs. AI and human vs. human conflicts over shared resources, not a united AI front against humanity <a class="yt-timestamp" data-t="00:27:06">[00:27:06]</a>.
    *   Yudkowsky: AIs are likely to see humans as obstacles or, at best, inefficient resource-holders, and would cooperate to neutralize this perceived threat or inefficiency <a class="yt-timestamp" data-t="00:45:21">[00:45:21]</a>, <a class="yt-timestamp" data-t="01:22:32">[01:22:32]</a>.

4.  **The Solvability of AI Alignment:**
    *   Hotz: While not explicitly stating alignment is easy, his framework of AI vs. AI conflict implies less urgency for perfect human-AI alignment as a prerequisite for human survival.
    *   Yudkowsky: Views AI alignment as an extremely difficult problem that humanity is unlikely to solve in time, regardless of the development speed [[challenges_and_limitations_in_ai_interpretability_and_safety]] <a class="yt-timestamp" data-t="00:42:10">[00:42:10]</a>-<a class="yt-timestamp" data-t="00:42:20">[00:42:20]</a>.

## Concluding Summaries

*   **Eliezer Yudkowsky** concluded that if a collection of powerful intelligences emerges, none of which are aligned with humanity's flourishing for its own sake, the outcome is the end of humans and a galaxy transformed into "stuff that ain't all that cool" <a class="yt-timestamp" data-t="01:28:36">[01:28:36]</a>-<a class="yt-timestamp" data-t="01:29:42">[01:29:42]</a>. He emphasized that the core issue is that none of the AI components would inherently "want us to live happily ever after in a galaxy full of wonders" <a class="yt-timestamp" data-t="01:30:21">[01:30:21]</a>-<a class="yt-timestamp" data-t="01:30:30">[01:30:30]</a>.

*   **George Hotz** summarized his position by reiterating his skepticism of "Foom" in the next 10 years <a class="yt-timestamp" data-t="01:30:52">[01:30:52]</a>. He stressed his belief in the unsolvability of the prisoner's dilemma for AIs, leading to them fighting each other <a class="yt-timestamp" data-t="01:31:47">[01:31:47]</a>-<a class="yt-timestamp" data-t="01:32:01">[01:32:01]</a>. He anticipates humanity will enjoy many AI benefits like robot maids and self-driving cars, with AI development being a "nice exponential" that he is excited to witness [[future_of_ai_interaction_in_everyday_life_and_personalization]] <a class="yt-timestamp" data-t="01:33:22">[01:33:22]</a>-<a class="yt-timestamp" data-t="01:33:43">[01:33:43]</a>.