---
title: Applications and future of AI in various industries
videoId: JAgHUDhaTU0
---

From: [[nikhil.kamath]] <br/> 

## Defining AI and Its Evolution
Artificial Intelligence (AI) can be understood through its historical development and different conceptual approaches to intelligence itself <a class="yt-timestamp" data-t="00:11:20">[00:11:20]</a>. The pursuit of [[understanding_ai | AI]] is driven by the obsession to uncover the mysteries of intelligence, with the belief that the only way to do so is to build intelligent machines <a class="yt-timestamp" data-t="00:02:41">[00:02:41]</a>. The consequences of building such machines could be profoundly important for humanity <a class="yt-timestamp" data-t="00:03:04">[00:03:04]</a>.

Historically, the definition of intelligence within AI has been viewed from multiple perspectives, likened to the parable of the blind men and the elephant <a class="yt-timestamp" data-t="00:11:20">[00:11:20]</a> <a class="yt-timestamp" data-t="00:13:22">[00:13:22]</a>.
*   **Reasoning and Search**: One early approach (1950s) defined intelligence as the ability to reason and search for solutions to problems, such as the "traveling salesman problem" <a class="yt-timestamp" data-t="00:11:46">[00:11:46]</a> <a class="yt-timestamp" data-t="00:12:00">[00:12:00]</a>. This often involved optimization, finding the best solution among many possibilities <a class="yt-timestamp" data-t="00:12:50">[00:12:50]</a>. This branch, known as "good old-fashioned AI" (GOFAI), was dominant until the 1990s <a class="yt-timestamp" data-t="00:14:04">[00:14:04]</a> <a class="yt-timestamp" data-t="00:29:00">[00:29:00]</a>. It included rule-based systems, logical inference, and heuristic programming (which avoided exhaustive searches) <a class="yt-timestamp" data-t="00:18:27">[00:18:27]</a> <a class="yt-timestamp" data-t="00:19:23">[00:19:23]</a> <a class="yt-timestamp" data-t="00:29:00">[00:29:00]</a>.
*   **Learning and Neural Networks**: A parallel branch, also starting in the 1950s, focused on reproducing the learning mechanisms observed in animal and human brains <a class="yt-timestamp" data-t="00:15:27">[00:15:27]</a> <a class="yt-timestamp" data-t="00:15:43">[00:15:43]</a>. The idea was that intelligence emerges from networks of simple, connected elements that learn by modifying the strength of their connections (like neurons) <a class="yt-timestamp" data-t="00:15:56">[00:15:56]</a> <a class="yt-timestamp" data-t="00:16:05">[00:16:05]</a>. Early examples include the perceptron (1957) for recognizing simple shapes <a class="yt-timestamp" data-t="00:21:00">[00:21:00]</a>. This approach was initially limited but led to fields like pattern recognition <a class="yt-timestamp" data-t="00:16:57">[00:16:57]</a> <a class="yt-timestamp" data-t="00:17:09">[00:17:09]</a>.

### Machine Learning and Deep Learning
Machine learning is a subfield of AI where machines are trained from data rather than being explicitly programmed <a class="yt-timestamp" data-t="00:29:11">[00:29:11]</a> <a class="yt-timestamp" data-t="00:31:42">[00:31:42]</a>.
*   **Supervised Learning**: The system is given an input and a desired output, and its internal parameters are adjusted to make its output closer to the desired one <a class="yt-timestamp" data-t="00:27:36">[00:27:36]</a>. This process requires vast amounts of labeled data <a class="yt-timestamp" data-t="00:27:52">[00:27:52]</a>.
*   **Reinforcement Learning**: The system is not given correct answers but rather feedback on whether its output was "good" or "bad" <a class="yt-timestamp" data-t="00:32:23">[00:32:23]</a>. It's highly inefficient for general tasks, requiring many trials, but excels in environments like games where systems can play millions of times against themselves (e.g., chess, Go, poker) <a class="yt-timestamp" data-t="00:43:37">[00:43:37]</a> <a class="yt-timestamp" data-t="00:43:43">[00:43:43]</a>.
*   **Self-Supervised Learning**: This modern approach, highly prominent in the last 5-6 years, bridges the gap between supervised and unsupervised learning <a class="yt-timestamp" data-t="00:33:21">[00:33:21]</a>. Data itself is used to generate the "supervision" <a class="yt-timestamp" data-t="00:34:31">[00:34:31]</a>. For example, by masking parts of a text or corrupting an image, the system is trained to predict the missing words or recover the original image <a class="yt-timestamp" data-t="00:34:00">[00:34:00]</a> <a class="yt-timestamp" data-t="00:34:40">[00:34:40]</a>. This means no manual labeling of millions of examples is needed <a class="yt-timestamp" data-t="00:34:53">[00:34:53]</a>.

### Deep Learning and Large Language Models
Deep learning is a subcategory of machine learning that utilizes neural networks with multiple layers <a class="yt-timestamp" data-t="00:28:28">[00:28:28]</a> <a class="yt-timestamp" data-t="00:39:26">[00:39:26]</a>. The breakthrough in the 1980s was the ability to stack multiple layers of "neurons" (mathematical functions that compute weighted sums with a threshold) and train them using algorithms like backpropagation <a class="yt-timestamp" data-t="00:39:26">[00:39:26]</a> <a class="yt-timestamp" data-t="00:40:19">[00:40:19]</a>. This allowed for the computation of complex functions, overcoming the limitations of earlier perceptrons <a class="yt-timestamp" data-t="00:28:02">[00:28:02]</a> <a class="yt-timestamp" data-t="00:41:20">[00:41:20]</a>.

Key architectural components of deep neural networks include:
*   **Convolutional Neural Networks (ConvNets)**: Inspired by the visual cortex, ConvNets excel at processing natural data like images and audio signals <a class="yt-timestamp" data-t="00:45:12">[00:45:12]</a> <a class="yt-timestamp" data-t="00:45:28">[00:45:28]</a>. They use "neurons" that look at small, local areas of the input, detecting motifs that are then integrated by subsequent layers <a class="yt-timestamp" data-t="00:45:42">[00:45:42]</a>.
*   **Transformers**: A different architecture for arranging neurons, particularly effective for processing sequences of "tokens" (like words in text) <a class="yt-timestamp" data-t="00:46:23">[00:46:23]</a>. Transformers are "equivariant to permutation," meaning the order of input items doesn't fundamentally change the output if permuted <a class="yt-timestamp" data-t="00:46:47">[00:46:47]</a>.

**Large Language Models (LLMs)**, such as those powering chatbots like ChatGPT, are a special case of self-supervised, auto-regressive Transformers <a class="yt-timestamp" data-t="00:36:46">[00:36:46]</a> <a class="yt-timestamp" data-t="00:59:17">[00:59:17]</a>. They are trained to predict the next word in a sequence given the preceding words, using vast public text datasets from the internet <a class="yt-timestamp" data-t="00:56:11">[00:56:11]</a> <a class="yt-timestamp" data-t="00:56:23">[00:56:23]</a>. With billions of parameters, LLMs can store immense knowledge and manipulate language impressively, capturing grammar and syntax across multiple languages <a class="yt-timestamp" data-t="00:56:52">[00:56:52]</a> <a class="yt-timestamp" data-t="00:57:31">[00:57:31]</a>.

## Current Applications and Limitations of LLMs
LLMs excel at tasks involving language manipulation and information retrieval <a class="yt-timestamp" data-t="00:57:31">[00:57:31]</a> <a class="yt-timestamp" data-t="00:57:51">[00:57:51]</a>. They can answer questions, write essays, and even pass professional exams <a class="yt-timestamp" data-t="00:56:46">[00:56:46]</a> <a class="yt-timestamp" data-t="01:03:30">[01:03:30]</a>.

However, LLMs have significant limitations:
*   **Discrete vs. Continuous Data**: LLMs work best for text because text is discrete (finite number of words/tokens) <a class="yt-timestamp" data-t="01:00:00">[01:00:00]</a>. They struggle with continuous, high-dimensional data like images and videos, where predicting every pixel is mathematically intractable <a class="yt-timestamp" data-t="01:00:00">[01:00:00]</a> <a class="yt-timestamp" data-t="01:05:52">[01:05:52]</a>.
*   **Lack of Physical World Understanding**: LLMs do not understand the physical world <a class="yt-timestamp" data-t="01:02:07">[01:02:07]</a>. They can make "stupid mistakes" that reveal a lack of real-world comprehension <a class="yt-timestamp" data-t="01:02:23">[01:02:23]</a>. This is why we have LLMs that can pass the bar exam but no fully autonomous self-driving cars or domestic robots that truly understand their environment <a class="yt-timestamp" data-t="01:02:37">[01:02:37]</a>.
*   **Limited Memory**: LLMs primarily have two types of memory: inherent knowledge encoded in their parameters during training (like a human's general understanding after reading a novel) and a limited working memory within their input prompt <a class="yt-timestamp" data-t="01:03:26">[01:03:26]</a> <a class="yt-timestamp" data-t="01:04:09">[01:04:09]</a>. They lack persistent, long-term memory similar to the human hippocampus <a class="yt-timestamp" data-t="01:03:09">[01:03:09]</a> <a class="yt-timestamp" data-t="01:04:26">[01:04:26]</a>.

## Future of AI
The next major challenge in [[history_and_future_of_ai | AI]] is to build systems that can learn how the world works by observing videos and images <a class="yt-timestamp" data-t="01:03:11">[01:03:11]</a>. This will require new architectures, different from the auto-regressive models used for LLMs <a class="yt-timestamp" data-t="01:05:04">[01:05:04]</a>.

### Next-Generation Architectures
*   **Joint Embedding Predictive Architectures (JEPAs)**: This promising approach, being developed by researchers, aims to predict abstract representations of future video frames rather than every pixel <a class="yt-timestamp" data-t="01:00:26">[01:00:26]</a> <a class="yt-timestamp" data-t="01:01:19">[01:01:19]</a> <a class="yt-timestamp" data-t="01:10:25">[01:10:25]</a>. By eliminating unpredictable details from the representation, systems can learn fundamental structures of the world by predicting what comes next in a video <a class="yt-timestamp" data-t="01:11:47">[01:11:47]</a> <a class="yt-timestamp" data-t="01:12:38">[01:12:38]</a>.
*   **World Models**: These models will be able to imagine the consequences of actions and plan complex sequences of actions hierarchically, making accurate short-term predictions while also enabling long-term, more abstract planning <a class="yt-timestamp" data-t="01:13:35">[01:13:35]</a> <a class="yt-timestamp" data-t="01:13:54">[01:13:54]</a>. This represents "system 2" reasoning (deliberate planning) as opposed to LLMs' "system 1" reactive capabilities <a class="yt-timestamp" data-t="01:08:07">[01:08:07]</a> <a class="yt-timestamp" data-t="01:16:14">[01:16:14]</a>.

Achieving human-level intelligence could potentially happen within a decade, though this is an optimistic timeline that assumes no unexpected obstacles <a class="yt-timestamp" data-t="01:14:50">[01:14:50]</a>. It will require new architectures like JEPAs, not just scaling up current LLMs <a class="yt-timestamp" data-t="01:15:57">[01:15:57]</a>.

## Applications of AI in Industries
The current and [[the_future_of_ai_and_its_impact_on_jobs | future of AI]] offer vast opportunities across various industries.

### Business-to-Business (B2B) Applications
*   **Legal and Accounting**: These sectors are ripe for disruption through AI, as highlighted by Bill Gates <a class="yt-timestamp" data-t="01:23:57">[01:23:57]</a> <a class="yt-timestamp" data-t="01:24:02">[01:24:02]</a>.
*   **Business Information**: Generating reports on competitive situations in specific market segments (e.g., fintech, finance) <a class="yt-timestamp" data-t="01:24:14">[01:24:14]</a>.
*   **Internal Information Systems**: LLM-powered systems can provide quick answers to employee questions about administrative or company information, eliminating the need to search through multiple internal websites <a class="yt-timestamp" data-t="01:24:26">[01:24:26]</a>. This area involves fine-tuning models for specific vertical applications <a class="yt-timestamp" data-t="01:24:51">[01:24:51]</a>.

### Consumer-Oriented Applications
*   **Education**: While not always highly lucrative without government contracts, education is a strong application area <a class="yt-timestamp" data-t="01:25:02">[01:25:02]</a>.
*   **Healthcare**: LLMs can provide medical assistance, helping users understand symptoms and decide whether to seek professional medical help, especially in areas with limited access to doctors <a class="yt-timestamp" data-t="01:25:10">[01:25:10]</a>.
*   **Rural Areas and Literacy**: AI assistants that can speak local languages and interact via speech can serve populations uncomfortable with literacy, opening up applications in agriculture and other sectors <a class="yt-timestamp" data-t="01:25:48">[01:25:48]</a>.
*   **Smart Devices**: AI will increasingly integrate with devices like smartphones and smart glasses, enabling real-time translation and other helpful features <a class="yt-timestamp" data-t="01:28:30">[01:28:30]</a> <a class="yt-timestamp" data-t="01:29:02">[01:29:02]</a>.

## Investing and Entrepreneurship in AI
For entrepreneurs and investors, the current landscape is dominated by open-source foundation models like Llama <a class="yt-timestamp" data-t="01:23:19">[01:23:19]</a>. The most viable business model involves taking an open-source model, fine-tuning it for a particular vertical application, and becoming an expert in that niche <a class="yt-timestamp" data-t="01:23:40">[01:23:40]</a>.

### Key Trends for Investment:
*   **Open Source Dominance**: The future of [[perspectives_on_ai_and_its_significance | AI]] platforms will likely be open source, similar to Linux's dominance in operating systems <a class="yt-timestamp" data-t="01:26:48">[01:26:48]</a>. Open source offers portability, flexibility, and security, and its performance is catching up to proprietary engines <a class="yt-timestamp" data-t="01:27:08">[01:27:08]</a>.
*   **Distributed Training**: Future AI models will be trained in a distributed fashion, not controlled by a single company <a class="yt-timestamp" data-t="01:27:32">[01:27:32]</a>. This requires local computing infrastructure, making investments in data centers and local AI training capabilities crucial <a class="yt-timestamp" data-t="01:19:17">[01:19:17]</a>.
*   **Cost of Inference**: The cost of AI inference is rapidly decreasing (by a factor of 100 in two years for LLMs) <a class="yt-timestamp" data-t="01:20:25">[01:20:25]</a>. This drives wider deployment and requires significant innovation in hardware beyond what is currently dominated by companies like Nvidia <a class="yt-timestamp" data-t="01:20:18">[01:20:18]</a>.
*   **Data Quality**: Collecting and filtering high-quality data will remain an expensive but crucial part of training models <a class="yt-timestamp" data-t="01:17:01">[01:17:01]</a>. There is a need for more encompassing datasets that cover all the world's languages, cultures, and value systems, which necessitates a collaborative, global effort <a class="yt-timestamp" data-t="01:18:00">[01:18:00]</a> <a class="yt-timestamp" data-t="01:18:33">[01:18:33]</a>.

For individuals considering a [[career_paths_in_ai | career in AI]], advanced studies like PhDs or Master's degrees are highly recommended <a class="yt-timestamp" data-t="01:21:25">[01:21:25]</a> <a class="yt-timestamp" data-t="01:21:56">[01:21:56]</a>. These train individuals to invent new things, understand existing possibilities and limitations, and gain legitimacy for hiring talented people in this complex field <a class="yt-timestamp" data-t="01:21:38">[01:21:38]</a> <a class="yt-timestamp" data-t="01:22:06">[01:22:06]</a>.

## Impact on Human Intelligence and Society
The [[impact_of_ai_on_future_work_and_lifestyle | impact of AI]] on human intelligence and society is transformative. AI will amplify human intelligence, helping to solve many of the world's problems, such as climate change, by enabling more rational decision-making and better mental models of how the world works <a class="yt-timestamp" data-t="01:00:01">[01:00:01]</a> <a class="yt-timestamp" data-t="00:09:56">[00:09:56]</a> <a class="yt-timestamp" data-t="00:08:10">[00:08:10]</a>.

### Reshaping Human Roles and Jobs
*   **Shift in Tasks**: Human intelligence will shift to different tasks <a class="yt-timestamp" data-t="01:29:25">[01:29:25]</a>. AI will handle many "doing" tasks, allowing humans to focus on "deciding what to do" and "figuring out what to do" <a class="yt-timestamp" data-t="01:29:40">[01:29:40]</a>. This means we will become "high-level managers" for AI systems <a class="yt-timestamp" data-t="01:30:06">[01:30:06]</a>.
*   **Enhanced Creativity and Productivity**: AI will lift the abstraction level at which humans can operate, making us more creative and productive <a class="yt-timestamp" data-t="01:31:17">[01:31:17]</a>. Just as calculators eliminated the need for complex mental arithmetic, AI will automate many current human tasks <a class="yt-timestamp" data-t="01:31:01">[01:31:01]</a>.
*   **Job Market Evolution**: Economists predict that humans will not run out of jobs because we will not run out of problems <a class="yt-timestamp" data-t="01:31:55">[01:31:55]</a>. Instead, AI will help us find better solutions to those problems <a class="yt-timestamp" data-t="01:32:04">[01:32:04]</a>.

### Defining Intelligence in the AI Age
Intelligence can be defined as a combination of <a class="yt-timestamp" data-t="01:32:11">[01:32:11]</a>:
*   A collection of existing skills and experience in solving problems and accomplishing tasks <a class="yt-timestamp" data-t="01:32:19">[01:32:19]</a>.
*   The ability to learn new skills very quickly with few trials <a class="yt-timestamp" data-t="01:32:57">[01:32:57]</a>.
*   The ability to solve new problems "zero-shot," without prior learning or similar experience, by using a mental model of the situation <a class="yt-timestamp" data-t="01:32:24">[01:32:24]</a> <a class="yt-timestamp" data-t="01:33:06">[01:33:06]</a>.

Overall, the journey of [[history_and_future_of_ai | AI]] is one of continuous discovery and evolution, pushing the boundaries of what machines can do and how they can augment human capabilities. The [[speculations_on_future_job_market_disruptions_due_to_ai | future of AI]] promises profound societal changes, transforming industries and redefining human roles.