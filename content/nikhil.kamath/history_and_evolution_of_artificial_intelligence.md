---
title: History and evolution of artificial intelligence
videoId: JAgHUDhaTU0
---

From: [[nikhil.kamath]] <br/> 

The field of artificial intelligence (AI) has a rich history, evolving from early theoretical concepts to complex modern systems. The journey involves understanding what intelligence truly means and how to replicate its mechanisms.

## Defining Intelligence and Early Approaches
The concept of intelligence itself is difficult to define, often compared to the story of the blind man and the elephant, where different aspects are perceived in isolation <a class="yt-timestamp" data-t="00:11:17">[00:11:17]</a>. Historically, `[[understanding_and_defining_artificial_intelligence|AI]]` has addressed different facets of intelligence <a class="yt-timestamp" data-t="00:11:36">[00:11:36]</a>.

### Intelligence as Reasoning and Problem Solving (1950s-1990s)
One early perspective, dominant from the 1950s until the 1990s, viewed `[[understanding_and_defining_artificial_intelligence|intelligence]]` primarily as the ability to reason and search for solutions to problems <a class="yt-timestamp" data-t="00:11:46">[00:11:46]</a>.

*   **Logical Reasoning and Search:** This involved formulating problems, like the "traveling salesman problem," in terms of searching for optimal solutions within a space of possibilities. This approach is rooted in mathematical optimization <a class="yt-timestamp" data-t="00:12:00">[00:12:00]</a>.
*   **Heuristic Programming:** Since exhaustively searching all possible solutions is impractical due to exponential growth (e.g., in chess), systems employed "heuristics" â€“ rules of thumb to guide the search and find good solutions without checking every single one <a class="yt-timestamp" data-t="00:17:50">[00:17:50]</a>.
*   **Rule-Based Systems/Expert Systems:** Prevalent in the 1980s, these systems used logical inference to deduce new facts from existing rules and facts <a class="yt-timestamp" data-t="00:19:23">[00:19:23]</a>. This category is sometimes jokingly referred to as "Good Old-Fashioned `[[understanding_and_defining_artificial_intelligence|AI]]`" (GOI) <a class="yt-timestamp" data-t="00:28:58">[00:28:58]</a>.

### Intelligence as Learning (1950s onwards)
A parallel branch of `[[understanding_and_defining_artificial_intelligence|AI]]`, also beginning in the 1950s, focused on reproducing the learning mechanisms observed in animal and human brains <a class="yt-timestamp" data-t="00:15:27">[00:15:27]</a>.

*   **Neural Networks:** Inspired by the brain's organization of simple connected elements (neurons), this approach recognized that intelligence emerges from these networks learning by modifying the strength of connections between neurons <a class="yt-timestamp" data-t="00:15:56">[00:15:56]</a>.
*   **The Perceptron (1957):** This was an early machine designed to learn <a class="yt-timestamp" data-t="00:21:00">[00:21:00]</a>. It could be trained to recognize simple shapes by adjusting "weights" (simulated resistors) associated with pixel inputs. If an output was incorrect, weights were incrementally adjusted until the desired output was achieved <a class="yt-timestamp" data-t="00:23:18">[00:23:18]</a>.
*   **Limitations and the "AI Winter":** Early neural networks like the perceptron faced limitations in the types of complex functions they could learn <a class="yt-timestamp" data-t="00:28:02">[00:28:02]</a>. Criticisms, notably from Marvin Minsky in the mid-1960s, contributed to a decline in interest in this area, though the techniques evolved into fields like "statistical pattern recognition" and "adaptive filter theory" <a class="yt-timestamp" data-t="00:25:52">[00:25:52]</a>, finding applications in areas like finance for pattern recognition in data <a class="yt-timestamp" data-t="00:26:26">[00:26:26]</a>.

## Modern AI: Machine Learning and Deep Learning
The hierarchy of modern `[[understanding_and_defining_artificial_intelligence|AI]]` can be broken down:
*   **`[[understanding_and_defining_artificial_intelligence|AI]]`:** The overarching field of investigation <a class="yt-timestamp" data-t="00:28:51">[00:28:51]</a>.
*   **Machine Learning (ML):** A specific approach to `[[understanding_and_defining_artificial_intelligence|AI]]` where machines are trained from data rather than being fully programmed <a class="yt-timestamp" data-t="00:29:11">[00:29:11]</a>.
*   **Deep Learning (DL):** A subcategory of `[[understanding_and_defining_artificial_intelligence|machine learning]]` that uses neural networks with multiple layers, forming the foundation of much of today's `[[understanding_and_defining_artificial_intelligence|AI]]` <a class="yt-timestamp" data-t="00:29:22">[00:29:22]</a>.

### The Rise of Deep Learning (1980s onwards)
A significant breakthrough in the 1980s was the ability to "stack multiple layers of neurons" in neural networks <a class="yt-timestamp" data-t="00:39:26">[00:39:26]</a>. Each neuron computes a weighted sum and passes it through a non-linear threshold function <a class="yt-timestamp" data-t="00:39:33">[00:39:33]</a>.

*   **Backpropagation:** The "backpropagation algorithm," conceptually existing before but rediscovered for `[[understanding_and_defining_artificial_intelligence|machine learning]]` in the 1980s, enabled efficient training of these multi-layer networks <a class="yt-timestamp" data-t="00:40:19">[00:40:19]</a>. This algorithm adjusts parameters (weights) by propagating signals backward from the output layer to correct errors <a class="yt-timestamp" data-t="00:40:35">[00:40:35]</a>.
*   **Initial Challenges:** Despite the breakthrough, widespread adoption was slow because training `[[understanding_and_defining_artificial_intelligence|neural networks]]` required vast amounts of data (unavailable before the internet) and fast computers (not yet common) <a class="yt-timestamp" data-t="00:41:36">[00:41:36]</a>.
*   **Convolutional Neural Networks (CNNs):** Developed in the late 1980s and early 1990s, CNNs were inspired by the architecture of the visual cortex <a class="yt-timestamp" data-t="00:45:54">[00:45:54]</a>. They connect neurons in specific ways to detect local motifs in data like images or audio signals, making them highly effective for pattern recognition in natural data due to their shift equivariance <a class="yt-timestamp" data-t="00:44:59">[00:44:59]</a>.

### Types of Machine Learning Paradigms
Within `[[understanding_and_defining_artificial_intelligence|machine learning]]`, several paradigms have emerged:

*   **Supervised Learning:** The system is given an input and told the correct output, adjusting its parameters to get closer to the desired outcome. This requires labeled data <a class="yt-timestamp" data-t="00:32:08">[00:32:08]</a>.
*   **Reinforcement Learning (RL):** Instead of a correct answer, the system receives a "good or bad" signal (a single number) about its performance. It's often inefficient, requiring many trials, making it ideal for games (e.g., chess, Go, poker) where systems can play millions of games against themselves <a class="yt-timestamp" data-t="00:32:23">[00:32:23]</a>. While there was significant interest around a dozen years ago, RL is not currently the primary focus for general `[[understanding_and_defining_artificial_intelligence|AI]]` progress <a class="yt-timestamp" data-t="00:43:03">[00:43:03]</a>.
*   **Self-Supervised Learning (SSL):** Highly prominent in recent years, SSL doesn't require explicit labels. It learns by corrupting data (e.g., masking words in text, transforming images) and training the system to recover the original, understanding the input's internal structure <a class="yt-timestamp" data-t="00:34:10">[00:34:10]</a>.

## The Age of Large Language Models (LLMs)
The success of `[[role_and_evolution_of_ai_assistants|chatbots]]` and natural language understanding systems is largely due to self-supervised learning, specifically in the form of Large Language Models (LLMs) <a class="yt-timestamp" data-t="00:33:31">[00:33:31]</a>.

*   **Language Models:** The concept originated with Claude Shannon in the 1940s, who explored predicting the next symbol in a sequence (e.g., the next letter or word) based on previous context <a class="yt-timestamp" data-t="00:50:24">[00:50:24]</a>. As context increases, models become more readable but require exponentially larger tables of probabilities, making them impractical <a class="yt-timestamp" data-t="00:53:31">[00:53:31]</a>.
*   **Neural Network Language Models:** In the late 1990s, researchers like Yann LeCun and Yoshua Bengio proposed using `[[understanding_and_defining_artificial_intelligence|neural networks]]` to predict the next word instead of large probability tables <a class="yt-timestamp" data-t="00:55:07">[00:55:07]</a>.
*   **Transformers:** These are a key architectural component in modern LLMs. Transformers are designed so that if inputs are permuted, the outputs are permuted similarly, treating inputs as a set where order doesn't strictly matter for internal processing <a class="yt-timestamp" data-t="00:46:23">[00:46:23]</a>.
*   **Autoregressive LLMs:** Current `[[role_and_evolution_of_ai_assistants|LLMs]]` are largely autoregressive Transformers trained to predict the next word in a sequence given the preceding words <a class="yt-timestamp" data-t="00:59:17">[00:59:17]</a>. When trained on vast amounts of internet text, these systems exhibit "emerging properties" like answering questions and manipulating language impressively, understanding grammar and syntax across multiple languages <a class="yt-timestamp" data-t="00:56:39">[00:56:39]</a>.

### Limitations of LLMs
Despite their impressive capabilities, LLMs have significant limitations, particularly because they excel in discrete worlds (like text) but struggle with continuous, high-dimensional data like video <a class="yt-timestamp" data-t="01:00:02">[01:00:02]</a>.

*   **Lack of Physical World Understanding:** LLMs "do not understand the physical world" and can make "very stupid mistakes" that reveal this fundamental gap <a class="yt-timestamp" data-t="01:02:07">[01:02:07]</a>. For example, while they can pass a bar exam, they lack the common-sense understanding of a house cat <a class="yt-timestamp" data-t="01:02:32">[01:02:32]</a>.
*   **Limited Memory:** LLMs have limited "persistent memory." Their "memory" primarily resides in their learned parameters (from training data) and the current "context" (the prompt and generated text) which acts as a very limited working memory <a class="yt-timestamp" data-t="01:03:09">[01:03:09]</a>. They lack a separate, more robust memory system akin to the human hippocampus <a class="yt-timestamp" data-t="01:04:26">[01:04:26]</a>.
*   **System 1 vs. System 2 `[[understanding_and_defining_artificial_intelligence|Intelligence]]`:** LLMs primarily operate as "System 1" intelligence â€“ reactive, subconscious processing. The goal for future `[[understanding_and_defining_artificial_intelligence|AI]]` is to develop "System 2" capabilities, which involve deliberate planning, reasoning, and thinking <a class="yt-timestamp" data-t="01:08:07">[01:08:07]</a>.

## The Future of `[[history_and_future_of_ai|Artificial Intelligence]]`
The next phase of `[[history_and_future_of_ai|AI]]` development focuses on overcoming the limitations of current LLMs and achieving a deeper understanding of the physical world.

*   **Learning from Video and Images:** The challenge is to build `[[understanding_and_defining_artificial_intelligence|AI systems]]` that can learn how the world works by watching videos and processing images <a class="yt-timestamp" data-t="01:03:11">[01:03:11]</a>.
*   **Joint Embedding Predictive Architecture (JEPA):** This proposed architecture addresses the problem of predicting continuous data. Instead of predicting every pixel in a video, JEPA predicts abstract representations of the future video segments, filtering out unpredictable details <a class="yt-timestamp" data-t="01:10:25">[01:10:25]</a>.
*   **World Models:** These future `[[understanding_and_defining_artificial_intelligence|AI systems]]` will develop "world models" that are action-conditioned, meaning they can imagine the consequences of actions and plan complex, hierarchical sequences of actions. This is crucial for tasks like robotics and autonomous driving <a class="yt-timestamp" data-t="01:13:23">[01:13:23]</a>.
*   **Timeline for Human-Level `[[history_and_future_of_ai|Intelligence]]`:** Achieving human-level `[[understanding_and_defining_artificial_intelligence|intelligence]]` might be possible within a decade, though this is an optimistic projection assuming no unexpected obstacles <a class="yt-timestamp" data-t="01:14:50">[01:14:50]</a>. It will require new architectures like JEPA, not just scaling up existing LLMs <a class="yt-timestamp" data-t="01:15:57">[01:15:57]</a>.
*   **Role of Open Source:** The future of `[[history_and_future_of_ai|AI]]` is likely to be dominated by open-source platforms, similar to Linux in operating systems. This democratizes the technology, makes it more portable, flexible, and secure, and enables startups to fine-tune models for specific applications <a class="yt-timestamp" data-t="01:26:48">[01:26:48]</a>.
*   **Decentralized Training and Data:** To overcome biases in current datasets and encompass all languages, cultures, and value systems, future `[[history_and_future_of_ai|AI]]` development will need collaborative, distributed training efforts around the world, with local computing infrastructure becoming crucial <a class="yt-timestamp" data-t="01:18:00">[01:18:00]</a>.
*   **Societal `[[the_impact_of_artificial_intelligence_on_society|Impact]]`:** `[[role_and_evolution_of_ai_assistants|AI assistants]]` will amplify human `[[understanding_and_defining_artificial_intelligence|intelligence]]`, shifting human tasks towards more abstract activities like deciding and figuring out what to do, rather than the execution of tasks themselves <a class="yt-timestamp" data-t="01:29:30">[01:29:30]</a>. This will enable greater creativity and productivity, lifting the abstraction level of human endeavors <a class="yt-timestamp" data-t="01:31:17">[01:31:17]</a>.

In summary, `[[understanding_and_defining_artificial_intelligence|intelligence]]` is seen as a combination of possessing a set of skills, the ability to learn new skills quickly with few trials, and the capacity to solve new problems without prior learning ("zero-shot" capability) <a class="yt-timestamp" data-t="01:32:19">[01:32:19]</a>. The pursuit of `[[history_and_future_of_ai|AI]]` is the quest to replicate and amplify these facets of intelligence.