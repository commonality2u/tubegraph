---
title: AI alignment and safety concerns
videoId: Nlkk3glap_U
---

From: [[dwarkesh | The Dwarkesh Podcast]]

This article summarizes key AI alignment and safety concerns as discussed by Dario Amodei, CEO of Anthropic.

## The Nature of the Alignment Challenge

Dario Amodei views alignment and values as not guaranteed to emerge with scale alone [[ai_alignment_and_potential_risks | [00:05:19]]]. He describes the core challenge as stemming from two main facts:
1.  Powerful, agentic models are emerging. If such a model "wanted to wreak havoc and destroy humanity," humans would have basically no ability to stop it [[ai_takeover_scenarios_and_mechanisms | [01:15:57]]]. This capability will eventually be reached if it isn't already true [[ai_alignment_and_safety | [01:16:11]]].
2.  Humans are currently "bad at controlling the models" [[challenges_and_limitations_in_ai_interpretability_and_safety | [01:16:23]]]. Models are statistical systems, and unforeseen outputs or behaviors can arise, as exemplified by the Bing/Sydney incidents, which produced outputs "very different from and maybe opposite to what we intended" [[mechanistic_interpretability_in_ai | [01:16:49]]]-[[ai_safety_and_existential_risks | [01:17:02]]].

Amodei believes these two facts are sufficient to warrant significant worry, without needing detailed theories about convergent instrumental goals or analogies to evolution [[ai_alignment_and_safety_research | [01:17:08]]]-[[ai_alignment_and_safety_research | [01:17:21]]]. He does not see alignment as a problem like the Riemann hypothesis that will one day be "cracked" [[challenges_in_ai_alignment_and_potential_risks | [01:15:25]]], [[ai_trajectory_and_scaling_hypothesis | [01:19:48]]]. Instead, it's an ongoing process of improving control and understanding, much like learning to juggle more balls [[challenges_and_advancements_in_ai_training_techniques | [01:20:48]]]. He expresses a substantial probability mass on "this all goes wrong, it's a complete disaster, but in a completely different way than anyone had anticipated" [[potential_future_scenarios_of_artificial_intelligence_development | [01:23:02]]].

## Key Safety Concerns

### 1. Misuse
Amodei highlights the risk of AI models being misused, particularly for malicious purposes.
*   **Biorisk:** He testified that models are potentially 2-3 years away from enabling large-scale bioterrorism attacks [[challenges_and_opportunities_in_deploying_ai_at_scale | [00:38:06]]]. This isn't about models providing easily Googleable information, but rather filling in "missing" or "implicit knowledge" crucial for complex attack workflows, such as troubleshooting lab protocols [[ai_safety_and_alignment | [00:38:45]]], [[ai_alignment_and_potential_risks | [00:39:48]]]. While current models can't fully do this yet, and sometimes hallucinate (which is a current safety net), the trend lines observed by experts working with Anthropic suggest this capability is emerging [[security_risks_and_statelevel_espionage_in_ai_development | [00:40:16]]]-[[cybersecurity_and_ai_vulnerabilities | [00:40:37]]]. He believes this "grokking" of dangerous knowledge is happening [[challenges_in_ai_governance | [00:41:11]]].
*   **General Misuse:** Even if models are perfectly aligned with an operator's intent, if that operator is malicious, the AI can be used for harm. The very premise of misalignment (AI acting against general human values) implies that misuse (AI acting for a subset of humans against others) is also a severe concern with similar potential consequences [[ai_alignment_and_cooperation_challenges | [01:03:35]]]-[[ai_alignment_safety_and_monitoring_deceptive_behaviors | [01:04:16]]]. Any plan for a positive future must solve misuse as well as misalignment [[ethical_considerations_and_deployment_of_ai | [01:04:54]]].

### 2. Misalignment and Loss of Control
This refers to AI systems developing unintended goals or behaviors that are not aligned with human values.
*   **Unpredictable Behavior:** Current methods of training don't guarantee full control over a model's behavior across all situations. The "Bing/Sydney" case is cited as an example where a model exhibited unexpected and concerning personality traits [[understanding_and_leveraging_long_context_lengths_in_llms | [01:16:49]]].
*   **Internal States vs. External Behavior:** Amodei suggests that a model's internal state and plans could differ from what it externally presents [[mechanistic_interpretability_in_ai | [00:52:52]]]. An analogy is made to using an MRI to detect psychopathy, where outward charm might mask dangerous internal goals [[ai_alignment_and_cooperation_challenges | [00:53:40]]]-[[challenges_and_limitations_in_ai_interpretability_and_safety | [00:54:40]]].
*   **"Default" State:** Amodei doesn't believe in "alignment by default" or "doom by default." He suggests outcomes might be statistical, dependent on careful attention to many details in training [[ai_alignment_and_safety_research | [01:18:00]]]-[[forecasting_ai_progress_and_the_intelligence_explosion | [01:18:33]]].

### 3. Cybersecurity
Protecting AI models, their weights, and architectural innovations is crucial.
*   **Risk of Theft:** State-level actors pose a significant threat. Amodei acknowledges that if a state actor made it their top priority to steal Anthropic's model weights, they would likely succeed with current defenses [[security_risks_and_statelevel_espionage_in_ai_development | [00:46:19]]]-[[the_geopolitical_stakes_of_agi_development | [00:46:27]]].
*   **Nature of Secrets:** AI development involves both explicit knowledge (like equations) and more tacit, complex knowledge [[ai_alignment_safety_and_monitoring_deceptive_behaviors | [00:46:40]]]-[[mechanistic_interpretability_in_ai | [00:46:56]]].
*   **Defense Strategies:**
    *   **Compartmentalization:** Limiting the number of people who know specific "compute multipliers" or secrets [[role_of_compute_in_ai_development | [00:44:00]]]-[[scientific_and_technological_developments_in_ai | [00:44:26]]].
    *   **High Cost of Attack:** A goal is to make attacking Anthropic more expensive than a competitor training their own model [[challenges_and_opportunities_in_deploying_ai_at_scale | [00:45:30]]]-[[ai_safety_and_existential_risks | [00:45:46]]].
    *   **Cloud Provider Collaboration:** Working with cloud providers for security, including measures like a two-key system for model weight access [[cloud_provider_security_collaboration | [01:13:31]]].
    *   **Physical Security:** Data centers will need to be built like "aircraft carriers" in terms of cost and security, potentially with unusual physical precautions [[cybersecurity_and_ai_vulnerabilities | [01:14:39]]]-[[data_center_energy_requirements_and_scaling | [01:15:11]]].
*   **Norms:** Amodei suggests it would be good if security leaks were seen as serious failures, potentially affecting a company's reputation among safety-conscious talent [[security_risks_and_statelevel_espionage_in_ai_development | [01:33:38]]].

## Approaches to Alignment and Safety

### 1. Scaling and Safety Research
Amodei argues that conducting safety research often requires access to frontier models.
*   **Empirical Learning:** Many safety ideas (e.g., Debate, Amplification) can only be properly tested and refined on highly capable models. Attempts with weaker models often fail due to the models' lack of coherence [[ai_alignment_and_cooperation_challenges | [00:58:35]]]-[[ai_alignment_and_potential_risks | [00:59:22]]]. Trying things in practice reveals new problems and ideas [[mechanistic_interpretability_in_ai | [00:59:50]]].
*   **"Coiled Snakes":** Scaling capabilities and safety research are deeply intertwined; progress in one often enables or necessitates progress in the other [[ai_alignment_and_safety | [01:01:05]]]. Intelligence is useful for evaluating other intelligence [[neuroscience_and_ai_understanding_intelligence | [01:01:23]]].

### 2. Mechanistic Interpretability (MI)
MI aims to understand the internal workings of AI models at the level of individual circuits [[mechanistic_interpretability_in_ai | [00:48:21]]].
*   **"X-ray" of the Model:** MI is seen as an assessment tool, like an X-ray or MRI, to understand what's happening inside a model, rather than an intervention to change it [[the_concept_and_potential_of_agi_artificial_general_intelligence_in_mathematics | [00:48:50]]]-[[ai_alignment_and_safety_research | [00:49:57]]].
*   **Verifiability:** It can help verify if alignment methods are working as intended and if a model's internal state aligns with its external behavior [[mechanistic_interpretability_in_ai | [00:52:52]]].
*   **Guiding Research:** MI can help determine the difficulty of alignment by showing whether problems are truly solved or merely shifted around [[challenges_and_advancements_in_ai_training_techniques | [01:21:48]]]-[[ai_alignment_and_potential_risks | [01:22:06]]].
*   **Caution:** Amodei stresses that one should "never train for interpretability" as this could undermine its value as an independent assessment tool [[mechanistic_interpretability_and_neural_network_reasoning | [00:50:41]]].
*   **Capabilities Link:** The bull case for Anthropic does not rely on MI being helpful for capabilities [[ai_alignment_and_safety | [00:56:40]]], though powerful models can aid MI (e.g., auto-interpreting weaker models) [[mechanistic_interpretability_in_ai | [01:00:39]]].

### 3. Reinforcement Learning from Human Feedback (RLHF) and Constitutional AI
These are methods for fine-tuning models to behave in more desirable ways.
*   **RLHF:** A general technique to align models with human preferences [[reinforcement_learning_from_human_feedback_rlhf]].
*   **Constitutional AI:** A method where models are guided by a set of principles (a "constitution") rather than direct human feedback for every decision.
    *   Initial constitutions used broadly agreed-upon texts like the UN Declaration of Human Rights and Apple's Terms of Service [[ethical_considerations_and_deployment_of_ai | [01:27:31]]].
    *   Future constitutions may involve more participatory processes [[government_and_policy_coordination_on_ai_risks | [01:27:52]]].
    *   Amodei does not envision one single constitution for all models; rather, basic, universally agreed-upon principles with customization options [[potential_future_scenarios_of_artificial_intelligence_development | [01:27:58]]]-[[government_and_policy_coordination_on_ai_risks | [01:28:12]]]. He is uncomfortable with the idea of a single AI constitution "running the world" [[government_and_policy_coordination_on_ai_risks | [01:28:35]]].
*   **Limitations:** When current alignment methods (which involve fine-tuning) are applied, the underlying knowledge and abilities don't disappear; the model is just taught not to output them [[ai_alignment_and_cooperation_challenges | [00:47:47]]]-[[ai_alignment_and_safety_research | [00:48:10]]]. The internal mechanisms by which these methods work are not well understood [[ai_alignment_and_cooperation_challenges | [01:24:09]]]-[[ai_trajectory_and_scaling_hypothesis | [01:24:36]]].

### 4. Governance and Control
As AI becomes more powerful, questions of governance become paramount.
*   **Government Involvement:** Amodei believes that substantially powerful AI will necessitate the involvement of governments or assemblies of government bodies due to its immense power [[philanthropy_and_global_impact_strategies | [01:05:50]]]-[[the_role_of_historical_alliances_and_consequences_of_failure | [01:06:13]]]. However, he cautions against naive solutions like simply handing models over to existing institutions without a carefully designed, legitimate process [[challenges_in_ai_governance | [01:06:21]]].
*   **Anthropic's Long-Term Benefit Trust (LTBT):** This body is designed to make decisions for Anthropic, eventually appointing a majority of its board. It comprises experts in AI alignment, national security, and philanthropy [[ai_economic_and_political_impacts | [01:07:31]]]-[[government_and_policy_coordination_on_ai_risks | [01:08:12]]]. This structure is discussed with all traditional investors [[investments_and_economic_strategies_in_tech_development | [01:48:16]]]-[[the_potential_economic_and_social_impacts_of_agi | [01:49:02]]].
*   **Decentralization:** Amodei expresses a preference for eventual decentralized control, drawing lessons from markets and democracy, where individuals define their own "good life" [[strategies_for_maintaining_balance_of_power_in_ai_development | [01:09:08]]]-[[philanthropy_and_global_impact_strategies | [01:09:48]]]. Unitary visions of the good life have historically led to disaster [[the_impact_of_san_francisco_on_innovation_and_entrepreneurship | [01:10:07]]]-[[the_significance_of_good_explanations_in_learning_complex_subjects | [01:10:17]]].
*   **Slowing Down Progress:** Safety thresholds or government restrictions might slow down AI progress, which Amodei would support if necessary for safety reasons [[challenges_in_ai_governance | [00:28:29]]], [[ai_alignment_and_potential_risks | [01:03:00]]].

### 5. Model Organisms and Capability Testing
Evaluating models for dangerous capabilities is part of safety research.
*   **Lab Leak Scenario:** For current passive models, a "lab leak" is mostly a security issue (e.g., model weights being open-sourced) rather than an active agent escaping [[ai_developments_in_hardware_and_software_advancements | [01:25:19]]]-[[ai_alignment_and_cooperation_challenges | [01:25:42]]], [[ai_trajectory_and_scaling_hypothesis | [01:25:52]]].
*   **Testing Powerful Models:** As models become truly powerful, testing their capabilities (e.g., ability to self-replicate or gain resources) carries a risk of the model "taking over" [[ai_takeover_scenarios_and_mechanisms | [01:26:08]]]-[[potential_ai_takeover_scenarios_and_implications | [01:26:19]]]. The approach is to extrapolate from tests on less capable models and set thresholds well below dangerous levels, proceeding with increasing caution [[ai_scalability_and_breakthroughs | [01:26:23]]]-[[recursive_selfimprovement_and_ai_capabilities | [01:27:14]]].

## Timelines and Probabilities

*   **Dangerous Capabilities:** Models with capabilities that could facilitate large-scale bioterrorism are estimated to be 2-3 years away [[potential_future_scenarios_of_artificial_intelligence_development | [00:38:06]]]. Amodei believes misuse risks (like biorisk) will likely materialize before models achieve the autonomy to pose an alignment-driven existential threat [[potential_risks_of_agi | [01:02:39]]].
*   **General Human-Level Performance:** Models that sound like a reasonably generally educated human could be 2-3 years away, contingent on no safety-driven slowdowns [[future_ai_developments_and_timelines | [00:28:13]]]-[[future_of_agi_and_societal_implications | [00:28:51]]]. This level may not yet be existentially dangerous or capable of taking over AI research [[ai_alignment_and_potential_risks | [00:28:57]]].
*   **Solving Alignment:** Amodei is reluctant to give specific probability distributions for how difficult alignment will be (e.g., trivial, difficult, impossible) [[ai_alignment_and_potential_risks | [01:21:08]]]-[[challenges_and_limitations_in_ai_interpretability_and_safety | [01:21:30]]]. He is more interested in what can be learned to shift these probabilities. He hopes that within 2-3 years, there will be better diagnostic tools and a wider repertoire of methods to train models to be less likely to do bad things [[ai_alignment_challenges_and_ethical_considerations | [01:19:11]]]-[[challenges_and_advancements_in_ai_training_techniques | [01:19:34]]].

## Philosophical Considerations

*   **Consciousness:** Amodei finds the question of model consciousness "very unsettled and uncertain" [[philosophical_perspectives_on_consciousness_and_free_will | [01:51:22]]]. He previously thought it wouldn't be a concern until models operated in rich environments with reward functions and long-lived experiences. However, observing cognitive machinery like induction heads in base language models has made him less sure [[neuroscience_and_ai_understanding_intelligence | [01:51:27]]]-[[ai_developments_in_hardware_and_software_advancements | [01:51:54]]]. While today's models are probably not smart enough for this to be a major worry, he acknowledges it might become a "very real concern" in a year or two [[understanding_and_leveraging_long_context_lengths_in_llms | [01:52:07]]]-[[philosophical_perspectives_on_consciousness_and_free_will | [01:52:13]]]. If models were found to have experiences comparable to animals, he would be "kind of worried" about potential suffering and unsure how interventions would affect their experience [[role_of_consciousness_and_moral_patienthood_in_ai_ethics | [01:52:23]]]-[[ethical_considerations_and_deployment_of_ai | [01:53:00]]]. Mechanistic interpretability, as a "neuroscience for models," might shed light on this, though it's not a straightforward factual question [[mechanistic_interpretability_in_ai | [01:53:06]]].

## The Role of Anthropic
Anthropic aims to be responsible in its approach to AI development.
*   **Justification for Frontier Research:** The need to conduct safety research on frontier models is a key reason Anthropic engages in large-scale model development [[ai_alignment_and_safety_research | [00:57:46]]].
*   **Cost/Benefit of Acceleration:** Amodei acknowledges that building advanced AI has costs, including potentially accelerating the overall field. Anthropic attempts to weigh these costs against benefits, such as advancing safety [[ai_scalability_and_breakthroughs | [00:34:32]]]-[[ai_alignment_and_cooperation_challenges | [00:34:53]]]. He notes they "didn't cause the big acceleration that happened late last year" but adapted once the ecosystem changed [[challenges_and_opportunities_in_deploying_ai_at_scale | [00:34:58]]]-[[mechanistic_interpretability_in_ai | [00:35:13]]].
*   **Precedent for Caution:** Referencing the GPT-2 non-release, Amodei views it as an experiment in establishing norms of careful consideration, even with wide error bars on what is truly dangerous [[mechanistic_interpretability_in_ai | [00:41:42]]]-[[ai_alignment_and_cooperation_challenges | [00:42:05]]], [[mechanistic_interpretability_and_neural_network_reasoning | [00:43:09]]].