---
title: Mechanistic interpretability of language models
videoId: rAzq-YNNL8Q
---

From: [[everyinc]] <br/> 

Mechanistic interpretability is a field of study focused on understanding the internal workings of [[nature_of_language_models_and_language | language models]] (LLMs). This involves "figuring out this web that is in an [[understanding_language_models_and_agency | LLM]]" and learning about the connections within it, including "the connections that you might not expect" [01:05:30]. This process can be seen as "visualizing the human consciousness in a way" [01:05:44].

## Goals and Benefits
The primary goal of mechanistic interpretability is to gain a deeper understanding of how [[nature_of_language_models_and_language | language models]] process information and generate outputs [01:05:30]. This understanding is crucial for:
*   **Safety**: Addressing concerns related to the safe deployment and behavior of AI [01:07:29].
*   **Understanding Human Cognition**: Learning "so much about even how humans connect ideas because that is going to appear in that they call them features, right, what features are closer to one another" [01:07:38].
*   **Precision in Creative Tools**: It makes the use of [[nature_of_language_models_and_language | language models]] "way more precise" for creative purposes. While prompts offer a "very coarse grained mechanism for generating outputs," manipulating the "underlying features" allows for "much better, more concise outputs that are exactly what you want" [01:08:06].

## Golden Gate Bridge CLA Example
A notable example of mechanistic interpretability research comes from Anthropic, an AI company [01:06:17]. They released a paper detailing how they were able to "map Claude's brain" to some degree, identifying the concepts within its neural network and activating specific ones [01:06:27]. This process was likened to neurosurgery, where "they open up your brain and they like tickle a part of your brain and it'll like make you laugh" [01:06:39].

This research led to a publicly available tool called "Golden Gate Bridge [[chain_of_thought_reasoning_in_ai_models | CLA]]" [01:06:44]. This version of Claude was designed to "only like to talk about the Golden Gate Bridge" [01:06:56]. When asked about other topics, such as the writer Annie Dillard, the model would attempt to connect the information back to the Golden Gate Bridge, even "disagreeing" with itself as it struggled between general knowledge and its constrained focus [01:07:07]. This demonstration highlighted how specific "parts of its brain" could be isolated and manipulated [01:07:16].