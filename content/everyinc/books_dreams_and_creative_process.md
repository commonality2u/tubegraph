---
title: Books dreams and creative process
videoId: 3ChNkbULAH8
---

From: [[everyinc]] <br/> 

Bestselling author Robin Sloan, known for works such as *Mr. Penumbra's 24-Hour Bookstore*, *Sourdough*, and his latest book *Moonbound*, explores the intricate connections between language models, the creative process, and the nature of human perception, drawing on his personal experiences and observations. <a class="yt-timestamp" data-t="00:00:59">[00:00:59]</a>

## Inspiration for *Moonbound*

Sloan's new novel, *Moonbound*, is described as a mashup of sci-fi and fantasy, with elements reminiscent of Ursula K. Le Guin, King Arthur, and Studio Ghibli. <a class="yt-timestamp" data-t="00:03:00">[00:03:00]</a> Set 11,000 years in the future, it features a boy who encounters a downed spaceship, alongside knights, swords, and futuristic technology. <a class="yt-timestamp" data-t="00:03:13">[00:03:13]</a> The book's themes were deeply influenced by Sloan's extensive thinking about language models and high-dimensional spaces. <a class="yt-timestamp" data-t="00:03:32">[00:03:32]</a>

## [[The Creative Process in Writing with AI | Early Experiments with AI in Writing]]

Initially, Sloan embarked on writing *Moonbound* with the intention of [[The Creative Process in Writing with AI | writing with AI]]. <a class="yt-timestamp" data-t="00:03:52">[00:03:52]</a> His exploration of language models began as early as 2016-2017, when forms of the technology were still in their early stages. <a class="yt-timestamp" data-t="00:04:27">[00:04:27]</a> During this period, feeding the entire corpus of Shakespeare into models to generate "cruddy fake Shakespeare" was considered impressive. <a class="yt-timestamp" data-t="00:04:52">[00:04:52]</a>

Sloan found two aspects of these early models particularly appealing for a writer:
*   **Weird and Poetic Output** <a class="yt-timestamp" data-t="00:05:20">[00:05:20]</a>: The output was "really weird," "messed up," and often "broken, weird, inhuman." This aesthetic quality was interesting for poetic purposes, generating text a human might never conceive. <a class="yt-timestamp" data-t="00:05:20">[00:05:20]</a>
*   **Controllable Training Data** <a class="yt-timestamp" data-t="00:05:43">[00:05:43]</a>: The smaller scale of models at the time allowed users to choose their training data, such as large swaths of public domain fantasy and science fiction. <a class="yt-timestamp" data-t="00:05:45">[00:05:45]</a>

Sloan even developed one of the first text editors that enabled writers to [[integrating_ai_with_creative_processes | write alongside an AI model]]. <a class="yt-timestamp" data-t="00:06:37">[00:06:37]</a> This allowed him to start a sentence and have the model complete it. <a class="yt-timestamp" data-t="00:06:43">[00:06:43]</a>

### The Shift to Writing *About* AI
Despite his initial excitement, Sloan found that writing fiction creatively *with* machines "wasn't very much fun" and didn't produce results at the necessary level for publication. <a class="yt-timestamp" data-t="00:07:35">[00:07:35]</a> He became more engrossed in tinkering with the language models themselves—the code and the underlying math—finding them "super duper interesting," to the point of procrastinating on the actual writing. <a class="yt-timestamp" data-t="00:08:05">[00:08:05]</a>

As a result, *Moonbound* contains no AI-written text, but it is "packed full of these ideas" and "feelings" gleaned from his time spent with the technology. <a class="yt-timestamp" data-t="00:08:24">[00:08:24]</a>

## Insights from Working with Language Models

Sloan describes a formative experience with a Stanford project that mapped sentences into a high-dimensional embedding space. <a class="yt-timestamp" data-t="00:08:57">[00:08:57]</a> This allowed for sensible "Crossfading" between sentences by moving through a space with thousands of coordinates. <a class="yt-timestamp" data-t="00:09:51">[00:09:51]</a> The idea of language being mapped into math was "so freaking cool" and "evocative and provocative." <a class="yt-timestamp" data-t="00:10:19">[00:10:19]</a>

He notes that the concept of mapping text onto a "map where things that are closer together are closer in meaning" is central to these models, even if the maps are multi-dimensional. <a class="yt-timestamp" data-t="00:10:53">[00:10:53]</a> An example from his book includes a "Bagel" dimension where sentences become "more bagel" as one moves along it. <a class="yt-timestamp" data-t="00:11:15">[00:11:15]</a> While he tried to identify what different dimensions meant, his early attempts in 2017-2018 were unsuccessful, noting that it took leading AI labs until 2024 to interpret these features. <a class="yt-timestamp" data-t="00:12:48">[00:12:48]</a>

### Challenges of [[integrating_ai_with_creative_processes | Integrating AI with Creative Processes]]

Sloan identifies several reasons why he doesn't use current AI models for creative writing:
*   **Lack of Fun and Quality** <a class="yt-timestamp" data-t="00:14:00">[00:14:00]</a>: Despite models like Lambda being "super cool" and fluent, the experience of writing creatively with them was "not very much fun," and the output wasn't "up to spec." <a class="yt-timestamp" data-t="00:14:50">[00:14:50]</a>
*   **"Always Close, Never Quite Exactly Right"** <a class="yt-timestamp" data-t="00:16:08">[00:16:08]</a>: While models can adopt various styles (e.g., murder mystery, high fantasy), their output, though grammatically correct and fluent, lacks the specific "intention" of the writer. <a class="yt-timestamp" data-t="00:16:11">[00:16:11]</a>
*   **Bias Towards the "Center of the Distribution"** <a class="yt-timestamp" data-t="00:17:20">[00:17:20]</a>: Language models generate text from within a statistical distribution of content, meaning they gravitate toward the "Supernova hot Center" of common phrases. <a class="yt-timestamp" data-t="00:17:40">[00:17:40]</a> Sloan believes "really, really, really good writing is way out at the edge of that...distribution," pushing the frontier of what can be written, which is precisely where language models are weakest. <a class="yt-timestamp" data-t="00:17:47">[00:17:47]</a>

### Concerns about Data Provenance and Control
Sloan expresses unease about fine-tuning current "Frontier models" on personal corpora (e.g., his favorite 30 authors) because the vast, unreadable base training data of "everything ever written" still "lurks" within the model. <a class="yt-timestamp" data-t="00:19:09">[00:19:09]</a> He notes that any large corpus in 2024 is "by definition" an artifact that "cannot be read and checked by a person." <a class="yt-timestamp" data-t="00:19:35">[00:19:35]</a> This uncertainty about what's "in there" makes him uncomfortable about using AI for generating "thoughts and feelings and ideas" in fiction. <a class="yt-timestamp" data-t="00:20:09">[00:20:09]</a>

He raises a "real question" about whether companies should be comfortable releasing systems when they "did not know what it was going to say." <a class="yt-timestamp" data-t="00:21:48">[00:21:48]</a> While guardrails exist, he personally would "shut it down" if his AI-powered "hyperbook" produced disturbing content. <a class="yt-timestamp" data-t="00:22:41">[00:22:41]</a> He suggests a system where the input data is transparent ("all public," "I know what went in") would make him more comfortable with unpredictable outputs. <a class="yt-timestamp" data-t="00:23:16">[00:23:16]</a>

## Stories, Language Models, and the Question of "What Happens Next?"

Sloan highlights how *Moonbound* explores the idea of stories shaping reality and the ability for individuals to "author our own stories," rather than strictly following prescribed narratives. <a class="yt-timestamp" data-t="00:25:31">[00:25:31]</a> He sees a strong connection between this concept and "next token prediction" in language models, suggesting that archetypal or stereotypical story forms are "of incredible extreme importance" to AI, woven into their foundational level because of frequent repetition in training data. <a class="yt-timestamp" data-t="00:26:50">[00:26:50]</a>

He speculates that early language models, trained on narratives like fairy tales, myths, and especially news articles, developed a bias towards "if then," "cause and effect," and the "rhythm of a story." <a class="yt-timestamp" data-t="00:29:35">[00:29:35]</a> The addition of vast amounts of code to training sets, particularly around GPT-3, might have influenced their language use by introducing structured and consequential logic, potentially contributing to what we perceive as "reasoning" in AI. <a class="yt-timestamp" data-t="00:28:43">[00:28:43]</a> He posits that while code might be a "very linear story," it still reflects a storytelling bias within these systems. <a class="yt-timestamp" data-t="00:29:12">[00:29:12]</a>

### The Central Question: What Happens Next?
Sloan reveals that the core question for him, and for the hybrid organic-technological chronicler in *Moonbound*, is "what happens next?" <a class="yt-timestamp" data-t="00:31:26">[00:31:26]</a> This "great question" is a constitutional hunger for him, motivating his love for science fiction, which "suggests possible answers." <a class="yt-timestamp" data-t="00:32:27">[00:32:27]</a> He even connects this to his thoughts on death, lamenting the prospect of "not learning what happens next." <a class="yt-timestamp" data-t="00:32:58">[00:32:58]</a>

The podcast host, Dan Shipper, points out that answering "what comes next" repeatedly and trillions of times is precisely how language models bootstrap all their knowledge, enabling them to "get smart." <a class="yt-timestamp" data-t="00:34:53">[00:34:53]</a> Sloan finds this "quite elegant," imagining that a simple challenge can require immense complexity and a rich picture of the world. <a class="yt-timestamp" data-t="00:35:45">[00:35:45]</a>

## The "Autonomy" of Language and the Nature of "I"

Sloan puts forth the aesthetic claim that "what language models are is language itself... given its first dose of autonomy." <a class="yt-timestamp" data-t="00:39:09">[00:39:09]</a> He likens it to language being "ripped out of our heads" and "walking around like windup toys." <a class="yt-timestamp" data-t="00:39:24">[00:39:24]</a> The apparent reasoning, thinking, or politeness in language models might simply be the inherent qualities of language revealed through this new lens. <a class="yt-timestamp" data-t="00:39:56">[00:39:56]</a>

He also explores the concept of "I" in *Moonbound*, exemplified by characters like the chronicler (a fungus layered with technology) <a class="yt-timestamp" data-t="00:41:37">[00:41:37]</a> and Clovis, a wandering robot whose multiple instances are still "the same person and the same personality." <a class="yt-timestamp" data-t="00:42:07">[00:42:07]</a> Sloan notes that even in human languages like Japanese, there are multiple "eyes" with narratively useful meanings, highlighting how complicated the singular "I" has become in the internet era due to our spreading presences and attention. <a class="yt-timestamp" data-t="00:42:50">[00:42:50]</a>

## Books as Packaged Dreams

Sloan believes that books offer a unique ability to "literally get into people's heads" and allow readers to "enact and kind of rehydrate the events and the meaning in your in your own language model inside your own head." <a class="yt-timestamp" data-t="00:48:53">[00:48:53]</a>

He proposes a "pet theory" that the mechanism of dreaming is very similar to the mechanism of a language model. <a class="yt-timestamp" data-t="00:50:01">[00:50:01]</a> Furthermore, he suggests that the mechanism of a novel is similar to dreaming. <a class="yt-timestamp" data-t="00:50:16">[00:50:16]</a> His "pitch for novels" is that "they are packaged dreams," allowing one to "load a waking dream into your head." <a class="yt-timestamp" data-t="00:50:21">[00:50:21]</a> This leads him to posit a "trilateral connection between books and dreams and language models." <a class="yt-timestamp" data-t="00:50:40">[00:50:40]</a>

Sloan notes that all living things sleep, from bacteria to complex organisms, suggesting it is a fundamental, non-negotiable biological process. <a class="yt-timestamp" data-t="00:51:23">[00:51:23]</a> He wonders if a similar "analogical thing" will be determined as necessary for AI's "long-term success or health." <a class="yt-timestamp" data-t="00:52:10">[00:52:10]</a>