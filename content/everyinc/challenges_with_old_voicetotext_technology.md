---
title: Challenges with Old VoicetoText Technology
videoId: IXm1m0OheLc
---

From: [[everyinc]] <br/> 

Before recent advancements, voice mode in AI models like ChatGPT faced significant [[challenges_in_realtime_text_analysis | challenges in realtime text analysis]] due to its underlying architecture <a class="yt-timestamp" data-t="00:00:16">[00:00:16]</a>. The "old ChatGPT" voice mode operated by translating spoken words into text, which was then fed into the GPT-4 model <a class="yt-timestamp" data-t="00:00:37">[00:00:37]</a>. This two-step process led to several limitations.

## Loss of Nuance and Context

A primary issue was the "loss in translation" <a class="yt-timestamp" data-t="00:00:46">[00:00:46]</a>. The translation software was described as "kind of dumb" <a class="yt-timestamp" data-t="00:00:50">[00:00:50]</a>, meaning it failed to capture the subtleties of human speech. As a result, GPT-4 only received a "compressed facsimile" of the spoken input <a class="yt-timestamp" data-t="00:00:54">[00:00:54]</a>.

This lack of nuance could lead to misinterpretations. For instance, when asked, "What does a baby computer say to its father?", the old system interpreted "dat" as "data" <a class="yt-timestamp" data-t="00:00:28">[00:00:28]</a>, missing the intended playful sound.

## Inability to Differentiate Speakers

Another significant limitation was the voice translation element's inability to tell the difference between multiple speakers <a class="yt-timestamp" data-t="00:02:13">[00:02:13]</a>. This "reduces the complexity of the kinds of conversations that it could handle" <a class="yt-timestamp" data-t="00:02:17">[00:02:17]</a>, making multi-party interactions or scenarios requiring speaker identification extremely difficult or impossible.

## Interruption and User Experience

The old voice mode also presented user experience challenges, particularly for activities like brainstorming or dictation. It would frequently interrupt the user <a class="yt-timestamp" data-t="00:03:13">[00:03:13]</a>, making it "sort of impossible" to ramble or pause naturally <a class="yt-timestamp" data-t="00:03:18">[00:03:18]</a>. Users felt a "pressure" to constantly keep talking to avoid confusion <a class="yt-timestamp" data-t="00:05:30">[00:05:30]</a>.

A workaround was introduced where users could hold their finger down on a button to prevent interruptions <a class="yt-timestamp" data-t="00:03:20">[00:03:20]</a>. However, this was considered "hacky" and impractical for extended use, such as while walking <a class="yt-timestamp" data-t="00:03:26">[00:03:26]</a>. The overall experience was characterized by "friction" when interacting with voice interfaces <a class="yt-timestamp" data-t="00:04:26">[00:04:26]</a>.