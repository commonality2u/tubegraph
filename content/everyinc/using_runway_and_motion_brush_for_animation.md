---
title: Using Runway and motion brush for animation
videoId: DCSGZIHXWeQ
---

From: [[everyinc]] <br/> 

[[runway_video_editing_tool | Runway]] is a versatile tool that allows users to perform text-to-image-to-video generation within a single application, making it efficient for [[creating_cinematic_experiences_with_AI_tools | creating cinematic experiences with AI tools]] in a time crunch <a class="yt-timestamp" data-t="00:49:09">[00:49:09]</a>. It is particularly useful for animating cinematic images due to its level of control <a class="yt-timestamp" data-t="00:49:20">[00:49:20]</a>.

## [[motion_brush_animation_technique | Motion Brush]] Feature

The [[motion_brush_animation_technique | Motion Brush]] is a key feature within [[runway_video_editing_tool | Runway]] that allows for precise control over animation <a class="yt-timestamp" data-t="00:49:58">[00:49:58]</a>. It was released as a new tool from [[runway_video_editing_tool | Runway]] <a class="yt-timestamp" data-t="00:00:23">[00:00:23]</a>.

### How to Use Motion Brush

1.  **Select Areas for Movement**: Users can select a brush and color over specific areas of an image that they want to move <a class="yt-timestamp" data-t="00:00:26">[00:00:26]</a>.
2.  **Define Movement Type**: After selecting an area, users can specify the kind of movement they want, such as ambient movement for subtle effects like hair rustling <a class="yt-timestamp" data-t="00:50:09">[00:50:09]</a>. Vertical brush can be used to push elements like smoke upwards <a class="yt-timestamp" data-t="00:51:30">[00:51:30]</a>.
3.  **Adjust Parameters**: Users can set the intensity of the ambient movement, for example, using values like "0.5" or "2" for ambient noise <a class="yt-timestamp" data-t="00:50:21">[00:50:21]</a>. For simple movements like flickering candles, a small amount of ambient noise suffices <a class="yt-timestamp" data-t="00:52:03">[00:52:03]</a>.
4.  **Camera Controls**: In addition to object movement, [[runway_video_editing_tool | Runway]] also offers camera movement controls such as zoom in or zoom out <a class="yt-timestamp" data-t="00:52:50">[00:52:50]</a>. Camera rolls can also be applied for dynamic shots <a class="yt-timestamp" data-t="00:59:19">[00:59:19]</a>.
5.  **Generate Clips**: After setting the desired movements, users can generate the animated clips. It's recommended to generate multiple clips (e.g., five at a time) to increase the chances of getting the desired output, as not every generation will be perfect <a class="yt-timestamp" data-t="00:52:17">[00:52:17]</a>.

### Practical Application

*   **Subtle Character Movement**: For animating characters, users can brush over the character and apply ambient movement for natural effects like head tilts or subtle leaning <a class="yt-timestamp" data-t="00:50:16">[00:50:16]</a>.
*   **Environmental Effects**: The [[motion_brush_animation_technique | Motion Brush]] is well-suited for images with elements like smoke or light rays, allowing users to make them move realistically <a class="yt-timestamp" data-t="00:52:43">[00:52:43]</a>.
*   **Adding Visual Effects**: Users can add text prompts to influence the animation, such as making a light source glow or change color (e.g., from yellow to red to create a threatening ambiance) <a class="yt-timestamp" data-t="00:58:47">[00:58:47]</a>.

### Post-Production Techniques

*   **Upscaling**: Generated clips, often initially around 4 seconds in length, can be upscaled to 4K resolution using tools like Topaz Labs for improved quality <a class="yt-timestamp" data-t="00:55:58">[00:55:58]</a>.
*   **Frame Rate Manipulation**: Clips generated at 24 frames per second (fps) can be converted to higher frame rates (e.g., 60 fps or 120 fps) to allow for [[extending_clips_with_AI_tools | extending clips with AI tools]] by slowing them down in post-production <a class="yt-timestamp" data-t="00:57:31">[00:57:31]</a>. This technique can help create longer shots and vary the pacing, akin to traditional [[editing_techniques_in_ai_films | editing techniques in AI films]] seen in films like *Man on Fire* or *300* <a class="yt-timestamp" data-t="01:02:19">[01:02:19]</a>.
*   **Layering and Masking**: Multiple generations of clips can be layered in video [[editing_techniques_in_ai_films | editing tools]] like Premiere Pro. Users can combine parts from different clips (e.g., smoke from one, character movement from another) using masking techniques. To maintain high-quality character likeness, the original face can be masked back into the animated clip <a class="yt-timestamp" data-t="00:56:09">[00:56:09]</a>.

### Limitations and Considerations

*   **Clip Length**: [[runway_video_editing_tool | Runway]] typically generates clips around 4 seconds in length <a class="yt-timestamp" data-t="00:17:04">[00:17:04]</a>. While some tools like Pika Labs can extend to 15 seconds, this may lead to a loss of fidelity <a class="yt-timestamp" data-t="00:17:06">[00:17:06]</a>.
*   **Consistency**: Maintaining consistent character appearances across different shots can be challenging <a class="yt-timestamp" data-t="00:16:40">[00:16:40]</a>.
*   **Iterative Process**: [[using_ai_tools_for_creative_projects | Using AI tools for creative projects]] like [[runway_video_editing_tool | Runway]] requires an iterative approach. It's not a "one-button press" solution to instantly generate a full movie <a class="yt-timestamp" data-t="00:59:50">[00:59:50]</a>. Most initial generations may be suboptimal, and significant effort is required to refine outputs <a class="yt-timestamp" data-t="01:00:00">[01:00:00]</a>. Users act as "model managers" to coax the best results from the AI <a class="yt-timestamp" data-t="01:00:11">[01:00:11]</a>. This process is likened to traditional filmmaking where multiple takes are done to get the best shot <a class="yt-timestamp" data-t="01:01:01">[01:01:01]</a>.