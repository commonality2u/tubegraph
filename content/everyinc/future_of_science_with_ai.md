---
title: Future of science with AI
videoId: tNRMWNfrkxc
---

From: [[everyinc]] <br/> 

The scientific landscape is undergoing a profound transformation with the advent of artificial intelligence (AI), particularly large language models (LLMs). This shift is driven by the exponentially increasing volume of scientific literature, which has become unmanageable for human researchers to keep up with <a class="yt-timestamp" data-t="02:10:00">[02:10:00]</a>. As a result, the traditional emphasis on human-understandable explanations in science may evolve, with a greater focus on prediction and the integration of AI tools for discovery <a class="yt-timestamp" data-t="02:51:00">[02:51:00]</a>.

## BrainGPT: AI for Neuroscience Research

Bradley Love, a professor of cognitive and decision sciences and experimental psychology at UCL, is a key builder of BrainGPT, a large language model designed to aid neuroscience research <a class="yt-timestamp" data-t="01:07:07">[01:07:07]</a>.

### Motivation and Core Function
The primary motivation for BrainGPT stems from the overwhelming volume of scientific literature, which is no longer "human readable" <a class="yt-timestamp" data-t="02:14:00">[02:14:00]</a>. Unlike many existing [[ai_and_its_impact_on_science | AI]] tools that focus on summarization, BrainGPT is "forward-looking," prioritizing the prediction of study outcomes before they happen <a class="yt-timestamp" data-t="02:52:00">[02:52:00]</a>.

To achieve this, BrainGPT was fine-tuned on 20 years of neuroscience scientific literature <a class="yt-timestamp" data-t="03:11:00">[03:11:00]</a>. The model can draw on thousands of findings from different literatures and at multiple levels, from psychology and behavior to cellular and molecular findings, including DNA <a class="yt-timestamp" data-t="04:05:00">[04:05:00]</a>. No single human can assimilate this vast amount of information <a class="yt-timestamp" data-t="04:21:00">[04:21:00]</a>.

### Performance and Methodology
BrainGPT has demonstrated superior predictive accuracy compared to human neuroscience experts, including professors <a class="yt-timestamp" data-t="03:22:00">[03:22:00]</a>. This was tested using a benchmark called "BrainBench," which involved taking recent abstracts from the *Journal of Neuroscience* and subtly altering the results <a class="yt-timestamp" data-t="07:28:00">[07:28:00]</a>. The model and human neuroscientists were then tasked with identifying the original abstract <a class="yt-timestamp" data-t="08:01:00">[08:01:00]</a>.

Initially, prompting models like GPT-4 to act as neuroscientists yielded only human-level accuracy (around 61%) <a class="yt-timestamp" data-t="13:34:00">[13:34:00]</a>. However, when direct access to the model's weights (e.g., Llama family models) was used to calculate "perplexity" – how surprising a text is given the model's training – the accuracy significantly improved to 83% <a class="yt-timestamp" data-t="14:08:00">[14:08:00]</a>. The perplexity difference also provides a reliable measure of the model's confidence, which is calibrated to its accuracy <a class="yt-timestamp" data-t="14:55:00">[14:55:00]</a>. This method allows for effective human-machine teaming, leading to better results than either working alone <a class="yt-timestamp" data-t="15:13:00">[15:13:00]</a>.

### Impact on Scientific Discovery
The ability of AI to predict study outcomes has several implications:
*   **More Informative Studies**: If BrainGPT predicts a study will work as expected with high certainty (e.g., 99.9%), there's little information gain in running it <a class="yt-timestamp" data-t="04:46:00">[04:46:00]</a>. Conversely, if the system predicts unexpected results, it could signal a systematic bias in existing literature, potentially leading to impactful discoveries <a class="yt-timestamp" data-t="05:00:00">[05:00:00]</a>.
*   **Addressing Reproducibility Crisis**: Many scientific findings do not replicate <a class="yt-timestamp" data-t="05:25:00">[05:25:00]</a>. [[ai_and_its_impact_on_science | AI]] systems like BrainGPT can help determine what findings are reliable and guide future research steps <a class="yt-timestamp" data-t="05:29:00">[05:29:00]</a>.
*   **Aggregating Noisy Data**: While individual scientific papers may be "flawed, noisy, incomplete," aggregating thousands of them through an [[ai_and_its_impact_on_science | AI]] model can reveal underlying signals, much like an ensemble in machine learning <a class="yt-timestamp" data-t="06:11:00">[06:11:00]</a>.
*   **Guiding Experiment Design**: The predictive capability can evolve into tools that suggest what experiments to run next or evaluate the plausibility of hypothetical outcomes <a class="yt-timestamp" data-t="05:00:00">[05:00:00]</a>.

## Shifting Paradigms in Science: Explanation vs. Prediction

Traditionally, science has been "obsessed with explanations" because they are powerful for making predictions and provide a beautiful understanding of how the world works, particularly in fields like physics and chemistry <a class="yt-timestamp" data-t="18:33:00">[18:33:00]</a>. However, in "soft sciences" like psychology, finding parsimonious causal explanations is extremely difficult, leading to replication issues <a class="yt-timestamp" data-t="19:01:00">[19:01:00]</a>.

### Unbundling Prediction from Explanation
[[potential_future_developments_and_implications_of_ai_technologies | AI]] and machine learning approaches allow scientists to "unbundle explanations from predictions," enabling good predictions without explicit theories <a class="yt-timestamp" data-t="19:47:00">[19:47:00]</a>. This transforms science problems into engineering problems; for instance, instead of explaining depression, one can predict it, which helps in intervention <a class="yt-timestamp" data-t="20:03:00">[20:03:00]</a>. The theory might exist within a neural network, which could potentially be more interpretable than the human brain <a class="yt-timestamp" data-t="20:21:00">[20:21:00]</a>.

### The "Dog Analogy" and Human Limitations
It's possible that theories for higher-level phenomena, like depression, are so vast they "don't fit in our rational brains" <a class="yt-timestamp" data-t="20:50:00">[20:50:00]</a>. This echoes the analogy of explaining quantum mechanics to a dog: the theory is well-specified, but it's beyond the dog's comprehension <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a>. It's plausible that humans, in relation to [[future_of_ai_and_agi_development | advanced AI]], might become the "dogs," unable to fully understand the intricate workings of complex systems <a class="yt-timestamp" data-t="00:07:00">[00:07:00]</a>.

### The Future of Explanation
In such a [[the_future_of_ai_and_human_interaction | predictive world]], explanations might take "different forms," becoming more general or akin to "stories" <a class="yt-timestamp" data-t="00:26:00">[00:26:00]</a>. Scientists might become "the new priests," interpreting and providing meaning to systems rather than offering crisp, simple equations <a class="yt-timestamp" data-t="00:37:00">[00:37:00]</a>. The complex, multi-level nature of biology, with its 10,000 interacting variables from DNA to behavior, makes a clean, human-understandable explanation increasingly difficult <a class="yt-timestamp" data-t="23:12:00">[23:12:00]</a>.

### A Non-Dystopian Predictive Future
Despite the potential loss of traditional explanation, this future need not be dystopian. If [[potential_future_developments_and_implications_of_ai_technologies | AI]] enables predictions that lead to improved health, economic growth, and solutions for climate change, most people would be content <a class="yt-timestamp" data-t="29:20:00">[29:20:00]</a>. The definition of a "satisfying" explanation might also change over time, much like how understanding the world differs today compared to 500 years ago <a class="yt-timestamp" data-t="31:09:00">[31:09:00]</a>.

## Challenges and Misinterpretations in Scientific Understanding

Human intuition often gravitates towards simple, coherent narratives, which are effective in politics or legal trials <a class="yt-timestamp" data-t="41:05:00">[41:05:00]</a>. However, this preference can be limiting in complex scientific domains, particularly biology and the real world, where systems involve many variables and interactions <a class="yt-timestamp" data-t="41:47:00">[41:47:00]</a>.

### The Inevitability and Superfluousness of Cell Types
Love's paper, "The Inevitability and Superfluousness of Cell Types and Spatial Cognition," illustrates this danger <a class="yt-timestamp" data-t="37:15:00">[37:15:00]</a>. Major neuroscience discoveries, often Nobel Prize-winning, have involved identifying "intuitive cell types," such as "place cells" that light up when a rodent is in a specific location, leading to the simple explanation of a "brain GPS system" <a class="yt-timestamp" data-t="37:34:00">[37:34:00]</a>.

However, Love's research found that similar intuitive cell types emerge even in random deep networks placed in virtual reality environments, without serving a navigation function <a class="yt-timestamp" data-t="38:26:00">[38:26:00]</a>. This suggests that scientists might be "forcing an intuitive understanding on the system," finding what they expect even if it's not how the system truly works <a class="yt-timestamp" data-t="38:45:00">[38:45:00]</a>. Often, 30 years of subsequent research then explains why these initial simple explanations are incomplete or distorted <a class="yt-timestamp" data-t="39:12:00">[39:12:00]</a>.

### Confirmatory Bias
This human preference for simple stories is further reinforced by "confirmatory work in science" <a class="yt-timestamp" data-t="44:18:00">[44:18:00]</a>. If scientists already believe a particular framework or set of relevant variables, they tend to collect data that confirms that mindset, potentially prolonging simple stories beyond their utility <a class="yt-timestamp" data-t="44:20:00">[44:20:00]</a>.

## Reimagining Science for the AI Era

To adapt to this evolving landscape, science needs a new orientation:
*   **Philosophical Reflection**: Scientists should be more philosophical about the nature of explanation and its limits <a class="yt-timestamp" data-t="45:22:00">[45:22:00]</a>.
*   **Computational Skills**: Increased emphasis on computational skills, large-scale simulations, and handling complex, naturalistic environments is crucial <a class="yt-timestamp" data-t="45:34:00">[45:34:00]</a>.
*   **Big Data Integration**: The field should embrace large data sets and different types of data (e.g., massive DNA databases, disease databases, brain recordings) <a class="yt-timestamp" data-t="26:48:00">[26:48:00]</a>.
*   **Interplay of Lab and Real-World Studies**: Laboratory studies should complement and interact with real-world, naturalistic, big-data approaches to avoid creating "fake science" that doesn't relate to real-world problems <a class="yt-timestamp" data-t="46:44:00">[46:44:00]</a>.

A historical example of this shift is the field of computer vision. The advent of AlexNet, a convolutional network developed by [[ai_and_its_impact_on_science | AI]] researchers rather than traditional vision scientists, revolutionized object recognition <a class="yt-timestamp" data-t="47:49:00">[47:49:00]</a>. By training on a million natural images, this model achieved a "bigger advance than 100 years of vision science" <a class="yt-timestamp" data-t="48:47:00">[48:47:00]</a>, demonstrating the power of large-scale, real-world training <a class="yt-timestamp" data-t="48:38:00">[48:38:00]</a>. This success was "humbling to scientists" and led many to adopt similar approaches <a class="yt-timestamp" data-t="49:02:00">[49:02:00]</a>.

## Philosophical Considerations

To foster deeper reflection on the methods of science, Love recommends reading Thomas Nagel's "The View from Nowhere" <a class="yt-timestamp" data-t="50:01:00">[50:01:00]</a>. The book highlights that science offers a "disembodied perspective" – a view from nowhere – which is its strength (e.g., launching a probe to Mars) but also its limitation <a class="yt-timestamp" data-t="50:48:00">[50:48:00]</a>.

### Science and Human Experience
While experimental psychology strives to be scientific, it often encounters the inherent tension between objective operationalization and subjective human experience <a class="yt-timestamp" data-t="51:46:00">[51:46:00]</a>. Defining concepts like "depression" requires quantifiable criteria <a class="yt-timestamp" data-t="52:31:00">[52:31:00]</a>. However, science, from the "view from nowhere," has limited ability to offer deep insights into "subjective experiences" or "qualia" (e.g., why blue makes one feel a certain way) <a class="yt-timestamp" data-t="53:09:00">[53:09:00]</a>.

This limitation is why other forms of human knowledge, such as literature, religion, and music, remain valuable <a class="yt-timestamp" data-t="53:55:00">[53:55:00]</a>. Literature, in particular, offers an "undervalued" exploration of psychology <a class="yt-timestamp" data-t="54:09:00">[54:09:00]</a>. Even practices like psychoanalysis, despite not being scientific in the conventional sense, can yield good outcomes because practitioners build valuable intuition within their frameworks, similar to how cultural knowledge (e.g., Italian cooking) develops over centuries of tweaking <a class="yt-timestamp" data-t="55:09:00">[55:09:00]</a>.

In essence, the [[impact_of_ai_on_human_intellect | impact of AI on human intellect]] and scientific practice may lead to a realization that our natural inclination towards simple causal stories, while useful in everyday life, may not be adequate for understanding the full complexity of high-dimensional phenomena in the world <a class="yt-timestamp" data-t="43:07:00">[43:07:00]</a>.