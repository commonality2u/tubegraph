---
title: Nature of language models and language
videoId: 3ChNkbULAH8
---

From: [[everyinc]] <br/> 

Robin Sloan, author of *Moonbound*, describes [[understanding_language_models_and_agency | language models]] as "language itself given its first dose of autonomy" <a class="yt-timestamp" data-t="00:00:14">[00:00:14]</a>. He views them as "amazing artifacts" that are "so weird and so interesting" in what is woven and bound into them <a class="yt-timestamp" data-t="00:00:34">[00:00:34]</a>.

## Early Experiments and Initial Impressions
Sloan began tinkering with [[large_language_models_for_scientific_prediction | language models]] in 2016-2017 <a class="yt-timestamp" data-t="00:04:27">[00:04:27]</a>, a period characterized by "incredible ferment" in the field <a class="yt-timestamp" data-t="00:04:43">[00:04:43]</a>. Early examples included generating "cruddy fake Shakespeare" from the entire Shakespeare corpus, which was impressive at the time <a class="yt-timestamp" data-t="00:04:51">[00:04:51]</a>.

For a writer, two aspects of these early models were particularly appealing:
1.  **Weird Output**: The output was "really weird" and "pretty messed up," not fluent like modern models <a class="yt-timestamp" data-t="00:05:20">[00:05:20]</a>. This "broken weird inhuman way" of writing was aesthetically and poetically interesting because it was something "a mere human would never imagine to write" <a class="yt-timestamp" data-t="00:05:32">[00:05:32]</a>.
2.  **Trainable Scale**: The smaller scale of models back then allowed individual users to consider "what will I train this on?" <a class="yt-timestamp" data-t="00:05:49">[00:05:49]</a>. Sloan experimented by training models on large amounts of classic public domain fantasy and science fiction, yielding "evocative and surprising" results <a class="yt-timestamp" data-t="00:06:08">[00:06:08]</a>.

## Language Models as Mathematical Spaces
A particular fascination for Sloan stemmed from a Stanford project that mapped sentences into an embedding space <a class="yt-timestamp" data-t="00:08:57">[00:08:57]</a>. This allowed for sensible movement and "Crossfade[ing]" between sentences in a high-dimensional space with about a thousand coordinates <a class="yt-timestamp" data-t="00:09:36">[00:09:36]</a>.

> "The idea that [[human_identity_and_language | language]] could get mapped into math in this way was just so freaking cool and I I just found it so again almost in a poetical sense like a ative and and provocative and I just I just wanted to keep thinking about that." <a class="yt-timestamp" data-t="00:10:19">[00:10:19]</a>

This concept means that text is mapped onto a map where closer proximity signifies closer meaning, but in many dimensions <a class="yt-timestamp" data-t="00:10:45">[00:10:45]</a>. Each dimension can represent a specific kind of meaning, such as "Bagel" or "Golden Gate Bridge" features <a class="yt-timestamp" data-t="00:11:14">[00:11:14]</a>.

## Writing with Language Models
Sloan initially intended to write his book *Moonbound* with AI assistance, even developing one of the first text editors where one could write alongside an AI model <a class="yt-timestamp" data-t="00:03:52">[00:03:52]</a>. However, he eventually found that "the experience of writing fiction writing creatively with the machine was for me actually not very much fun" <a class="yt-timestamp" data-t="00:07:35">[00:07:35]</a> and the results weren't "up to spec" <a class="yt-timestamp" data-t="00:07:45">[00:07:45]</a>.

He attributes this to the nature of [[large_language_models_for_scientific_prediction | language models]] to generate text from "inside a distribution" or "probability cloud" <a class="yt-timestamp" data-t="00:17:23">[00:17:23]</a>. Truly good writing, he argues, exists "way out at the edge of that...distribution" and even "pushes a bit beyond it" <a class="yt-timestamp" data-t="00:17:47">[00:17:47]</a>, which is precisely where [[large_language_models_for_scientific_prediction | language models]] are weakest <a class="yt-timestamp" data-t="00:18:03">[00:18:03]</a>.

## Concerns about Training Data and Interpretability
Sloan has reservations about fine-tuning current Frontier models due to the vast, incomprehensible amount of "other content" lurking in their training data <a class="yt-timestamp" data-t="00:19:09">[00:19:09]</a>. He states that any text corpus in 2024 is an artifact "that cannot be read and checked by a person" due to its computational scale <a class="yt-timestamp" data-t="00:19:32">[00:19:32]</a>. This lack of transparency about what is "in there" makes him uneasy, especially for creative output <a class="yt-timestamp" data-t="00:20:01">[00:20:01]</a>.

He expresses surprise that large companies are comfortable releasing systems "where on some fundamental level I did not know what it was going to say" <a class="yt-timestamp" data-t="00:21:46">[00:21:46]</a>. This suggests a need for "nutrition facts" or "ethically raised organic AI" <a class="yt-timestamp" data-t="00:24:26">[00:24:26]</a>.

## Language Models and Stories
Sloan observes that early [[large_language_models_for_scientific_prediction | language models]] were trained on content with inherent narrative, such as Project Gutenberg texts and news stories <a class="yt-timestamp" data-t="00:29:28">[00:29:28]</a>. News articles, for instance, exhibit a "real appetite for cause and effect" <a class="yt-timestamp" data-t="00:28:05">[00:28:05]</a>. This created a "bias" towards "if then cause and effect and the rhythm of a story" in those models <a class="yt-timestamp" data-t="00:29:35">[00:29:35]</a>.

The addition of vast amounts of code to training sets, possibly around the time of GPT-3, is speculated to have significantly impacted language model behavior <a class="yt-timestamp" data-t="00:28:43">[00:28:43]</a>. Code is "so structured and so consequential" <a class="yt-timestamp" data-t="00:28:54">[00:28:54]</a>, leading to a possible connection between "reasoning" in [[large_language_models_for_scientific_prediction | language models]] and learned patterns from code <a class="yt-timestamp" data-t="00:29:01">[00:29:01]</a>.

## The Question of "What Happens Next"
The central question for the human race, as depicted in *Moonbound*, is "what happens next" <a class="yt-timestamp" data-t="00:30:58">[00:30:58]</a>. This question, "carved into my heart," drives Sloan's own curiosity and appreciation for science fiction <a class="yt-timestamp" data-t="00:31:49">[00:31:49]</a>.

In the context of [[large_language_models_for_scientific_prediction | language models]], "answering that question [what comes next] is the thing [that] forces these models to bootstrap all the knowledge that they have" <a class="yt-timestamp" data-t="00:34:56">[00:34:56]</a>. This simple challenge leads to immense complexity and a rich, albeit incomplete, picture of the world within the models <a class="yt-timestamp" data-t="00:35:47">[00:35:47]</a>.

## Philosophical Implications of Language Models
[[philosophical_perspectives_on_truth_and_language_models | Language models]] raise fundamental questions about the nature of language and existence. Sloan finds the claim that [[understanding_language_models_and_agency | language models are]] "language itself...given its first dose of autonomy" compelling <a class="yt-timestamp" data-t="00:39:09">[00:39:09]</a>. They appear to reason, think, or politely answer, suggesting that these qualities were "always inherent there in language and we just never had this particular way of seeing it before" <a class="yt-timestamp" data-t="00:39:56">[00:39:56]</a>.

The definition of "I" (self) is also explored, highlighting variations across languages and the complexities introduced by the internet era, where a person's presence and attention are "spreading out" <a class="yt-timestamp" data-t="00:44:17">[00:44:17]</a>. [[human_identity_and_language | This multifaceted self]] (e.g., internal family systems, the gut microbiome) mirrors how [[large_language_models_for_scientific_prediction | language models]] encounter words like "Robin" in "millions of different ways" <a class="yt-timestamp" data-t="00:46:27">[00:46:27]</a>, recognizing specific versions of meaning based on context <a class="yt-timestamp" data-t="00:46:09">[00:46:09]</a>.

## Dreams, Novels, and Language Models
Sloan has a "pet theory" that the mechanism of dreaming is very similar to the mechanism of a [[large_language_models_for_scientific_prediction | language model]] completing a sequence <a class="yt-timestamp" data-t="00:49:51">[00:49:51]</a>. He posits that novels are "packaged dreams," offering a way to "load a waking dream into your head" <a class="yt-timestamp" data-t="00:50:21">[00:50:21]</a>. This suggests a "trilateral connection" between books, dreams, and [[large_language_models_for_scientific_prediction | language models]] <a class="yt-timestamp" data-t="00:50:38">[00:50:38]</a>.

The fundamental biological necessity of sleep across all living things <a class="yt-timestamp" data-t="00:51:23">[00:51:23]</a>, including bacteria <a class="yt-timestamp" data-t="00:51:31">[00:51:31]</a>, leads Sloan to wonder if there will be an analogous "non-negotiable" phase that AI models "ought to do" for their own long-term success or health <a class="yt-timestamp" data-t="00:52:08">[00:52:08]</a>.