---
title: Editing techniques in AI films
videoId: VWj8Bk2Zj6Y
---

From: [[everyinc]] <br/> 

The landscape of [[ai_in_filmmaking | AI]] tools is continually evolving, with new methods emerging to extend and refine [[ai_in_film_making | AI-generated footage]] <a class="yt-timestamp" data-t="00:00:04">[00:00:04]</a>. Current limitations for generative video tools include clip length, such as Runway's 4-second limit, though Pika Labs can extend up to 15 seconds <a class="yt-timestamp" data-t="00:00:20">[00:00:20]</a>. However, extending clips directly can lead to a loss of fidelity <a class="yt-timestamp" data-t="00:00:26">[00:00:26]</a>.

## Extending Clip Length and Fidelity

To overcome these limitations and make [[ai_in_film_production | AI-generated clips]] more usable for storytelling, specific post-production techniques are employed:

*   **Upscaling and Frame Rate Manipulation**
    *   Tools like Topaz Labs can be used to process [[ai_in_film_making | AI-generated clips]] <a class="yt-timestamp" data-t="00:00:32">[00:00:32]</a>.
    *   Clips can be exported at higher resolutions, such as 4K or 8K <a class="yt-timestamp" data-t="00:00:37">[00:00:37]</a>.
    *   Frame rates can be changed from 24 frames per second to 60 or even 120 frames per second <a class="yt-timestamp" data-t="00:00:43">[00:00:43]</a>. This allows for slowing down the footage in post-production, effectively extending the clip's duration <a class="yt-timestamp" data-t="00:01:06">[00:01:06]</a>.
    *   An example of this technique was in the sci-fi film "Borrowing Time," particularly for time travel scenes where lights whipped around, utilizing high frame rate generation that was later slowed down and sped up in post <a class="yt-timestamp" data-t="00:00:51">[00:00:51]</a>.

## Storytelling Through Editing Cadence

Editing plays a crucial role in [[ai_in_film_making | AI film making]] <a class="yt-timestamp" data-t="00:01:34">[00:01:34]</a>. By extending clips, filmmakers can avoid relying solely on short, 3-second segments <a class="yt-timestamp" data-t="00:01:40">[00:01:40]</a>. This allows for a mix of longer clips (e.g., 8 seconds after slowing down) and shorter ones (e.g., 1 second), creating a dynamic cadence that aids in storytelling <a class="yt-timestamp" data-t="00:01:46">[00:01:46]</a>. This approach makes the [[creating_cinematic_experiences_with_ai_tools | filmmaking]] more akin to traditional television and cinema, reminiscent of directors like Tony Scott (e.g., *Man on Fire*) with his quick cuts, or Zack Snyder (e.g., *300*, *Dawn of the Dead*) <a class="yt-timestamp" data-t="00:01:17">[00:01:17]</a>.

## Voice Generation in Post-Production

Beyond visuals, [[ai_in_film_making | AI]] also impacts audio post-production:

*   **AI Voice Tools**
    *   Tools like Eleven Labs offer [[voice_generation_in_films_using_ai | AI voice generation]] <a class="yt-timestamp" data-t="00:01:53">[00:01:53]</a>.
    *   While some [[voice_generation_in_films_using_ai | AI voices]] can sound distinctly artificial, Eleven Labs' "speech to speech" feature allows users to record their own performance and have the [[voice_generation_in_films_using_ai | AI]] generate a voice based on that acting <a class="yt-timestamp" data-t="00:02:00">[00:02:00]</a>.
    *   In "Borrowing Time," this feature was used for character voices, such as the white judge and the mother, by speaking the lines naturally into the system <a class="yt-timestamp" data-t="00:02:11">[00:02:11]</a>.
    *   For voiceovers, a natural speaking voice with its inherent pauses and cadences can be preferred for a more natural feel <a class="yt-timestamp" data-t="00:02:24">[00:02:24]</a>.