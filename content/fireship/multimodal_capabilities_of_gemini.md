---
title: Multimodal capabilities of Gemini
videoId: q5qAVmXSecQ
---

From: [[fireship]] <br/> 

[[google_gemini_15_ai_model | Google's Gemini]] model, unleashed on December 7, 2023, was designed to be a significant advancement in artificial intelligence, following the "AI war of 2023" <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a>. First introduced at Google I/O, [[google_gemini_15_ai_model | Gemini]] is described as a multimodal large language model that will replace previous models like Lambda and Palm 2 <a class="yt-timestamp" data-t="00:00:35">[00:00:35]</a>. Its core capability lies in its multimodal nature, meaning it is trained on diverse data types including text, sound, images, and video, similar to [[comparison_between_gemini_and_gpt_4 | GPT 4]] <a class="yt-timestamp" data-t="00:00:39">[00:00:39]</a>.

## Real-time Video Understanding
[[google_gemini_15_ai_model | Gemini]] demonstrates impressive real-time video understanding, capable of recognizing activities within a video feed and responding instantly <a class="yt-timestamp" data-t="00:00:47">[00:00:47]</a>.
Examples include:
*   Identifying a drawn duck as it's being sketched <a class="yt-timestamp" data-t="00:00:51">[00:00:51]</a>.
*   Tracking an object in an ongoing video feed, such as the "find the ball under the cup" game, even after the cups are scrambled <a class="yt-timestamp" data-t="00:00:58">[00:00:58]</a>.
*   Performing complex tasks like "connect the dots" <a class="yt-timestamp" data-t="00:01:07">[00:01:07]</a>.
These capabilities are supported across multiple languages <a class="yt-timestamp" data-t="00:00:54">[00:00:54]</a>.

## Multimodal Outputs
Beyond understanding various input types, [[google_gemini_15_ai_model | Gemini]] can also generate multimodal outputs:
*   **Image Generation**: It can generate images on the fly, similar to Stable Diffusion <a class="yt-timestamp" data-t="00:01:11">[00:01:11]</a>.
*   **Music Generation**: It generates music from a prompt, supporting not only text-to-audio but also image-to-audio conversions <a class="yt-timestamp" data-t="00:01:15">[00:01:15]</a>.
This makes [[google_gemini_15_ai_model | Gemini]] an "anything to anything" model <a class="yt-timestamp" data-t="00:01:23">[00:01:23]</a>.

## Advanced Logic and Spatial Reasoning
[[google_gemini_15_ai_model | Gemini]] exhibits strong capabilities in logic and spatial reasoning, using visual information to make deductions:
*   It can analyze two pictures to determine which car would go faster based on its aerodynamics <a class="yt-timestamp" data-t="00:01:26">[00:01:26]</a>.
*   In the future, it is envisioned that a civil engineer could take a picture of land, and the AI could instantly generate blueprints for a bridge <a class="yt-timestamp" data-t="00:01:33">[00:01:33]</a>.

## Training Infrastructure
The development of [[google_gemini_15_ai_model | Gemini]] utilized Google's newly unveiled version 5 Tensor Processing Units (TPUs) <a class="yt-timestamp" data-t="00:03:40">[00:03:40]</a>. These TPUs are deployed in "super PODS" of 4,096 chips, with each super pod featuring a dedicated optical switch to facilitate rapid data transfer and parallel training <a class="yt-timestamp" data-t="00:03:43">[00:03:43]</a>. The system can dynamically reconfigure into 3D torus topologies to reduce latency between chips <a class="yt-timestamp" data-t="00:03:54">[00:03:54]</a>. The scale of [[google_gemini_15_ai_model | Gemini]] Ultra required communication between multiple data centers <a class="yt-timestamp" data-t="00:04:01">[00:04:01]</a>.

The training dataset for [[google_gemini_15_ai_model | Gemini]] included vast amounts of data from the internet, such as web pages and YouTube videos, as well as scientific papers and books <a class="yt-timestamp" data-t="00:04:07">[00:04:07]</a>. This data was filtered for quality and then fine-tuned using reinforcement learning with human feedback to enhance quality and minimize hallucinations <a class="yt-timestamp" data-t="00:04:14">[00:04:14]</a>.

## Availability
[[google_gemini_15_ai_model | Gemini]] is available in three sizes: Nano, Pro, and Ultra <a class="yt-timestamp" data-t="00:02:07">[00:02:07]</a>. The Nano and Pro models were made available on Google Cloud on December 13th <a class="yt-timestamp" data-t="00:04:24">[00:04:24]</a>. The smallest Nano version is designed for embedding on devices like Android phones, while the Pro version is a general-purpose model <a class="yt-timestamp" data-t="00:02:11">[00:02:11]</a>. The most advanced version, [[google_gemini_15_ai_model | Gemini]] Ultra, is expected to be available in 2024 after additional safety tests are completed <a class="yt-timestamp" data-t="00:04:27">[00:04:27]</a>. In the United States, users can currently access [[google_gemini_15_ai_model | Gemini]] Pro via the Bard chatbot <a class="yt-timestamp" data-t="00:02:22">[00:02:22]</a>.