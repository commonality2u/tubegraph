---
title: Comparison of AI models costs and effectiveness
videoId: FW2XOIxaNqg
---

From: [[fireship]] <br/> 

The artificial intelligence landscape is experiencing a "sigmoid of sorrow" rather than a technological singularity, marked by the underwhelming release of GPT 4.5 <a class="yt-timestamp" data-t="00:28:44">[00:28:44]</a>. This has led to a re-evaluation of current AI models, their costs, and their actual effectiveness.

## GPT 4.5: High Cost, Underwhelming Performance

Released by OpenAI, GPT 4.5 is touted as the most expensive AI model ever produced <a class="yt-timestamp" data-t="00:08:18">[00:08:18]</a>. Despite its cost, it has failed to crush any benchmarks, win awards, or offer novel capabilities <a class="yt-timestamp" data-t="00:10:48">[00:10:48]</a>.

### Cost Analysis
*   **Input Tokens:** GPT 4.5 costs $75 per million input tokens <a class="yt-timestamp" data-t="01:25:31">[01:25:31]</a>.
*   **Output Tokens:** It costs $150 per million output tokens <a class="yt-timestamp" data-t="01:27:32">[01:27:32]</a>.
*   **Subscription:** To chat with GPT 4.5, it is currently only available to $200 per month Pro users <a class="yt-timestamp" data-t="01:29:43">[01:29:43]</a>.
    *   For comparison, Claude was considered expensive at $15 per million tokens <a class="yt-timestamp" data-t="01:15:37">[01:15:37]</a>.

### Performance and Capabilities
Its primary selling point is "Vibes," aiming to chat in a more natural, human-like way <a class="yt-timestamp" data-t="00:16:03">[00:16:03]</a>. OpenAI introduced a new "Vibes Benchmark" to measure creative thinking <a class="yt-timestamp" data-t="01:41:25">[01:41:25]</a>. While it does seem to emit "Chill Vibes," this is a highly subjective metric <a class="yt-timestamp" data-t="01:33:14">[01:33:14]</a>.

Despite claims of a far lower hallucination rate <a class="yt-timestamp" data-t="01:53:57">[01:53:57]</a>, GPT 4.5 still makes many silly mistakes <a class="yt-timestamp" data-t="01:55:54">[01:55:54]</a>. It is not self-aware and is unaware of its own identity as GPT 4.5, stating its training cutoff as October 2023 <a class="yt-timestamp" data-t="01:58:36">[01:58:36]</a>. While it could correctly count the "R"s in "Strawberry," it failed to give the correct number of "L"s in "Lollapalooza" <a class="yt-timestamp" data-t="02:06:06">[02:06:06]</a>.

For programming and science tasks, GPT 4.5 is not expected to perform as well as "deep thinking models" like 03 <a class="yt-timestamp" data-t="02:18:29">[02:18:29]</a>. On the AER polyglot coding Benchmark, GPT 4.5 is worse at programming than [[comparison_with_other_ai_models_in_coding | Deep Seek]] and is hundreds of times more expensive <a class="yt-timestamp" data-t="02:22:15">[02:22:15]</a>.

## Other Notable AI Models

### Deep Seek
[[comparison_with_other_ai_models_in_coding | Deep Seek]] outperforms GPT 4.5 in programming on the AER polyglot coding Benchmark <a class="yt-timestamp" data-t="02:24:06">[02:24:06]</a>, while also being significantly more cost-effective.

### Grok (xAI)
Currently, xAI's Grok is considered the best model in the world, according to the betting market <a class="yt-timestamp" data-t="02:32:00">[02:32:00]</a>.

## Future of AI Models: GPT 5 and Scaling Challenges

OpenAI's Sam Altman believes models can scale almost infinitely <a class="yt-timestamp" data-t="02:51:17">[02:51:17]</a>, contingent on securing trillions of dollars for data center infrastructure <a class="yt-timestamp" data-t="02:55:04">[02:55:04]</a>. However, there's a theory that despite scaling up parameters and compute, GPT 5 has failed to achieve significant improvements <a class="yt-timestamp" data-t="03:01:21">[03:01:21]</a>. GPT 4.5 is the largest model OpenAI has ever created <a class="yt-timestamp" data-t="03:07:37">[03:07:37]</a>.

Sam Altman has described GPT 5 as being more like a "router that automatically chooses the best model based on your prompt" <a class="yt-timestamp" data-t="03:13:06">[03:13:06]</a>. This suggests a shift in strategy, possibly acknowledging a plateau in the capabilities of current [[training_and_finetuning_ai_models | pre-training]] methods for generative pre-trained transformers <a class="yt-timestamp" data-t="00:52:10">[00:52:10]</a>.

For computer science students, this plateau is seen as positive news, as [[open_source_advancements_in_ai_model_technology | AI coding tools]] are most useful to human programmers who understand what they are doing <a class="yt-timestamp" data-t="03:33:59">[03:33:59]</a>.