---
title: Comparison with other AI video models
videoId: tWP6z0hvw1M
---

From: [[fireship]] <br/> 

On February 16th, 2024, [[OpenAI Sora AI video model capabilities | OpenAI Sora]] was unveiled, significantly impacting the landscape of AI video generation <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a>. While not the first AI video model, Sora set new benchmarks for the technology <a class="yt-timestamp" data-t="00:00:53">[00:00:53]</a>.

## Sora's Dominance
Prior to Sora's announcement, Google had introduced Gemini 1.5 with an impressive context window of up to 10 million tokens <a class="yt-timestamp" data-t="00:00:34">[00:00:34]</a>. However, this achievement was swiftly "overshadowed" by Sam Altman's preview of Sora <a class="yt-timestamp" data-t="00:00:40">[00:00:40]</a>.

Sora distinguishes itself from existing models in several key areas:
*   **Realism and Length** It is described as the first AI to create realistic videos up to a minute long <a class="yt-timestamp" data-t="00:00:18">[00:00:18]</a>.
*   **Cohesion** Sora maintains cohesion between frames throughout the video <a class="yt-timestamp" data-t="00:01:03">[00:01:03]</a>.
*   **Aspect Ratios** Videos can be rendered in various aspect ratios <a class="yt-timestamp" data-t="00:01:07">[00:01:07]</a>.
*   **Input Flexibility** Content can be generated from a text prompt or by animating a starting image <a class="yt-timestamp" data-t="00:01:09">[00:01:09]</a>.

### Contrast with Existing Models
The field already included [[open_source_ai_models | open models]] such as Stable Video Diffusion and private products like Pika <a class="yt-timestamp" data-t="00:00:55">[00:00:55]</a>. However, Sora is noted for "blowing everything out of the water" due to its enhanced realism, longer video capabilities, and consistent frame cohesion <a class="yt-timestamp" data-t="00:00:59">[00:00:59]</a>.

## Technical Distinctions
While traditional video models typically crop their training data and outputs to specific times and resolutions, Sora can train on its native resolution and produce variable resolutions <a class="yt-timestamp" data-t="00:01:14">[00:01:14]</a>. Like other diffusion models such as DALL-E and Stable Diffusion, Sora starts with random noise and gradually refines it into a coherent image <a class="yt-timestamp" data-t="00:02:21">[00:02:21]</a>.

A key aspect of [[technical_aspects_and_challenges_of_ai_video_modeling | Sora's technical operation]] involves processing data differently due to the massive scale of video information <a class="yt-timestamp" data-t="00:02:53">[00:02:53]</a>. Unlike large language models that tokenize text, Sora tokenizes "visual patches"â€”small, compressed chunks of images that capture both visual information and movement over time <a class="yt-timestamp" data-t="00:02:57">[00:02:57]</a>.

## Current Limitations
Despite its impressive capabilities, Sora, like other early AI video models, exhibits some flaws. Videos generated often have a "subtle but distinctive AI look," and they do not yet perfectly model physics or humanoid interactions <a class="yt-timestamp" data-t="00:03:58">[00:03:58]</a>. However, it is anticipated that these limitations will be overcome in the future <a class="yt-timestamp" data-t="00:04:07">[00:04:07]</a>.