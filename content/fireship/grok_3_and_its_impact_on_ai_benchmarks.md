---
title: Grok 3 and its impact on AI benchmarks
videoId: b0XI-cbel1U
---

From: [[fireship]] <br/> 

Just hours prior to February 18th, 2025, [[elon_musks_advancements_in_ai_with_grok_3 | Grok 3]], a new large language model developed by Elon Musk's xAI, emerged as a leading contender in the AI space <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a>. It quickly climbed to the number one spot on the LM Marina leaderboard, a platform for blind taste tests comparing different LLMs <a class="yt-timestamp" data-t="00:00:05">[00:00:05]</a><a class="yt-timestamp" data-t="00:01:57">[00:01:57]</a>.

## [[features_and_capabilities_of_grok_3 | Features and Capabilities]]

[[features_and_capabilities_of_grok_3 | Grok 3]] is noted for its intelligence and its largely uncensored nature, which allows it to generate content that may be illegal in some parts of the world <a class="yt-timestamp" data-t="00:01:14">[00:01:14]</a><a class="yt-timestamp" data-t="00:01:16">[00:01:16]</a><a class="yt-timestamp" data-t="00:01:43">[00:01:43]</a>. Key features include:
*   **Deep Thinking Mode**: Similar to DeepCcar 1, it possesses a deep thinking mode <a class="yt-timestamp" data-t="00:00:19">[00:00:19]</a>.
*   **Text-to-Video**: The model can reportedly perform text-to-video generation <a class="yt-timestamp" data-t="00:00:22">[00:00:22]</a>.
*   **Direct Data Access**: Unique among models, [[features_and_capabilities_of_grok_3 | Grok 3]] has direct access to the "fire hose" of data from Twitter <a class="yt-timestamp" data-t="00:01:16">[00:01:16]</a>.
*   **Truth-Seeking Optimization**: xAI developers have optimized the model for "maximum truth seeking," even if it means being politically incorrect <a class="yt-timestamp" data-t="00:01:20">[00:01:20]</a><a class="yt-timestamp" data-t="00:01:23">[00:01:23]</a>. This allows it to generate content like images of celebrities or profanity-laced poems about racial stereotypes in the name of science, content which other LLMs typically block <a class="yt-timestamp" data-t="00:01:26">[00:01:26]</a><a class="yt-timestamp" data-t="00:01:34">[00:01:34]</a><a class="yt-timestamp" data-t="00:01:36">[00:01:36]</a>.
*   **Availability**: Despite its uncensored nature, [[features_and_capabilities_of_grok_3 | Grok 3]] is expected to be available in countries like Germany and the UK soon <a class="yt-timestamp" data-t="00:01:48">[00:01:48]</a><a class="yt-timestamp" data-t="00:01:50">[00:01:50]</a>.

## Benchmark Performance and [[comparison_with_other_ai_models_in_coding | Comparisons]]

[[features_and_capabilities_of_grok_3 | Grok 3]]'s initial performance on the LM Marina leaderboard positioned it at the top, indicating high quality based on human comparisons <a class="yt-timestamp" data-t="00:01:55">[00:01:55]</a><a class="yt-timestamp" data-t="00:02:01">[00:02:01]</a>.

Additional benchmarks show [[comparison_with_other_ai_models_in_coding | Grok 3 beating Gemini, Claude, DeepSeek, and GPT-4]] in math, science, and coding tasks <a class="yt-timestamp" data-t="00:02:03">[00:02:03]</a><a class="yt-timestamp" data-t="00:02:08">[00:02:08]</a>. However, it's noted that these benchmarks conveniently omit OpenAI's O3 model, and when included, the picture changes <a class="yt-timestamp" data-t="00:02:10">[00:02:10]</a><a class="yt-timestamp" data-t="00:02:14">[00:02:14]</a>. The model also lacked results from CodeForces and Arc AGI benchmarks <a class="yt-timestamp" data-t="00:02:16">[00:02:16]</a>. It's suggested that benchmarks are often "cherry-picked" <a class="yt-timestamp" data-t="00:02:19">[00:02:19]</a>.

In practical terms, [[features_and_capabilities_of_grok_3 | Grok 3]] demonstrated the ability to generate valid Svelte 5 code and assist in building a Godot game, performing well overall <a class="yt-timestamp" data-t="00:02:25">[00:02:25]</a><a class="yt-timestamp" data-t="00:02:28">[00:02:28]</a>. It appears to be plateauing at a similar level to other state-of-the-art models <a class="yt-timestamp" data-t="00:02:31">[00:02:31]</a>.

## [[training_infrastructure_and_future_developments_of_grok_3 | Training and Infrastructure]]

Details have been provided regarding [[training_infrastructure_and_future_developments_of_grok_3 | how Grok 3 was trained]]. It was trained at the Colossus supercomputer in Memphis, Tennessee, which is believed to be the world's largest AI supercomputer <a class="yt-timestamp" data-t="00:02:44">[00:02:44]</a><a class="yt-timestamp" data-t="00:02:47">[00:02:47]</a><a class="yt-timestamp" data-t="00:02:51">[00:02:51]</a>.
*   **GPUs**: Colossus contains a cluster of over 200,000 Nvidia H100 GPUs, with plans to expand to 1 million GPUs <a class="yt-timestamp" data-t="00:02:53">[00:02:53]</a><a class="yt-timestamp" data-t="00:02:58">[00:02:58]</a>.
*   **Power Consumption**: The facility's electricity demands are so high that portable diesel generators are required to supplement grid power <a class="yt-timestamp" data-t="00:03:00">[00:03:00]</a><a class="yt-timestamp" data-t="00:03:05">[00:03:05]</a>.

## Future Developments: Super Grok

[[training_infrastructure_and_future_developments_of_grok_3 | Super Grok]], an even more powerful version, is expected to be released in the near future <a class="yt-timestamp" data-t="00:00:25">[00:00:25]</a><a class="yt-timestamp" data-t="00:03:07">[00:03:07]</a>. It is projected to cost $30 per month, a highly competitive price compared to services like ChatGPT Pro, which is priced at $200 per month <a class="yt-timestamp" data-t="00:03:09">[00:03:09]</a><a class="yt-timestamp" data-t="00:03:15">[00:03:15]</a>.

## Context in the AI Landscape

The AI landscape is fiercely competitive, with major players like Elon Musk (xAI), OpenAI, and Mark Zuckerberg (Meta) vying for dominance <a class="yt-timestamp" data-t="00:00:52">[00:00:52]</a>. For example, Elon Musk recently attempted to acquire OpenAI, an offer that was rejected by the OpenAI board <a class="yt-timestamp" data-t="00:00:42">[00:00:42]</a><a class="yt-timestamp" data-t="00:00:48">[00:00:48]</a>. Meanwhile, Meta faced scrutiny for allegedly using 82 terabytes of pirated books from the Library Genesis Project to train their LLaMA models <a class="yt-timestamp" data-t="00:00:57">[00:00:57]</a><a class="yt-timestamp" data-t="00:01:06">[00:01:06]</a>.

The focus in AI development has recently shifted from creating larger, better base models to developing more effective prompting frameworks like "deep research" and "big brain mode" <a class="yt-timestamp" data-t="00:02:35">[00:02:35]</a><a class="yt-timestamp" data-t="00:02:41">[00:02:41]</a>.

Despite the advancements in AI, a developer noted that paying for multiple AI coding tools like Claude, Cursor, Gemini, ChatGPT, Co-pilot, Codium, Midjourney, and WatsonX has paradoxically led to [[impact_of_ai_on_coding_and_programming_skills | worse code quality]] <a class="yt-timestamp" data-t="00:03:15">[00:03:15]</a><a class="yt-timestamp" data-t="00:03:23">[00:03:23]</a>.