---
title: Technical aspects and challenges of AI video modeling
videoId: tWP6z0hvw1M
---

From: [[fireship]] <br/> 

[[OpenAI Sora AI video model capabilities | OpenAI Sora]] represents a significant advancement in AI video modeling, showcasing the ability to generate realistic videos up to a minute long <a class="yt-timestamp" data-t="00:00:18">[00:00:18]</a>. This text-to-video model, whose name "Sora" comes from the Japanese word for "sky" <a class="yt-timestamp" data-t="00:00:45">[00:00:45]</a>, produces content that surpasses previous models in realism and coherence <a class="yt-timestamp" data-t="00:00:59">[00:00:59]</a>.

## Core Capabilities of Sora

[[OpenAI Sora AI video model capabilities | Sora]] can generate video clips from either a text prompt describing the desired scene or from a starting image that is then brought to life <a class="yt-timestamp" data-t="00:01:09">[00:01:09]</a>. Its outputs are notably realistic and can maintain cohesion between frames for up to a minute <a class="yt-timestamp" data-t="00:01:02">[00:01:02]</a>. Additionally, videos can be rendered in various aspect ratios <a class="yt-timestamp" data-t="00:01:07">[00:01:07]</a>. Initial concerns about OpenAI cherry-picking examples were alleviated when Sam Altman generated videos on request via Twitter, like "two golden retrievers doing a podcast on top of a mountain" <a class="yt-timestamp" data-t="00:01:16">[00:01:16]</a>.

## Underlying Mechanism

[[OpenAI Sora AI video model capabilities | Sora]] functions as a [[training and finetuning AI models | diffusion model]], a technique also employed by models like DALL-E and Stable Diffusion <a class="yt-timestamp" data-t="00:02:21">[00:02:21]</a>. This process begins with random noise, which is then gradually refined into a coherent image or video <a class="yt-timestamp" data-t="00:02:24">[00:02:24]</a>.

### Data Handling and Scale
Generating video presents a much greater challenge than still images due to the exponential increase in data points and the added dimension of time <a class="yt-timestamp" data-t="00:02:53">[00:02:53]</a>:
*   A 1000x1000 pixel image with three color channels contains approximately 3 million data points <a class="yt-timestamp" data-t="00:02:30">[00:02:30]</a>.
*   A one-minute video at 60 frames per second contains over 10 billion data points <a class="yt-timestamp" data-t="00:02:38">[00:02:38]</a>. To put this in perspective, 1 million seconds is about 11.5 days, while 10 billion seconds is approximately 317.7 years <a class="yt-timestamp" data-t="00:02:45">[00:02:45]</a>.

To manage this scale, [[OpenAI Sora AI video model capabilities | Sora]] adopts an approach similar to large language models (LLMs) <a class="yt-timestamp" data-t="00:02:57">[00:02:57]</a>. Instead of tokenizing text, [[OpenAI Sora AI video model capabilities | Sora]] tokenizes "visual patches"â€”small, compressed segments of images that capture both visual information and movement over time <a class="yt-timestamp" data-t="00:03:02">[00:03:02]</a>. Unlike typical video models that crop training data to specific resolutions, [[OpenAI Sora AI video model capabilities | Sora]] can train on native resolutions and output variable resolutions <a class="yt-timestamp" data-t="00:03:14">[00:03:14]</a>.

## Computational Demands

The [[development of video content using AI technology | development of video content using AI technology]] at this scale requires immense computing power <a class="yt-timestamp" data-t="00:02:00">[00:02:00]</a>. Sam Altman, CEO of OpenAI, reportedly requested $7 trillion to acquire the necessary GPUs for scaling such models <a class="yt-timestamp" data-t="00:02:04">[00:02:04]</a>, a figure even NVIDIA's CEO Jensen Huang found high, suggesting it should be closer to $2 trillion <a class="yt-timestamp" data-t="00:02:10">[00:02:10]</a>.

## [[Limitations and challenges of current AI technologies | Limitations and Challenges]]

Despite its impressive capabilities, [[OpenAI Sora AI video model capabilities | Sora]] still faces [[Limitations and challenges of current AI technologies | limitations and challenges]]:
*   **"AI Look":** Generated videos often exhibit a subtle but distinctive "AI look" <a class="yt-timestamp" data-t="00:04:00">[00:04:00]</a>.
*   **Physics and Interactions:** The model doesn't always perfectly simulate physics or complex humanoid interactions <a class="yt-timestamp" data-t="00:04:04">[00:04:04]</a>. However, these are expected to be overcome over time <a class="yt-timestamp" data-t="00:04:07">[00:04:07]</a>.

## Availability and Content Provenance

It is highly unlikely that [[OpenAI Sora AI video model capabilities | Sora]] will ever be [[open_source_advancements_in_ai_model_technology | open source]] <a class="yt-timestamp" data-t="00:01:48">[00:01:48]</a>. When released, videos generated by [[OpenAI Sora AI video model capabilities | Sora]] are expected to include c2p metadata, a surveillance apparatus that records content origin and modification history <a class="yt-timestamp" data-t="00:01:51">[00:01:51]</a>.