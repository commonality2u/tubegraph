---
title: Training infrastructure and future developments of Grok 3
videoId: b0XI-cbel1U
---

From: [[fireship]] <br/> 

## Training Infrastructure

[[elon_musks_advancements_in_ai_with_grok_3 | Grok]] 3 was trained at the Colossus supercomputer located in Memphis, Tennessee <a class="yt-timestamp" data-t="00:02:47">[00:02:47]</a>. This facility is currently believed to be the world's largest AI supercomputer <a class="yt-timestamp" data-t="00:02:51">[00:02:51]</a>.

It consists of a cluster with over 200,000 Nvidia H100 GPUs <a class="yt-timestamp" data-t="00:02:54">[00:02:54]</a>, with future plans to expand this to 1 million GPUs <a class="yt-timestamp" data-t="00:02:58">[00:02:58]</a>. The power demands of Colossus are so significant that it cannot rely solely on the electrical grid, necessitating the use of portable diesel generators to meet its energy needs <a class="yt-timestamp" data-t="00:03:01">[00:03:01]</a>.

## Future Developments

### Super Grok

A more powerful version of Grok, named Super Grok, is anticipated for release in the near future <a class="yt-timestamp" data-t="00:02:25">[00:02:25]</a>. This enhanced model is expected to be offered as a paid subscription service <a class="yt-timestamp" data-t="00:02:23">[00:02:23]</a>, costing approximately $30 per month <a class="yt-timestamp" data-t="00:03:10">[00:03:10]</a>. This price point aims to be highly competitive, especially when compared to services like ChatGPT Pro, which is priced at $200 per month <a class="yt-timestamp" data-t="00:03:12">[00:03:12]</a>.

### Accessibility

[[grok_3_and_its_impact_on_ai_benchmarks | Grok 3]] is expected to become available in countries such as Germany and the UK soon <a class="yt-timestamp" data-t="00:01:48">[00:01:48]</a>.