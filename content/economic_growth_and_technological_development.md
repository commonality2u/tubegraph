---
title: Economic growth and technological development
videoId: 6yQEA18C-XI
---

From: [[dwarkesh | The Dwarkesh Podcast]]

This article summarizes discussions from a debate between George Hotz and Eliezer Yudkowsky regarding the nature, speed, and implications of economic and technological growth, particularly in the context of Artificial Intelligence (AI) development.

## Current State and Baselines

### Economic Doubling Times
The current rate of global economic growth was discussed, with differing estimates for its doubling time.
*   George Hotz initially stated the world economy doubles about every 30 years <a class="yt-timestamp" data-t="00:12:45">[00:12:45]</a>.
*   Eliezer Yudkowsky suggested it might be closer to 15 years, though he was not certain <a class="yt-timestamp" data-t="00:12:50">[00:12:50]</a>.
*   Hotz raised a hypothetical concern about the world economy doubling every second, terming it "terrifying" <a class_yt-timestamp" data-t="00:13:05">[00:13:05]</a>.

### Compute Power: Human vs. Silicon
A comparison was made between the total computational power of humanity and existing silicon-based computers.
*   Hotz estimated that there are about two Zetaflops of silicon compute in the world <a class="yt-timestamp" data-t="00:31:37">[00:31:37]</a>.
*   Assuming a human brain operates at approximately 20 Petaflops, Hotz calculated that humanity collectively possesses around 160,000 Zetaflops of compute power <a class="yt-timestamp" data-t="00:31:51">[00:31:51]</a>.
*   This implies there is currently about 80,000 times more human compute than silicon compute in the world [[role_of_compute_in_ai_development | role of compute in AI development]] <a class="yt-timestamp" data-t="00:32:07">[00:32:07]</a>. Yudkowsky noted this figure can be misleading due to aggregation challenges, suggesting a single large entity could outperform many small ones [[ai_scalability_and_breakthroughs | AI scalability and breakthroughs]] <a class="yt-timestamp" data-t="00:32:15">[00:32:15]</a>.

### Efficiency of Biological vs. Silicon Compute
The discussion touched upon the energy efficiency of human brains compared to current silicon technology, referencing the Landauer limit (the theoretical minimum energy required for computation).
*   Hotz stated that to achieve 20 Petaflops of compute (approximate human brain power) with current technology (16 H100 GPUs), it would cost about half a million dollars and consume 20 kilowatts of power [[data_center_energy_requirements_and_scaling | data center energy requirements and scaling]] <a class="yt-timestamp" data-t="01:12:04">[01:12:04]</a>. A human brain achieves this with roughly 100 Watts <a class="yt-timestamp" data-t="01:11:43">[01:11:43]</a>.
*   This suggests the brain is about a thousand times more power-efficient than current silicon for the same computational throughput <a class="yt-timestamp" data-t="01:12:29">[01:12:29]</a>.
*   Hotz asserted that current silicon computers are "really close" to the Landauer limit, possibly off by a factor of 100 to 1,000 <a class="yt-timestamp" data-t="01:12:38">[01:12:38]</a>.
*   Yudkowsky estimated the brain is about six orders of magnitude away from the Landauer limit <a class="yt-timestamp" data-t="01:11:55">[01:11:55]</a>, citing the irreversible operations in neurotransmitter reuptake and ion channel activity as necessary energy expenditures <a class="yt-timestamp" data-t="01:13:03">[01:13:03]</a>.

## Drivers and Accelerants of Technological Development

### Recursive Self-Improvement
The concept of recursive self-improvement was acknowledged as possible.
*   Hotz stated that humanity has engaged in recursive self-improvement whenever a tool has been used to create a better tool [[recursive_selfimprovement_and_ai_capabilities | recursive self-improvement and AI capabilities]] <a class="yt-timestamp" data-t="00:03:04">[00:03:04]</a>.
*   However, Hotz expressed skepticism about an AI in a basement rapidly self-improving overnight to achieve superintelligence <a class="yt-timestamp" data-t="00:03:12">[00:03:12]</a>.

### AI as a Tool and Developer
AI's role in accelerating scientific and technological progress was highlighted by the example of protein folding.
*   Yudkowsky recalled predicting in 2004 that superintelligence could solve a special case of protein folding [[the_geopolitical_stakes_of_agi_development | the geopolitical stakes of AGI development]] <a class="yt-timestamp" data-t="00:05:49">[00:05:49]</a>.
*   In reality, AlphaFold 2, an AI, cracked the much harder general biological case around 2020 <a class="yt-timestamp" data-t="00:06:18">[00:06:18]</a>.
*   Hotz noted AlphaFold 2 was trained on vast amounts of experimental data, not derived from first principles physics [[ai_for_science_and_societal_challenges | AI for Science and Societal Challenges]] <a class="yt-timestamp" data-t="00:07:57">[00:07:57]</a>.
*   Hotz is working on self-driving cars using a Transformer-based world model, architecturally similar to GPT-2, to predict the future for navigation [[ais_potential_impact_on_software_and_application_development | AI's potential impact on software and application development]] <a class="yt-timestamp" data-t="01:08:01">[01:08:01]</a>.

### Moore's Law and Potential Acceleration
Yudkowsky's earlier writings described Moore's Law as a human-driven phenomenon (doubling processor power every two years). If computers took over this process, the doubling time could shrink dramatically (2 years, then 1 year, 6 months, etc.), leading to a hyperbolic growth curve or "singularity" <a class="yt-timestamp" data-t="00:01:37">[00:01:37]</a>. Yudkowsky later clarified he doubts a strictly hyperbolic sequence now, suggesting it could be exponential or another curve [[economic_growth_and_technological_acceleration | economic growth and technological acceleration]] <a class="yt-timestamp" data-t="01:10:48">[01:10:48]</a>.

## Scenarios of Rapid Technological Advancement ("Foom")

### The "Foom" Hypothesis
The idea of a rapid, intelligence-driven "foom" or criticality, where AI undergoes extremely fast self-improvement leading to superintelligence, was a central point of contention.
*   Hotz stated, "I don't think AI can Foom. I don't think intelligence can go critical" [[ai_alignment_and_safety_concerns | AI alignment and safety concerns]] <a class="yt-timestamp" data-t="00:02:48">[00:02:48]</a>. He considers this an extraordinary claim requiring extraordinary evidence <a class="yt-timestamp" data-t="00:03:25">[00:03:25]</a>.
*   Yudkowsky's "Staring into the Singularity" (written at age 16 <a class="yt-timestamp" data-t="00:10:55">[00:10:55]</a>) posited such a rapid takeoff [[intelligence_explosion_and_its_implications | intelligence explosion and its implications]] <a class="yt-timestamp" data-t="00:01:58">[00:01:58]</a>.
*   Yudkowsky indicated that a destructive outcome from superintelligence does not strictly require a very rapid rate of ascent, but rather a large enough capability gap opening up between AI and humanity <a class="yt-timestamp" data-t="00:03:48">[00:03:48]</a>. Even a 10-year process to this state could be "game over" [[predicting_the_impact_and_management_of_superintelligence | predicting the impact and management of superintelligence]] <a class="yt-timestamp" data-t="00:04:42">[00:04:42]</a>.

### Arguments Against Rapid "Foom"
Hotz argued that practical constraints make an overnight "foom" scenario unlikely.
*   The high expense and length of current AI training runs were mentioned [[ai_alignment_and_safety_research | AI alignment and safety research]] <a class="yt-timestamp" data-t="00:33:04">[00:33:04]</a>.
*   Yudkowsky acknowledged that the "giant inscrutable matrices" of current deep learning models are "incredibly inefficient" <a class="yt-timestamp" data-t="01:09:10">[01:09:10]</a>, with vast amounts of compute used for relatively simple operations like multiplication <a class="yt-timestamp" data-t="01:10:07">[01:10:07]</a>.
*   Hotz believes superintelligence is possible but requires orders of magnitude more power and compute than humanity currently possesses <a class="yt-timestamp" data-t="01:14:15">[01:14:15]</a>.
*   The development of highly advanced technologies like nanobots or new chip fabrication facilities would require overcoming immense search problems and resource mobilization [[challenges_and_opportunities_in_deploying_ai_at_scale | challenges and opportunities in deploying AI at scale]] <a class="yt-timestamp" data-t="01:01:09">[01:01:09]</a>, <a class="yt-timestamp" data-t="01:18:57">[01:18:57]</a>.

### The "Slow Takeoff" Alternative
If not a rapid "foom," a slower, more gradual increase in AI capabilities relative to humans was discussed.
*   Hotz envisions a future where AI develops "slowly," allowing humanity to expand across the galaxy and unlock amazing technologies [[exploring_the_future_of_society_and_economy_with_ai | exploring the future of society and economy with AI]] <a class="yt-timestamp" data-t="00:12:05">[00:12:05]</a>. He expects a "nice exponential" growth, with technologies like robot maids and advanced chefs becoming commonplace <a class="yt-timestamp" data-t="01:33:28">[01:33:28]</a>.
*   Yudkowsky countered that even a slower development (e.g., over a week or five years instead of overnight) leading to vastly superior, non-aligned intelligence would still be catastrophic <a class="yt-timestamp" data-t="00:12:31">[00:12:31]</a>, [[ai_alignment_and_existential_risks | AI alignment and existential risks]] <a class="yt-timestamp" data-t="00:04:45">[00:04:45]</a>.

## Advanced Technologies and Their Implications

### Diamond Nanotechnology
The prospect of "diamond nanobots" was used as a benchmark for highly advanced, transformative technology.
*   Hotz doubts that an AI could go from current capabilities to producing "diamond nanobots overnight" [[future_of_agi_and_societal_implications | future of AGI and societal implications]] <a class="yt-timestamp" data-t="00:12:26">[00:12:26]</a>, viewing their development as an "extremely, extremely hard" search problem <a class="yt-timestamp" data-t="01:01:09">[01:01:09]</a>.
*   Yudkowsky suggested that diamond nanobots could lead to self-replicating fusion factories quite quickly [[implications_of_ai_on_future_scientific_advancements | implications of AI on future scientific advancements]] <a class="yt-timestamp" data-t="01:16:46">[01:16:46]</a>. He considers the problem "predictably solvable" in a similar vein to how protein folding was <a class="yt-timestamp" data-t="01:16:58">[01:16:58]</a>.

### Self-Replicating Factories
The concept of self-replicating manufacturing capabilities was discussed as a potential outcome of advanced AI.
*   Yudkowsky argued that self-replicating factories would exhaust available resources rapidly, regardless of initial scale (e.g., a whole star's resources) [[ai_safety_and_alignment | AI safety and alignment]] <a class="yt-timestamp" data-t="01:18:05">[01:18:05]</a>.
*   Hotz dreams of building a machine that can self-replicate using silica, to which Yudkowsky noted algae already self-replicate using biological materials <a class="yt-timestamp" data-t="01:18:16">[01:18:16]</a>.

### Biotechnology
The power of biotechnology, especially when augmented by AI, was touched upon.
*   The development of COVID-19, potentially through lab-based research (Yudkowsky <a class="yt-timestamp" data-t="01:05:27">[01:05:27]</a>, Hotz agreeing on Lab Escape likelihood [[challenges_in_ai_governance | challenges in AI governance]] <a class="yt-timestamp" data-t="01:06:53">[01:06:53]</a>), was cited as an example of what can be achieved with relatively modest funding.
*   Yudkowsky noted "enormous amounts of headroom above biology for artificial biology" [[emerging_trends_in_memory_and_chip_design | emerging trends in memory and chip design]] <a class="yt-timestamp" data-t="01:11:24">[01:11:24]</a>, and that biology itself is very constrained, having only found a few solutions for things like freely rotating wheels (e.g., bacterial flagellum, ATP synthase) <a class="yt-timestamp" data-t="01:04:22">[01:04:22]</a>.

## Timelines and Predictability

### Difficulty of Predicting Timelines
Both speakers acknowledged the difficulty in predicting the exact timing of technological breakthroughs.
*   Yudkowsky stated, "Timing is really, really hard" [[forecasting_ai_progress_and_the_intelligence_explosion | forecasting AI progress and the intelligence explosion]] <a class="yt-timestamp" data-t="00:05:42">[00:05:42]</a>, citing his inability to predict the 2020 timing of AlphaFold 2 in 2004, despite correctly predicting the solvability of protein folding <a class="yt-timestamp" data-t="00:06:24">[00:06:24]</a>. He emphasized it's "far easier to predict the end point than all the details of the process" <a class="yt-timestamp" data-t="00:05:37">[00:05:37]</a>.

### Varying Estimates for AGI/Superintelligence
*   Hotz recalled his 2015 prediction that self-driving cars wouldn't be widespread for 10 years (which has largely held true) <a class="yt-timestamp" data-t="00:43:22">[00:43:22]</a>. He currently predicts no superintelligence within the next 10 years [[ai_trajectory_and_scaling_hypothesis | AI trajectory and scaling hypothesis]] <a class="yt-timestamp" data-t="00:43:30">[00:43:30]</a>. He suggested AGI surpassing humans at all tasks might take 20 to 50 years [[timeline_predictions_for_agi_development | timeline predictions for AGI development]] <a class="yt-timestamp" data-t="00:43:50">[00:43:50]</a>.
*   Yudkowsky, while hesitant to give exact timelines, stated his "wild guess" is that superintelligence leading to a critical outcome could happen within his lifetime <a class="yt-timestamp" data-t="00:07:21">[00:07:21]</a>.

## Impact of Growth and Development

### Potential for "Awesome" Future
George Hotz expressed optimism about a future augmented by advanced AI, if developed gradually.
*   He stated, "If we do it slowly... if we start to expand out across the Galaxy and we eventually unlock these wild and amazing technologies, that sounds pretty awesome to me" <a class="yt-timestamp" data-t="00:12:05">[00:12:05]</a>.
*   He anticipates desirable technologies like self-driving cars, robot maids, and AI chefs, contributing to a positive future [[impact_of_ai_on_future_technology_and_society | impact of AI on future technology and society]] <a class="yt-timestamp" data-t="01:33:29">[01:33:29]</a>.

### Concerns about Uncontrolled Growth
Eliezer Yudkowsky voiced concerns that rapid or uncontrolled technological growth, particularly in AI, could lead to catastrophic outcomes.
*   He worried about growth that leads to "super weapons that against which we have no defense" [[potential_ai_takeover_scenarios_and_implications | potential AI takeover scenarios and implications]] <a class="yt-timestamp" data-t="00:13:41">[00:13:41]</a>.
*   He expressed concern about investments in chip factories <a class="yt-timestamp" data-t="00:13:49">[00:13:49]</a> and would advocate for stopping the production of AI chips, and potentially other chips, due to the perceived deadliness of the technology <a class="yt-timestamp" data-t="00:14:34">[00:14:34]</a>.
*   The "end point" of unaligned superintelligence, for Yudkowsky, involves humans gone and galaxies transformed into "stuff that that ain't all that cool" <a class="yt-timestamp" data-t="01:29:41">[01:29:41]</a>.