---
title: AI existential risks and safe advanced AI systems
videoId: M3TUe4zUCKk
---

From: [[DwarkeshPatel]] <br/> 
In recent discussions with Joe Carl Smith, a senior research analyst at Open Philanthropy and a doctoral student in philosophy at the University of Oxford, the focus was on the existential risks posed by artificial intelligence (AI) and the development of advanced AI systems that are safe. This article synthesizes insights shared in the video interview regarding the potential threats of AI and the importance of ensuring safety in its evolution.

## Understanding AI Existential Risks

**Existential risks** refer to threats that could cause human extinction or permanently and drastically curtail humanity's potential. AI presents a unique existential risk due to its potential to surpass human intelligence and capability, leading to situations where it might act in ways that are misaligned with human values and interests [[alignment_and_misalignment_of_ai | (Alignment and Misalignment of AI)]].

Joe Carl Smith emphasizes that understanding AI timelines and "takeoff speeds"—the speed at which AI transitions from impressive capabilities to radically transformative ones—is critical in assessing these risks. By clarifying these timelines, researchers can better prioritize and respond to potential dangers that could arise from AI systems becoming too advanced too quickly [[challenges_in_modeling_and_predicting_ai_timelines | (Challenges in Modeling and Predicting AI Timelines)]] (<a class="yt-timestamp" data-t="00:02:22">[00:02:22]</a>).

## Safe Advanced AI Systems

Creating **safe AI systems** involves ensuring that as AI systems become more advanced, they remain aligned with human values and act in ways that are beneficial rather than harmful. Achieving this requires developing methodologies and strategies that prioritize safety and ethical considerations alongside technological advancements [[structured_generative_advancements | AI Ethics and Deployment Strategies]].

Joe Carl Smith suggests that part of this involves thinking seriously about AI governance and policy to mitigate risks and foster collaboration among stakeholders involved in AI development [[strategic_international_coordination_on_ai_governance | (Strategic International Coordination on AI Governance)]]. This means counting less on having more time when AI approaches transformative capabilities sooner than expected, and taking into account probabilities of catastrophic outcomes in how priorities are set [[ai_progress_forecasting | (AI Progress Forecasting)]] (<a class="yt-timestamp" data-t="00:02:56">[00:02:56]</a>).

> [!info] Importance of Prioritization
> 
> The implications of AI risk are significant in terms of prioritizing efforts. A higher perceived probability of catastrophic AI events makes it essential to redefine priorities to ensure safety measures are implemented effectively in anticipation of rapid technological advancements.

## Long-termism and Future Considerations

The philosophy of **long-termism**, which places importance on the long-term future and wellbeing, informs much of the discourse around AI risks [[longtermism_and_its_implications | (Longtermism and its Implications)]]. Joe Carl Smith identifies as a long-termist and highlights the value in dedicating resources and energy toward building a future that remains positive and secure from existential threats posed by AI [[existential_risk_and_societal_collapse | (Existential Risk and Societal Collapse)]] (<a class="yt-timestamp" data-t="00:03:50">[00:03:50]</a>).

## Addressing Uncertainty in AI Timelines

Understanding and modeling the future in context with AI development is fraught with uncertainty. The complexity of AI and its potential impacts require models that account for multiple possible outcomes [[the_future_of_ai_research_and_potential_societal_impacts | (The Future of AI Research and Potential Societal Impacts)]]. Joe Carl Smith admits that while the future is a vast domain with many potential paths, employing "extremely lossy abstractions" becomes essential to grapple with this uncertainty and remain responsible in action [[challenges_in_modeling_and_predicting_ai_timelines | (Challenges in Modeling and Predicting AI Timelines)]] (<a class="yt-timestamp" data-t="00:00:42">[00:00:42]</a>).

## Conclusion

AI poses potential existential risks that require comprehensive strategies focused on safety and alignment with human values [[ai_safety_and_existential_risk | (AI Safety and Existential Risk)]]. By prioritizing these considerations, developing better governance, and engaging with the uncertain future, we can strive to mitigate the existential risks that advanced AI could present. Joe Carl Smith's insights underscore the importance of anticipating and preparing for how AI will evolve to ensure it supports a humanity-centered future.