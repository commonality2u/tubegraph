---
title: Backpropagation algorithm in neural networks
videoId: Ilg3gGewQ5U
---

From: [[3blue1brown]] <br/> 

[[Backpropagation algorithm walkthrough | Backpropagation]] is the fundamental algorithm that enables [[Structure and function of a neural network | neural networks]] to learn <a class="yt-timestamp" data-t="00:00:04">[00:00:04]</a>. It provides a method for computing the complex gradient of the [[Cost function and its role in neural networks | cost function]] <a class="yt-timestamp" data-t="00:01:45">[00:01:45]</a>.

## Prerequisites and Context

To understand [[Backpropagation algorithm walkthrough | backpropagation]], a basic understanding of [[Structure and function of a neural network | neural networks]], how they process information forward (feed-forward), and [[understanding_gradients_in_backpropagation | gradient descent]] is helpful <a class="yt-timestamp" data-t="00:00:27">[00:00:27]</a> <a class="yt-timestamp" data-t="00:00:31">[00:00:31]</a> <a class="yt-timestamp" data-t="00:00:50">[00:00:50]</a>. Learning in [[Structure and function of a neural network | neural networks]] involves finding the [[Weight and bias adjustment in neural networks | weights and biases]] that minimize a specific [[Cost function and its role in neural networks | cost function]] <a class="yt-timestamp" data-t="00:00:56">[00:00:56]</a>.

### Cost Function Recap

For a single training example, the cost is calculated by comparing the network's output to the desired output and summing the squares of the differences between each component <a class="yt-timestamp" data-t="00:01:02">[00:01:02]</a> <a class="yt-timestamp" data-t="00:01:05">[00:01:05]</a>. The total cost of the network is the average of these costs across all training examples <a class="yt-timestamp" data-t="00:01:15">[00:01:15]</a> <a class="yt-timestamp" data-t="00:01:18">[00:01:18]</a>. The goal is to find the negative gradient of this [[Cost function and its role in neural networks | cost function]], which indicates how to change all the [[Weight and bias adjustment in neural networks | weights and biases]] to efficiently decrease the cost <a class="yt-timestamp" data-t="00:01:26">[00:01:26]</a> <a class="yt-timestamp" data-t="00:01:30">[00:01:30]</a>.

### Understanding the Gradient

While the gradient vector can exist in thousands of dimensions, it can also be understood as a measure of how sensitive the [[Cost function and its role in neural networks | cost function]] is to each [[Weight and bias adjustment in neural networks | weight and bias]] <a class="yt-timestamp" data-t="00:02:04">[00:02:04]</a> <a class="yt-timestamp" data-t="00:02:07">[00:02:07]</a>. For example, if a component of the negative gradient for one weight is 3.2 and for another is 0.1, it means the [[Cost function and its role in neural networks | cost function]] is 32 times more sensitive to changes in the first weight <a class="yt-timestamp" data-t="00:02:11">[00:02:11]</a> <a class="yt-timestamp" data-t="00:02:20">[00:02:20]</a> <a class="yt-timestamp" data-t="00:02:26">[00:02:26]</a>.

## [[Intuitive explanation of backpropagation | Intuitive Walkthrough]] of Backpropagation

[[Intuitive explanation of backpropagation | Backpropagation]] calculates the necessary "nudges" for each [[Weight and bias adjustment in neural networks | weight and bias]] <a class="yt-timestamp" data-t="00:03:07">[00:03:07]</a> <a class="yt-timestamp" data-t="00:03:11">[00:03:11]</a>.

### Impact of a Single Training Example

The adjustment of [[Weight and bias adjustment in neural networks | weights and biases]] depends, in principle, on every training example because the [[Cost function and its role in neural networks | cost function]] averages across all examples <a class="yt-timestamp" data-t="00:03:17">[00:03:17]</a> <a class="yt-timestamp" data-t="00:03:21">[00:03:21]</a>. Let's consider a single training example, such as an image of the digit '2' <a class="yt-timestamp" data-t="00:03:42">[00:03:42]</a> <a class="yt-timestamp" data-t="00:03:46">[00:03:46]</a>.

1.  **Output Layer Adjustments**: If the network is untrained, its output activations will be somewhat random <a class="yt-timestamp" data-t="00:03:52">[00:03:52]</a> <a class="yt-timestamp" data-t="00:03:56">[00:03:56]</a>. Since the network should classify the image as a '2', the activation for the '2' neuron should increase, while others should decrease <a class="yt-timestamp" data-t="00:04:13">[00:04:13]</a> <a class="yt-timestamp" data-t="00:04:16">[00:04:16]</a>. The size of these nudges should be proportional to how far each current value is from its target <a class="yt-timestamp" data-t="00:04:22">[00:04:22]</a> <a class="yt-timestamp" data-t="00:04:25">[00:04:25]</a>.

2.  **Influencing a Single Neuron**: A neuron's activation is determined by a weighted sum of activations from the previous layer, plus a bias, passed through an activation function like a sigmoid or ReLU <a class="yt-timestamp" data-t="00:04:42">[00:04:42]</a> <a class="yt-timestamp" data-t="00:04:44">[00:04:44]</a> <a class="yt-timestamp" data-t="00:04:48">[00:04:48]</a>. To increase a neuron's activation, three avenues can be utilized <a class="yt-timestamp" data-t="00:05:01">[00:05:01]</a>:
    *   Increase its bias <a class="yt-timestamp" data-t="00:05:07">[00:05:07]</a>.
    *   Increase the weights connecting it to the previous layer <a class="yt-timestamp" data-t="00:05:09">[00:05:09]</a>.
    *   Change the activations of neurons in the previous layer <a class="yt-timestamp" data-t="00:05:10">[00:05:10]</a>.

3.  **Weight Adjustments**: [[Weight and bias adjustment in neural networks | Weights]] have differing levels of influence <a class="yt-timestamp" data-t="00:05:17">[00:05:17]</a>. Connections from highly active neurons in the preceding layer have the biggest effect on the cost function when their weights are adjusted, as those weights are multiplied by larger activation values <a class="yt-timestamp" data-t="00:05:21">[00:05:21]</a> <a class="yt-timestamp" data-t="00:05:25">[00:05:25]</a>. This concept is somewhat reminiscent of [[Neural networks and their inspiration from the brain | Hebbian theory]] in neuroscience, often summarized as "neurons that fire together wire together" <a class="yt-timestamp" data-t="00:05:55">[00:05:55]</a> <a class="yt-timestamp" data-t="00:05:58">[00:05:58]</a> <a class="yt-timestamp" data-t="00:06:02">[00:06:02]</a>.

4.  **Propagating Backwards**: The third way to influence a neuron's activation is by changing the activations in the previous layer <a class="yt-timestamp" data-t="00:06:41">[00:06:41]</a> <a class="yt-timestamp" data-t="00:06:45">[00:06:45]</a>. If neurons connected with positive [[Weight and bias adjustment in neural networks | weights]] become brighter, and those with negative [[Weight and bias adjustment in neural networks | weights]] become dimmer, the target neuron will become more active <a class="yt-timestamp" data-t="00:06:49">[00:06:49]</a>. These desired changes are proportional to the size of the corresponding [[Weight and bias adjustment in neural networks | weights]] <a class="yt-timestamp" data-t="00:07:02">[00:07:02]</a> <a class="yt-timestamp" data-t="00:07:06">[00:07:06]</a>.
    *   All output neurons have desires for how the second-to-last [[Neural network structure and layers | layer]] should change <a class="yt-timestamp" data-t="00:07:29">[00:07:29]</a> <a class="yt-timestamp" data-t="00:07:33">[00:07:33]</a>. These desires are added together, proportional to corresponding [[Weight and bias adjustment in neural networks | weights]] and the amount each neuron needs to change <a class="yt-timestamp" data-t="00:07:42">[00:07:42]</a> <a class="yt-timestamp" data-t="00:07:47">[00:07:47]</a>. This is where the concept of propagating backwards originates <a class="yt-timestamp" data-t="00:08:01">[00:08:01]</a> <a class="yt-timestamp" data-t="00:08:05">[00:08:05]</a>.
    *   Once these desired nudges for a [[Neural network structure and layers | layer]] are established, the same recursive process is applied to the [[Weight and bias adjustment in neural networks | weights and biases]] of preceding [[Neural network structure and layers | layers]], moving backward through the network <a class="yt-timestamp" data-t="00:08:14">[00:08:14]</a> <a class="yt-timestamp" data-t="00:08:17">[00:08:17]</a>.

### Averaging Desired Changes

The process described above for a single training example is repeated for every other training example <a class="yt-timestamp" data-t="00:08:28">[00:08:28]</a> <a class="yt-timestamp" data-t="00:08:33">[00:08:33]</a> <a class="yt-timestamp" data-t="00:08:44">[00:08:44]</a>. The desired changes from each example are then averaged together <a class="yt-timestamp" data-t="00:08:49">[00:08:49]</a> <a class="yt-timestamp" data-t="00:08:53">[00:08:53]</a>. This collection of averaged nudges represents the negative gradient of the [[Cost function and its role in neural networks | cost function]] <a class="yt-timestamp" data-t="00:09:01">[00:09:01]</a> <a class="yt-timestamp" data-t="00:09:05">[00:09:05]</a>.

## Stochastic Gradient Descent

While a true gradient descent step would involve averaging the influence of every single training example, which is computationally intensive <a class="yt-timestamp" data-t="00:09:33">[00:09:33]</a> <a class="yt-timestamp" data-t="00:09:38">[00:09:38]</a>, a more common and efficient approach is **stochastic gradient descent** <a class="yt-timestamp" data-t="00:10:31">[00:10:31]</a>:
*   Training data is randomly shuffled and divided into mini-batches, often around 100 examples <a class="yt-timestamp" data-t="00:09:45">[00:09:45]</a> <a class="yt-timestamp" data-t="00:09:48">[00:09:48]</a>.
*   Each step of the gradient descent is computed based on one mini-batch <a class="yt-timestamp" data-t="00:09:52">[00:09:52]</a>.
*   Although this isn't the exact gradient for the entire dataset, it provides a good approximation and significantly speeds up computation <a class="yt-timestamp" data-t="00:09:56">[00:09:56]</a> <a class="yt-timestamp" data-t="00:10:00">[00:10:00]</a> <a class="yt-timestamp" data-t="00:10:09">[00:10:09]</a>.

Repeatedly processing all mini-batches and making these adjustments allows the network to converge towards a local minimum of the [[Cost function and its role in neural networks | cost function]], improving its performance on training examples <a class="yt-timestamp" data-t="00:11:14">[00:11:14]</a> <a class="yt-timestamp" data-t="00:11:17">[00:11:17]</a>.

## Summary of Backpropagation

[[Backpropagation algorithm walkthrough | Backpropagation]] is the algorithm that determines how each individual training example influences the necessary "nudges" for [[Weight and bias adjustment in neural networks | weights and biases]] <a class="yt-timestamp" data-t="00:10:40">[00:10:40]</a> <a class="yt-timestamp" data-t="00:10:44">[00:10:44]</a>. These nudges indicate not just direction (up or down) but also the relative proportions that lead to the most rapid decrease in the [[Cost function and its role in neural networks | cost]] <a class="yt-timestamp" data-t="00:10:47">[00:10:47]</a> <a class="yt-timestamp" data-t="00:10:50">[00:10:50]</a>. While a full gradient descent step involves averaging these changes across all training examples, stochastic gradient descent uses mini-batches for computational efficiency <a class="yt-timestamp" data-t="00:10:56">[00:10:56]</a> <a class="yt-timestamp" data-t="00:11:00">[00:11:00]</a> <a class="yt-timestamp" data-t="00:11:04">[00:11:04]</a> <a class="yt-timestamp" data-t="00:11:08">[00:11:08]</a>.

## Further Exploration

For those interested in the underlying [[Calculus in the context of neural networks | calculus]] of [[Backpropagation algorithm walkthrough | backpropagation]], the mathematical details can provide a deeper understanding of these concepts <a class="yt-timestamp" data-t="00:00:17">[00:00:17]</a> <a class="yt-timestamp" data-t="00:00:20">[00:00:20]</a> <a class="yt-timestamp" data-t="00:11:44">[00:11:44]</a> <a class="yt-timestamp" data-t="00:11:48">[00:11:48]</a>.

A crucial aspect for the success of [[Backpropagation algorithm walkthrough | backpropagation]] and other machine learning algorithms is the availability of sufficient training data <a class="yt-timestamp" data-t="00:11:57">[00:11:57]</a> <a class="yt-timestamp" data-t="00:12:00">[00:12:00]</a> <a class="yt-timestamp" data-t="00:12:04">[00:12:04]</a>. Datasets like the MNIST database, containing many human-labeled examples of handwritten digits, are invaluable <a class="yt-timestamp" data-t="00:12:06">[00:12:06]</a> <a class="yt-timestamp" data-t="00:12:10">[00:12:10]</a> <a class="yt-timestamp" data-t="00:12:15">[00:12:15]</a>. Obtaining sufficient labeled training data is a common challenge in machine learning <a class="yt-timestamp" data-t="00:12:19">[00:12:19]</a>.