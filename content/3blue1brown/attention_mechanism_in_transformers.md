---
title: Attention Mechanism in Transformers
videoId: wjZofJX0v4M
---

From: [[3blue1brown]] <br/> 
The [[Transformers and attention mechanism | Attention Mechanism]] is a pivotal component within [[Transformers and attention mechanism | transformer]] architectures, which are a specific kind of [[Neural Networks and Transformers | neural network]] and machine learning model, serving as the core invention behind the recent surge in AI advancements <a class="yt-timestamp" data-t="00:00:20">[00:00:20]</a>.

### What is a Transformer?
The term GPT, as in ChatGPT, stands for [[generative_pretrained_transformers | Generative Pretrained Transformer]] <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a>. These bots are designed to generate new text <a class="yt-timestamp" data-t="00:00:05">[00:00:05]</a>. The "pretrained" aspect indicates that the model has undergone extensive learning from a massive amount of data, with the potential for further fine-tuning on specific tasks <a class="yt-timestamp" data-t="00:00:09">[00:00:09]</a>. The "transformer" part is the fundamental innovation driving this technology <a class="yt-timestamp" data-t="00:00:20">[00:00:20]</a>.

[[Transformers and attention mechanism | Transformers]] can be adapted for various tasks, including:
*   Converting audio into transcripts <a class="yt-timestamp" data-t="00:00:47">[00:00:47]</a>.
*   Generating synthetic speech from text <a class="yt-timestamp" data-t="00:00:51">[00:00:51]</a>.
*   Creating images from text descriptions, as seen in tools like DALL-E and Midjourney <a class="yt-timestamp" data-t="00:00:56">[00:00:56]</a>.
*   The original [[Transformers and attention mechanism | transformer]], introduced by Google in 2017, was specifically designed for translating text between languages <a class="yt-timestamp" data-t="00:01:13">[00:01:13]</a>.

The variant of [[Transformers and attention mechanism | transformer]] that underpins tools like ChatGPT is trained to receive a piece of text (and potentially accompanying images or sound) and predict the subsequent content <a class="yt-timestamp" data-t="00:01:22">[00:01:22]</a>. This prediction manifests as a probability distribution across various potential text chunks <a class="yt-timestamp" data-t="00:01:38">[00:01:38]</a>. A prediction model can generate longer text by taking an initial snippet, sampling a random chunk from its generated distribution, appending it, and repeating the process <a class="yt-timestamp" data-t="00:01:45">[00:01:45]</a>. This iterative prediction and sampling process is what enables models like ChatGPT to produce text word by word <a class="yt-timestamp" data-t="00:02:41">[00:02:41]</a>. While smaller models like GPT-2 might produce nonsensical stories with this method <a class="yt-timestamp" data-t="00:02:12">[00:02:12]</a>, larger models like GPT-3 can generate surprisingly coherent narratives <a class="yt-timestamp" data-t="00:02:26">[00:02:26]</a>.

### Data Flow Through a Transformer
Data flows through a [[Transformers and attention mechanism | transformer]] in several key steps:

1.  **Tokenization**: The input is first broken down into smaller units called tokens, which can be words, parts of words, or common character combinations for text <a class="yt-timestamp" data-t="00:03:19">[00:03:19]</a>. For images or sound, tokens might be small patches or chunks <a class="yt-timestamp" data-t="00:03:30">[00:03:30]</a>.
2.  **Vector Association (Embeddings)**: Each token is converted into a vector (a list of numbers) that encodes its meaning <a class="yt-timestamp" data-t="00:03:37">[00:03:37]</a>. In this high-dimensional space, words with similar meanings tend to have vectors that are close to each other <a class="yt-timestamp" data-t="00:03:45">[00:03:45]</a>. This process of converting words into vectors is called "embedding" <a class="yt-timestamp" data-t="00:13:36">[00:13:36]</a>. The embedding matrix, whose columns define the vector for each word, represents the first set of learned weights in the model <a class="yt-timestamp" data-t="00:15:50">[00:15:50]</a>. In GPT-3, the vocabulary size is 50,257 tokens, and the embedding dimension is 12,288, resulting in about 617 million weights for this matrix alone <a class="yt-timestamp" data-t="00:18:00">[00:18:00]</a>.
3.  **[[Transformers and attention mechanism | Attention Block]]**: The sequence of vectors then passes through an operation known as an [[Transformers and attention mechanism | attention block]] <a class="yt-timestamp" data-t="00:03:55">[00:03:55]</a>. This block allows the vectors to "talk to each other" and exchange information to update their values <a class="yt-timestamp" data-t="00:03:58">[00:03:58]</a>. Its role is to determine which words in a given context are relevant for updating the meanings of other words and how those meanings should be adjusted <a class="yt-timestamp" data-t="00:04:12">[00:04:12]</a>. For example, it differentiates the meaning of "model" in "machine learning model" versus "fashion model" <a class="yt-timestamp" data-t="00:04:04">[00:04:04]</a>. The meaning of a word is entirely encoded in the entries of these vectors <a class="yt-timestamp" data-t="00:04:22">[00:04:22]</a>.
4.  **[[Multiheaded attention in transformers | Multi-layer Perceptron]] (Feed-Forward Layer)**: After the attention block, the vectors proceed through another operation, often called a [[Multiheaded attention in transformers | multi-layer perceptron]] or feed-forward layer <a class="yt-timestamp" data-t="00:04:29">[00:04:29]</a>. In this step, the vectors do not interact with each other but undergo the same operation in parallel <a class="yt-timestamp" data-t="00:04:38">[00:04:38]</a>. This step can be conceptualized as asking a series of questions about each vector and updating them based on the answers <a class="yt-timestamp" data-t="00:04:45">[00:04:45]</a>.
5.  **Repetition**: This process of alternating between [[Transformers and attention mechanism | attention blocks]] and [[Multiheaded attention in transformers | multi-layer perceptron]] blocks repeats multiple times <a class="yt-timestamp" data-t="00:05:13">[00:05:13]</a>. The goal is for the essential meaning of the passage to be distilled into the final vector in the sequence <a class="yt-timestamp" data-t="00:05:20">[00:05:20]</a>.
6.  **Prediction (Unembedding)**: An operation is performed on the last vector to generate a probability distribution over all possible next tokens <a class="yt-timestamp" data-t="00:05:28">[00:05:28]</a>. This involves using an "Unembedding matrix" (WU) that maps the last vector to a list of 50,000 values (one for each vocabulary token) <a class="yt-timestamp" data-t="00:20:58">[00:20:58]</a>. This matrix also contributes about 617 million parameters <a class="yt-timestamp" data-t="00:21:56">[00:21:56]</a>.
7.  **Softmax Function**: Finally, a [[Activation functions in neural networks | softmax function]] normalizes these values into a valid probability distribution, ensuring each value is between 0 and 1 and all values sum to 1 <a class="yt-timestamp" data-t="00:23:00">[00:23:00]</a>. The largest values from the preceding step (often called "logits") end up with probabilities closest to 1 <a class="yt-timestamp" data-t="00:23:04">[00:23:04]</a>. A "temperature" parameter (T) can be added to the softmax function to control the randomness of sampling: higher temperatures lead to a more uniform distribution and potentially more creative but less coherent outputs, while lower temperatures make the distribution sharper, favoring the most predictable words <a class="yt-timestamp" data-t="00:24:04">[00:24:04]</a>.

### Mathematical Foundations
Most of the operations within [[Transformers and attention mechanism | transformers]] are complex series of [[Matrix representation of transformations | matrix multiplications]] <a class="yt-timestamp" data-t="00:04:54">[00:04:54]</a>. The learned parameters of the model, known as weights, are organized into these matrices <a class="yt-timestamp" data-t="00:10:17">[00:10:17]</a>. For example, GPT-3's 175 billion parameters are arranged into nearly 28,000 distinct matrices across eight categories <a class="yt-timestamp" data-t="00:11:06">[00:11:06]</a>.

A key concept for understanding the [[Transformers and attention mechanism | attention mechanism]] is the dot product of two vectors <a class="yt-timestamp" data-t="00:16:37">[00:16:37]</a>. The dot product measures how well two vectors align: it's positive if they point in similar directions, zero if they are perpendicular, and negative if they point in opposite directions <a class="yt-timestamp" data-t="00:16:55">[00:16:55]</a>. This mathematical operation allows the model to quantify semantic similarity between word embeddings, which is crucial for contextual understanding within the [[Transformers and attention mechanism | attention block]].

The understanding of word embeddings, the [[Activation functions in neural networks | softmax function]], how dot products measure similarity, and the principle that computations primarily involve [[Matrix representation of transformations | matrix multiplication]] with tunable parameters are all foundational for grasping the workings of the [[Transformers and attention mechanism | attention mechanism]] <a class="yt-timestamp" data-t="00:26:03">[00:26:03]</a>.