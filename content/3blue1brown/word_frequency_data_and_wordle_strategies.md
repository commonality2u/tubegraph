---
title: Word frequency data and Wordle strategies
videoId: v68zYyaEmEA
---

From: [[3blue1brown]] <br/> 

The game [[wordle_game_mechanics | Wordle]] has gained significant popularity, serving as a practical example for lessons in [[information_theory_and_entropy_in_Wordle | information theory]] and [[information_theory_and_entropy_in_Wordle | entropy]] <a class="yt-timestamp" data-t="00:00:08">[00:00:08]</a>. Programmers, in particular, have been drawn to developing [[algorithms_for_optimal_wordle_play | algorithms]] to play the game as optimally as possible <a class="yt-timestamp" data-t="00:00:19">[00:00:19]</a>. Initial approaches to [[wordle_solving_strategies_and_information_theory | solving Wordle]] often involve maximizing the expected information gained from each guess, a concept known as [[information_theory_and_entropy_in_Wordle | entropy]] <a class="yt-timestamp" data-t="00:14:05">[00:14:05]</a>.

## Incorporating Word Commonality

An early [[wordle_solving_strategies_and_information_theory | Wordle strategy]] might involve looking at the relative frequencies of letters in the English language to choose an [[optimal_wordle_opening_guesses_and_their_analysis | opening guess]] <a class="yt-timestamp" data-t="00:02:50">[00:02:50]</a>. For instance, "other" followed by "nails" was a favored initial pair of guesses, aiming to hit frequent letters <a class="yt-timestamp" data-t="00:02:59">[00:02:59]</a>. However, this approach doesn't account for letter order <a class="yt-timestamp" data-t="00:03:21">[00:03:21]</a>, nor does it quantify the quality of a guess like "weary," which contains less common letters such as W and Y <a class="yt-timestamp" data-t="00:03:32">[00:03:32]</a>.

Initially, a basic [[algorithms_for_optimal_wordle_play | Wordle bot]] would assume all possible words are equally likely answers <a class="yt-timestamp" data-t="00:05:58">[00:05:58]</a>. This first pass calculates the [[information_theory_and_entropy_in_Wordle | entropy]] for each of the ~13,000 valid guesses and selects the one that maximizes it <a class="yt-timestamp" data-t="00:14:09">[00:14:09]</a>. This method, applied recursively, yields an average score of about 4.124 in simulations against the 2315 actual [[wordle_game_mechanics | Wordle]] answers <a class="yt-timestamp" data-t="00:17:54">[00:17:54]</a>. While not bad, the goal is often to achieve more "birdies" (guesses in 3) <a class="yt-timestamp" data-t="00:18:02">[00:18:02]</a>. The "obvious low-hanging fruit" for improvement is to incorporate the commonality of words <a class="yt-timestamp" data-t="00:18:11">[00:18:11]</a>.

## Sourcing and Applying Word Frequency Data

To incorporate word commonality, a list of relative frequencies for English words can be used <a class="yt-timestamp" data-t="00:18:25">[00:18:25]</a>. One source is Mathematica's word frequency data function, which in turn draws from the Google Books English Ngram public dataset <a class="yt-timestamp" data-t="00:18:30">[00:18:30]</a>. This data reveals the most common five-letter words, such as "which" and "there" <a class="yt-timestamp" data-t="00:18:40">[00:18:40]</a>.

However, simply making the probability proportional to frequency isn't ideal, as both "which" and "braid" are common enough to be considered, despite a thousand-fold difference in raw frequency <a class="yt-timestamp" data-t="00:19:07">[00:19:07]</a>. Instead, a more binary cutoff is desired <a class="yt-timestamp" data-t="00:19:18">[00:19:18]</a>. This is achieved by sorting the words by frequency and applying a sigmoid function <a class="yt-timestamp" data-t="00:19:25">[00:19:25]</a>. The sigmoid function produces an output that is largely binary (0 or 1) but with a smooth transition, allowing for a probability assignment to each word based on its position in the sorted list <a class="yt-timestamp" data-t="00:19:33">[00:19:33]</a>. The parameters of the sigmoid function (steepness and cutoff point) can be adjusted, often by intuitively determining a window where about half the words are plausible answers <a class="yt-timestamp" data-t="00:20:02">[00:20:02]</a>.

## Entropy with Non-Uniform Distributions

With a non-uniform distribution of probabilities across words, [[information_theory_and_entropy_in_Wordle | entropy]] becomes an even more useful measurement <a class="yt-timestamp" data-t="00:20:19">[00:20:19]</a>.

For example, if four words are equally likely, the [[information_theory_and_entropy_in_Wordle | entropy]] is 2 bits (log base 2 of 4) <a class="yt-timestamp" data-t="00:20:50">[00:20:50]</a>. If there are 16 matching words, but 12 of them are very obscure (e.g., 1 in 1000 probability), the actual uncertainty or [[information_theory_and_entropy_in_Wordle | entropy]] is not log base 2 of 16 (4 bits). Instead, it would be closer to 2 bits (e.g., 2.11 bits), reflecting that the true uncertainty is primarily among the more common words <a class="yt-timestamp" data-t="00:21:46">[00:21:46]</a>. This demonstrates how entropy can simultaneously measure both the flatness of a distribution and the number of possibilities <a class="yt-timestamp" data-t="00:12:54">[00:12:54]</a> <a class="yt-timestamp" data-t="00:13:17">[00:13:17]</a>.

This leads to two applications for [[information_theory_and_entropy_in_Wordle | entropy]] in [[wordle_solving_strategies_and_information_theory | Wordle]]:
1.  **Expected Information from a Guess**: Measures the expected information gained from a given guess, taking into account the unequal weighting of words <a class="yt-timestamp" data-t="00:22:05">[00:22:05]</a>.
2.  **Remaining Uncertainty**: Quantifies the remaining uncertainty among all possible words after a guess <a class="yt-timestamp" data-t="00:22:09">[00:22:09]</a>.

## Wordle Bot Version 2.0 and Performance

In "version two" of the [[algorithms_for_optimal_wordle_play | Wordle bot]], the entropy calculation (expected information of a guess) now incorporates the probability that a given word is the actual answer <a class="yt-timestamp" data-t="00:23:33">[00:23:33]</a>.

The bot also maintains a model of the probability for each word being the actual answer, influencing its top picks <a class="yt-timestamp" data-t="00:23:47">[00:23:47]</a>. The "uncertainty value" (in bits) on the left of the display is no longer merely redundant with the number of possible matches. It reflects the equivalent number of equally likely outcomes <a class="yt-timestamp" data-t="00:24:07">[00:24:07]</a> <a class="yt-timestamp" data-t="00:24:24">[00:24:24]</a>. This means the [[algorithms_for_optimal_wordle_play | algorithm]] is "less uncertain" because it discounts obscure words <a class="yt-timestamp" data-t="00:24:34">[00:24:34]</a>.

For end-game strategy, the [[algorithms_for_optimal_wordle_play | bot]] doesn't just maximize [[information_theory_and_entropy_in_Wordle | entropy]]; it considers the expected final score of the game <a class="yt-timestamp" data-t="00:25:23">[00:25:23]</a>. This involves estimating future guesses based on the remaining uncertainty. A function 'f' was created, derived from simulation data of version one, to associate uncertainty (bits) with an expected number of guesses <a class="yt-timestamp" data-t="00:26:14">[00:26:14]</a> <a class="yt-timestamp" data-t="00:27:11">[00:27:11]</a>. For instance, one bit of uncertainty (two possibilities) typically requires about 1.5 more guesses on average <a class="yt-timestamp" data-t="00:27:15">[00:27:15]</a>.

Simulations with this version 2.0 [[algorithms_for_optimal_wordle_play | algorithm]] playing against all 2315 actual [[wordle_game_mechanics | Wordle]] answers show an average score of about 3.6 <a class="yt-timestamp" data-t="00:28:04">[00:28:04]</a>. This is an improvement over the initial naive method. However, this version occasionally fails to solve the puzzle within six guesses <a class="yt-timestamp" data-t="00:28:09">[00:28:09]</a>, presumably due to the tradeoff between maximizing information and aiming for the win <a class="yt-timestamp" data-t="00:28:15">[00:28:15]</a>.

## Further Optimizations and Limitations

Further sophistication, such as incorporating the true list of [[wordle_game_mechanics | Wordle]] answers and performing a two-step lookahead search for expected information, can yield an average score of around 3.43 <a class="yt-timestamp" data-t="00:28:35">[00:28:35]</a> <a class="yt-timestamp" data-t="00:28:52">[00:28:52]</a>. With such advanced strategies, "Crane" emerges as a top [[optimal_wordle_opening_guesses_and_their_analysis | opening guess]] <a class="yt-timestamp" data-t="00:29:08">[00:29:08]</a>.

Even with [[optimal_wordle_opening_guesses_and_their_analysis | perfectly optimal play]], it's highly unlikely to consistently achieve an average score as low as 3 <a class="yt-timestamp" data-t="00:29:44">[00:29:44]</a>. The initial uncertainty for the true [[wordle_game_mechanics | Wordle]] list is over 11 bits <a class="yt-timestamp" data-t="00:29:17">[00:29:17]</a>. A brute-force search suggests the maximum expected information after the first two guesses is around 10 bits <a class="yt-timestamp" data-t="00:29:26">[00:29:26]</a>, leaving approximately one bit of uncertainty, equivalent to two possible answers <a class="yt-timestamp" data-t="00:29:34">[00:29:34]</a>. This implies that there isn't enough information available to guarantee the answer in the third guess every time <a class="yt-timestamp" data-t="00:29:51">[00:29:51]</a>.