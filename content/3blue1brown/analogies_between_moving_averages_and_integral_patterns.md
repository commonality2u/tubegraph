---
title: Analogies between moving averages and integral patterns
videoId: 851U557j6HE
---

From: [[3blue1brown]] <br/> 

Sometimes, mathematical patterns can seem to defy expectation, appearing stable for a long time before subtly changing. This phenomenon is vividly illustrated by certain integral computations, which surprisingly align with patterns observed in a sequence of moving averages <a class="yt-timestamp" data-t="00:00:10">[00:00:10]</a>.

## The Borwein Integrals: An Unexpected Stability

A sequence of computations, involving integrals of the sinc function, appears to consistently equal pi for several iterations <a class="yt-timestamp" data-t="00:00:14">[00:00:14]</a>. This stability, however, is not infinite; at a certain point, the value subtly deviates, becoming "just barely, barely less than pi" <a class="yt-timestamp" data-t="00:00:27">[00:00:27]</a>.

The primary function in these integrals is `sinc(x)`, defined as `sine(x) / x` <a class="yt-timestamp" data-t="00:00:41">[00:00:41]</a>. This function is often visualized as an oscillating sine curve that is "squished down" by `1/x` as it moves away from zero <a class="yt-timestamp" data-t="00:00:53">[00:00:53]</a>. To maintain continuity at `x = 0`, `sinc(0)` is redefined as `1` <a class="yt-timestamp" data-t="00:01:15">[00:01:15]</a>.

The integral of `sinc(x)` from negative infinity to infinity, representing the signed area between the curve and the x-axis, remarkably evaluates to exactly pi <a class="yt-timestamp" data-t="00:01:24">[00:01:24]</a>. This result is not straightforward to obtain using standard [[derivatives_and_integrals | calculus]] tools <a class="yt-timestamp" data-t="00:01:50">[00:01:50]</a>.

### The Expanding Product Sequence

The sequence of integrals that exhibits this peculiar pattern involves multiplying progressively more stretched versions of the sinc function <a class="yt-timestamp" data-t="00:01:59">[00:01:59]</a>:
*   The first integral is `sinc(x)` <a class="yt-timestamp" data-t="00:01:24">[00:01:24]</a>.
*   The second integral includes `sinc(x/3)`, stretching the graph horizontally by a factor of 3, and multiplying it with the original `sinc(x)` <a class="yt-timestamp" data-t="00:02:03">[00:02:03]</a>.
*   Subsequent iterations multiply by `sinc(x/5)`, `sinc(x/7)`, and so on, using new odd numbers for the stretching factor <a class="yt-timestamp" data-t="00:02:31">[00:02:31]</a>.

Intuitively, one might expect these multiplications to significantly alter the signed area, potentially making it smaller since most parts of the function are multiplied by values less than 1 <a class="yt-timestamp" data-t="00:03:00">[00:03:00]</a>. However, the integral miraculously continues to equal pi for several steps <a class="yt-timestamp" data-t="00:02:24">[00:02:24]</a>.

The pattern eventually breaks when the stretching factor reaches `1/15` <a class="yt-timestamp" data-t="00:03:16">[00:03:16]</a>. At this point, the integral becomes slightly less than pi, by a "tiniest tiny amount" <a class="yt-timestamp" data-t="00:03:16">[00:03:16]</a>. This is a real phenomenon, not a numerical error, and was described in a paper by Jonathan and David Borwein <a class="yt-timestamp" data-t="00:03:21">[00:03:21]</a>.

### Extended Stability with a Cosine Factor

Adding an extra factor, `2 * cosine(x)`, to these integrals further extends their stability <a class="yt-timestamp" data-t="00:03:58">[00:03:58]</a>. The integral continues to equal pi for much longer, only breaking when the stretching factor reaches `1/113` <a class="yt-timestamp" data-t="00:04:09">[00:04:09]</a>. Again, the deviation is minuscule <a class="yt-timestamp" data-t="00:04:15">[00:04:15]</a>.

## The Moving Average Analogy

To understand this peculiar behavior, an analogy can be drawn from a seemingly unrelated process involving moving averages <a class="yt-timestamp" data-t="00:04:28">[00:04:28]</a>.

Consider a simple `rect(x)` function, which equals 1 for `x` between -0.5 and 0.5, and 0 otherwise <a class="yt-timestamp" data-t="00:04:52">[00:04:52]</a>. Let this be `f1(x)`.

A sequence of new functions (`f2(x)`, `f3(x)`, etc.) is generated by taking a moving average of the previous function <a class="yt-timestamp" data-t="00:05:07">[00:05:07]</a>:
*   `f2(x)` is the moving average of `f1(x)` with a window width of `1/3` <a class="yt-timestamp" data-t="00:05:15">[00:05:15]</a>. This process creates a smoothed version of the previous function, but importantly, it retains a central plateau where the value is still 1 <a class="yt-timestamp" data-t="00:06:00">[00:06:00]</a>. The width of this plateau is `1 - 1/3` <a class="yt-timestamp" data-t="00:06:33">[00:06:33]</a>.
*   `f3(x)` uses a window width of `1/5` on `f2(x)` <a class="yt-timestamp" data-t="00:06:49">[00:06:49]</a>. The plateau further shrinks to `(1 - 1/3) - 1/5` <a class="yt-timestamp" data-t="00:07:15">[00:07:15]</a>.
*   This continues with window widths of `1/7`, `1/9`, and so on <a class="yt-timestamp" data-t="00:07:38">[00:07:38]</a>.

The value of each function at `x = 0` (`f_n(0)`) is the quantity of interest <a class="yt-timestamp" data-t="00:06:37">[00:06:37]</a>. Initially, as long as the window is entirely contained within the previous function's plateau of 1, `f_n(0)` remains exactly 1 <a class="yt-timestamp" data-t="00:06:00">[00:06:00]</a>.

The pattern breaks when the sum of the reciprocals of the odd numbers (`1/3 + 1/5 + 1/7 + ...`) becomes greater than the initial plateau width (which is 1) <a class="yt-timestamp" data-t="00:08:20">[00:08:20]</a>. This occurs when including `1/15`, making the sum greater than 1 <a class="yt-timestamp" data-t="00:08:29">[00:08:29]</a>. At this point, the moving average window at `x=0` can no longer be entirely within a plateau of 1, causing `f_n(0)` to fall slightly below 1 <a class="yt-timestamp" data-t="00:08:10">[00:08:10]</a>.

This phenomenon is "analogous, and... more than just analogous" to the integral patterns <a class="yt-timestamp" data-t="00:09:00">[00:09:00]</a>. The constant by which the moving average process falls short of 1 is precisely the factor that multiplies pi in the corresponding integral <a class="yt-timestamp" data-t="00:09:10">[00:09:10]</a>.

When the integral includes the `2 * cosine(x)` term, the analogy starts with an initial `rect` function plateau of length 2 (from `x = -1` to `x = 1`) <a class="yt-timestamp" data-t="00:09:36">[00:09:36]</a>. This means it takes much longer for the sum of reciprocals to exceed the initial plateau length, explaining why the pattern extends until `1/113` <a class="yt-timestamp" data-t="00:09:51">[00:09:51]</a>.

The specific odd numbers (`1/3, 1/5, 1/7`) are not inherently special; any sequence of positive numbers for which the sum eventually exceeds the initial plateau width would cause a similar breakdown <a class="yt-timestamp" data-t="00:10:12">[00:10:12]</a>.

## The Mathematical Connection: [[Fourier transforms and convolutions in integral computation | Fourier Transforms]] and [[Fourier transforms and convolutions in integral computation | Convolutions]]

The deep connection between these two seemingly disparate phenomena lies in the realms of [[Fourier transforms and convolutions in integral computation | Fourier transforms]] and [[Fourier transforms and convolutions in integral computation | convolutions]] <a class="yt-timestamp" data-t="00:10:41">[00:10:41]</a>.

To facilitate analysis, the `sinc(x)` function is often normalized to `sinc(pi*x)`, which has an integral value of 1 <a class="yt-timestamp" data-t="00:11:29">[00:11:29]</a>.

### The [[Fourier transforms and convolutions in integral computation | Fourier Transform]] of Sinc and Rect

The crucial insight is that the `sinc(pi*x)` function is related to the `rect(x)` function via a [[Fourier transforms and convolutions in integral computation | Fourier transform]] <a class="yt-timestamp" data-t="00:12:18">[00:12:18]</a>.
<a class="yt-timestamp" data-t="00:12:32">[00:12:32]</a>
> "if you want to break down a function as the sum of a bunch of pure frequencies... the Fourier transform will tell you all the strength and phases for all those constituent parts."

For symmetric functions like `sinc` and `rect`, the [[Fourier transforms and convolutions in integral computation | Fourier transform]] of one is the other, and vice versa <a class="yt-timestamp" data-t="00:13:09">[00:13:09]</a>. More generally, stretching `sinc(x)` by a factor `k` corresponds to a stretched and squished version of the `rect` function in its [[Fourier transforms and convolutions in integral computation | Fourier transform]] <a class="yt-timestamp" data-t="00:13:17">[00:13:17]</a>.

One key property of [[Fourier transforms and convolutions in integral computation | Fourier transforms]] is that the integral of a function from negative infinity to infinity is equivalent to evaluating its [[Fourier transforms and convolutions in integral computation | Fourier transform]] at zero <a class="yt-timestamp" data-t="00:13:55">[00:13:55]</a>.
*   This explains why the integral of `sinc(pi*x)` is 1: it's equivalent to `rect(0)`, which is 1 <a class="yt-timestamp" data-t="00:14:39">[00:14:39]</a>.

### The [[Fourier transforms and convolutions in integral computation | Convolution Theorem]]

The connection between the integral sequence and the moving average sequence is solidified by the [[Fourier transforms and convolutions in integral computation | convolution theorem]] <a class="yt-timestamp" data-t="00:15:15">[00:15:15]</a>:
<a class="yt-timestamp" data-t="00:15:11">[00:15:11]</a>
> "if you have two different functions and you take their product, and then you take the sum of the Fourier transform of that product, it will be the same thing as if you individually took the Fourier transforms of your original function and then combined them using a new kind of operation... known as a convolution."

For the specific `rect` functions involved in the moving average process, a [[Fourier transforms and convolutions in integral computation | convolution]] *is* precisely the moving average operation <a class="yt-timestamp" data-t="00:15:31">[00:15:31]</a>.

Therefore, the multiplication of `sinc` functions in the integral domain (which corresponds to calculating an area) translates via the [[Fourier transforms and convolutions in integral computation | Fourier transform]] and [[Fourier transforms and convolutions in integral computation | convolution theorem]] to the operation of repeated moving averages in the frequency domain (which corresponds to evaluating a function at zero) <a class="yt-timestamp" data-t="00:15:55">[00:15:55]</a>. This explains the observed stability of the values and their eventual subtle breakdown, as the plateaus in the moving average analogy shrink and eventually disappear <a class="yt-timestamp" data-t="00:16:03">[00:16:03]</a>.

[!NOTE]
[[Fourier transforms and convolutions in integral computation | Fourier transforms]] provide a "shift in perspective" that can make complex problems appear simpler <a class="yt-timestamp" data-t="00:16:37">[00:16:37]</a>. The [[Fourier transforms and convolutions in integral computation | convolution theorem]] itself has broad [[application_of_integrals_in_real_world_problems | applications]], including efficient algorithms for multiplying large numbers <a class="yt-timestamp" data-t="00:16:49">[00:16:49]</a>.