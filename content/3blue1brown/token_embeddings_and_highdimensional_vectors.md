---
title: Token embeddings and highdimensional vectors
videoId: eMlx5fFNoYc
---

From: [[3blue1brown]] <br/> 

In large language models (LLMs) and other modern AI tools, a key piece of technology is the transformer <a class="yt-timestamp" data-t="00:00:04">[00:00:04]</a>. The initial step within a transformer involves representing text through **token embeddings**, which are [[word_embeddings_and_vectors | high-dimensional vectors]] <a class="yt-timestamp" data-t="00:00:51">[00:00:51]</a>.

## What are Token Embeddings?
Text input into the model is first broken down into "tokens," which are typically words or parts of words <a class="yt-timestamp" data-t="00:00:36">[00:00:36]</a>, <a class="yt-timestamp" data-t="00:00:40">[00:00:40]</a>. Each of these tokens is then associated with a [[word_embeddings_and_vectors | high-dimensional vector]], known as its embedding <a class="yt-timestamp" data-t="00:00:51">[00:00:51]</a>.

Initially, this token embedding functions like a lookup table, meaning it contains no information about the context of the word within a given text <a class="yt-timestamp" data-t="00:02:21">[00:02:21]</a>. It primarily encodes the meaning of the individual word without context <a class="yt-timestamp" data-t="00:04:59">[00:04:59]</a>. However, these vectors also encode the position of the word within the text <a class="yt-timestamp" data-t="00:05:05">[00:05:05]</a>, allowing the entries of the vector to indicate both the word itself and its location in the context <a class="yt-timestamp" data-t="00:05:11">[00:05:11]</a>. Embeddings are often denoted by the letter 'e' <a class="yt-timestamp" data-t="00:05:19">[00:05:19]</a>.

### High-Dimensional Space and Semantic Meaning
A crucial concept is that directions within this [[abstract_vector_spaces | high-dimensional space]] of all possible embeddings correspond to semantic meaning <a class="yt-timestamp" data-t="00:00:57">[00:00:57]</a>, <a class="yt-timestamp" data-t="00:01:02">[00:01:02]</a>. For example, moving in a specific direction in this space can transform the embedding of a masculine noun into that of its corresponding feminine noun, illustrating how direction can correspond to attributes like gender <a class="yt-timestamp" data-t="00:01:07">[00:01:07]</a>, <a class="yt-timestamp" data-t="00:01:11">[00:01:11]</a>. This principle extends to countless other aspects of a word's meaning <a class="yt-timestamp" data-t="00:01:20">[00:01:20]</a>, <a class="yt-timestamp" data-t="00:01:23">[00:01:23]</a>.

### Updating Embeddings for Contextual Meaning
The primary goal of a transformer is to progressively adjust these embeddings so that they reflect richer contextual meaning, rather than just encoding individual words <a class="yt-timestamp" data-t="00:01:28">[00:01:28]</a>, <a class="yt-timestamp" data-t="00:01:32">[00:01:32]</a>, <a class="yt-timestamp" data-t="00:01:35">[00:01:35]</a>. This adjustment happens in subsequent steps of the transformer, allowing surrounding embeddings to pass information to each other <a class="yt-timestamp" data-t="00:02:26">[00:02:26]</a>, <a class="yt-timestamp" data-t="00:02:29">[00:02:29]</a>.

For instance, the word "mole" has different meanings based on context (e.g., "American shrew mole," "one mole of carbon dioxide," "take a biopsy of the mole") <a class="yt-timestamp" data-t="00:02:00">[00:02:00]</a>, <a class="yt-timestamp" data-t="00:02:06">[00:02:06]</a>. While the initial embedding for "mole" is the same in all these cases <a class="yt-timestamp" data-t="00:02:18">[00:02:18]</a>, a well-trained attention block calculates what needs to be added to this generic embedding to shift it towards one of its specific contextual meanings <a class="yt-timestamp" data-t="00:02:42">[00:02:42]</a>, <a class="yt-timestamp" data-t="00:02:47">[00:02:47]</a>.

Similarly, the generic embedding for "tower" (associated with large, tall nouns) <a class="yt-timestamp" data-t="00:02:57">[00:02:57]</a>, <a class="yt-timestamp" data-t="00:03:01">[00:03:01]</a> could be updated if preceded by "Eiffel" to specifically encode the Eiffel Tower, correlating with vectors related to Paris, France, or steel <a class="yt-timestamp" data-t="00:03:04">[00:03:04]</a>, <a class="yt-timestamp" data-t="00:03:06">[00:03:06]</a>, <a class="yt-timestamp" data-t="00:03:10">[00:03:10]</a>. If also preceded by "miniature," the vector would be further updated to no longer correlate with large, tall things <a class="yt-timestamp" data-t="00:03:19">[00:03:19]</a>, <a class="yt-timestamp" data-t="00:03:22">[00:03:22]</a>.

## The Attention Mechanism and Embedding Transformation
The attention block allows the model to transfer information from one embedding to another, even if they are far apart in the text, enabling richer contextual understanding <a class="yt-timestamp" data-t="00:03:32">[00:03:32]</a>, <a class="yt-timestamp" data-t="00:03:35">[00:03:35]</a>, <a class="yt-timestamp" data-t="00:03:39">[00:03:39]</a>. The final vector in a sequence must encode all relevant contextual information to accurately predict the next word <a class="yt-timestamp" data-t="00:04:11">[00:04:11]</a>, <a class="yt-timestamp" data-t="00:04:16">[00:04:16]</a>, <a class="yt-timestamp" data-t="00:04:20">[00:04:20]</a>.

### Queries, Keys, and Values
The [[the_process_of_updating_embeddings_using_attention | process of updating embeddings using attention]] involves generating three types of vectors from each initial token embedding:
1.  **Query (q):** A vector representing a "question" a word is asking about other words in the context (e.g., "Are there any adjectives sitting in front of me?") <a class="yt-timestamp" data-t="00:06:14">[00:06:14]</a>, <a class="yt-timestamp" data-t="00:06:18">[00:06:18]</a>. The query vector has a smaller dimension (e.g., 128) than the embedding vector <a class="yt-timestamp" data-t="00:06:32">[00:06:32]</a>, <a class="yt-timestamp" data-t="00:06:36">[00:06:36]</a>. It is computed by multiplying the embedding by a "query matrix" ($W_Q$) <a class="yt-timestamp" data-t="00:06:42">[00:06:42]</a>, <a class="yt-timestamp" data-t="00:06:46">[00:06:46]</a>.
2.  **Key (k):** A vector representing a potential "answer" to a query (e.g., "Yes, I'm an adjective and I'm in that position") <a class="yt-timestamp" data-t="00:06:22">[00:06:22]</a>, <a class="yt-timestamp" data-t="00:06:25">[00:06:25]</a>. Keys are also produced by multiplying embeddings by a "key matrix" <a class="yt-timestamp" data-t="00:07:47">[00:07:47]</a>, <a class="yt-timestamp" data-t="00:07:51">[00:07:51]</a> and live in the same smaller dimensional space as queries <a class="yt-timestamp" data-t="00:08:07">[00:08:07]</a>. Keys are designed to match queries when they align closely <a class="yt-timestamp" data-t="00:08:12">[00:08:12]</a>.
3.  **Value (v):** A vector representing the information to be added to another embedding if the current word is deemed relevant <a class="yt-timestamp" data-t="00:13:44">[00:13:44]</a>, <a class="yt-timestamp" data-t="00:13:47">[00:13:47]</a>. Value vectors live in the same high-dimensional space as the embeddings themselves <a class="yt-timestamp" data-t="00:14:02">[00:14:02]</a>, <a class="yt-timestamp" data-t="00:14:07">[00:14:07]</a>. They are typically generated by a "value matrix" <a class="yt-timestamp" data-t="00:13:44">[00:13:44]</a>, which in practice is often factored into two smaller matrices: a "value down" matrix (mapping to a smaller space) and a "value up" matrix (mapping back to the embedding space) <a class="yt-timestamp" data-t="00:17:06">[00:17:06]</a>, <a class="yt-timestamp" data-t="00:17:11">[00:17:11]</a>.

### Attention Pattern and Embedding Update
The relevance between each key and query pair is measured using a dot product, which results in a grid of scores <a class="yt-timestamp" data-t="00:08:27">[00:08:27]</a>, <a class="yt-timestamp" data-t="00:08:30">[00:08:30]</a>. For example, the dot products between "fluffy" or "blue" (keys) and "creature" (query) would be large positive numbers if they align <a class="yt-timestamp" data-t="00:08:47">[00:08:47]</a>, <a class="yt-timestamp" data-t="00:08:52">[00:08:52]</a>. These scores are then normalized using a softmax function applied column-wise, transforming them into values between 0 and 1 that sum to 1, effectively becoming weights or a probability distribution <a class="yt-timestamp" data-t="00:09:36">[00:09:36]</a>, <a class="yt-timestamp" data-t="00:09:40">[00:09:40]</a>, <a class="yt-timestamp" data-t="00:09:49">[00:09:49]</a>. This normalized grid is called an **attention pattern** <a class="yt-timestamp" data-t="00:10:15">[00:10:15]</a>.

The attention pattern then guides how information is passed. For each target word (column in the attention pattern), the corresponding value vectors from all other words are multiplied by their respective weights from the attention pattern column <a class="yt-timestamp" data-t="00:14:42">[00:14:42]</a>, <a class="yt-timestamp" data-t="00:14:45">[00:14:45]</a>. These rescaled value vectors are summed to produce a change vector, $\Delta e$, which is then added to the original embedding of the target word <a class="yt-timestamp" data-t="00:15:06">[00:15:06]</a>, <a class="yt-timestamp" data-t="00:15:13">[00:15:13]</a>, <a class="yt-timestamp" data-t="00:15:16">[00:15:16]</a>. This results in a more refined vector that encodes a richer, contextually informed meaning <a class="yt-timestamp" data-t="00:15:19">[00:15:19]</a>, <a class="yt-timestamp" data-t="00:15:23">[00:15:23]</a>.

This entire process is referred to as a **single head of attention** <a class="yt-timestamp" data-t="00:04:47">[00:04:47]</a>, <a class="yt-timestamp" data-t="00:15:44">[00:15:44]</a>.

### Masking
To prevent later words from "giving away" the answer for preceding words during the training process, a technique called **masking** is applied <a class="yt-timestamp" data-t="00:11:49">[00:11:49]</a>, <a class="yt-timestamp" data-t="00:11:52">[00:11:52]</a>, <a class="yt-timestamp" data-t="00:12:26">[00:12:26]</a>. This involves setting the scores for later tokens influencing earlier ones to negative infinity before applying softmax, which effectively turns them into zero after normalization while maintaining column sums of 1 <a class="yt-timestamp" data-t="00:12:13">[00:12:13]</a>, <a class="yt-timestamp" data-t="00:12:16">[00:12:16]</a>, <a class="yt-timestamp" data-t="00:12:19">[00:12:19]</a>, <a class="yt-timestamp" data-t="00:12:23">[00:12:23]</a>. Masking is always applied in GPT-like models <a class="yt-timestamp" data-t="00:12:37">[00:12:37]</a>.

### Multi-Headed Attention
A full attention block in a transformer employs **multi-headed attention**, running many distinct attention heads in parallel <a class="yt-timestamp" data-t="00:20:35">[00:20:35]</a>, <a class="yt-timestamp" data-t="00:20:38">[00:20:38]</a>, <a class="yt-timestamp" data-t="00:20:43">[00:20:43]</a>. Each head has its own unique set of key, query, and value matrices <a class="yt-timestamp" data-t="00:20:43">[00:20:43]</a>, <a class="yt-timestamp" data-t="00:20:47">[00:20:47]</a>, enabling the model to learn various ways context can influence word meaning <a class="yt-timestamp" data-t="00:21:52">[00:21:52]</a>, <a class="yt-timestamp" data-t="00:21:56">[00:21:56]</a>. For instance, GPT-3 utilizes 96 attention heads per block <a class="yt-timestamp" data-t="00:20:47">[00:20:47]</a>.

Each head produces a proposed change for an embedding at a given position <a class="yt-timestamp" data-t="00:21:17">[00:21:17]</a>, <a class="yt-timestamp" data-t="00:21:21">[00:21:21]</a>. These changes from all heads are summed together and added to the original embedding, producing a single refined embedding as output from the multi-headed attention block <a class="yt-timestamp" data-t="00:21:27">[00:21:27]</a>, <a class="yt-timestamp" data-t="00:21:32">[00:21:32]</a>, <a class="yt-timestamp" data-t="00:21:36">[00:21:36]</a>, <a class="yt-timestamp" data-t="00:21:41">[00:21:41]</a>.

### Parameters and Scale
In GPT-3, with an embedding dimension of 12,288 and a key/query space dimension of 128, each key and query matrix adds approximately 1.5 million parameters <a class="yt-timestamp" data-t="00:16:09">[00:16:09]</a>, <a class="yt-timestamp" data-t="00:16:15">[00:16:15]</a>, <a class="yt-timestamp" data-t="00:16:20">[00:16:20]</a>. Factoring the value matrix ensures its parameter count is similar <a class="yt-timestamp" data-t="00:16:52">[00:16:52]</a>, <a class="yt-timestamp" data-t="00:16:54">[00:16:54]</a>, <a class="yt-timestamp" data-t="00:16:57">[00:16:57]</a>. A single attention head thus contributes about 6.3 million parameters <a class="yt-timestamp" data-t="00:18:16">[00:18:16]</a>. With 96 attention heads, a multi-headed attention block contains around 600 million parameters <a class="yt-timestamp" data-t="00:22:03">[00:22:03]</a>, <a class="yt-timestamp" data-t="00:22:07">[00:22:07]</a>, <a class="yt-timestamp" data-t="00:22:10">[00:22:10]</a>.

Considering GPT-3 has 96 distinct layers, the total parameters dedicated to all attention heads reach nearly 58 billion <a class="yt-timestamp" data-t="00:24:16">[00:24:16]</a>, <a class="yt-timestamp" data-t="00:24:21">[00:24:21]</a>, <a class="yt-timestamp" data-t="00:24:27">[00:24:27]</a>. While substantial, this accounts for only about a third of the network's total 175 billion parameters, with the majority coming from other blocks within the transformer <a class="yt-timestamp" data-t="00:24:34">[00:24:34]</a>, <a class="yt-timestamp" data-t="00:24:37">[00:24:37]</a>, <a class="yt-timestamp" data-t="00:24:41">[00:24:41]</a>, <a class="yt-timestamp" data-t="00:24:44">[00:24:44]</a>.

The effectiveness of the attention mechanism largely stems from its high parallelizability, allowing a vast number of computations to run quickly on GPUs, which is crucial for scaling up models and achieving performance improvements <a class="yt-timestamp" data-t="00:24:54">[00:24:54]</a>, <a class="yt-timestamp" data-t="00:24:58">[00:24:58]</a>, <a class="yt-timestamp" data-t="00:25:02">[00:25:02]</a>, <a class="yt-timestamp" data-t="00:25:09">[00:25:09]</a>.