---
title: Softmax function and its role in machine learning
videoId: wjZofJX0v4M
---

From: [[3blue1brown]] <br/> 

The Softmax function is a standard method used in deep learning to convert an arbitrary list of real numbers into a valid probability distribution <a class="yt-timestamp" data-t="00:23:00">[00:23:00]</a>. This is crucial because outputs from typical deep learning operations, such as those involving [[matrix_vector_multiplication_explained | matrix-vector multiplication]], often produce values that are negative, greater than one, or do not sum to one <a class="yt-timestamp" data-t="00:22:45">[00:22:45]</a>.

## Purpose and Requirements of a Probability Distribution

For a sequence of numbers to act as a probability distribution (e.g., over possible next words), each value must be between 0 and 1, and their sum must equal 1 <a class="yt-timestamp" data-t="00:21:31">[00:21:31]</a>. The Softmax function ensures these conditions are met <a class="yt-timestamp" data-t="00:23:00">[00:23:00]</a>.

## How Softmax Works

The process involves two main steps <a class="yt-timestamp" data-t="00:23:13">[00:23:13]</a>:
1.  **Exponentiation**: Each number in the input list is raised to the power of `e` (Euler's number), resulting in a list of positive values <a class="yt-timestamp" data-t="00:23:17">[00:23:17]</a>.
2.  **Normalization**: The sum of all these positive values is calculated, and then each term in the list is divided by that sum <a class="yt-timestamp" data-t="00:23:21">[00:23:21]</a>. This ensures that the final list adds up to 1 <a class="yt-timestamp" data-t="00:23:25">[00:23:25]</a>.

The function maps the largest input values to probabilities closest to 1, while smaller input values correspond to probabilities very close to 0 <a class="yt-timestamp" data-t="00:23:04">[00:23:04]</a>. If one input number is significantly larger than the rest, its corresponding output term will dominate the distribution <a class="yt-timestamp" data-t="00:23:30">[00:23:30]</a>. However, it's "softer" than simply picking the maximum; if other values are similarly large, they also receive meaningful weight in the distribution <a class="yt-timestamp" data-t="00:23:42">[00:23:42]</a>.

## Role in Transformers and Language Models

In models like [[large_language_models_and_their_function | Large Language Models]] (LLMs) such as ChatGPT, the Softmax function is used at the very end of the network to produce a probability distribution over all possible next "tokens" (words or sub-word units) <a class="yt-timestamp" data-t="00:20:29">[00:20:29]</a>, <a class="yt-timestamp" data-t="00:21:08">[00:21:08]</a>. It also appears within [[role_of_transformers_in_language_models | transformer]] architecture when diving into the "attention blocks" <a class="yt-timestamp" data-t="00:21:24">[00:21:24]</a>.

The raw, unnormalized output values that are fed into the Softmax function are often referred to as "logits" <a class="yt-timestamp" data-t="00:25:33">[00:25:33]</a>, <a class="yt-timestamp" data-t="00:25:46">[00:25:46]</a>.

### Temperature Parameter

When sampling from the probability distribution generated by Softmax to create a next word, a constant 'T' (called "temperature") can be introduced into the denominator of the exponents <a class="yt-timestamp" data-t="00:24:04">[00:24:04]</a>, <a class="yt-timestamp" data-t="00:24:09">[00:24:09]</a>.
*   **Larger T**: Gives more weight to lower values, making the distribution more uniform and the output less predictable or more "creative" <a class="yt-timestamp" data-t="00:24:17">[00:24:17]</a>, <a class="yt-timestamp" data-t="00:24:22">[00:24:22]</a>. This comes with a risk of generating nonsense <a class="yt-timestamp" data-t="00:24:56">[00:24:56]</a>.
*   **Smaller T**: Causes bigger values to dominate more aggressively <a class="yt-timestamp" data-t="00:24:24">[00:24:24]</a>. In the extreme, setting T equal to zero means all the weight goes to the maximum value, resulting in the most predictable word choice <a class="yt-timestamp" data-t="00:24:26">[00:24:26]</a>, <a class="yt-timestamp" data-t="00:24:31">[00:24:31]</a>, <a class="yt-timestamp" data-t="00:24:43">[00:24:43]</a>.