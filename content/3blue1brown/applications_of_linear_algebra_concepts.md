---
title: Applications of Linear Algebra Concepts
videoId: TgKwz5Ikpc8
---

From: [[3blue1brown]] <br/> 

Linear algebra provides a powerful framework whose concepts extend far beyond the intuitive visualization of arrows in 2D or 3D space. While vectors are often first understood as arrows or lists of numbers, their deeper essence transcends these specific manifestations <a class="yt-timestamp" data-t="00:00:38">[00:00:38]</a>. This abstract nature allows the tools and techniques of linear algebra to be applied across diverse mathematical and real-world domains.

## The Nature of Vectors and Coordinate Independence

Initially, a vector might be considered either an arrow on a plane described by coordinates or fundamentally a pair of real numbers <a class="yt-timestamp" data-t="00:00:24">[00:00:24]</a>. Defining vectors as lists of numbers offers clarity, making concepts like four-dimensional or 100-dimensional vectors seem concrete <a class="yt-timestamp" data-t="00:00:49">[00:00:49]</a>. However, a common understanding among those fluent in linear algebra is that they deal with a space existing independently of the chosen coordinates <a class="yt-timestamp" data-t="00:01:05">[00:01:05]</a>. Coordinates are somewhat arbitrary, depending on the chosen basis vectors <a class="yt-timestamp" data-t="00:01:16">[00:01:16]</a>. Core concepts like determinants and eigenvectors remain unchanged regardless of the coordinate system <a class="yt-timestamp" data-t="00:01:24">[00:01:24]</a>. The determinant reflects how much a [[linear_transformations_in_linear_algebra | transformation]] scales areas, and eigenvectors are those that maintain their span during a [[linear_transformations_in_linear_algebra | transformation]]; both are inherently spatial properties unaffected by coordinate changes <a class="yt-timestamp" data-t="00:01:31">[00:01:31]</a>.

## Functions as Vectors

A significant application of [[understanding_linear_algebra | linear algebra]] concepts is to functions, which can be seen as another type of vector <a class="yt-timestamp" data-t="00:02:13">[00:02:13]</a>. Just as vectors can be added, functions f and g can be added to yield a new function, (f + g)(x) = f(x) + g(x) <a class="yt-timestamp" data-t="00:02:22">[00:02:22]</a>. Similarly, functions can be scaled by a real number, analogous to scaling vector coordinates <a class="yt-timestamp" data-t="00:03:11">[00:03:11]</a>. This means the problem-solving techniques of [[understanding_linear_algebra | linear algebra]] originally conceived for arrows in space can be applied to functions <a class="yt-timestamp" data-t="00:03:33">[00:03:33]</a>.

## Linear Transformations and the Derivative

The concept of a [[linear_transformations_in_linear_algebra | linear transformation]] also extends to functions <a class="yt-timestamp" data-t="00:03:46">[00:03:46]</a>. A prime example from calculus is the derivative, which transforms one function into another <a class="yt-timestamp" data-t="00:04:03">[00:04:03]</a>. In this context, these transformations are sometimes called operators <a class="yt-timestamp" data-t="00:04:08">[00:04:08]</a>.

A transformation of functions is linear if it satisfies two properties <a class="yt-timestamp" data-t="00:04:39">[00:04:39]</a>:
*   **Additivity**: Applying the transformation to the sum of two functions (v + w) yields the same result as adding the transformed versions of v and w separately <a class="yt-timestamp" data-t="00:04:46">[00:04:46]</a>.
*   **Scaling**: Applying the transformation to a scaled function (c*v) yields the same result as scaling the transformed version of v by that same amount (c*T(v)) <a class="yt-timestamp" data-t="00:05:04">[00:05:04]</a>.

These properties mean [[linear_transformations_in_linear_algebra | linear transformations]] preserve vector addition and scalar multiplication <a class="yt-timestamp" data-t="00:05:21">[00:05:21]</a>. A significant consequence is that a [[linear_transformations_in_linear_algebra | linear transformation]] is entirely defined by where it maps the basis vectors <a class="yt-timestamp" data-t="00:05:44">[00:05:44]</a>. This principle holds true for functions as it does for traditional vectors <a class="yt-timestamp" data-t="00:06:12">[00:06:12]</a>. Calculus students implicitly use the derivative's additivity and scaling properties when working with it <a class="yt-timestamp" data-t="00:06:18">[00:06:18]</a>.

### Representing the Derivative with a Matrix

The derivative can even be described using a matrix, demonstrating the deep connection between these seemingly disparate concepts <a class="yt-timestamp" data-t="00:06:53">[00:06:53]</a>. For polynomial functions, a natural choice for basis functions are powers of x (1, x, x², x³, etc.) <a class="yt-timestamp" data-t="00:07:28">[00:07:28]</a>. Since polynomials can have arbitrarily large degrees, this creates an infinite set of basis functions, meaning polynomials can be represented as vectors with infinitely many coordinates (though any individual polynomial will have a finite number of non-zero coordinates) <a class="yt-timestamp" data-t="00:08:02">[00:08:02]</a>.

In this coordinate system, the derivative is represented by an infinite matrix with positive integers counting down on an offset diagonal <a class="yt-timestamp" data-t="00:09:06">[00:09:06]</a>. Multiplying this matrix by the coordinate vector of a polynomial yields the coordinate vector of its derivative, effectively performing differentiation through matrix multiplication <a class="yt-timestamp" data-t="00:09:24">[00:09:24]</a>. This highlights that matrix-vector multiplication and taking a derivative are fundamentally related <a class="yt-timestamp" data-t="00:10:59">[00:10:59]</a>.

## Vector Spaces: The Abstract Framework

The generalization of linear algebra concepts leads to the abstract definition of "vector spaces" <a class="yt-timestamp" data-t="00:12:13">[00:12:13]</a>. A vector space is any set of objects where there is a sensible notion of scaling and adding, and these operations adhere to a specific list of rules called axioms <a class="yt-timestamp" data-t="00:12:32">[00:12:32]</a>. These eight axioms ensure that the expected properties of vector addition and scalar multiplication are met <a class="yt-timestamp" data-t="00:12:40">[00:12:40]</a>.

This abstract approach allows the entire theory of linear algebra—including concepts like the dot product (or inner product for functions) and eigenvectors (or eigenfunctions)—to apply to any set of "vectorish" things, be they arrows, lists of numbers, functions, or even more exotic mathematical constructs <a class="yt-timestamp" data-t="00:11:40">[00:11:40]</a>. The axioms serve as an interface, allowing mathematicians to develop general results without needing to consider every specific embodiment <a class="yt-timestamp" data-t="00:13:02">[00:13:02]</a>. This is why [[educational_resources_for_teaching_linear_algebra | linear algebra]] textbooks often define [[introduction_to_linear_transformations | linear transformations]] abstractly in terms of additivity and scaling, rather than focusing on visual examples like gridlines <a class="yt-timestamp" data-t="00:14:01">[00:14:01]</a>, despite the latter's intuitive appeal for initial learning <a class="yt-timestamp" data-t="00:14:13">[00:14:13]</a>.

In essence, [[understanding_linear_algebra | linear algebra]] abstracts the common properties of diverse mathematical objects that can be added and scaled, providing a universal set of tools applicable to a vast array of problems <a class="yt-timestamp" data-t="00:14:59">[00:14:59]</a>. This abstract power underlies many [[applications_of_linear_algebra_in_data_analysis_and_computer_graphics | applications of linear algebra in data analysis and computer graphics]], among other fields.