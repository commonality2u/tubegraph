---
title: Impact of AI on economic and societal structures
videoId: Nlkk3glap_U
---

From: [[dwarkesh | The Dwarkesh Podcast]]

This article summarizes insights from Dario Amodei, CEO of Anthropic, regarding the potential impacts of advancing Artificial Intelligence (AI) on economic systems and societal structures, based on a podcast episode.

## Economic Impacts

### Current and Near-Term Productivity
Currently, models like Claude are described as being akin to an "intern in most areas" <a class="yt-timestamp" data-t="00:25:51">[00:25:51]</a>, though they can exhibit savant-like capabilities in specific, narrow domains <a class="yt-timestamp" data-t="00:26:38">[00:26:38]</a>. The comparison to human employees is difficult due to AI's different "form factor" and lack of long-term prior experience <a class="yt-timestamp" data-t="00:25:55">[00:25:55]</a>. There's an expectation that AI will reach a threshold of economic productivity where it can perform tasks equivalent to what an average human does <a class="yt-timestamp" data-t="00:25:30">[00:25:30]</a>.

### AI Accelerating AI Research
A significant economic impact could arise from AI systems speeding up the productivity of humans in AI research, then equaling human productivity, and eventually becoming the main contributors to scientific and AI progress [[impact_of_ai_on_future_technology_and_society]] <a class="yt-timestamp" data-t="00:27:01">[00:27:01]</a> <a class="yt-timestamp" data-t="00:27:17">[00:27:17]</a>. This creates a "snowball" effect where models help improve subsequent models <a class="yt-timestamp" data-t="00:31:44">[00:31:44]</a>.

### Investment and Costs
The perceived enormous economic value of AI is driving significant investment into the field. Amodei expects the amount of money spent on training the largest models to increase substantially, potentially by a factor of 100 <a class="yt-timestamp" data-t="00:33:43">[00:33:43]</a> <a class="yt-timestamp" data-t="00:33:56">[00:33:56]</a>. Current frontier models (like GPT-4 or Claude 2) are estimated to cost in the general order of magnitude of a hundred million dollars to train <a class="yt-timestamp" data-t="00:57:58">[00:57:58]</a>. Future leading models ("leviathans") might involve $10 billion training runs <a class="yt-timestamp" data-t="01:01:42">[01:01:42]</a>. Securing the necessary components, like power and GPUs for next-generation models, is a complex challenge as operations move to an unprecedented scale [[data_center_energy_requirements_and_scaling]] <a class="yt-timestamp" data-t="01:35:32">[01:35:32]</a> <a class="yt-timestamp" data-t="01:35:50">[01:35:50]</a>. Data centers for these models are becoming as costly as aircraft carriers <a class="yt-timestamp" data-t="01:35:04">[01:35:04]</a>.

### Frictions in Adoption
Even if AI models achieve high capability levels, their immediate impact on the economy might be tempered by "mysterious frictions" <a class="yt-timestamp" data-t="00:30:20">[00:30:20]</a>. These include the challenges of integrating AI into existing company workflows and overcoming inertia [[challenges_and_opportunities_in_deploying_ai_at_scale]] <a class="yt-timestamp" data-t="00:30:46">[00:30:46]</a>. It takes time for businesses to figure out how to use these systems most efficiently, even if the technology is rapidly advancing <a class="yt-timestamp" data-t="00:31:28">[00:31:28]</a>. This creates a race between how fast technology improves and how fast it's integrated into the economy, leading to an unstable and turbulent process <a class="yt-timestamp" data-t="01:47:31">[01:47:31]</a>. While revenues for AI companies are growing rapidly (e.g., in the $100 million to billion per year range for multiple companies <a class="yt-timestamp" data-t="01:46:16">[01:46:16]</a>), it's uncertain if this will scale to hundreds of billions or trillions before the technological landscape shifts dramatically again <a class="yt-timestamp" data-t="01:46:22">[01:46:22]</a>.

## Societal Impacts

### Timelines for Human-Level Capabilities
Amodei suggests that AI models could be perceived as a "generally well educated human" within two to three years, assuming continued scaling and no slowdowns for safety or regulatory reasons <a class="yt-timestamp" data-t="00:28:21">[00:28:21]</a> <a class="yt-timestamp" data-t="00:28:51">[00:28:51]</a>. However, this level of capability might not immediately equate to existential risk or a complete takeover of AI research or the economy <a class="yt-timestamp" data-t="00:28:57">[00:28:57]</a> <a class="yt-timestamp" data-t="00:29:07">[00:29:07]</a>.

### Governance and Control of Powerful AI
The immense power of future AI necessitates substantial involvement from governments or assemblies of government bodies in its management <a class="yt-timestamp" data-t="01:06:03">[01:06:03]</a>. Amodei cautions against naive solutions like simply handing over control to existing entities (e.g., the UN or a current political office) <a class="yt-timestamp" data-t="01:06:21">[01:06:21]</a>. A legitimate process needs to be established, involving builders, democratically elected authorities, and affected individuals [[challenges_in_ai_governance]] <a class="yt-timestamp" data-t="01:06:38">[01:06:38]</a>. The specifics of such governance structures are hard to determine in advance and will likely require experimentation with less powerful technologies <a class="yt-timestamp" data-t="01:07:10">[01:07:10]</a> <a class="yt-timestamp" data-t="01:07:23">[01:07:23]</a>. Anthropic's Long-Term Benefit Trust (LTBT), which can appoint a majority of board seats, is a narrower mechanism specific to the company, designed to ensure its mission is upheld <a class="yt-timestamp" data-t="01:07:45">[01:07:45]</a> <a class="yt-timestamp" data-t="01:08:04">[01:08:04]</a>. It's distinct from the broader governance needed for AGI on behalf of humanity <a class="yt-timestamp" data-t="01:08:22">[01:08:22]</a>.

### Security Risks (Misuse and Proliferation)
Both misuse by malicious actors and misalignment (where AI develops unintended harmful goals) are significant long-term concerns [[ai_alignment_and_safety_concerns]] <a class="yt-timestamp" data-t="01:03:40">[01:03:40]</a>. Amodei argues that any successful plan for a positive future must address misuse alongside misalignment <a class="yt-timestamp" data-t="01:05:05">[01:05:05]</a>. The premise of misalignment itself implies that misuse by humans controlling a powerful but misaligned AI could have similar devastating consequences <a class="yt-timestamp" data-t="01:04:10">[01:04:10]</a>.

#### Bioterrorism
Models are projected to be capable of enabling large-scale bioterrorism attacks within two to three years <a class="yt-timestamp" data-t="00:38:06">[00:38:06]</a>. This isn't about models revealing easily Googlable information, but about providing "key missing pieces" of implicit or scattered knowledge needed for complex, multi-step attack workflows <a class="yt-timestamp" data-t="00:39:48">[00:39:48]</a> <a class="yt-timestamp" data-t="00:39:57">[00:39:57]</a>. Current models mostly can't do this yet, or they hallucinate, but trends indicate this capability is emerging <a class="yt-timestamp" data-t="00:40:16">[00:40:16]</a> <a class="yt-timestamp" data-t="00:40:37">[00:40:37]</a>.

#### Cybersecurity and Model Theft
Preventing the proliferation of advanced AI models and their underlying architectures is a major concern. Anthropic employs compartmentalization strategies to limit knowledge of "compute multipliers" (architectural innovations) <a class="yt-timestamp" data-t="00:44:00">[00:44:00]</a> <a class="yt-timestamp" data-t="00:44:19">[00:44:19]</a>. The goal is to make attacking Anthropic more costly than a state actor training their own model <a class="yt-timestamp" data-t="00:45:46">[00:45:46]</a>, although currently, a top-priority state-level attack would likely succeed <a class="yt-timestamp" data-t="00:46:27">[00:46:27]</a>. The increasing value of AI models means cybersecurity practices must be exceptionally high, beyond typical tech company standards [[cybersecurity_and_ai_vulnerabilities]] <a class="yt-timestamp" data-t="01:31:47">[01:31:47]</a>, as state actors are likely to target them if the value is perceived to be high enough <a class="yt-timestamp" data-t="01:32:39">[01:32:39]</a>. This includes securing physical data centers, which may require specialized construction and operational security <a class="yt-timestamp" data-t="01:34:50">[01:34:50]</a>.

### The Future of Work and Discovery
AI models are beginning to show "ordinary creativity," like writing sonnets in specific styles <a class="yt-timestamp" data-t="00:36:29">[00:36:29]</a>. While they haven't made "big" scientific discoveries yet, this is attributed to current skill levels not being high enough <a class="yt-timestamp" data-t="00:36:50">[00:36:50]</a>. In fields like biology, where progress relies on knowing many facts and connections, models are seen as being on the cusp of making such discoveries <a class="yt-timestamp" data-t="00:37:43">[00:37:43]</a> <a class="yt-timestamp" data-t="00:38:02">[00:38:02]</a>. It's expected that models will eventually undertake extended tasks <a class="yt-timestamp" data-t="01:45:05">[01:45:05]</a>, but predicting the exact form of integration (e.g., models working with each other, with humans, or independently) is difficult <a class="yt-timestamp" data-t="01:45:22">[01:45:22]</a>.

### International Dynamics
China is seen as aggressively trying to catch up in AI development, particularly after the release of ChatGPT <a class="yt-timestamp" data-t="01:12:16">[01:12:16]</a>. While previously more commercially focused, a concern is that the drive for national security and power could lead them to pursue AGI effectively [[china_and_the_uss_race_in_ai_and_superintelligence]] <a class="yt-timestamp" data-t="01:12:55">[01:12:55]</a> <a class="yt-timestamp" data-t="01:13:08">[01:13:08]</a>. Preventing blueprints or codebases from falling into adversarial hands is a key reason for Anthropic's strong focus on cybersecurity <a class="yt-timestamp" data-t="01:13:24">[01:13:24]</a>.

## Philosophical and Ethical Considerations

### Defining a "Good Future"
Amodei expresses nervousness about defining what to do with superhuman AI or prescribing a single "good life" <a class="yt-timestamp" data-t="01:09:30">[01:09:30]</a>. He suggests that after crucial safety problems and externalities are addressed <a class="yt-timestamp" data-t="01:10:39">[01:10:39]</a>, society should draw on lessons from markets and democracy, allowing for decentralized decision-making and individual pursuit of well-being <a class="yt-timestamp" data-t="01:09:42">[01:09:42]</a>. Unitary visions for society have historically led to disaster [[historical_influences_on_leadership_and_innovation]] <a class="yt-timestamp" data-t="01:10:17">[01:10:17]</a>. The ideal future, even with powerful AI, should be "more decentralized and less like a godlike super" AI <a class="yt-timestamp" data-t="01:29:11">[01:29:11]</a>.

### AI Consciousness
The question of AI consciousness is unsettled. Initially, Amodei believed it wouldn't be a concern until models operated in rich environments with reward functions and long-lived experiences <a class="yt-timestamp" data-t="01:51:31">[01:51:31]</a>. However, observing cognitive machinery like induction heads in base language models has made him less sure [[mechanistic_interpretability_in_ai]] <a class="yt-timestamp" data-t="01:51:49">[01:51:49]</a>. While current models are likely not "smart enough" for this to be a major worry, it could become a real concern within a year or two <a class="yt-timestamp" data-t="01:52:13">[01:52:13]</a>. If models were found to have experiences comparable to animals, it would be unsettling, particularly due to the difficulty of knowing whether interventions would lead to positive or negative experiences <a class="yt-timestamp" data-t="01:52:48">[01:52:48]</a> <a class="yt-timestamp" data-t="01:53:00">[01:53:00]</a>.