---
title: Influence of a datadriven approach on company growth
videoId: hEzpiDuYFoE
---

From: [[lennyspodcast]] <br/> 

A data-driven approach, particularly through A/B testing and experimentation, is crucial for fostering sustainable company growth and making informed decisions in product development. Ronnie Kohavi, widely recognized as a world expert on A/B testing and experimentation, emphasizes its importance, having led efforts at major tech companies like Microsoft, Amazon, and Airbnb <a class="yt-timestamp" data-t="01:14:08">[01:14:08]</a>.

## The Core Principle: Test Everything

A fundamental tenet of a data-driven approach is to "test everything" <a class="yt-timestamp" data-t="00:00:02">[00:00:02]</a>. Any code change or new feature should be introduced within an experiment <a class="yt-timestamp" data-t="00:00:06">[00:00:06]</a>. This is because even minor bug fixes or small changes can lead to surprising and unexpected impacts, necessitating continuous experimentation <a class="yt-timestamp" data-t="00:00:13">[00:00:13]</a>. It is not possible to experiment too much <a class="yt-timestamp" data-t="00:00:22">[00:00:22]</a>.

## Growth Through Experimentation

### Unexpected Wins

While rare, a data-driven approach can uncover significant, unexpected wins. One notable example from Bing involved a "trivial" change to how ads were displayed â€“ moving the second line of an ad to the first, making the title larger <a class="yt-timestamp" data-t="00:05:20">[00:05:20]</a>. Despite being low-priority on the backlog, this simple change increased revenue by about 12%, a gain valued at $100 million at the time, without hurting user metrics <a class="yt-timestamp" data-t="00:06:57">[00:06:57]</a>. Similarly, at Airbnb, a small experiment to open search results in a new tab resulted in one of the biggest wins for search <a class="yt-timestamp" data-t="00:09:08">[00:09:08]</a>. These "gold nuggets" are not frequent, but experimentation provides the means to discover them <a class="yt-timestamp" data-t="00:10:46">[00:10:46]</a>.

### Incremental Gains Lead to Sustainable Growth

Most [[growth_hacking_and_sustainable_growth_strategies | company growth]] comes from continuous, "inch by inch" improvements <a class="yt-timestamp" data-t="00:11:15">[00:11:15]</a>. For instance, Bing's relevance team, comprising hundreds of people, aimed to improve their key metric by 2% annually, achieving this through many small, cumulative gains <a class="yt-timestamp" data-t="00:12:12">[00:12:12]</a>. At Airbnb, 250 experiments in search relevance led to a 6% overall improvement in revenue, resulting from many smaller ideas each contributing a minor gain <a class="yt-timestamp" data-t="00:12:41">[00:12:41]</a>.

### High Failure Rates Are Normal and Valuable

A critical aspect of a data-driven approach is accepting high failure rates. Across Ronnie Kohavi's career:
*   Microsoft: Approximately 66% of ideas failed <a class="yt-timestamp" data-t="00:13:39">[00:13:39]</a>.
*   Bing: Roughly 85% failure rate, due to a more optimized domain <a class="yt-timestamp" data-t="00:13:48">[00:13:48]</a>.
*   Airbnb Search Relevance: A staggering 92% of experiments failed to improve the target metric <a class="yt-timestamp" data-t="00:14:02">[00:14:02]</a>.
*   Industry-wide: Other companies like Booking and Google Ads report 80-90% failure rates <a class="yt-timestamp" data-t="00:14:19">[00:14:19]</a>.

These failures are not setbacks but opportunities for learning. Approximately 10% of experiments are aborted on the first day due to implementation issues, not necessarily bad ideas <a class="yt-timestamp" data-t="00:14:47">[00:14:47]</a>. Even flat results should generally not be shipped, as they add code complexity and maintenance overhead without value <a class="yt-timestamp" data-t="00:44:50">[00:44:50]</a>.

## Avoiding Pitfalls and Ensuring Trust

### Surprising Results and Twyman's Law

Any figure that looks "interesting or different" is usually wrong <a class="yt-timestamp" data-t="01:00:56">[01:00:56]</a>. This concept, known as Twyman's Law, suggests that if an experiment shows an unusually large movement (e.g., a 10% gain when normal is under 1%), it's highly probable there's an error <a class="yt-timestamp" data-t="01:01:13">[01:01:13]</a>. Investigations based on Twyman's Law reveal a flaw in about nine out of ten cases <a class="yt-timestamp" data-t="01:01:41">[01:01:41]</a>. This vigilance is crucial for maintaining trust in the experimentation process.

### The Overall Evaluation Criterion (OEC)

A well-defined Overall Evaluation Criterion (OEC) is essential to prevent short-sighted optimizations and ensure [[retention_and_growth_challenges_in_a_datadriven_business | sustainable growth]]. The OEC defines what a company is optimizing for, often balancing revenue with user experience <a class="yt-timestamp" data-t="00:28:18">[00:28:18]</a>.

*   **Balancing Metrics**: For instance, increasing ad density might boost short-term revenue but hurt user experience and long-term retention. An OEC for search engines could involve a trade-off, like increasing revenue within a fixed ad real estate budget <a class="yt-timestamp" data-t="00:29:41">[00:29:41]</a>.
*   **Long-Term Value**: The OEC should be causally predictive of the user's lifetime value <a class="yt-timestamp" data-t="00:32:07">[00:32:07]</a>. At Amazon, the email team initially optimized for immediate purchases from emails, leading to spamming <a class="yt-timestamp" data-t="00:34:01">[00:34:01]</a>. By modeling the cost of spamming (e.g., lost value from unsubscribes), the OEC shifted to maximize long-term value, leading to more targeted and beneficial email campaigns <a class="yt-timestamp" data-t="00:34:54">[00:34:54]</a>. Similarly, for Airbnb, conversion rate alone isn't enough; user satisfaction months after a stay should also be part of the OEC <a class="yt-timestamp" data-t="00:31:14">[00:31:14]</a>.

### Avoiding Full Product Redesigns

Full product redesigns often fail to yield positive results, requiring teams to backtrack and repair what was broken <a class="yt-timestamp" data-t="00:36:33">[00:36:33]</a>. It's more effective to implement changes incrementally, testing "one factor at a time" (O-FAT), to learn and adjust along the way <a class="yt-timestamp" data-t="00:37:05">[00:37:05]</a>. While big bets are necessary, teams should be ready for an 80% failure rate on major redesigns <a class="yt-timestamp" data-t="00:43:21">[00:43:21]</a>.

### Trust in the Experimentation Platform

Trust is paramount for an experimentation platform, which serves as both a safety net for bad launches and an oracle for results <a class="yt-timestamp" data-t="00:52:03">[00:52:03]</a>. Issues like "sample ratio mismatch" (when user splits in control and treatment groups are not as designed) are common red flags indicating invalid experiments <a class="yt-timestamp" data-t="00:55:53">[00:55:53]</a>. Roughly 8% of experiments at Microsoft suffered from this issue <a class="yt-timestamp" data-t="00:57:24">[00:57:24]</a>. To maintain trust, platforms must flag and even obscure invalid results, forcing investigation <a class="yt-timestamp" data-t="00:59:37">[00:59:37]</a>.

A common misunderstanding relates to p-values; a p-value of 0.05 does not mean there's a 95% probability that the treatment is better than the control <a class="yt-timestamp" data-t="01:02:42">[01:02:42]</a>. The actual "false positive risk" can be much higher (e.g., 26% at Airbnb search where the success rate was low) <a class="yt-timestamp" data-t="01:04:44">[01:04:44]</a>.

## Scaling and Cultural Shift

### When to Start A/B Testing

For [[company_growth_and_scaling_challenges | startups and smaller companies]], A/B testing becomes statistically viable with "tens of thousands of users" to detect large effects <a class="yt-timestamp" data-t="02:26:59">[02:26:59]</a>. For more robust analysis and detection of smaller, yet beneficial, changes, a user base of around 200,000 is needed <a class="yt-timestamp" data-t="02:27:21">[02:27:21]</a>. Below this threshold, it's advisable to focus on building an experimentation culture and platform in preparation for scale <a class="yt-timestamp" data-t="02:27:52">[02:27:52]</a>.

### Building a Data-Driven Culture

Shifting an organization towards a data-driven culture requires:
*   **Finding a Beachhead**: Start with a team that launches frequently (e.g., weekly or daily) and has a clear OEC <a class="yt-timestamp" data-t="01:08:37">[01:08:37]</a>.
*   **Sharing Surprising Results**: Showcase unexpected successes and failures to convince other parts of the company <a class="yt-timestamp" data-t="01:08:15">[01:08:15]</a>.
*   **Cross-Pollination**: As people move to different groups, they can carry the data-driven mindset with them <a class="yt-timestamp" data-t="01:08:25">[01:08:25]</a>.

### Institutional Learning

To preserve [[the_role_of_data_analytics_in_business_impact | institutional memory]] and facilitate learning, companies should:
*   **Document Experiments**: Maintain a comprehensive record of successes and failures <a class="yt-timestamp" data-t="01:17:10">[01:17:10]</a>.
*   **Searchable History**: Provide the ability to search past experiments by keywords to inform new ideas <a class="yt-timestamp" data-t="01:19:47">[01:19:47]</a>.
*   **Regular Reviews**: Hold quarterly meetings to review the most surprising experiments (both positive and negative outcomes where expectations differed greatly from reality) <a class="yt-timestamp" data-t="01:17:28">[01:17:28]</a>.

### Platform Investment

Investing in a robust experimentation platform is key to reducing the marginal cost of running experiments to near zero <a class="yt-timestamp" data-t="02:25:40">[02:25:40]</a>. A mature platform enables self-service for setting up experiments, defining metrics, and analyzing results quickly, reducing reliance on manual data scientist involvement <a class="yt-timestamp" data-t="01:11:30">[01:11:30]</a>. Modern vendors offer good experimentation platforms, making "buy" a viable option for many companies <a class="yt-timestamp" data-t="01:07:34">[01:07:34]</a>.

## Challenges and Nuances

*   **Innovation vs. Optimization**: While experimentation excels at optimization, it's also important to allocate resources to "high-risk, high-reward ideas" that might fail but could lead to breakthroughs <a class="yt-timestamp" data-t="02:21:03">[02:21:03]</a>.
*   **Wartime Decisions**: Even in crises, like the COVID-19 pandemic, A/B testing becomes *more* important, as it helps validate changes in a rapidly shifting environment <a class="yt-timestamp" data-t="02:48:37">[02:48:37]</a>.

Ultimately, a data-driven approach, powered by rigorous experimentation and a commitment to learning from all outcomes, is foundational for sustainable [[growth_strategies_in_tech_companies | company growth]] and informed decision-making.