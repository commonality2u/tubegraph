---
title: Common misconceptions about research within organizations
videoId: L6RKi9ZvkT4
---

From: [[lennyspodcast]] <br/> 

Many organizations harbor misconceptions about the role and effectiveness of user research, leading to practices that hinder its true impact. Jud Anon, a former head of research at Airbnb and Facebook, highlights several of these [[challenges_and_evolution_in_the_user_research_field | challenges]] and offers insights into how to rectify them.

## User-Centered Performance

A significant issue is what Jud Anon terms "user-centered performance" <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a>. This refers to customer obsession or user-centered practices that are symbolic rather than genuinely focused on learning <a class="yt-timestamp" data-t="00:00:04">[00:00:04]</a>. It's often work done to signal customer obsession, not to inform a different decision <a class="yt-timestamp" data-t="00:00:15">[00:00:15]</a>.

> [!EXAMPLE] Examples of User-Centered Performance
> *   A Product Manager (PM) requesting a quick user study at the end of a product process "just to validate our assumptions." This is user-centered performance because it's too late to matter; the goal is to "check the box" rather than make a different decision <a class="yt-timestamp" data-t="00:00:26">[00:00:26]</a>.
> *   Executive listening sessions or focus groups, where participants primarily want to "get close to the customer" but are largely engaging in performance, not genuine learning that drives better outcomes <a class="yt-timestamp" data-t="00:25:27">[00:25:27]</a>.

Many PMs and designers are not looking to be wrong; they are looking to validate their existing ideas <a class="yt-timestamp" data-t="00:00:45">[00:00:45]</a>. Jud's mantra is: "We don't validate, we falsify. We are looking to be wrong" <a class="yt-timestamp" data-t="00:00:39">[00:00:39]</a>. This mindset shift is crucial for effective research.

## The Research "Reckoning"

The user research field is currently undergoing a "reckoning," implying a need for significant change in how companies leverage user research <a class="yt-timestamp" data-t="00:01:27">[00:01:27]</a>. The discipline of UX research as it existed over the last 15 years is "dying," but the discipline itself can survive and thrive through adaptation <a class="yt-timestamp" data-t="00:05:46">[00:05:46]</a>. This period of layoffs in UX and UX research fields highlights that the system may be "broken," and research is not consistently driving the value or impact it should <a class="yt-timestamp" data-t="00:08:00">[00:08:00]</a>.

### Doing the "Wrong Type" of Research

A core problem is that organizations often engage in too much of what Jud Anon considers the "wrong type of research" <a class="yt-timestamp" data-t="00:08:50">[00:08:50]</a>. He categorizes research into three types:

1.  **Macro Research** <a class="yt-timestamp" data-t="00:08:57">[00:08:57]</a>
    *   **Focus**: Big-picture, strategic, business-focused, forward-looking innovation <a class="yt-timestamp" data-t="00:09:09">[00:09:09]</a>.
    *   **Examples**: Understanding competitive landscapes, concept car projects, strategic planning, Total Addressable Market (TAM) studies, long-term product direction <a class="yt-timestamp" data-t="00:09:13">[00:09:13]</a>.
    *   **Impact**: Crucial for long-term vision and [[common_pitfalls_in_strategic_planning | strategic planning]] <a class="yt-timestamp" data-t="00:12:33">[00:12:33]</a>.

2.  **Micro Research** <a class="yt-timestamp" data-t="00:09:22">[00:09:22]</a>
    *   **Focus**: Technical usability, enabling pixel-perfect execution, understanding A/B test results <a class="yt-timestamp" data-t="00:09:23">[00:09:23]</a>.
    *   **Impact**: Drives significant business value by improving user experience and conversion <a class="yt-timestamp" data-t="00:35:16">[00:35:16]</a>. Can provide rapid insights, sometimes in 48 hours <a class="yt-timestamp" data-t="00:35:26">[00:35:26]</a>.
    *   **Misconception**: Often seen as "scut work" or only for junior researchers <a class="yt-timestamp" data-t="00:38:00">[00:38:00]</a>.

3.  **Middle-Range Research** <a class="yt-timestamp" data-t="00:09:38">[00:09:38]</a>
    *   **Focus**: Middle-altitude research questions, often centered on core user understanding (e.g., how users think, feel, behave, use a product) <a class="yt-timestamp" data-t="00:09:41">[00:09:41]</a>.
    *   **Problem**: While interesting, it's often not impactful enough for the business <a class="yt-timestamp" data-t="00:10:04">[00:10:04]</a>.
    *   **Causes**:
        *   Yields interesting but hard-to-operationalize results <a class="yt-timestamp" data-t="00:10:46">[00:10:46]</a>.
        *   Triggers post-hoc bias ("that was kind of obvious; we knew that already") <a class="yt-timestamp" data-t="00:10:53">[00:10:53]</a>.
        *   Fulfills a need to feel user-centered without actual change <a class="yt-timestamp" data-t="00:11:03">[00:11:03]</a>.
        *   Often requested by PMs who haven't deeply engaged with researchers or by those seeking validation <a class="yt-timestamp" data-t="00:14:15">[00:14:15]</a>.
        *   Contributes to the stereotype that research is slow because it's not "pointed enough" at a business problem <a class="yt-timestamp" data-t="00:38:31">[00:38:31]</a>.

### Over-Reliance on Research

PMs and designers too often rely on research to answer the wrong questions <a class="yt-timestamp" data-t="00:01:40">[00:01:40]</a>. Organizations hired many researchers with good intentions but often failed to integrate them properly, treating them as a "service function" rather than strategic partners <a class="yt-timestamp" data-t="00:11:42">[00:11:42]</a>. This leads to reactive research that is less impactful, causing executives to sideline or lay off researchers, perpetuating a vicious cycle <a class="yt-timestamp" data-t="00:16:00">[00:16:00]</a>.

> [!QUOTE] Jud Anon on Middle-Range Research Requests
> "Product managers love to ask for Middle range research that they can use to justify decisions they're reluctant to make on their own. User designers love to ask for Middle range research because it fits their model of what proper design process should look like Executives love to to ask for Middle range because they don't really understand what research is for it helps them do performative user centeredness in the end they will decide based on their own opinions" <a class="yt-timestamp" data-t="00:26:43">[00:26:43]</a>

## Common Tropes and Misconceptions about Research

Jud Anon identifies several common complaints or "tropes" about research from product managers that are often rooted in misconceptions:

1.  **"Research just slows us down" / "Research is too slow."** <a class="yt-timestamp" data-t="00:33:52">[00:33:52]</a>
    *   **Reality**: A skilled research team can deliver insights quickly, often in days or weeks, depending on the depth required <a class="yt-timestamp" data-t="00:34:01">[00:34:01]</a>. Good research actually speeds up development by preventing costly mistakes and providing clarity <a class="yt-timestamp" data-t="00:34:34">[00:34:34]</a>. The perception of slowness often stems from being asked to do the less impactful "middle-range" research <a class="yt-timestamp" data-t="00:38:31">[00:38:31]</a>.

2.  **"I can do my own research."** <a class="yt-timestamp" data-t="00:39:03">[00:39:03]</a>
    *   **Reality**: While anyone can talk to a user, that doesn't constitute rigorous research or insights work <a class="yt-timestamp" data-t="00:39:22">[00:39:22]</a>. A professional researcher knows how to extract the heart of an issue, avoid bias, and situate individual feedback within a broader context <a class="yt-timestamp" data-t="00:39:33">[00:39:33]</a>.

3.  **"AB test everything."** <a class="yt-timestamp" data-t="00:40:07">[00:40:07]</a>
    *   **Reality**: A/B tests are excellent for establishing causality (what happened), but they rarely explain *why* it happened <a class="yt-timestamp" data-t="00:40:12">[00:40:12]</a>. Without understanding the "how and why," teams risk endless experimentation without deeper learning. [[strategies_for_effective_user_research_and_its_business_impact | User research]] provides the critical qualitative data to understand the underlying reasons, making for a powerful partnership with data science <a class="yt-timestamp" data-t="00:41:40">[00:41:40]</a>.

4.  **The Apocryphal Henry Ford Quote.** <a class="yt-timestamp" data-t="00:42:26">[00:42:26]</a>
    *   **Misconception**: The widely quoted "If I had asked my users what they wanted, they would have said faster horses" implies that users cannot articulate their needs and research is useless for innovation.
    *   **Reality**: Henry Ford likely never said this quote <a class="yt-timestamp" data-t="00:42:31">[00:42:31]</a>. More importantly, professional researchers don't simply ask customers what they want. Instead, they delve into underlying needs, behaviors, and challenges to inform innovation <a class="yt-timestamp" data-t="00:42:56">[00:42:56]</a>.

5.  **"We knew this already" / Hindsight Bias.** <a class="yt-timestamp" data-t="00:43:18">[00:43:18]</a>
    *   **Misconception**: After research insights are presented, stakeholders claim the findings were obvious or already known.
    *   **Reality**: This is often a result of hindsight bias, where information seems obvious once it's revealed <a class="yt-timestamp" data-t="00:43:40">[00:43:40]</a>. Humans selectively remember and construct narratives to make sense of the past, creating a "narrative fallacy" <a class="yt-timestamp" data-t="00:43:55">[00:43:55]</a>. Good research often exposes blind spots and challenges intuition, helping to continuously improve mental models <a class="yt-timestamp" data-t="00:27:27">[00:27:27]</a>.

## Misconceptions about Specific Metrics

### Net Promoter Score (NPS)

Jud Anon has a strong opinion that NPS is an example of the marketing industry marketing itself <a class="yt-timestamp" data-t="01:03:53">[01:03:53]</a>.

> [!CAUTION] Problems with NPS
> *   The "likelihood to recommend" question is flawed for various reasons, including its 0-10 scale, often unlabeled points, and the fact that 11 items reduce precision (which typically goes down after five items) <a class="yt-timestamp" data-t="01:04:20">[01:04:20]</a>.
> *   The question assumes a person regularly recommends products like operating systems, which is often not the case <a class="yt-timestamp" data-t="01:05:00">[01:05:00]</a>.
> *   NPS is often idiosyncratic, making internal and external benchmarking unreliable due to inconsistencies in how it's asked <a class="yt-timestamp" data-t="01:06:19">[01:06:19]</a>.

Instead, a simpler **customer satisfaction (CSAT)** metric (e.g., "Overall how satisfied are you with your experience with [Product/Service]?") has better data properties, is more precise, and is more correlated to business outcomes <a class="yt-timestamp" data-t="01:05:17">[01:05:17]</a>.

## The Product Walkthrough Misconception

While dogfooding one's own product and doing product walkthroughs are important <a class="yt-timestamp" data-t="01:07:28">[01:07:28]</a>, a key misconception for PMs is relying solely on their own intuition when doing so <a class="yt-timestamp" data-t="01:07:33">[01:07:33]</a>.

> [!IMPORTANT] PMs are not like users
> PMs are "nothing like the user" in ways that can bias their understanding of a product's strengths and weaknesses <a class="yt-timestamp" data-t="01:07:41">[01:07:41]</a>. Many problems require understanding the context of use, priorities, and constraints that PMs, through their own usage, simply don't have <a class="yt-timestamp" data-t="01:08:03">[01:08:03]</a>. While walkthroughs can identify potential issues, prioritizing and understanding the impact of those issues requires external user insights <a class="yt-timestamp" data-t="01:08:24">[01:08:24]</a>.