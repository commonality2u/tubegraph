---
title: Importance of AB testing and experimentation
videoId: hEzpiDuYFoE
---

From: [[lennyspodcast]] <br/> 

Ronny Kohavi is widely recognized as a leading expert in A/B testing and experimentation <a class="yt-timestamp" data-t="00:01:17">[00:01:17]</a>. His expertise highlights the crucial role of robust experimentation in product development and growth <a class="yt-timestamp" data-t="00:01:07">[00:01:07]</a>.

## Core Principle: Test Everything

A fundamental principle in experimentation is to "test everything" <a class="yt-timestamp" data-t="00:00:02">[00:00:02]</a>. Any code change or new feature should be part of an experiment <a class="yt-timestamp" data-t="00:00:06">[00:00:06]</a>. This approach is vital because even minor bug fixes or small changes can lead to surprising and unexpected impacts <a class="yt-timestamp" data-t="00:00:13">[00:00:13]</a>. It's not possible to experiment too much <a class="yt-timestamp" data-t="00:21:30">[00:21:30]</a>.

## Surprising Results and Learnings

Experimentation frequently yields surprising results, highlighting the [[importance_of_learning_and_adapting_in_sales | importance of learning]] and the limitations of human intuition in predicting outcomes <a class="yt-timestamp" data-t="00:05:01">[00:05:01]</a> <a class="yt-timestamp" data-t="00:08:52">[00:08:52]</a>.

*   **Bing Ad Title Change**: A simple idea to move the second line of an ad to the first line on Bing, making the title larger, was on the backlog for months, deemed less important than other tasks <a class="yt-timestamp" data-t="00:05:29">[00:05:29]</a> <a class="yt-timestamp" data-t="00:06:01">[00:06:01]</a>. However, when implemented for a few days, it unexpectedly increased revenue by approximately 12%, an impact worth $100 million at the time, without hurting user metrics <a class="yt-timestamp" data-t="00:06:28">[00:06:28]</a> <a class="yt-timestamp" data-t="00:06:57">[00:06:57]</a> <a class="yt-timestamp" data-t="00:07:22">[00:07:22]</a>. This single change was the biggest revenue impact in Bing's history <a class="yt-timestamp" data-t="00:07:53">[00:07:53]</a>.
*   **Airbnb New Tab Opening**: At Airbnb, a small experiment to open search results in a new browser tab instead of the current one led to one of the biggest wins in search <a class="yt-timestamp" data-t="00:09:08">[00:09:08]</a> <a class="yt-timestamp" data-t="00:09:16">[00:09:16]</a>. This concept had previously been tested and found beneficial at Microsoft in 2008 <a class="yt-timestamp" data-t="00:09:21">[00:09:21]</a> <a class="yt-timestamp" data-t="00:09:50">[00:09:50]</a>.
*   **Windows Indexer Battery Drain**: An experiment to improve the Windows indexer showed better offline indexing and relevance, but the A/B test revealed it significantly killed battery life by consuming more CPU on laptops <a class="yt-timestamp" data-t="00:18:37">[00:18:37]</a> <a class="yt-timestamp" data-t="00:19:00">[00:19:00]</a>.
*   **Amazon Email Campaigns**: An initial system for Amazon's recommendation email team credited revenue solely based on purchases originating from emails, leading to excessive email sending and spamming users <a class="yt-timestamp" data-t="00:33:31">[00:33:31]</a> <a class="yt-timestamp" data-t="00:34:07">[00:34:07]</a>. By modeling the cost of user unsubscribes (lost long-term value), it was discovered that over half of the campaigns were actually negative in terms of net value <a class="yt-timestamp" data-t="00:34:36">[00:34:36]</a> <a class="yt-timestamp" data-t="00:35:07">[00:35:07]</a>.

These examples illustrate that even small changes can have a massive impact, and human prediction of outcomes is often poor <a class="yt-timestamp" data-t="00:08:34">[00:08:34]</a> <a class="yt-timestamp" data-t="00:08:52">[00:08:52]</a>.

## [[Challenges and success rates in experimentation | Challenges and Success Rates in Experimentation]]

While "gold nuggets" or breakthrough results exist, they are very rare <a class="yt-timestamp" data-t="00:10:46">[00:10:46]</a> <a class="yt-timestamp" data-t="00:11:13">[00:11:13]</a>. Most gains come incrementally, "inch by inch" <a class="yt-timestamp" data-t="00:11:19">[00:11:19]</a>.

*   **Failure Rates**: Most ideas fail to improve key metrics <a class="yt-timestamp" data-t="00:13:08">[00:13:08]</a>.
    *   Across Microsoft, about two-thirds (66%) of ideas fail <a class="yt-timestamp" data-t="00:13:39">[00:13:39]</a>.
    *   At Bing, a more optimized domain, the failure rate was around 85% <a class="yt-timestamp" data-t="00:13:48">[00:13:48]</a>.
    *   At Airbnb search relevance, 92% of experiments failed to improve the target metric <a class="yt-timestamp" data-t="00:13:56">[00:13:56]</a>.
    *   Other companies like Booking.com and Google Ads report failure rates of 80% to 90% <a class="yt-timestamp" data-t="00:14:19">[00:14:19]</a>.
*   **Shipping on Flat or Negative Results**: It's a mistake to ship a feature if an experiment shows flat or negative results, even if significant effort was invested <a class="yt-timestamp" data-t="00:44:40">[00:44:40]</a>. Shipping such projects increases maintenance overhead and complicates the codebase without adding value <a class="yt-timestamp" data-t="00:44:52">[00:44:52]</a>.

### Redesigns and Big Bets
Large redesigns often fail dramatically <a class="yt-timestamp" data-t="00:36:59">[00:36:59]</a>. It's often better to implement redesigns incrementally, testing each step, rather than launching a full overhaul at once <a class="yt-timestamp" data-t="00:37:05">[00:37:05]</a>. When taking big bets, such as attempting a complete product redesign to break out of a local maximum, it's essential to be ready for failure; approximately 80% of such large bets are likely to fail <a class="yt-timestamp" data-t="00:22:08">[00:22:08]</a> <a class="yt-timestamp" data-t="00:43:21">[00:43:21]</a> <a class="yt-timestamp" data-t="00:43:38">[00:43:38]</a>.

## Overall Evaluation Criterion (OEC)

The Overall Evaluation Criterion (OEC) is crucial for defining what an experiment is optimizing for <a class="yt-timestamp" data-t="00:28:18">[00:28:18]</a>. It addresses the concern that A/B testing only leads to micro-optimizations <a class="yt-timestamp" data-t="00:20:57">[00:20:57]</a>.

*   **Long-Term Focus**: Simply optimizing for short-term revenue is often detrimental, as it can harm the user experience <a class="yt-timestamp" data-t="00:28:38">[00:28:38]</a>. The OEC should incorporate "countervailing metrics" that measure user experience or [[longterm_experimentation_and_decision_making_at_shopify | long-term growth]], like user churn or time to achieve a task <a class="yt-timestamp" data-t="00:28:45">[00:28:45]</a> <a class="yt-timestamp" data-t="00:30:31">[00:30:31]</a>.
*   **Constraint Optimization**: A good approach is to frame the problem as constraint optimization, e.g., "increase revenue within a fixed amount of average ad real estate" <a class="yt-timestamp" data-t="00:29:43">[00:29:43]</a>.
*   **Lifetime Value**: Ultimately, the OEC should be causally predictive of the user's lifetime value <a class="yt-timestamp" data-t="00:32:07">[00:32:07]</a>. This encourages a focus on long-term benefits rather than short-term gains that might negatively impact user retention <a class="yt-timestamp" data-t="00:32:22">[00:32:22]</a>.
*   **Predicting Long-Term Outcomes**: Long-term holdouts or building models based on historical data can help predict long-term impacts when immediate data isn't available <a class="yt-timestamp" data-t="00:33:02">[00:33:02]</a> <a class="yt-timestamp" data-t="00:33:18">[00:33:18]</a>.

## When to Start A/B Testing

Companies need a sufficient number of users for the statistics to work out <a class="yt-timestamp" data-t="00:25:18">[00:25:18]</a>.

*   **User Volume**: Companies typically need at least tens of thousands of users to begin A/B testing <a class="yt-timestamp" data-t="00:26:53">[00:26:53]</a>. To detect beneficial changes of 5% or more, a retail site might need around 200,000 users <a class="yt-timestamp" data-t="00:27:12">[00:27:12]</a> <a class="yt-timestamp" data-t="00:27:21">[00:27:21]</a>.
*   **Early Stages**: Below this threshold, it's too early for A/B testing but a good time to start [[building_a_culture_and_platform_for_experimentation | building the culture]] and platform for experimentation, to be ready as the company scales <a class="yt-timestamp" data-t="00:27:52">[00:27:52]</a>.

## Importance of Trust

Trust is the most critical element of a successful experiment culture and platform <a class="yt-timestamp" data-t="00:02:08">[00:02:08]</a> <a class="yt-timestamp" data-t="00:51:08">[00:51:08]</a>. The experimentation platform serves as a safety net, allowing quick abortion of bad launches, and an oracle that provides reliable results <a class="yt-timestamp" data-t="00:52:06">[00:52:06]</a>. Trust, once lost, is difficult to regain <a class="yt-timestamp" data-t="00:52:43">[00:52:43]</a>. Early implementations of A/B testing tools, like Optimizely, initially made statistical errors (e.g., stopping experiments when p-value was significant, inflating false positives), which eroded trust <a class="yt-timestamp" data-t="00:53:31">[00:53:31]</a> <a class="yt-timestamp" data-t="00:55:16">[00:55:16]</a>.

### Signs of Invalid Experiments
*   **Sample Ratio Mismatch (SRM)**: If an experiment designed for a 50/50 user split deviates significantly (e.g., 50.2% vs. 49.8% with a million users), it's a red flag indicating an issue with the experiment, such as data pipeline problems or bot interference <a class="yt-timestamp" data-t="00:55:53">[00:55:53]</a> <a class="yt-timestamp" data-t="00:56:11">[00:56:11]</a> <a class="yt-timestamp" data-t="00:57:01">[00:57:01]</a>. At Microsoft, about 8% of experiments suffered from SRM <a class="yt-timestamp" data-t="00:57:24">[00:57:24]</a>.
*   **Twyman's Law**: This law states, "If any figure that looks interesting or different is usually wrong" <a class="yt-timestamp" data-t="01:00:00">[01:00:00]</a> <a class="yt-timestamp" data-t="01:00:56">[01:00:56]</a>. If an experiment shows unusually large improvements (e.g., 10% when typical movements are under 1%), it should be investigated for flaws before celebrating <a class="yt-timestamp" data-t="01:01:13">[01:01:13]</a>. About nine out of ten times, such "too good to be true" results are indeed found to be flawed <a class="yt-timestamp" data-t="01:01:41">[01:01:41]</a>.
*   **Misinterpreting P-Values**: The common interpretation of a p-value (e.g., 0.05 meaning 95% probability that treatment is better than control) is incorrect <a class="yt-timestamp" data-t="01:02:42">[01:02:42]</a>. P-values assume the null hypothesis is true. The actual "false positive risk" can be much higher; for instance, at Airbnb search (with an 8% success rate), a p-value less than 0.05 had a 26% chance of being a false positive <a class="yt-timestamp" data-t="01:04:42">[01:04:42]</a> <a class="yt-timestamp" data-t="01:04:47">[01:04:47]</a>. For critical experiments, a lower p-value threshold (e.g., 0.01) or replication may be needed to ensure higher confidence <a class="yt-timestamp" data-t="01:05:01">[01:05:01]</a>.

## [[Building a culture and platform for experimentation | Building a Culture and Platform for Experimentation]]

*   **Cultural Shift**: Shifting a company's culture to be more experiment-driven requires time and effort <a class="yt-timestamp" data-t="00:02:01">[00:02:01]</a>.
    *   Start by finding a team that launches frequently (e.g., weekly or bi-weekly) and has a clear OEC <a class="yt-timestamp" data-t="01:08:37">[01:08:37]</a>.
    *   Share surprising successful (and negative) results from these early experiments to demonstrate the value of data-driven decisions <a class="yt-timestamp" data-t="01:08:15">[01:08:15]</a>.
    *   Document learnings from experiments in a searchable database to build institutional memory <a class="yt-timestamp" data-t="01:17:02">[00:17:02]</a> <a class="yt-timestamp" data-t="01:17:10">[01:17:10]</a>.
    *   Hold regular meetings to review the most interesting (surprising) experiments to reinforce learning <a class="yt-timestamp" data-t="01:17:28">[01:17:28]</a>.
*   **Platform Investment**: A robust experimentation platform is key to bringing the marginal cost of running experiments close to zero <a class="yt-timestamp" data-t="00:25:40">[00:25:40]</a> <a class="yt-timestamp" data-t="01:10:34">[01:10:34]</a>.
    *   The platform should enable self-service setup of experiments, clear definition of targets, and automated analysis of metrics <a class="yt-timestamp" data-t="01:10:39">[01:10:39]</a>.
    *   Automation in analysis reduces reliance on data scientists, allowing them to focus on deeper issues <a class="yt-timestamp" data-t="01:11:27">[01:11:27]</a>.
    *   Consider whether to build an internal platform or buy a third-party solution <a class="yt-timestamp" data-t="01:06:46">[01:06:46]</a>. Modern vendors offer trustworthy platforms that may be suitable for companies starting out <a class="yt-timestamp" data-t="01:07:34">[01:07:34]</a>.

## Speeding Up Experiments

*   **Automated Scorecards**: Once an experiment finishes, the platform should provide a scorecard quickly, ideally within a day, without requiring a week-long wait for a data scientist to analyze <a class="yt-timestamp" data-t="01:12:44">[01:12:44]</a>.
*   **Variance Reduction Techniques**: Methods like capping metrics (e.g., capping revenue at $1,000 or nights booked at 30 days) reduce the variance of metrics, requiring fewer users to achieve statistically significant results faster <a class="yt-timestamp" data-t="01:13:04">[01:13:04]</a>.
*   **Cupid**: This technique uses pre-experiment data to adjust results, leading to unbiased outcomes with lower variance and faster conclusions with fewer users <a class="yt-timestamp" data-t="01:13:55">[01:13:55]</a>.

## Hierarchy of Evidence

When evaluating information, especially in the news, it's important to understand the "hierarchy of evidence" <a class="yt-timestamp" data-t="01:20:34">[01:20:34]</a>. Anecdotal evidence should be trusted least, while observational studies offer some trust, and multiple controlled experiments provide the highest level of trust <a class="yt-timestamp" data-t="01:20:42">[01:20:42]</a>. This critical thinking is crucial in various aspects of life, not just product development <a class="yt-timestamp" data-t="01:21:01">[01:21:01]</a>.