---
title: Experimentation and decisionmaking in business growth
videoId: BVzTfsUMaK8
---

From: [[lennyspodcast]] <br/> 

[[role_of_data_and_experimentation_in_growth | Experimentation]] is critical for driving growth and understanding the performance of new features, particularly within complex online marketplaces <a class="yt-timestamp" data-t="00:01:49">[00:01:49]</a>. Ramesh Johari, a professor at Stanford University, emphasizes that while many businesses claim to be "experiment driven" and "test everything," there are nuances and potential pitfalls to consider <a class="yt-timestamp" data-t="00:41:03">[00:41:03]</a>.

## The Importance of Experimentation

A fundamental aspect of managing online marketplaces involves continually adjusting and refining the platform, a process that can be likened to a game of "whack-a-mole" <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a>. Because changes can be made and unmade easily due to underlying technology, businesses can continuously [[inventiveness_and_innovation_in_business | innovate]] and adapt <a class="yt-timestamp" data-t="00:09:07">[00:09:07]</a>. This dynamic environment necessitates [[role_of_data_and_experimentation_in_growth | experimentation]] to understand the true impact of decisions <a class="yt-timestamp" data-t="00:30:00">[00:30:00]</a>.

## Prediction vs. Decision-Making

Data scientists are often tasked with building machine learning models to predict outcomes, such as predicting which job applicant is most likely to be hired <a class="yt-timestamp" data-t="00:30:05">[00:30:05]</a>. However, the ultimate goal of data science in business is to help make informed decisions, not just predictions <a class="yt-timestamp" data-t="00:33:00">[00:33:00]</a>.

### The Correlation vs. Causation Challenge
Prediction relies on identifying correlations in past data, while effective decision-making requires understanding causation <a class="yt-timestamp" data-t="00:34:17">[00:34:17]</a>. For example, knowing a customer's lifetime value (LTV) is a prediction, but the crucial question for a marketing manager is how much more a customer will spend *because* they received a promotion <a class="yt-timestamp" data-t="00:33:05">[00:33:05]</a>. This distinction, often referred to as "causal inference," shifts the focus from simply recreating past patterns to understanding how decisions will impact future business value <a class="yt-timestamp" data-t="00:34:51">[00:34:51]</a>.

For instance, when evaluating two different ranking algorithms for an Airbnb listing search, the key question should be whether one leads to more bookings or better matches, rather than which one better predicts past user choices <a class="yt-timestamp" data-t="00:36:48">[00:36:48]</a>.

## Common Pitfalls in Experimentation

### Micro-Optimization and Risk Aversion
A common concern with [[role_of_data_and_experimentation_in_growth | experimentation]], especially when focusing heavily on A/B testing, is the risk of micro-optimizing and missing larger opportunities <a class="yt-timestamp" data-t="00:40:00">[00:40:00]</a>. This often stems from:
*   **Incremental Design:** What gets built and tested tends to be incremental due to organizational incentives and risk aversion <a class="yt-timestamp" data-t="00:41:56">[00:41:56]</a>.
*   **Long Experiment Durations:** Experiments are often run for too long, delaying learning and iterating on new ideas <a class="yt-timestamp" data-t="00:42:09">[00:42:09]</a>.

### The "Winners and Losers" Dynamic
Marketplace changes often create winners and losers <a class="yt-timestamp" data-t="00:48:48">[00:48:48]</a>. For example, introducing a "Superhost" badge on Airbnb might redirect attention to badged hosts, potentially taking attention away from unbadged ones <a class="yt-timestamp" data-t="00:44:37">[00:44:37]</a>. A key challenge is determining if the benefits for winners outweigh the negative impacts on losers <a class="yt-timestamp" data-t="00:51:57">[00:51:57]</a>. It's often difficult to articulate that a feature might hurt some users <a class="yt-timestamp" data-t="00:52:05">[00:52:05]</a>.

### The Cost of Learning
[[role_of_data_and_experimentation_in_growth | Learning]] through [[role_of_data_and_experimentation_in_growth | experimentation]] is not free; it comes with a cost. For instance, running a holdout group in a marketing campaign means sacrificing potential revenue from that segment to learn the true impact of the campaign <a class="yt-timestamp" data-t="00:59:03">[00:59:03]</a>. Businesses must be willing to "pay to learn" by allocating resources to both treatment and control groups, even if one performs better <a class="yt-timestamp" data-t="01:00:12">[01:00:12]</a>. The language of "winners and losers" in A/B testing can reinforce the idea that time spent on "losing" experiments is wasted, which is a cultural impediment to learning <a class="yt-timestamp" data-t="01:00:29">[01:00:29]</a>.

## Cultivating an Effective Experimentation Culture

### Prioritizing Learning
To counter the pitfalls of micro-optimization and risk aversion, businesses should foster a culture where [[role_of_data_and_experimentation_in_growth | learning]] is valued as a "win" <a class="yt-timestamp" data-t="00:45:05">[00:45:05]</a>. This means:
*   **Hypothesis-Driven Testing:** Clearly articulate hypotheses in experiment documentation, stating what insights will be gained about business flows, funnels, or user preferences, regardless of a direct "win" in metrics <a class="yt-timestamp" data-t="00:55:02">[00:55:02]</a>.
*   **Cultural Expectation:** Leaders should expect data scientists to do more than just deliver statistically rigorous results; they should expect them to share what they learn about the business in the process <a class="yt-timestamp" data-t="00:54:35">[00:54:35]</a>.
*   **Increased Velocity:** Instead of running experiments for extended periods, prioritize increasing experimentation velocity by trying more diverse ideas and accepting that some may "fail" but still provide valuable lessons <a class="yt-timestamp" data-t="00:43:11">[00:43:11]</a>.

### Incorporating Past Learnings
Traditionally, frequentist statistical methods used in A/B testing do not incorporate past knowledge, treating each experiment in isolation <a class="yt-timestamp" data-t="00:55:51">[00:55:51]</a>. However, employing Bayesian A/B testing allows for building a "prior belief" based on previous experiments, which is then updated with new experimental data <a class="yt-timestamp" data-t="00:56:25">[00:56:25]</a>. This approach rewards contributors for generating information that refines these prior beliefs, creating a positive "information externality" for future experiments <a class="yt-timestamp" data-t="00:56:55">[00:56:55]</a>.

## The Impact of AI on Experimentation
AI and large language models (LLMs) have significantly expanded the frontier of possible ideas and hypotheses to test <a class="yt-timestamp" data-t="01:09:50">[01:09:50]</a>. While AI can automate tasks like code generation and visualization, it places more pressure on humans to identify what truly matters from this "astronomical explosion of explanations and ideas" <a class="yt-timestamp" data-t="01:10:00">[01:10:00]</a>. This means humans become even more crucial in the data science loop, guiding the "funneling down process" and evaluating the quality of insights gained from a vast array of potential experiments <a class="yt-timestamp" data-t="01:10:10">[01:10:10]</a>.