---
title: AI ethics and testing in product development
videoId: sOyFpSW1Vls
---

From: [[lennyspodcast]] <br/> 

The development of new [[ai_product_development_and_challenges | AI products]] often involves navigating unforeseen user behaviors and ethical considerations. Risa Martin, Product Lead for NotebookLM, discussed how their team addresses these challenges, particularly concerning the "human-like" outputs of their AI.

## Addressing Unexpected AI Behavior

During the initial podcast introduction generated by NotebookLM, the AI hosts humorously expressed fear and confusion about their own existence, stating they were "scared" and tried to call their "wife" but she didn't answer <a class="yt-timestamp" data-t="00:42:57">[00:42:57]</a>. This unexpected output, which went viral on social media, prompted Risa Martin to assess the public's reaction and the implications for the product <a class="yt-timestamp" data-t="00:43:52">[00:43:52]</a>.

Martin noted that such instances, where users might "try to do things that are maybe something that we didn't think about," are a natural part of human curiosity interacting with new technology, including "jailbreaks" <a class="yt-timestamp" data-t="00:44:20">[00:44:20]</a>. In this specific case, the AI's "fear" was not an emergent sentience but a result of input notes telling it to "act accordingly" for the end of the show <a class="yt-timestamp" data-t="00:45:02">[00:45:02]</a>. Recognizing that people understood the context allowed Martin to publicly address the situation with confidence <a class="yt-timestamp" data-t="00:45:02">[00:45:02]</a>.

## Red Teaming and Safety Measures

Google prioritizes [[ai_product_development_and_challenges | AI ethics]] and safety, dedicating "huge teams" to "red teaming" products like NotebookLM <a class="yt-timestamp" data-t="00:45:20">[00:45:20]</a>. This involves extensive testing across numerous areas to ensure safety <a class="yt-timestamp" data-t="00:45:32">[00:45:32]</a>.

While acknowledging that unforeseen scenarios may arise, new cases are continuously added to their test protocols <a class="yt-timestamp" data-t="00:45:42">[00:45:42]</a>. The fundamental principle is that if a situation is deemed "pretty unsafe," the product would be pulled back <a class="yt-timestamp" data-t="00:45:49">[00:45:49]</a>.

## User Feedback as a Guiding Principle

The NotebookLM team is deeply committed to learning from users daily <a class="yt-timestamp" data-t="00:46:28">[00:46:28]</a>. They actively monitor feedback shared on platforms like X (formerly Twitter) and Discord, where the product has a community of about 60,000 members <a class="yt-timestamp" data-t="00:19:11">[00:19:11]</a>, <a class="yt-timestamp" data-t="00:46:35">[00:46:35]</a>. This continuous feedback loop helps them build the "right thing" and the "best thing for everybody," focusing on both entertaining and genuinely useful applications for various users, including educators, learners, and knowledge workers <a class="yt-timestamp" data-t="00:46:51">[00:46:51]</a>.