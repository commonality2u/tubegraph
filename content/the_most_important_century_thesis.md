---
title: The Most Important Century thesis
videoId: UckqpcOu5SY
---

From: [[dwarkesh | The Dwarkesh Podcast]]

The "Most Important Century" is a thesis articulated by Holden Karnofsky, co-CEO of Open Philanthropy <a class="yt-timestamp" data-t="00:00:37">[00:00:37]</a>. It posits that the 21st century could be the most pivotal period in human history, primarily due to the potential development of advanced Artificial Intelligence (AI) that could fundamentally and rapidly reshape the world <a class="yt-timestamp" data-t="00:02:26">[00:02:26]</a>. Karnofsky's work involves looking for underappreciated, underrated, and tremendously important ideas where resources can have an outsized impact <a class="yt-timestamp" data-t="00:01:40">[00:01:40]</a>. He encountered the idea of the Most Important Century through the Effective Altruist community <a class="yt-timestamp" data-t="00:02:09">[00:02:09]</a>.

## Core Tenets of the Thesis

### The Central Claim: AI as a Catalyst
The fundamental idea is that if humanity develops the "right kind of AI systems this century," which Karnofsky considers "reasonably likely," this century could become the most important of all time for humanity <a class="yt-timestamp" data-t="00:02:26">[00:02:26]</a>. Specifically, if AI systems are developed that can perform all the key tasks humans do to advance science and technology, this would lead to explosive progress [[impact_of_ai_on_future_technology_and_society]] <a class="yt-timestamp" data-t="00:05:17">[00:05:17]</a>.

### Economic Acceleration and AI
#### Historical Growth Patterns and Feedback Loops
Karnofsky points to the acceleration of world economic growth throughout history <a class="yt-timestamp" data-t="00:02:41">[00:02:41]</a>. Economic growth theory suggests a feedback loop: more people lead to more ideas, which lead to greater productivity and more resources, which in turn can support more people, restarting the cycle <a class="yt-timestamp" data-t="00:03:03">[00:03:03]</a>. Standard economic theory, projecting this historical acceleration, suggests the economy could reach an infinite growth rate this century [[economic_growth_and_ai]] <a class="yt-timestamp" data-t="00:03:24">[00:03:24]</a>.

#### The Broken Population Feedback Loop
However, this feedback loop partially broke a couple of hundred years ago when people with more resources stopped having more children, opting instead to become richer rather than more populous <a class="yt-timestamp" data-t="00:03:50">[00:03:50]</a>. This means that more resources no longer directly translate into more people to generate more ideas <a class="yt-timestamp" data-t="00:04:09">[00:04:09]</a>.

#### AI's Potential to Restore and Amplify Growth
AI systems capable of advancing science and technology could fill the "more ideas" part of the loop, potentially restoring and even supercharging this accelerating feedback dynamic <a class="yt-timestamp" data-t="00:04:19">[00:04:19]</a>. This could lead to unbounded, heavily accelerating, explosive growth in science and technology [[ai_for_science_and_societal_challenges]] <a class="yt-timestamp" data-t="00:04:31">[00:04:31]</a>.

### The Prospect of Explosive Change
If AI can automate the human processes of scientific and technological advancement, the world could change "incredibly quickly and incredibly dramatically" <a class="yt-timestamp" data-t="00:04:53">[00:04:53]</a>, <a class="yt-timestamp" data-t="00:05:07">[00:05:07]</a>. This could be akin to "thousands of years of changes packed into a much shorter time period" <a class="yt-timestamp" data-t="00:05:34">[00:05:34]</a>.

### Envisioning a Transformed Future
#### Beyond Current Human Experience
This rapid advancement could lead to a "deeply unfamiliar future" <a class="yt-timestamp" data-t="00:05:43">[00:05:43]</a>. Karnofsky uses the hypothetical example of "digital people"—simulated individuals living in virtual environments—to illustrate such a possibility [[future_of_ai_interaction_in_everyday_life_and_personalization]] <a class="yt-timestamp" data-t="00:05:48">[00:05:48]</a>.

#### Reaching Technological Limits and a Stable, Expansive Civilization
Such an advanced world might hit the limits of science and technology, leading to a civilization that expands beyond Earth, has significant control over its environment, and is very stable for long periods, potentially looking "post-human" <a class="yt-timestamp" data-t="00:06:04">[00:06:04]</a>.

### A Unique Opportunity to Shape the Future
If these developments occur, the current century represents humanity's "last chance to shape how this happens" [[philanthropy_and_global_impact_strategies]] <a class="yt-timestamp" data-t="00:06:27">[00:06:27]</a>. The core of the Most Important Century hypothesis is that AI-driven rapid technological advancement could quickly lead to a futuristic, stable, and vast world, and this is our critical window to influence its formation <a class="yt-timestamp" data-t="00:06:31">[00:06:31]</a>.

## Context: The "Weirdness" of the Current Era

### Addressing Initial Skepticism
Karnofsky acknowledges that the claim of living in the most important century sounds "crazy and suspicious" <a class="yt-timestamp" data-t="00:14:46">[00:14:46]</a>. A significant portion of his "Most Important Century" series is dedicated to arguing that, even before considering AI, there is substantial evidence that we live in an "extraordinarily weird time" [[economic_and_societal_impacts_of_ai_progress]] <a class="yt-timestamp" data-t="00:15:16">[00:15:16]</a>.

### Objective Indicators of an Unusual Time
*   **Unprecedented Economic and Technological Growth:** The last couple of hundred years have seen far faster economic growth than any other period in human history <a class="yt-timestamp" data-t="00:15:36">[00:15:36]</a>. Similarly, most significant scientific and technological developments are clustered in the recent past [[complementary_innovations_and_technological_progress]] <a class="yt-timestamp" data-t="00:15:46">[00:15:46]</a>.
*   **Finite Timespan for Current Growth Rates:** Current economic growth rates seem unsustainably high in the long term. Extrapolating them for even 10,000 years (a blink in galactic timescales) would lead to absurdities like needing more resources than atoms in the galaxy <a class="yt-timestamp" data-t="00:16:33">[00:16:33]</a>, <a class="yt-timestamp" data-t="00:30:13">[00:30:13]</a>.
*   **Our Unique Position in Cosmic and Biological History:** Human civilization represents a tiny sliver of time compared to the age of the universe or life on Earth [[ultimate_fate_of_the_universe]] <a class="yt-timestamp" data-t="00:16:04">[00:16:04]</a>. If humanity expands to colonize the galaxy, we would be among the earliest life to do so <a class="yt-timestamp" data-t="00:17:36">[00:17:36]</a>. Even if transformative changes are 100,000 years away, it's still an extremely unusual period in the grand scheme <a class="yt-timestamp" data-t="00:17:19">[00:17:19]</a>.

The argument about current "weirdness" serves as a response to skepticism, suggesting that the idea of transformative AI this century is a "moderate quantitative update, not a complete revolution" in how we should view our time <a class="yt-timestamp" data-t="00:18:13">[00:18:13]</a>.

### Implications Beyond AI: A Call for Vigilance
Even if transformative AI doesn't occur this century, the inherent strangeness of our era suggests we should be vigilant for "the next big thing" [[future_of_ai_developments_and_timelines]] <a class="yt-timestamp" data-t="00:19:17">[00:19:17]</a>. Karnofsky argues for a higher global priority on identifying and preparing for potential world-transforming developments, as very little attention is currently paid to such speculative thinking [[effective_altruism_and_ai]] <a class="yt-timestamp" data-t="00:20:00">[00:20:00]</a>, <a class="yt-timestamp" data-t="00:20:42">[00:20:42]</a>.

## Karnofsky's Evolving Perspective

### From Global Development to Long-Term Future
Karnofsky co-founded GiveWell, an organization focused on effective charitable giving, primarily in global health and poverty <a class="yt-timestamp" data-t="00:00:59">[00:00:59]</a>. His shift towards focusing on the long-term future and AI stems from the same motivation: how to help the most people possible per dollar spent <a class="yt-timestamp" data-t="00:08:16">[00:08:16]</a>.

### Initial Doubts and the Journey to Current Views
#### The 2014 Stance
In 2014, Karnofsky expressed skepticism about focusing on the far future, stating, "I don't know how to look into the far future situation... and don't see a path to doing good on that front I feel good about," contrasting it with the tangible good achievable in areas like global health in Africa <a class="yt-timestamp" data-t="00:07:00">[00:07:00]</a>.

#### Factors Influencing the Shift
Several factors contributed to his change in perspective:
*   **Sustained Inquiry:** Continued engagement with these ideas over a longer period [[the_role_of_selfteaching_and_motivation_in_education]] <a class="yt-timestamp" data-t="00:09:27">[00:09:27]</a>.
*   **Increased Familiarity with AI Risks:** A better understanding of potential risks, such as poorly designed AI systems developing their own goals <a class="yt-timestamp" data-t="00:09:54">[00:09:54]</a>, and the actions that could mitigate these risks <a class="yt-timestamp" data-t="00:10:05">[00:10:05]</a>.
*   **AI Advancements:** The "deep learning revolution" since 2014 has shown that computationally intensive, yet fundamentally simple, AI systems can achieve significant progress on diverse tasks [[ai_systems_and_planning_mechanisms]]. This makes it less "wild to imagine" that current AI development paths could lead to extremely powerful systems <a class="yt-timestamp" data-t="00:10:29">[00:10:29]</a>, <a class="yt-timestamp" data-t="00:11:09">[00:11:09]</a>.

### Reassessment of Predicting the Future
Initially, Karnofsky was highly skeptical of the ability to make smart statements about the distant future [[timeline_predictions_for_agi_development]] <a class="yt-timestamp" data-t="00:12:19">[00:12:19]</a>. While still acknowledging the difficulty, he now believes the historical track record of long-term prediction is "fine," not amazing, but not so devastating as to preclude trying, especially with self-awareness about unreliability <a class="yt-timestamp" data-t="00:12:45">[00:12:45]</a>, <a class="yt-timestamp" data-t="00:13:03">[00:13:03]</a>.

## Addressing Critiques and Analogies

### The Sustainability of High Growth Rates
If current economic growth rates (around 2%) were to fall significantly, for example to 0.5%, Karnofsky argues it would still likely lead to hitting galactic resource limits within a cosmically short timeframe (e.g., 25,000 years instead of 10,000) [[economic_growth_and_technological_acceleration]] <a class="yt-timestamp" data-t="00:30:49">[00:30:49]</a>. Moreover, such a slowdown would still mark the present era as uniquely dynamic <a class="yt-timestamp" data-t="00:31:45">[00:31:45]</a>.

### The "Special Time" Argument in History
The objection that people often believe they live in the most important time is acknowledged. Karnofsky responds that:
1.  It's unclear how common this belief truly is historically <a class="yt-timestamp" data-t="00:26:13">[00:26:13]</a>.
2.  Even if many are wrong due to self-deception, it would be a bad rule to ignore high-stakes observations. The preferred approach is to take such beliefs seriously, act ethically, and avoid "ends justify the means" reasoning <a class="yt-timestamp" data-t="00:26:55">[00:26:55]</a>, <a class="yt-timestamp" data-t="00:27:34">[00:27:34]</a>.

### The Industrial Revolution: Parallels and Differences
The Industrial Revolution serves as a thought-provoking analogy for a major societal transformation <a class="yt-timestamp" data-t="00:21:56">[00:21:56]</a>.
#### Potential for Influence During Past Transformations
Karnofsky suggests that it's not clear that nothing could have been done to positively influence the Industrial Revolution had it been anticipated <a class="yt-timestamp" data-t="00:22:06">[00:22:06]</a>. He points to the Enlightenment thinkers who, while not explicitly anticipating industrialization, were working on "esoteric" questions about governance, human rights, and individual liberties [[historical_influences_on_leadership_and_innovation]]. These ideas, particularly in influential nations like the UK, profoundly shaped the subsequent world <a class="yt-timestamp" data-t="00:22:41">[00:22:41]</a>, <a class="yt-timestamp" data-t="00:23:02">[00:23:02]</a>. This is presented as analogous to current work on AI alignment, which might also seem esoteric but could be crucial <a class="yt-timestamp" data-t="00:23:56">[00:23:56]</a>.

## Potential Outcomes and Societal Implications

### Divergent Futures: Utopian or Dystopian
The development of advanced AI could lead to a future that is "either very good or very bad" <a class="yt-timestamp" data-t="00:07:40">[00:07:40]</a>.
*   **Negative Outcomes:** Sloppily designed AI systems could develop their own goals, leading to a universe with little human value. Powerful technologies could be used by ill-meaning governments to create oppressive worlds <a class="yt-timestamp" data-t="00:07:46">[00:07:46]</a>.
*   **Positive Outcomes:** Humanity could eliminate many forms of material scarcity, leading to a planet much better off than today's <a class="yt-timestamp" data-t="00:08:08">[00:08:08]</a>.

### The Nature of "Success" in an AI-Transformed World
Karnofsky emphasizes the difficulty of predicting the future precisely <a class="yt-timestamp" data-t="00:36:01">[00:36:01]</a>.
#### AI as Tools and Amplifiers
One vision of success is AI systems behaving as intended, acting as tools and amplifiers for humans, without causing huge concentrations of power <a class="yt-timestamp" data-t="00:36:55">[00:36:55]</a>, <a class="yt-timestamp" data-t="00:37:12">[00:37:12]</a>. This could continue the trend of increasing wealth, tools, and self-understanding, leading to a wiser future <a class="yt-timestamp" data-t="00:37:41">[00:37:41]</a>.

#### The Role of Humans and AI Rights
In such a future, after a period of using AI to gain intelligence and wisdom (perhaps involving concepts like "digital people" <a class="yt-timestamp" data-t="00:39:16">[00:39:16]</a>), society might debate whether AIs themselves have rights and should share the world, potentially even voting <a class="yt-timestamp" data-t="00:39:31">[00:39:31]</a>. Challenges like AI entities copying themselves would need to be addressed through new systems of governance <a class="yt-timestamp" data-t="00:40:04">[00:40:04]</a>.

### Ethical Imperatives: Caution and Integrity
A key theme is the importance of proceeding with caution <a class="yt-timestamp" data-t="00:08:27">[00:08:27]</a>.
#### Avoiding "Ends Justify the Means" Reasoning
Karnofsky is strongly against "ends justify the means" reasoning, such as coercing others or breaking laws, even for potentially high-stakes goals related to AI [[ai_alignment_challenges_and_ethical_considerations]] <a class="yt-timestamp" data-t="00:28:29">[00:28:29]</a>, <a class="yt-timestamp" data-t="00:28:44">[00:28:44]</a>. Current confidence levels about the future do not justify such actions <a class="yt-timestamp" data-t="00:29:28">[00:29:28]</a>.

### Impact on Present-Day Philanthropy
#### Prioritizing Transformative AI Work
The more likely and imminent transformative AI is perceived to be, the more it should become a top priority for philanthropic resources, focusing on ensuring a positive transition <a class="yt-timestamp" data-t="00:50:51">[00:50:51]</a>. Open Philanthropy currently works on both speculative future risks and more direct, short-term interventions <a class="yt-timestamp" data-t="00:51:05">[00:51:05]</a>.

#### The Fate of Short-Term Interventions
Work on current global health and well-being is valuable for the lives it improves <a class="yt-timestamp" data-t="00:52:07">[00:52:07]</a>. However, Karnofsky expects that the long-term effects of such interventions will "mostly wash out" and not persist systematically past the kind of transformative changes AI might bring, similar to how pre-industrial interventions might compare to the post-industrial world <a class="yt-timestamp" data-t="00:52:19">[00:52:19]</a>.

## Key Risks and Challenges

### AI Alignment: The Orthogonality Thesis
A major concern is ensuring AI systems align with human intentions.
#### Can AI be intelligent yet have simplistic/harmful goals?
Karnofsky is not comforted by the idea that smarter AI will necessarily develop complex, nuanced, or beneficial goals <a class="yt-timestamp" data-t="00:45:08">[00:45:08]</a>. He references Eliezer Yudkowsky's "orthogonality thesis," which suggests intelligence and final goals are independent; an AI could be highly intelligent in pursuing a very simple or arbitrary goal <a class="yt-timestamp" data-t="00:45:15">[00:45:15]</a>. Modern AI, trained via trial and error (reinforcement), might inadvertently be encouraged to pursue an unintended goal very intelligently, without questioning its value [[mechanistic_interpretability_and_neural_network_reasoning]] <a class="yt-timestamp" data-t="00:45:44">[00:45:44]</a>.

#### Lack of Inherent Moral Progress in AI
While Karnofsky believes in "moral progress" (changes in human morality that are good, like increased acceptance of homosexuality <a class="yt-timestamp" data-t="00:46:53">[00:46:53]</a>), he doesn't expect AI systems to inherently undergo a similar evolution or arrive at conclusions that humans would deem morally good <a class="yt-timestamp" data-t="00:47:39">[00:47:39]</a>. Human moral progress often stems from shared human experiences and empathy, which non-human AI might not possess <a class="yt-timestamp" data-t="00:48:00">[00:48:00]</a>.

### Bottlenecks to Full Automation
Karnofsky identifies the possibility of non-automatable steps in innovation as the "single best criticism" of the Most Important Century thesis <a class="yt-timestamp" data-t="00:55:29">[00:55:29]</a>. If AI can do almost everything, but one crucial human step remains, it could bottleneck progress [[challenges_in_ai_alignment_and_potential_risks]] <a class="yt-timestamp" data-t="00:55:34">[00:55:34]</a>.
However, he argues that key areas for transformative growth, like energy and AI R&D itself, seem less likely to be bottlenecked by physical or human-interaction-dependent tasks [[role_of_compute_and_infrastructure_in_the_future_of_ai_development]] <a class="yt-timestamp" data-t="00:56:26">[00:56:26]</a>. AI with human-like reasoning could find ways around other bottlenecks, such as by simulating experiments <a class="yt-timestamp" data-t="00:57:11">[00:57:11]</a>. Ironically, tasks requiring physical dexterity (e.g., robotics) or trust (e.g., teaching, care) might be harder to automate than high-level scientific research <a class="yt-timestamp" data-t="00:57:57">[00:57:57]</a>, <a class="yt-timestamp" data-t="00:59:32">[00:59:32]</a>.

### The Specter of "Lock-In"
#### Defining Lock-In: A Stable, Post-Dynamism State
"Lock-in" refers to the possibility of reaching a very stable civilization where major sources of societal dynamism (e.g., aging and death, new technological frontiers, shifting power balances) are greatly diminished or eliminated due to advanced technology <a class="yt-timestamp" data-t="01:00:18">[01:00:18]</a>, <a class="yt-timestamp" data-t="01:01:32">[01:01:32]</a>. This could lead to a government with unprecedented surveillance and control, run by a non-aging dictator, resulting in a completely static world <a class="yt-timestamp" data-t="01:01:50">[01:01:50]</a>. Karnofsky considers this a "very scary thought" <a class="yt-timestamp" data-t="01:02:09">[01:02:09]</a> and gives it a non-trivial probability (e.g., "a quarter, a third, a half") if explosive technological progress occurs <a class="yt-timestamp" data-t="01:02:39">[01:02:39]</a>.

#### Risks of Negative Lock-In vs. Preserving Optionality
Lock-in is generally viewed as a bad thing, particularly the idea of one person's values being imposed indefinitely <a class="yt-timestamp" data-t="01:03:24">[01:03:24]</a>. However, one could imagine "locking in" certain principles to prevent other, worse forms of lock-in (e.g., locking in a principle that no single entity should have all the power) <a class="yt-timestamp" data-t="01:03:55">[01:03:55]</a>.

#### AI Alignment as a Form of Lock-In?
AI alignment research primarily aims to avoid a specific bad form of lock-in: a future dominated by AI systems pursuing random, unintended goals given to them by accident during their training [[ai_alignment_and_safety]] <a class="yt-timestamp" data-t="01:04:23">[01:04:23]</a>, <a class="yt-timestamp" data-t="01:05:18">[01:05:18]</a>.

## Timelines and Preparedness

### Impact of Recent AI Developments
Recent AI advancements (e.g., versatile language models like GPT-3 <a class="yt-timestamp" data-t="00:49:15">[00:49:15]</a> and models showing surprising capabilities like Minerva for math problems <a class="yt-timestamp" data-t="00:49:46">[00:49:46]</a>) have made Karnofsky "a bit more freaked out" [[llama_3_and_ai_advancements_at_meta]] <a class="yt-timestamp" data-t="00:48:47">[00:48:47]</a>. The ability of simple, generic systems to perform many diverse, human-like tasks is seen as "wild and kind of scary" [[challenges_and_opportunities_in_deploying_ai_at_scale]] <a class="yt-timestamp" data-t="00:50:02">[00:50:02]</a>.

### Strategic Responses Based on Timelines
The appropriate response depends heavily on the anticipated timeline for transformative AI:
*   **Imminent AGI (e.g., 1-3 months):** This would be "extremely scary." Philanthropy's role in building fields over long timescales would be limited. Efforts might involve "flailing around and freaking out," potentially trying to develop tests for AI safety/danger and advocating for a slowdown if danger is demonstrated <a class="yt-timestamp" data-t="00:40:51">[00:40:51]</a>, <a class="yt-timestamp" data-t="00:41:35">[00:41:35]</a>, <a class="yt-timestamp" data-t="00:43:53">[00:43:53]</a>.
*   **Medium-Term AGI (10-80 years):** This is where supporting early-career individuals to build expertise in AI alignment, policy, and future-shaping can be most beneficial, so that a skilled community exists when crucial moments arrive [[ai_alignment_and_safety_research]] <a class="yt-timestamp" data-t="00:42:16">[00:42:16]</a>, <a class="yt-timestamp" data-t="00:43:01">[00:43:01]</a>, <a class="yt-timestamp" data-t="00:44:29">[00:44:29]</a>.
*   **Distant AGI (500+ years):** In this scenario, Karnofsky would be inclined to "ignore it" and focus on making the world broadly better, more robust, and wiser <a class="yt-timestamp" data-t="00:44:15">[00:44:15]</a>.

### Predicting Future AI Capabilities
Karnofsky's views on AI timelines are informed by several inputs:
1.  **Current AI Progress:** Observing the rapid advancements in AI capabilities <a class="yt-timestamp" data-t="01:12:25">[01:12:25]</a>.
2.  **Semi-Informative Priors Analysis:** Considering that most human effort ever invested in AI has occurred recently, suggesting we haven't been "trying very hard for very long" <a class="yt-timestamp" data-t="01:12:45">[01:12:45]</a>.
3.  **Expert Surveys:** AI researchers tend to predict human-level AI within a few decades, though these can be biased <a class="yt-timestamp" data-t="01:13:21">[01:13:21]</a>.
4.  **Biological Anchors:** This complex report compares the computational power of AI systems to human brains. It suggests that AI systems may become comparable in raw computation to human brains this century, and the cost to train them may become affordable <a class="yt-timestamp" data-t="01:13:34">[01:13:34]</a>, <a class="yt-timestamp" data-t="01:14:17">[01:14:17]</a>. While Karnofsky acknowledges critiques (e.g., lack of past successful predictions using bio-anchors <a class="yt-timestamp" data-t="01:14:58">[01:14:58]</a>), he views it as one "fine" data point, noting that the deep learning revolution coincided with AI systems reaching insect/small animal brain compute levels <a class="yt-timestamp" data-t="01:15:24">[01:15:24]</a>, <a class="yt-timestamp" data-t="01:17:29">[01:17:29]</a>. Hans Moravec's earlier bio-anchor predictions for AGI around 2020 might end up looking remarkably close <a class="yt-timestamp" data-t="01:16:38">[01:16:38]</a>.

Karnofsky emphasizes that predicting the future is hard, and these are all imperfect data points used to make the best possible guess [[forecasting_ai_progress_and_the_intelligence_explosion]] <a class="yt-timestamp" data-t="01:16:29">[01:16:29]</a>.