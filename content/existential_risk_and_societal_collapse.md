---
title: Existential risk and societal collapse
videoId: AHkmEnl55jo
---

From: [[dwarkesh | The Dwarkesh Podcast]]

Tyler Cowan discusses his views on existential risk, the potential for societal collapse, and humanity's long-term future, often in dialogue with or critique of [[effective_altruism_and_ai | Effective Altruism (EA) perspectives]]. He expresses a general concern about long-term survival but is skeptical about the efficacy of certain targeted interventions, particularly concerning Artificial General Intelligence (AGI).

## Cowan's General Stance on Humanity's Long-Term Future

Cowan believes that existential risk matters significantly more than most people think <a class="yt-timestamp" data-t="01:04:02">[01:04:02]</a>. However, he is skeptical about humanity's prospects for very long-term survival, such as another 100,000 years, primarily due to the accumulating risk of major wars over time <a class="yt-timestamp" data-t="01:06:17">[01:06:17]</a>. He notes that while the chance of such a war in any given year is low, the cumulative probability becomes substantial over extended periods <a class="yt-timestamp" data-t="01:06:25">[01:06:25]</a>.

Cowan also posits that "the more progress...advances the easier it is to destroy things" <a class="yt-timestamp" data-t="01:27:25">[01:27:25]</a>, highlighting a tension between advancement and increased destructive capability. He finds a metaphor for impermanence in biology, stating, "the existence of sex is the most pessimistic thing there is" <a class="yt-timestamp" data-t="01:09:49">[01:09:49]</a>. He explains this by noting that sex allows for randomization and change, a necessary defense mechanism because static entities tend to be destroyed, implying that "nothing survives for that long" <a class="yt-timestamp" data-t="01:09:30">[01:09:30]</a>.

## Consequences of Societal Collapse

Cowan paints a grim picture of the aftermath of a major global catastrophe, such as a nuclear war:

*   **Permanent Setback:** He suggests humanity would be "permanently set back kind of forever" <a class="yt-timestamp" data-t="00:00:15">[00:00:15]</a>, <a class="yt-timestamp" data-t="01:07:37">[01:07:37]</a>.
*   **Regression to Medieval Conditions:** Society would likely revert to "medieval living standards super small population feudal governance lots of violence rape whatever" <a class="yt-timestamp" data-t="00:00:25">[00:00:25]</a>, <a class="yt-timestamp" data-t="01:07:47">[01:07:47]</a>.
*   **Inability to Address Other Threats:** In such a diminished state, humanity would be unable to develop or maintain protections against other existential threats, such as failing to "build asteroid protection" <a class="yt-timestamp" data-t="00:00:21">[00:00:21]</a>, <a class="yt-timestamp" data-t="01:07:43">[01:07:43]</a>.

### The Difficulty of Rebuilding Civilization

Cowan is deeply skeptical about the prospects of rebuilding civilization after a collapse:

*   He dismisses the idea of an automatic recovery, calling it "crazy wrong" to assume that society could simply "read a copy of the Constitution in 400 years [and be] back on track" <a class="yt-timestamp" data-t="01:07:55">[01:07:55]</a>.
*   The fact that civilization emerged from feudalism once does not guarantee it would happen again, pointing to "hundreds of thousands of years of human history where we seem to make diddly squat progress" <a class="yt-timestamp" data-t="01:08:04">[01:08:04]</a>, <a class="yt-timestamp" data-t="01:08:09">[01:08:09]</a>. He argues, "Don't assume that it happened once means you always rebuild I don't think it does" <a class="yt-timestamp" data-t="01:08:15">[01:08:15]</a>.
*   He states, "I don't think we have good theories of [how our descendants could recover industrial civilization] at all" <a class="yt-timestamp" data-t="01:08:29">[01:08:29]</a>, especially considering potential compounding problems like "nuclear winter crop failures climate change" <a class="yt-timestamp" data-t="01:09:06">[01:09:06]</a>.
*   He observes that historically, many semi-independent parts of the world (circa 1500) did not make significant progress on their own <a class="yt-timestamp" data-t="01:08:33">[01:08:33]</a>.

## Specific Existential Risks Discussed

### Nuclear War
While a nuclear war is unlikely to kill *every* single person on Earth <a class="yt-timestamp" data-t="01:07:33">[01:07:33]</a>, Cowan believes it would lead to the severe, potentially permanent setbacks and regression to medieval conditions detailed above. The cumulative, long-term probability of such a conflict is a central reason for his skepticism about humanity's extended future <a class="yt-timestamp" data-t="01:06:17">[01:06:17]</a>.

### Artificial General Intelligence (AGI)
Cowan is particularly skeptical about current approaches to mitigating risks from AGI:
*   He believes the ability of contemporary efforts (often associated with the EA movement) to limit AGI risk is "basically zero" <a class="yt-timestamp" data-t="01:05:02">[01:05:02]</a>.
*   He argues that if AGI becomes a danger, it will stem from the "worst set of procedures" enacted by "the sloppiest people," who are unlikely to be influenced by current AGI alignment training or regulatory discussions [[ai_alignment_and_safety | AI alignment and safety concerns]] <a class="yt-timestamp" data-t="01:05:16">[01:05:16]</a>, <a class="yt-timestamp" data-t="01:05:35">[01:05:35]</a>.
*   Even if a responsible entity like OpenAI is first to develop AGI and prioritizes alignment, Cowan suggests "the more screwed up successors will just come 10 years later and you know Skynet goes live but 10 years later" [[comparisons_between_atomic_bomb_development_and_modern_ai_advancements | comparisons between atomic bomb development and modern AI advancements]] <a class="yt-timestamp" data-t="01:05:53">[01:05:53]</a>.

## Approaches to Mitigating Existential Risk

### Cowan's Preferred Strategies
Cowan advocates for:
*   **Practical Measures:** He favors tangible actions such as an "asteroid Protection Program" [[impact_and_future_of_ai_in_economic_systems | impact and future of AI in economic systems]] <a class="yt-timestamp" data-t="01:04:40">[01:04:40]</a>.
*   **Broad-Based Improvements:** He believes that effective actions to limit existential risk are often not fundamentally different from general societal improvements like "looking for more Talent growing GDP supporting science" <a class="yt-timestamp" data-t="01:04:13">[01:04:13]</a>.
*   **Epistemic Modesty:** He stresses the need for caution regarding overly specific hypotheses about future risks and their solutions <a class="yt-timestamp" data-t="01:04:54">[01:04:54]</a>.
*   **Present Action:** His personal response to these concerns is to "try actually to do something about it" by focusing on constructive actions in the present, while acknowledging the "extreme fallibility embedded in all such projections" <a class="yt-timestamp" data-t="01:27:50">[01:27:50]</a>, <a class="yt-timestamp" data-t="01:28:01">[01:28:01]</a>.

### Critique of Some Effective Altruism (EA) Approaches
While aligning with EAs on the high importance of existential risk <a class="yt-timestamp" data-t="01:04:02">[01:04:02]</a>, Cowan offers some critiques:
*   He suggests some within the EA community might "overvalue it a bit" <a class="yt-timestamp" data-t="01:04:10">[01:04:10]</a> or "flip out about it a bit too much" with highly specific and difficult-to-verify hypotheses, especially concerning AGI [[potential_risks_of_agi | potential risks of AGI]] <a class="yt-timestamp" data-t="01:04:29">[01:04:29]</a>, <a class="yt-timestamp" data-t="01:05:02">[01:05:02]</a>.
*   He points out that addressing complex issues like nuclear safety involves nuanced foreign policy debates, where merely "calling yourself worried about existential risk" does not inherently add substantial value beyond existing expertise. He also observes that some EA proponents may be "underinvested in historical and cultural forms of knowledge" relevant to these complex problems [[historical_influences_on_leadership_and_innovation | historical influences on leadership and innovation]] <a class="yt-timestamp" data-t="01:06:46">[01:06:46]</a>, <a class="yt-timestamp" data-t="01:07:07">[01:07:07]</a>.