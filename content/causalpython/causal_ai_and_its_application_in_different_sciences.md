---
title: Causal AI and its application in different sciences
videoId: UQ8j-DEkB98
---

From: [[causalpython]] <br/> 

[[causality_in_ai | Causality]] is a fundamental concept for understanding the world, distinguishing between mere prediction and the ability to intervene on a system <a class="yt-timestamp" data-t="02:03:15">[02:03:15]</a>. This article explores the philosophical and practical dimensions of [[causality_in_ai | causal AI]] and its applications across various scientific fields, drawing insights from Dr. Naftali Weinberger, a philosopher of science <a class="yt-timestamp" data-t="01:27:07">[01:27:07]</a>.

## Challenging Traditional Views on Causality
Over 100 years ago, philosopher Bertrand Russell famously stated that [[causality_in_ai | causality]] was "a relic of the bygone age" <a class="yt-timestamp" data-t="01:10:07">[01:10:07]</a>. This perspective was rooted in the idea that for causation to be scientifically legitimate, it needed to play a role in fundamental physics, a field Russell considered through the lens of gravitational astronomy before quantum mechanics <a class="yt-timestamp" data-t="01:37:07">[01:37:07]</a>.

However, modern scientific needs firmly refute Russell's argument <a class="yt-timestamp" data-t="02:11:07">[02:11:07]</a>. If one examines fields like epidemiology, sociology, or industry, the necessity of [[causal_reasoning_in_ai | causal knowledge]] becomes evident <a class="yt-timestamp" data-t="02:03:15">[02:03:15]</a>. The usefulness and scientific legitimacy of [[causality_in_ai | causation]] are not in question <a class="yt-timestamp" data-t="02:37:07">[02:37:07]</a>. While Russell might have been vindicated in the sense that [[causality_in_ai | causation]]'s role in fundamental physics remains controversial and debated, its importance in other sciences is clear <a class="yt-timestamp" data-t="02:28:07">[02:28:07]</a>.

## Causation at Different Scales
A crucial aspect of [[causal_reasoning_in_ai | causal reasoning]] is its sensitivity to time and spatio-temporal scales <a class="yt-timestamp" data-t="02:44:07">[02:44:07]</a>. Any scientific claim or phenomenon study must consider these scales <a class="yt-timestamp" data-t="02:52:07">[02:52:07]</a>. This concept applies universally, from physicists analyzing systems at quantum versus Newtonian scales <a class="yt-timestamp" data-t="03:06:07">[03:06:07]</a> to biologists distinguishing between cell biology and systems biology <a class="yt-timestamp" data-t="03:18:07">[03:18:07]</a>.

This idea, systematically explored by Herbert Simon and notably in a 1994 paper by Yumi Iwasaki and Herbert Simon on "Causality and Model Abstraction," highlights that causal relationships are relative to a specific time scale <a class="yt-timestamp" data-t="03:40:07">[03:40:07]</a>.

### The Flossing Example
A simple, everyday illustration of time-scale relativity in causation is the relationship between flossing and gum bleeding <a class="yt-timestamp" data-t="04:00:07">[04:00:07]</a>. If someone who doesn't floss starts, their gums will likely bleed *that day*. However, if they floss consistently for a week, their gums will cease to bleed <a class="yt-timestamp" data-t="04:22:07">[04:22:07]</a>. Both statements are true depending on the time scale considered <a class="yt-timestamp" data-t="04:40:07">[04:40:07]</a>.

### Hidden Confounders and Long-Term Factors
In industry, a common concern in [[causal_ai_in_business_applications | causal AI]] is the presence of hidden confounders <a class="yt-timestamp" data-t="05:05:07">[05:05:07]</a>. Not including potential confounders in a causal model is a strong assumption, as it implicitly supposes they are not there <a class="yt-timestamp" data-t="06:31:07">[06:31:07]</a>. This is why [[causal_ai_and_its_role_in_experiments | experiments]] are often preferred, as they can, by design, remove the influence of confounders <a class="yt-timestamp" data-t="06:37:07">[06:37:07]</a>.

Long-term factors, however, are not necessarily confounders if they remain constant over the time scale of interest <a class="yt-timestamp" data-t="09:06:07">[09:06:07]</a>. For example, the capitalist economy of a country might influence all trading behavior over centuries but is a background context rather than a confounder for shorter-term studies <a class="yt-timestamp" data-t="08:31:07">[08:31:07]</a>. They become problematic when one extrapolates across different time scales <a class="yt-timestamp" data-t="13:12:07">[13:12:07]</a>.

### Rainfall and Crop Growth Example
A classic example from Simon and Werr illustrates this:
*   On a short time scale, rainfall is an exogenous variable; it causes crop growth, but crop growth does not cause rainfall <a class="yt-timestamp" data-t="10:41:07">[10:41:07]</a>.
*   However, over a very long time scale (e.g., 100-200 years), agriculture can influence climate, creating a feedback loop where crop growth influences future rainfall <a class="yt-timestamp" data-t="11:49:07">[11:49:07]</a>.

Both models are appropriate for their respective time scales <a class="yt-timestamp" data-t="13:02:07">[13:02:07]</a>. The issue arises only when models are applied inappropriately across scales <a class="yt-timestamp" data-t="13:12:07">[13:12:07]</a>.

## [[causality_in_ai | Causality]] in Complex Systems and Emergence
The discussion of scales naturally leads to complex systems and phenomena like emergence <a class="yt-timestamp" data-t="15:39:07">[15:39:07]</a>. The main question is whether [[causality_in_ai | causation]] exists in complex systems <a class="yt-timestamp" data-t="16:06:07">[16:06:07]</a>. When [[causal_reasoning_in_ai | causal relationships]] are understood as relative to a time scale, much of what is called "emergence" can be understood <a class="yt-timestamp" data-t="16:39:07">[16:39:07]</a>. Patterns of regularity can emerge at higher levels of description, even if the lower levels appear chaotic <a class="yt-timestamp" data-t="16:57:07">[16:57:07]</a>.

The idea of "levels" in philosophy often assumes the same object is being described at different granularities (e.g., a physical object vs. a corkscrew) <a class="yt-timestamp" data-t="18:02:07">[18:02:07]</a>. However, when considering a system at different scales, one is effectively considering different objects or different "zooms" <a class="yt-timestamp" data-t="18:45:07">[18:45:07]</a>. For example, an oven heating a room:
*   In 5 minutes, the oven causes the room to get hotter.
*   In an hour, due to a thermostat, the room is not hotter <a class="yt-timestamp" data-t="19:02:07">[19:02:07]</a>.
These models are not contradictory; they describe different time scales <a class="yt-timestamp" data-t="19:18:07">[19:18:07]</a>. This perspective avoids metaphysical puzzles about where emergent properties "come from," as it acknowledges that the universe naturally exhibits different patterns of behavior at different spatio-temporal scales <a class="yt-timestamp" data-t="20:16:07">[20:16:07]</a>.

## Defining [[causality_in_ai | Causality]]: A Methodological Approach
From a philosophical standpoint, [[causality_in_ai | causality]] is best understood methodologically <a class="yt-timestamp" data-t="21:04:07">[21:04:07]</a>. Influenced by Nancy Cartwright's work, the core idea is that [[causal_reasoning_in_ai | causal knowledge]] is needed to understand the difference between mere prediction and effective interventions <a class="yt-timestamp" data-t="21:12:07">[21:12:07]</a>. For instance, knowing that giving a drug *causes* recovery, not just that drug-takers are more likely to recover <a class="yt-timestamp" data-t="21:27:07">[21:27:07]</a>.

Modern [[causal_ai_and_machine_learning_intersection | causal AI]] algorithms are built on this foundation, focusing on interventions <a class="yt-timestamp" data-t="21:45:07">[21:45:07]</a>. This methodological approach is not merely pragmatic; the effectiveness of [[causal_ai_and_its_role_in_experiments | causal methods]] stems from features of the world itself <a class="yt-timestamp" data-t="22:29:07">[22:29:07]</a>. By studying [[causal_ai_and_its_role_in_experiments | causal methodology]], we can infer properties about the nature of [[causality_in_ai | causation]] and the world <a class="yt-timestamp" data-t="23:02:07">[23:02:07]</a>.

## [[Application of Quantum Physics Principles in Causal AI | Causality in Physics]]
While Bertrand Russell's skepticism regarding [[causality_in_ai | causality]] in fundamental physics (like gravitational astronomy) was, in some sense, vindicated by later developments, it doesn't preclude its role elsewhere <a class="yt-timestamp" data-t="24:00:07">[24:00:07]</a>. The question of whether [[causality_in_ai | causation]] exists in fundamental physics remains controversial <a class="yt-timestamp" data-t="24:40:07">[24:40:07]</a>. However, [[causal reasoning in AI | causal reasoning]] is clearly present in other areas of physics, such as when building a large hadron collider, where interventions are made <a class="yt-timestamp" data-t="25:16:07">[25:16:07]</a>.

The idea of interventions, central to [[causality_in_ai | causality]], might stop making sense at certain physical levels. For example, quantum mechanical setups and well-confirmed results from Bell's inequalities and EPR experiments suggest difficulties in providing a coherent [[causal_reasoning_in_ai | causal interpretation]] <a class="yt-timestamp" data-t="26:08:07">[26:08:07]</a>. This indicates that at certain scales, [[causality_in_ai | causation]] might indeed break down <a class="yt-timestamp" data-t="26:33:07">[26:33:07]</a>.

## Interventions and Dynamical Systems
The paper "Intervening and Letting Go" by Naftali Weinberger was inspired by the ideal gas system <a class="yt-timestamp" data-t="04:09:07">[04:09:07]</a>. In such systems, different setups lead to different equilibrium [[causal_reasoning_in_ai | causal models]]:
*   A sealed container (fixed volume) where volume and temperature cause pressure <a class="yt-timestamp" data-t="04:41:07">[04:41:07]</a>.
*   A movable piston (constant pressure) where pressure and temperature cause volume <a class="yt-timestamp" data-t="04:54:07">[04:54:07]</a>.

A puzzle arises when an intervention (like fixing the volume with a pin) changes the causal relationships in a way not perfectly predicted by the standard *do*-operator in causal models <a class="yt-timestamp" data-t="04:40:07">[04:40:07]</a>. This highlights that these equilibrium models don't capture the full dynamics or transitions between states <a class="yt-timestamp" data-t="04:46:07">[04:46:07]</a>.

Clark Glymour directed Weinberger to the work of Denver Dash, who focused on the dynamics of such systems <a class="yt-timestamp" data-t="04:37:07">[04:37:07]</a>. Dynamic causal models, as described by Iwasaki and Simon, reveal a feedback loop in the system when away from equilibrium <a class="yt-timestamp" data-t="04:53:07">[04:53:07]</a>. This feedback loop is destroyed when an intervention, like "clamping" a variable, holds it fixed indefinitely <a class="yt-timestamp" data-t="04:58:07">[04:58:07]</a>.

This distinction between **clamp interventions** (holding a variable fixed indefinitely) and **shock interventions** (a brief influence at one time step) is crucial when dealing with time-varying systems <a class="yt-timestamp" data-t="04:58:07">[04:58:07]</a>. Real-world interventions often involve different durations and timings <a class="yt-timestamp" data-t="05:43:07">[05:43:07]</a>. For example, in [[causal_ai_in_medicine | medicine]], chemotherapy aims to change the entire state and stability properties of a system to bring it to a new equilibrium, influencing feedback loops rather than just isolated variables <a class="yt-timestamp" data-t="05:07:07">[05:07:07]</a>.

Research from Joris Mooij's lab at the University of Amsterdam has significantly advanced the understanding of linking differential equations (from dynamical systems) with causal models <a class="yt-timestamp" data-t="05:32:07">[05:32:07]</a>. These dynamic causal models allow for understanding relationships both at equilibrium and away from it, integrating concepts like time derivatives and integration into the [[causal_ai_and_machine_learning_intersection | causal framework]] <a class="yt-timestamp" data-t="05:41:07">[05:41:07]</a>. This establishes a bridge between different modeling paradigms, building on the pioneering work of Herbert Simon in structural equations <a class="yt-timestamp" data-t="05:51:07">[05:51:07]</a>.

## [[Applications of Causal Models in Biology and AI | Causality in Cognitive Science]]
The principles of [[causality_in_ai | causality]] and scale also apply to debates in cognitive science, particularly between computational and dynamical systems views of cognition <a class="yt-timestamp" data-t="02:06:07">[02:06:07]</a>. Rather than viewing them as conflicting paradigms, they can be seen as different valid descriptions of the same system at different scales <a class="yt-timestamp" data-t="03:00:07">[03:00:07]</a>.

Studies involving artificial organisms with multiple neurons evolving to perform tasks (e.g., finding food, avoiding poison) demonstrate that thinking about a system computationally or dynamically are simply two ways of characterizing it <a class="yt-timestamp" data-t="02:20:07">[02:20:07]</a>. Both approaches involve modeling assumptions, and neither is a "purer" description <a class="yt-timestamp" data-t="03:04:07">[03:04:07]</a>.

## Overcoming Fear in Causal Modeling
A common fear for those new to [[causality_in_ai | causality]] is building a Directed Acyclic Graph (DAG) that might be wrong <a class="yt-timestamp" data-t="03:47:07">[03:47:07]</a>. However, not building a graph doesn't guarantee correctness either <a class="yt-timestamp" data-t="03:55:07">[03:55:07]</a>. If one intends to make [[causal_reasoning_in_ai | causal claims]], model interventions, or interpret data in a causal way, then [[causal_ai_and_its_role_in_experiments | causal models]] are necessary <a class="yt-timestamp" data-t="03:30:07">[03:30:07]</a>.

A critical insight is that in [[causal_ai_and_its_role_in_experiments | causal models]], the assumptions are encoded in the *missing* arrows <a class="yt-timestamp" data-t="03:42:07">[03:42:07]</a>. A lack of an arrow between two variables, or a common cause, represents a very strong assumption <a class="yt-timestamp" data-t="03:52:07">[03:52:07]</a>. This "no causes in, no causes out" principle means causal assumptions are required to derive causal knowledge <a class="yt-timestamp" data-t="03:17:07">[03:17:07]</a>.

Building an explicit model, even if it's wrong, is a learning opportunity that allows for modification and improvement <a class="yt-timestamp" data-t="03:32:07">[03:32:07]</a>. In contrast, implicitly making causal assumptions without formal modeling (e.g., in psychology studies) prevents learning from mistakes <a class="yt-timestamp" data-t="03:39:07">[03:39:07]</a>. [[causal_ai_and_its_role_in_experiments | Causal models]] make assumptions systematic and explicit, clarifying the conditions needed to reach desired conclusions <a class="yt-timestamp" data-t="03:11:07">[03:11:07]</a>.

## The Future of [[causality_in_ai | Causal AI]]
Whether [[causality_in_ai | causality]] is a fleeting trend or a lasting methodology is an empirical question <a class="yt-timestamp" data-t="05:57:07">[05:57:07]</a>. However, [[causal_reasoning_in_ai | causal AI]] differs fundamentally from approaches like large language models that simply process vast amounts of data for prediction <a class="yt-timestamp" data-t="05:56:07">[05:56:07]</a>. [[causal_reasoning_in_ai | Causal reasoning]] is about enabling limited beings to interact with a complex world in a localized and manageable way, without needing universal knowledge <a class="yt-timestamp" data-t="05:48:07">[05:48:07]</a>. It's about distinguishing between prediction and effective intervention <a class="yt-timestamp" data-t="02:18:07">[02:18:07]</a>.

## Bridging Academia and Industry
The interaction between [[causality_in_ai | causal researchers]] in academia and industry is crucial <a class="yt-timestamp" data-t="01:10:15">[01:10:15]</a>. There is a need to build bridges between different "clans" or schools of thought (e.g., potential outcomes, single-world intervention graphs, DAGs, epidemiology, econometrics, computer science) <a class="yt-timestamp" data-t="01:11:04">[01:11:04]</a>. Often, these groups discuss the same concepts using slightly different terminology, leading to missed opportunities <a class="yt-timestamp" data-t="01:11:53">[01:11:53]</a>. Promoting cross-pollination of ideas and applications is vital for advancing the field and addressing real-world problems in [[application_of_causal_ai_in_marketing_and_business | business]] and other sectors <a class="yt-timestamp" data-t="01:11:53">[01:11:53]</a>.

## Resources for Learning [[causality_in_ai | Causality]]
For individuals starting their journey into [[causality_in_ai | causality]], several resources are recommended:
*   **Felix Elwert (Sociologist at Wisconsin)**: A chapter in a handbook for [[causality_in_ai | causality]] covering confounding, d-separation, endogenous selection bias, and time-varying treatments <a class="yt-timestamp" data-t="01:13:33">[01:13:33]</a>.
*   **Felix Elwert and Chris Winship (Harvard)**: A paper specifically on endogenous selection bias (conditioning on a collider), offering a flavor of causal graphs <a class="yt-timestamp" data-t="01:14:10">[01:14:10]</a>.
*   **Frederick Eberhardt (Caltech)**: A short introduction (around 10-12 pages) to [[causal_ai_and_machine_learning_intersection | causal inference algorithms]], including traditional and machine learning approaches, which can solve the two-variable problem (e.g., distinguishing X causes Y from Y causes X without other variables) <a class="yt-timestamp" data-t="01:14:39">[01:14:39]</a>.
*   **Textbook by Schoelkopf, Janzing, and Peters**: A more in-depth resource for algorithmic aspects <a class="yt-timestamp" data-t="01:15:53">[01:15:53]</a>.
*   **Michael Nielsen's website**: An accessible introduction to d-separation <a class="yt-timestamp" data-t="01:16:07">[01:16:07]</a>.
*   **Stanford Encyclopedia of Philosophy article on Simpson's Paradox**: An article discussing how [[causal_ai_and_its_role_in_experiments | causal modeling methods]] relate to earlier philosophical and probabilistic theories of [[causality_in_ai | causality]] <a class="yt-timestamp" data-t="01:16:33">[01:16:33]</a>.

Finding an aspect of [[causality_in_ai | causality]] that genuinely interests you is key, as it provides an entry point to explore the broader landscape <a class="yt-timestamp" data-t="01:05:28">[01:05:28]</a>. There are many avenues into [[causal_ai_and_its_role_in_experiments | causal inference]], whether through algorithms, philosophy, or specific applications <a class="yt-timestamp" data-t="01:06:19">[01:06:19]</a>.