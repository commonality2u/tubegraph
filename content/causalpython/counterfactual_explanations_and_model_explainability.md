---
title: Counterfactual Explanations and Model Explainability
videoId: Y4wMksHyMFg
---

From: [[causalpython]] <br/> 

[[explainable_ai_and_feature_importance | Counterfactual explanations]] aim to explain the behavior of blackbox models without the need to examine their internal workings [00:04:27]. They operate by identifying the minimal changes required in a model's input to produce a desired different output [00:04:34]. While these explanations can carry a [[Counterfactual reasoning in decision making | causal meaning]], this meaning is often not explicitly addressed in the literature [00:04:44].

## Faithfulness vs. Plausibility

Previous work in [[explainable_ai_and_feature_importance | counterfactual explanations]] has primarily focused on "plausibility" [00:04:55]. Plausibility implies that the generated counterfactual explanation is consistent with the true data generating process [00:05:13]. For applications such as explanations for human decision-makers or algorithmic recourse, the goal has been to generate explanations that appear intuitive and make sense to users [00:05:02]. For instance, a plausible counterfactual should reside in a region of high data density [00:05:46].

However, there is a recognized limitation when focusing solely on plausibility: it can distract from the primary objective of accurately explaining how a blackbox model behaves [00:05:58]. An illustrative example using MNIST digits highlights this:
*   A factual image, correctly classified as a '9', is given [00:06:18].
*   The task for the [[explainable_ai_and_feature_importance | counterfactual generator]] is to determine what changes are necessary for the model to classify it as a '7' [00:06:26].
*   Several approaches can produce "valid" counterfactuals (i.e., the classifier confidently predicts '7') [00:06:44].
*   Some of these valid counterfactuals resemble adversarial attacks, such as those generated by the "water" and "shoot" methods [00:06:50].
*   Only one method, "Revise" (which uses a variational autoencoder to understand the data generating process), produces a counterfactual that is intuitively "plausible" [00:07:03].
*   The concern is that if multiple valid explanations exist, presenting only the plausible one risks "whitewashing" the model by not accurately reflecting its true behavior [00:07:39].

The presented work advocates for prioritizing "faithfulness first and plausibility second" [00:08:09], arguing that generating plausible but unfaithful explanations for blackbox models has limited practical value [00:08:15]. It's important to note that "faithfulness" in this context differs from the "faithfulness assumption" used in [[structural_causal_models_and_causal_discovery | causal discovery]] [00:08:26].

## Causal Knowledge and Model Accountability

Incorporating [[structural_causal_models_and_causal_discovery | causal knowledge]] can lead to [[Counterfactual reasoning in decision making | causally valid counterfactuals]] that are more efficiently generated and at a lower cost to individuals [00:08:51]. This approach leverages external causal information.

Alternatively, the paper proposes relying directly on properties provided by the model itself, rather than external surrogate tools, to derive better explanations [00:09:06]. The idea is to place all accountability on the model [00:09:17]. It suggests that plausible counterfactuals can still be obtained if the model has learned plausible explanations for the data [00:09:24]. This is achieved by borrowing ideas from energy-based modeling and conformal prediction to quantify the generative capacity and predictive uncertainty of the classifier [00:09:29]. Both of these approaches are model-agnostic and can be applied to any differentiable classifier [00:09:49].