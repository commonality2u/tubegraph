---
title: Impacts of sample selection bias in LLMs
videoId: sljBU_HFnFs
---

From: [[causalpython]] <br/> 

Research presented at the Triple AI conference highlights critical considerations regarding the inherent limitations and potential biases in [[large_language_models_llms_learning_limitations | Large Language Models (LLMs)]] due to sample selection bias.

## Motivation and Problem Statement

Even though [[large_language_models_llms_learning_limitations | LLMs]] are trained on increasingly vast amounts of data, sometimes described as the "whole world's data," they fundamentally operate on a dataset that is a subsampled representation of the real-world target domain in which they are used <a class="yt-timestamp" data-t="00:04:50">[00:04:50]</a>. This act of sampling inherently introduces the potential for sample selection bias <a class="yt-timestamp" data-t="00:05:07">[00:05:07]</a>. Researchers are exploring various areas where these selection pressures might exist <a class="yt-timestamp" data-t="00:05:13">[00:05:13]</a>.

## Main Insights and Learnings

A primary insight is the importance of considering the data generating process when working with [[large_language_models_llms_learning_limitations | LLMs]] <a class="yt-timestamp" data-t="00:05:17">[00:05:17]</a>. It is crucial not to assume that the dataset is independent and identically distributed (IID), regardless of its size <a class="yt-timestamp" data-t="00:05:21">[00:05:21]</a>.

There are significant parallels between [[large_language_models_llms_and_causation | LLMs]] and the conditional probabilities that can be hypothesized with a causal directed acyclic graph (DAG) <a class="yt-timestamp" data-t="00:05:25">[00:05:25]</a>. This allows for proposing a DAG and measuring the conditional probabilities it entails within an [[large_language_models_llms_and_causation | LLM]] <a class="yt-timestamp" data-t="00:05:37">[00:05:37]</a>. It is also advised to look at the log probabilities generated by the [[large_language_models_llms_and_causation | LLM]] for deeper introspection beyond just the tokens <a class="yt-timestamp" data-t="00:05:45">[00:05:45]</a>.

## Real-World Impact

Understanding and addressing the "missing data problem" or sample selection bias in [[large_language_models_llms_learning_limitations | LLM]] training data is vital <a class="yt-timestamp" data-t="00:06:37">[00:06:37]</a>. It's hypothesized that very important elements might be omitted from these datasets, often mundane or "boring" common-sense reasoning that people learn from a young age <a class="yt-timestamp" data-t="00:06:43">[00:06:43]</a>. This missing, obvious information could be precisely what [[large_language_models_llms_learning_limitations | language models]] lack, as no one takes the time to explicitly write down what is considered self-evident <a class="yt-timestamp" data-t="00:06:56">[00:06:56]</a>.

## Related Concepts

*   **[[application_of_large_language_models_llms_in_causal_discovery | Application of Large Language Models (LLMs) in Causal Discovery]]**: Research explores using [[large_language_models_llms_and_causation | LLMs]] for mediation analysis by introspecting into the [[large_language_models_llms_and_causation | LLM]], freezing weights, and drawing parallels between causal DAGs and Transformer models <a class="yt-timestamp" data-t="00:07:21">[00:07:21]</a>. This method can validate hypotheses and provide a general approach for those interested in [[large_language_models_llms_and_causation | LLMs]] and causality <a class="yt-timestamp" data-t="00:07:33">[00:07:33]</a>.
*   **[[large_language_models_llms_and_causation | Large Language Models and Causation]]**: There is an ongoing discussion about whether [[large_language_models_llms_and_causation | LLMs]] can learn causal relations <a class="yt-timestamp" data-t="00:15:47">[00:15:47]</a>. A key argument against [[large_language_models_llms_and_causation | LLMs]] learning causation from pure observations is that causal relationships are typically not inferable from observational data alone <a class="yt-timestamp" data-t="00:17:33">[00:17:33]</a>. However, this theorem might be challenged in the context of [[large_language_models_llms_and_causation | LLMs]] because they access an enormous amount of data that already contains interventions, unlike the limited observational data humans typically access <a class="yt-timestamp" data-t="00:17:47">[00:17:47]</a>. This suggests [[large_language_models_llms_and_causation | LLMs]] might develop some form of causal understanding <a class="yt-timestamp" data-t="00:18:13">[00:18:13]</a>.
*   **[[fairness_and_bias_in_language_models | Fairness and Bias in Language Models]]**: Research also focuses on why [[fairness_and_bias_in_language_models | language models]] learn biases (e.g., gender, race, religion), how they can exacerbate existing societal biases, and methods to measure and mitigate these biases <a class="yt-timestamp" data-t="00:18:51">[00:18:51]</a>. One approach involves pruning specific parts of [[fairness_and_bias_in_language_models | language models]] found responsible for bias to make the model less biased <a class="yt-timestamp" data-t="00:19:20">[00:19:20]</a>.