---
title: causal AI and machine learning
videoId: relI7Q9A03g
---

From: [[causalpython]] <br/> 

The Causal Bandits podcast featured Dr. Andrew Lampinen, a Senior Research Scientist at Google DeepMind, to discuss the intersection of [[causality_and_machine_learning | causality and machine learning]], particularly in the context of large language models (LLMs) <a class="yt-timestamp" data-t="00:24:00">[00:24:00]</a>.

## Active vs. Passive Strategies in Language Models

Dr. Lampinen's recent paper, part of a series on large language models and [[causality_and_machine_learning | causality]] released in the second half of 2023, introduces the concepts of "active" and "passive" strategies <a class="yt-timestamp" data-t="00:11:00">[00:11:00]</a>. This distinction differs from traditional [[machine_learning_and_causality | machine learning]] terminology, which often uses "interventional," "observational," or "counterfactual" based on Pearl's causal hierarchy <a class="yt-timestamp" data-t="00:43:00">[00:43:00]</a>.

The key distinction in their paper is that while language models are trained passively—processing language data generated by others from the internet—this data is not necessarily purely observational <a class="yt-timestamp" data-t="00:12:00">[00:12:00]</a>. For instance, a scientific paper, though absorbed passively, contains interventional data from experiments. Similarly, Stack Overflow posts involve debugging and experimentation, and conversations include interventions from speakers <a class="yt-timestamp" data-t="00:24:00">[00:24:00]</a>. Therefore, LLMs learn passively from data that is inherently interventional <a class="yt-timestamp" data-t="00:27:00">[00:27:00]</a>.

This fact is explored to understand if it impacts the models' capabilities to generalize beyond their training data <a class="yt-timestamp" data-t="00:30:00">[00:30:00]</a>. The paper proposes two reasons why this training data could imbue LLMs with causal strategies or understanding that generalizes:
1.  **Causal Strategies**: By learning from others' interventions, models might discover a strategy for intervening in new situations to uncover causal structures for downstream goals <a class="yt-timestamp" data-t="00:39:00">[00:39:00]</a>. Empirically, they show that passively observing interventions can lead to a generalizable strategy for determining causal structures <a class="yt-timestamp" data-t="00:59:00">[00:59:00]</a>.

### Experimental Setup and Insights
To study this in a controlled manner, a simpler model was trained on data showing interventions on causal directed acyclic graphs (DAGs) <a class="yt-timestamp" data-t="00:41:00">[00:41:00]</a>. The model observed a series of interventions on a DAG and a goal (e.g., maximize a variable), along with interventions attempting to achieve that goal <a class="yt-timestamp" data-t="00:53:00">[00:53:00]</a>. The aim was to see if the model, trained passively on data where others discover and use causal structures, could generalize to actively intervene and discover new causal structures at test time <a class="yt-timestamp" data-t="01:14:00">[01:14:00]</a>.

The results indicate that the system, trained passively, can apply observed causal strategies to actively intervene at test time, discover new causal structures, and exploit them <a class="yt-timestamp" data-t="01:43:00">[01:43:00]</a>. The model's approach closely approximates correct causal reasoning compared to simpler heuristic or associational strategies <a class="yt-timestamp" data-t="01:52:00">[01:52:00]</a>.

## Causal Parrots and Meta-Structural Causal Models

The "Causal Parrots" paper posits that large language models learn a "meta-structural causal model" based on correlations of causal facts present in their training data <a class="yt-timestamp" data-t="06:13:00">[06:13:00]</a>. This paper concludes that LLMs can "talk causality" but do not "reason causally" <a class="yt-timestamp" data-t="06:40:00">[06:40:00]</a>.

Dr. Lampinen's results, however, suggest a contradiction, as their models demonstrate the capability to discover and apply a causal reasoning algorithm in new, generalizable settings, given a suitable training regime <a class="yt-timestamp" data-t="07:07:00">[07:07:00]</a>. While natural data might contain more correlations and less interventional data, leading LLMs to learn more correlational strategies, Lampinen's work shows that with sufficient interventional data and explicit explanations in the prompt, LLMs can effectively learn to experiment and discover new causal structures <a class="yt-timestamp" data-t="08:08:00">[08:08:08]</a>. This suggests that LLMs possess the ability to discover some causal strategies <a class="yt-timestamp" data-t="08:08:00">[08:08:08]</a>.

## Training Paradigms and Explanations

Current LLM training paradigms are largely driven by what works and allows for data use at scale <a class="yt-timestamp" data-t="08:46:00">[08:46:00]</a>. Dr. Lampinen suggests that integrating auxiliary tasks, such as providing natural language explanations of rewards in reinforcement learning environments, can significantly improve learning and shape out-of-distribution generalization, even with confounded data <a class="yt-timestamp" data-t="09:18:00">[09:18:00]</a>.

Knowledge about the data structure can be leveraged to better structure what the model learns and how it generalizes <a class="yt-timestamp" data-t="09:44:00">[09:44:00]</a>. A relatively new approach involves conditioning models on a quality signal (e.g., a reward estimate) during training. This can help disentangle training and encourage the system to generate high-quality responses at test time, even if it learned from lower-quality data <a class="yt-timestamp" data-t="10:29:00">[10:29:00]</a>.

## Language Models as Agents

The term "agent" is broad, and large language models can be considered passively trained agents <a class="yt-timestamp" data-t="11:19:00">[11:19:00]</a>. An agent is defined as a system that takes inputs (like observations of the world) and produces output actions in a sequential decision-making problem <a class="yt-timestamp" data-t="11:27:00">[11:27:00]</a>. This includes LLMs, which take natural language inputs and produce sequences of language in response, as well as game-playing engines (chess, Go, Atari) <a class="yt-timestamp" data-t="11:43:00">[11:43:00]</a>.

A key distinction is whether an agent interacts with its environment during training or only at testing time <a class="yt-timestamp" data-t="12:02:00">[12:02:00]</a>. Often, agents are partially trained on passive data (e.g., observing expert players) before reinforcement learning steps involve active interaction and reward signals <a class="yt-timestamp" data-t="12:25:00">[12:25:00]</a>. This is analogous to LLM training, where initial passive data training is followed by fine-tuning and reinforcement learning (e.g., training a reward model based on human preferences for language responses) <a class="yt-timestamp" data-t="12:47:00">[12:47:00]</a>.

Regarding error accumulation in autoregressive models, Dr. Lampinen notes that state-of-the-art LLMs have some ability to self-correct within sequences, especially with techniques like Chain of Thought, preventing errors from monotonically increasing <a class="yt-timestamp" data-t="14:01:00">[14:01:00]</a>. The architecture and context length can affect this ability <a class="yt-timestamp" data-t="14:30:00">[14:30:00]</a>.

Passive learning is less efficient and leads to worse generalization out of distribution <a class="yt-timestamp" data-t="14:42:00">[14:42:00]</a>. When a system trained passively becomes active, it may move off its training data distribution and break down <a class="yt-timestamp" data-t="14:59:00">[14:59:00]</a>. Techniques from offline reinforcement learning, like DAgger, provide interventional data to recover to the data distribution <a class="yt-timestamp" data-t="15:10:00">[15:10:00]</a>. The active reinforcement learning or supervised tuning performed on LLMs during deployment can be seen as a form of interventional data <a class="yt-timestamp" data-t="15:26:00">[15:26:00]</a>.

## Causal Data Fusion

There is a parallel between these ideas and [[causal_ai_and_its_connection_to_machine_learning | causal inference]] literature on "causal data fusion," which mixes observational and interventional data to recover causal structures and maximize inference efficiency <a class="yt-timestamp" data-t="16:05:00">[16:05:00]</a>. Lampinen's work has explored cases where systems use "prior knowledge" (akin to observational data) to efficiently determine causal structures, guiding interventions to only relevant variables <a class="yt-timestamp" data-t="16:42:00">[16:42:00]</a>.

## Useful Sequential Decision-Making Systems

Large language models are considered useful sequential decision-making systems, particularly when a human is in the loop <a class="yt-timestamp" data-t="17:41:00">[17:41:00]</a>. They are useful for tasks like writing, idea generation, and critiquing thinking <a class="yt-timestamp" data-t="17:48:00">[17:48:00]</a>. However, they are currently too unreliable for fully autonomous deployment <a class="yt-timestamp" data-t="18:11:00">[18:11:00]</a>. Lampinen is more interested in AI systems as tools to assist humans, for example, in scientific discovery or reasoning, rather than as autonomous agents <a class="yt-timestamp" data-t="18:19:00">[18:19:00]</a>.

## [[Causality and generalization | Causality and Generalization]]

While not always applicable (e.g., in pure mathematics, which deals with equivalences rather than causal directions), [[causality_and_machine_learning | causality]] offers highly useful intuitions for how real-world and agentic systems generalize and where they might break down <a class="yt-timestamp" data-t="21:24:00">[21:24:00]</a>. Concepts like confounding and intervention are practical for understanding generalization in agentic systems <a class="yt-timestamp" data-t="22:12:00">[22:12:00]</a>.

## Neuro-Symbolic AI and Rationality

The field of neuro-symbolic AI explores combining representation learning (soft, differentiable) with symbolic, discrete systems <a class="yt-timestamp" data-t="22:56:00">[22:56:00]</a>. Dr. Lampinen views symbolic systems and logical reasoning as useful tools for intelligent systems, but not inherent to intelligence itself <a class="yt-timestamp" data-t="23:31:00">[23:31:00]</a>. Humans, for example, are not naturally strong logical or mathematical reasoners without extensive training, leading them to develop tools like mathematical proving systems <a class="yt-timestamp" data-t="23:50:00">[23:50:50]</a>. Intelligence, in this view, tends to be more akin to a continuous, fuzzy reasoning system, with symbolic logic serving as a tool for specific constraint problems <a class="yt-timestamp" data-t="24:08:00">[24:08:00]</a>.

Regarding rationality in artificial systems, Dr. Lampinen notes that both humans and LLMs show similar patterns of content-entangled logical reasoning <a class="yt-timestamp" data-t="40:50:00">[40:50:00]</a>. This suggests that the objective of humans and LLMs is not necessarily to be perfectly rational reasoners, but to be adaptive to the situations they encounter <a class="yt-timestamp" data-t="41:21:00">[41:21:00]</a>. Being less "rational" in a strict logical sense might lead to better decisions across the common distribution of situations encountered daily <a class="yt-timestamp" data-t="41:31:00">[41:31:00]</a>. This concept aligns with "bounded rationality," where limitations or specific adaptations (like human social behaviors) lead to what appears as "irrationality" but serves an overall adaptive purpose <a class="yt-timestamp" data-t="41:59:00">[41:59:00]</a>.

## Discrepancy Between Lab and Reality

There is a significant gap between empirical demonstrations in controlled lab settings and real-world applications where tasks are less defined and distributions unknown <a class="yt-timestamp" data-t="46:13:00">[46:13:00]</a>. While controlled experiments are vital for understanding, the real world acts as a "forcing function" against overfitting <a class="yt-timestamp" data-t="47:01:00">[47:01:00]</a>. Designing an algorithm and its test simultaneously can lead to algorithms that only work perfectly in their specific, designed test cases <a class="yt-timestamp" data-t="47:15:00">[47:15:00]</a>.

Working with real-world data (e.g., natural images or language queries) helps test ideas rigorously and ensures they solve the actual problem, not just a toy version <a class="yt-timestamp" data-t="47:37:00">[47:37:00]</a>. Over-constraining a system's reasoning processes based on assumptions about how it "should" work can make it fragile; if the world deviates from these assumptions, the system may break down <a class="yt-timestamp" data-t="44:14:00">[44:14:00]</a>. Instead of hard constraints, softer methods like explanation prediction objectives can encourage desired representations without overly constraining internal computations, offering more effective and scalable solutions, especially in broad applications like language models or rich virtual environments <a class="yt-timestamp" data-t="44:43:00">[44:43:00]</a>. This aligns with "The Bitter Lesson" from Rich Sutton, which suggests that attempts to build in problem-solving methods often fail at scale <a class="yt-timestamp" data-t="45:44:00">[45:44:00]</a>.

## Lessons from Psychology

Dr. Lampinen emphasizes several key lessons from cognitive psychology applicable to [[machine_learning_and_causality | machine learning]]:
*   **Causality-Related Issues**: Cognitive scientists have extensively studied confounding and different underlying processes that explain observations <a class="yt-timestamp" data-t="48:15:00">[48:15:00]</a>.
*   **Experimental Design and Rigorous Statistics**: Psychology emphasizes careful experimental design and rigorous statistical analysis, a practice often overlooked in [[machine_learning_and_causality | machine learning]] where simpler metrics (e.g., "this number is bigger") might suffice <a class="yt-timestamp" data-t="48:27:00">[48:27:00]</a>.
*   **Abstract Thinking**: Bridging from observations to more abstract models that explain them is a valuable skill emphasized in computational cognitive science <a class="yt-timestamp" data-t="48:53:00">[48:53:00]</a>.

## Statistical Learning Theory

Statistical learning theory involves assumptions that may be too strong for modern [[machine_learning_and_causality | machine learning]] systems, or there's a lack of understanding of how these systems fit into the theory <a class="yt-timestamp" data-t="49:47:00">[49:47:00]</a>. Recent theoretical work attempts to bridge this gap by arguing for implicit inductive biases in model architectures or gradient descent processes, which make models effectively less overparameterized than they appear <a class="yt-timestamp" data-t="50:04:00">[50:04:00]</a>. This challenges the naive assumption that a system's capacity is uniformly distributed for fitting a function <a class="yt-timestamp" data-t="50:36:00">[50:36:00]</a>.

## The Future of [[Causality and Machine Learning | Causality and Machine Learning]]

The future of [[causal_ai_and_machine_learning | causality and machine learning]] involves combining formalisms like Pearl's with concepts from differential equations, chaos theory, and dynamical systems <a class="yt-timestamp" data-t="52:55:00">[52:55:00]</a>. It also means integrating soft learning with more formalized inference methods <a class="yt-timestamp" data-t="53:13:00">[53:13:00]</a>. The discussion highlights that human everyday reasoning often relies on soft learning skills rather than formal reasoning, which requires extensive training <a class="yt-timestamp" data-t="53:28:00">[53:28:00]</a>.

A unified perspective is needed to show that interventions (like AB tests) and simulations are special cases of [[causal_ai_and_machine_learning | causal inference]] and operationalizations of structural causal models <a class="yt-timestamp" data-t="56:32:00">[56:32:00]</a>. This perspective aims to make [[causality_and_machine_learning | causality]] more accessible and address common misconceptions from different communities <a class="yt-timestamp" data-t="55:54:00">[55:54:00]</a>.