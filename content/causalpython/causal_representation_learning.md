---
title: Causal representation learning
videoId: ubSFglvhBd0
---

From: [[causalpython]] <br/> 

[[Causal representation learning | Causal representation learning]] is a field focused on uncovering the underlying causal entities and structures from high-dimensional or complex data where these entities are not explicitly provided <a class="yt-timestamp" data-t="06:51:00">[06:51:00]</a> <a class="yt-timestamp" data-t="06:56:00">[06:56:00]</a>. It extends beyond traditional statistical representation learning by incorporating notions of intervention and action <a class="yt-timestamp" data-t="10:37:00">[10:37:00]</a>.

## Core Ideas and Motivation

Modern [[causal_ai_and_machine_learning_intersection | machine learning]] heavily relies on representation learning <a class="yt-timestamp" data-t="06:02:00">[06:02:00]</a>. However, much of this work traditionally operates under an Independent and Identically Distributed (IID) data assumption, relying on correlations and statistical dependencies <a class="yt-timestamp" data-t="06:17:00">[06:17:00]</a> <a class="yt-timestamp" data-t="09:52:00">[09:52:00]</a>.

[[Causal representation learning | Causal representation learning]] becomes crucial when dealing with real-world scenarios where conditions change, distributions shift, or the set of observed variables varies <a class="yt-timestamp" data-t="06:26:00">[06:26:00]</a> <a class="yt-timestamp" data-t="06:29:00">[06:29:00]</a> <a class="yt-timestamp" data-t="06:33:00">[06:33:00]</a>. These changing settings inherently involve causality <a class="yt-timestamp" data-t="06:40:00">[06:40:00]</a> <a class="yt-timestamp" data-t="06:43:00">[06:43:00]</a>.

### Distinctions from Classical AI and [[causal_discovery_and_learning | Causal Discovery]]

*   **Classical AI**: Unlike classical [[causal_reasoning_in_artificial_intelligence | AI]], which assumes that symbols (e.g., chess pieces on a board) are pre-defined and then algorithms process them <a class="yt-timestamp" data-t="08:03:00">[08:03:00]</a> <a class="yt-timestamp" data-t="08:06:00">[08:06:00]</a>, [[causal representation learning | causal representation learning]] aims to learn these fundamental objects or symbols directly from the data <a class="yt-timestamp" data-t="07:08:00">[07:08:00]</a> <a class="yt-timestamp" data-t="08:35:00">[08:35:00]</a>. For example, in a scene with objects, it seeks to learn which pixels form an object without prior explicit labeling <a class="yt-timestamp" data-t="07:01:00">[07:01:00]</a> <a class="yt-timestamp" data-t="07:03:00">[07:03:00]</a>.
*   **[[causal_discovery_and_learning | Causal Discovery]]**: While related to the classical problem of [[causal_discovery_and_learning | causal discovery]], [[causal representation learning | causal representation learning]] is not identical to it <a class="yt-timestamp" data-t="07:50:00">[07:50:00]</a>. It focuses more on the *representation* of the data's underlying causal factors.

## Connection to Intelligence and Biological Systems

The concept of thinking is likened to "acting in an imagined space" <a class="yt-timestamp" data-t="00:09:00">[00:09:00]</a> <a class="yt-timestamp" data-t="09:32:00">[09:32:00]</a>. For artificial intelligence to achieve this, representations must be *interventional*, allowing actions within the representation space <a class="yt-timestamp" data-t="10:25:00">[10:25:00]</a> <a class="yt-timestamp" data-t="10:29:00">[10:29:00]</a> <a class="yt-timestamp" data-t="10:49:00">[10:49:00]</a>.

### Finite Data and Modularity

Biological systems, unlike many large-scale [[generative_ai_and_causal_reasoning | generative AI]] models, operate with finite training data and computational resources <a class="yt-timestamp" data-t="13:43:00">[13:43:00]</a> <a class="yt-timestamp" data-t="13:45:00">[13:45:00]</a> <a class="yt-timestamp" data-t="14:02:00">[14:02:00]</a> <a class="yt-timestamp" data-t="14:06:00">[14:06:00]</a>. This necessitates a more clever, modular approach to learning <a class="yt-timestamp" data-t="14:15:00">[14:15:00]</a> <a class="yt-timestamp" data-t="15:20:00">[15:20:00]</a> <a class="yt-timestamp" data-t="15:22:00">[15:22:00]</a>. If the world is also composed of modules, then the modules learned by biological systems might structurally correspond to real-world modules <a class="yt-timestamp" data-t="15:38:00">[15:38:00]</a> <a class="yt-timestamp" data-t="15:41:00">[15:41:00]</a>.

### Commutative Diagram and Simulation

A key aspect of [[causal representation learning | causal representation learning]] involves the idea that if an object is represented in the brain, mentally simulating an evolution or intervention on that object should yield the same result as performing the evolution or intervention in the real world and then observing the evolved object <a class="yt-timestamp" data-t="16:55:00">[16:55:00]</a> <a class="yt-timestamp" data-t="17:02:00">[17:02:00]</a> <a class="yt-timestamp" data-t="17:05:00">[17:05:00]</a> <a class="yt-timestamp" data-t="17:08:00">[17:08:00]</a> <a class="yt-timestamp" data-t="17:10:00">[17:10:00]</a>. This concept captures a form of consistency through a commutative diagram <a class="yt-timestamp" data-t="17:38:00">[17:38:00]</a>.

Having an internal world model allows systems to learn without having to risk their "lives" every time, enabling more efficient and safer learning through simulation <a class="yt-timestamp" data-t="00:30:00">[00:30:00]</a> <a class="yt-timestamp" data-t="00:32:00">[00:32:00]</a> <a class="yt-timestamp" data-t="20:56:00">[20:56:00]</a> <a class="yt-timestamp" data-t="20:57:00">[20:57:00]</a>. While many tasks might benefit from direct learning in the real world, complex tasks or those involving high stakes benefit from internal models <a class="yt-timestamp" data-t="21:04:00">[21:04:00]</a> <a class="yt-timestamp" data-t="21:07:00">[21:07:00]</a> <a class="yt-timestamp" data-t="21:19:00">[21:19:00]</a>.

## Current Research and Future Directions

A forthcoming book on [[causal representation learning | causal representation learning]] aims to cover the basics of causality alongside modern representation learning, especially regarding high-dimensional [[generative_ai_and_causal_reasoning | generative models]] <a class="yt-timestamp" data-t="05:56:00">[05:56:00]</a> <a class="yt-timestamp" data-t="05:58:00">[05:58:00]</a> <a class="yt-timestamp" data-t="06:00:00">[06:00:00]</a> <a class="yt-timestamp" data-t="06:07:00">[06:07:00]</a>.

### Focus Areas

*   **Compelling Applications**: The field needs to develop compelling applications to convince the broader [[causal_reasoning_in_ai | AI]] community of its importance <a class="yt-timestamp" data-t="31:58:00">[31:58:00]</a> <a class="yt-timestamp" data-t="32:00:00">[32:00:00]</a> <a class="yt-timestamp" data-t="32:02:00">[32:02:00]</a> <a class="yt-timestamp" data-t="32:17:00">[32:17:00]</a> <a class="yt-timestamp" data-t="32:19:00">[32:19:00]</a>.
*   **Intersection with [[generative_ai_and_causal_reasoning | Generative Modeling]]**: Researchers should focus on the intersection of causality and [[generative_ai_and_causal_reasoning | generative modeling]], particularly controllable generation <a class="yt-timestamp" data-t="32:27:00">[32:27:00]</a> <a class="yt-timestamp" data-t="32:30:00">[32:30:00]</a> <a class="yt-timestamp" data-t="32:36:00">[32:36:00]</a> <a class="yt-timestamp" data-t="32:37:00">[32:37:00]</a>. Many working on controllable generation may not realize its connection to causality <a class="yt-timestamp" data-t="32:40:00">[32:40:00]</a> <a class="yt-timestamp" data-t="32:41:00">[32:41:00]</a> <a class="yt-timestamp" data-t="32:43:00">[32:43:00]</a>.
*   **High-Performance Models**: The community should engage with and understand how to train high-performance [[generative_ai_and_causal_reasoning | generative models]], including the use of neural networks in a way that connects to causality <a class="yt-timestamp" data-t="32:51:00">[32:51:00]</a> <a class="yt-timestamp" data-t="32:54:00">[32:54:00]</a> <a class="yt-timestamp" data-t="32:57:00">[32:57:00]</a> <a class="yt-timestamp" data-t="33:01:00">[33:01:00]</a> <a class="yt-timestamp" data-t="33:04:00">[33:04:00]</a>.

### Physics Perspective

The perspective from physics suggests modeling systems on multiple levels, from statistical dependencies to differential equations, which are the "gold standard" for simulating and reasoning about interventions <a class="yt-timestamp" data-t="23:20:00">[23:20:00]</a> <a class="yt-timestamp" data-t="23:23:00">[23:23:00]</a> <a class="yt-timestamp" data-t="23:25:00">[23:25:00]</a> <a class="yt-timestamp" data-t="23:32:00">[23:32:00]</a> <a class="yt-timestamp" data-t="23:33:00">[23:33:00]</a> <a class="yt-timestamp" data-t="23:47:00">[23:47:00]</a> <a class="yt-timestamp" data-t="23:50:00">[23:50:00]</a>. [[Causal inference and machine learning | Structural causal models]] are seen as an intermediate level, preserving some simplicity of [[causal_ai_and_machine_learning_intersection | machine learning]] methods while allowing reasoning about interventions <a class="yt-timestamp" data-t="24:07:00">[24:07:00]</a> <a class="yt-timestamp" data-t="24:10:00">[24:10:00]</a> <a class="yt-timestamp" data-t="24:12:00">[24:12:00]</a> <a class="yt-timestamp" data-t="24:14:00">[24:14:00]</a> <a class="yt-timestamp" data-t="24:17:00">[24:17:00]</a> <a class="yt-timestamp" data-t="24:26:00">[24:26:00]</a>.

Work in this area is consistent with physics, viewing causality through the lens of physical mechanisms rather than just statistical dependencies <a class="yt-timestamp" data-t="25:11:00">[25:11:00]</a> <a class="yt-timestamp" data-t="25:14:00">[25:14:00]</a> <a class="yt-timestamp" data-t="25:16:00">[25:16:00]</a> <a class="yt-timestamp" data-t="25:27:00">[25:27:00]</a> <a class="yt-timestamp" data-t="25:30:00">[25:30:00]</a>. It also explores additional assumptions, like independent mechanisms, to capture how causal systems are physically realized <a class="yt-timestamp" data-t="25:57:00">[25:57:00]</a> <a class="yt-timestamp" data-t="26:01:00">[26:01:00]</a> <a class="yt-timestamp" data-t="26:03:00">[26:03:00]</a> <a class="yt-timestamp" data-t="26:06:00">[26:06:00]</a> <a class="yt-timestamp" data-t="26:09:00">[26:09:00]</a>.

One recent approach proposes a new formalism for causality based on stochastic differential equations, motivated by time-series data and the prevalence of loops in practical causal problems <a class="yt-timestamp" data-t="26:28:00">[26:28:00]</a> <a class="yt-timestamp" data-t="26:30:00">[26:30:00]</a> <a class="yt-timestamp" data-t="26:44:00">[26:44:00]</a> <a class="yt-timestamp" data-t="26:47:00">[26:47:00]</a> <a class="yt-timestamp" data-t="26:50:00">[26:50:00]</a> <a class="yt-timestamp" data-t="26:52:00">[26:52:00]</a> <a class="yt-timestamp" data-t="26:54:00">[26:54:00]</a> <a class="yt-timestamp" data-t="26:57:00">[26:57:00]</a>. This includes a concept called the "kernel deviation from stationarity" <a class="yt-timestamp" data-t="27:31:00">[27:31:00]</a> <a class="yt-timestamp" data-t="27:33:00">[27:33:00]</a>.