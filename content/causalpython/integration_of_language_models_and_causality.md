---
title: integration of language models and causality
videoId: 8yWKQqNFrmY
---

From: [[causalpython]] <br/> 

The integration of [[large_language_models_and_causality | large language models (LLMs)]] and [[causality_and_large_language_models | causality]] is a rapidly evolving field, poised to become increasingly important, especially with the rise of autonomous agents <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a>. This synergy aims to enhance both causal inference through LLMs and the robustness of LLMs through causal principles <a class="yt-timestamp" data-t="00:36:21">[00:36:21]</a>.

## LLMs and Causality

[[large_language_models_and_causality | LLMs]] have demonstrated a new approach to [[causality_and_large_language_models | causality]] by revealing ways to learn domain knowledge <a class="yt-timestamp" data-t="00:03:52">[00:03:52]</a>. Historically, the field of [[causality_and_large_language_models | causality]] spent considerable time on discovery and reasoning from given datasets, often leaving the acquisition of domain knowledge vague <a class="yt-timestamp" data-t="00:03:35">[00:03:35]</a>. LLMs can now learn this domain knowledge, and in many situations, this enables reasoning even without requiring traditional data <a class="yt-timestamp" data-t="00:03:59">[00:03:59]</a>. While this often appears as reasoning, it is largely due to accumulated domain knowledge and generalization capabilities <a class="yt-timestamp" data-t="00:04:15">[00:04:15]</a>. For the future of causal inference, it's crucial to integrate methods that supercharge both domain knowledge acquisition and traditional discovery/reasoning <a class="yt-timestamp" data-t="00:04:26">[00:04:26]</a>.

## Agents and Causality

The emergence of agents interacting in the real world or web environments makes [[causality_and_large_language_models | causality]] critically important <a class="yt-timestamp" data-t="00:00:06">[00:00:06]</a>. Notions like action, reward, and behavior optimization or penalization are fundamentally causal abstractions <a class="yt-timestamp" data-t="00:00:18">[00:00:18]</a>. Questions such as "which agent is most useful?" or "which agent should be rewarded?" are inherently causal <a class="yt-timestamp" data-t="00:00:28">[00:00:28]</a>. Counterfactual notions of blame, like identifying agents performing poorly to remove or improve them, also fall into this category <a class="yt-timestamp" data-t="00:00:34">[00:00:34]</a>.

While causal concepts might not be explicit in the initial training of agents (e.g., in reinforcement learning, where focus is on reward optimization) <a class="yt-timestamp" data-t="00:05:12">[00:05:12]</a>, they become vital when agents interact with each other or collaborate <a class="yt-timestamp" data-t="00:05:21">[00:05:21]</a>. New causal questions will arise, akin to social science, focusing on designing systems from the ground up to achieve desired behaviors <a class="yt-timestamp" data-t="00:05:51">[00:05:51]</a>.

Research from Google DeepMind, such as a paper by Richardson and Everett, explores how agents can learn robust world models <a class="yt-timestamp" data-t="00:00:43">[00:00:43]</a>. These ideas often revolve around interventional distributionsâ€”making an intervention and learning the response <a class="yt-timestamp" data-t="00:00:58">[00:00:58]</a>. Such papers demonstrate that even without explicit causal reasoning, an agent can mimic an ideal causal agent's behavior if exposed to sufficiently diverse environments <a class="yt-timestamp" data-t="00:07:27">[00:07:27]</a>. This suggests that by simply striving to perform well across varied conditions, agents can learn actions that align with true causal understanding, and sometimes even recover the underlying world model by observing their actions <a class="yt-timestamp" data-t="00:07:38">[00:07:38]</a>.

This emerging trend highlights the power of diverse data <a class="yt-timestamp" data-t="00:08:10">[00:08:10]</a>. Work from Schulov's group on exchangeability further supports that having data from multiple distributions (akin to diverse environments) significantly improves [[application_of_large_language_models_llms_in_causal_discovery | causal discovery]] <a class="yt-timestamp" data-t="00:08:18">[00:08:18]</a>. The insight is that there are "multiple ways to reach a causal agent" <a class="yt-timestamp" data-t="00:09:17">[00:09:17]</a>. While traditional methods emphasize mechanisms, graphs, and inductive biases <a class="yt-timestamp" data-t="00:09:22">[00:09:22]</a>, evidence now shows that collecting abundant and diverse data can also lead to learning causal actions, even without explicit knowledge of the causal model <a class="yt-timestamp" data-t="00:09:44">[00:09:44]</a>.

Another way to achieve causal actions is by building foundations on domain knowledge, theorems, and established axioms <a class="yt-timestamp" data-t="00:10:56">[00:10:56]</a>. For example, LLMs can answer causal questions by applying theorems learned from physics books, much like humans learn from observations or existing knowledge <a class="yt-timestamp" data-t="00:10:26">[00:10:26]</a>. This approach, while sometimes called "cheating," may simply be a more attractive and efficient way to structure initial knowledge for models, allowing them to adapt more easily to new situations like varying gravitational constants on different planets <a class="yt-timestamp" data-t="00:14:06">[00:14:06]</a>.

## Challenges and Future Directions

### Verification in Agentic Frameworks

A significant missing piece in contemporary agentic frameworks is robust verification <a class="yt-timestamp" data-t="00:14:42">[00:14:42]</a>. Current frameworks often succeed in controlled environments (e.g., coding with compilers or API calls with clear failure signals) or where human feedback is provided <a class="yt-timestamp" data-t="00:14:48">[00:14:48]</a>. The challenge lies in applying these frameworks to vague real-world problems where even humans struggle to provide accurate rewards, or where human feedback isn't scalable <a class="yt-timestamp" data-t="00:15:25">[00:15:25]</a>.

The solution involves creating automated verifiers for outcomes and even for different actions taken by agents <a class="yt-timestamp" data-t="00:15:42">[00:15:42]</a>. Examples include:
*   **Summarizing Meetings**: Verifiers could assess precision, recall of important information, and relevance to specific individuals <a class="yt-timestamp" data-t="00:16:01">[00:16:01]</a>.
*   **Scientific Discovery**: Agents generating literature reviews, suggesting experiments, and receiving feedback to design better experiments <a class="yt-timestamp" data-t="00:16:35">[00:16:35]</a>.

This leads to the need for holistic verification strategies, creating causal systems that provide necessary properties for proof in specific domains <a class="yt-timestamp" data-t="00:16:54">[00:16:54]</a>.

### Leveraging LLMs for Causal Modeling Challenges

The integration of LLMs can address key bottlenecks in causal modeling:
1.  **Graph Generation**: One of the biggest challenges in causal inference is obtaining a credible causal graph for a system <a class="yt-timestamp" data-t="00:32:36">[00:32:36]</a>. LLMs can simplify this process by generating plausible graphs that experts can then critique and feed into causal analysis tools <a class="yt-timestamp" data-t="00:33:05">[00:33:05]</a>.
2.  **Reputation Tests**: Causal analysis often involves various reputation tests (e.g., sensitivity analysis, negative controls, placebo treatments, dummy outcomes) to verify the model's robustness <a class="yt-timestamp" data-t="00:19:57">[00:19:57]</a>. LLMs can provide suggestions for appropriate tests and guide users on interpreting their results <a class="yt-timestamp" data-t="00:33:53">[00:33:53]</a>.

### Making LLMs More Robust with Causality

Beyond using LLMs to aid causal inference, research also focuses on how [[causality_and_large_language_models | causality]] can make LLMs themselves more robust <a class="yt-timestamp" data-t="00:35:59">[00:35:59]</a>. Traditional [[machine_learning_and_causality | machine learning]] often prioritizes accuracy on a specific distribution, while [[causality_and_large_language_models | causality]] aims for accuracy even on worst-case distributions <a class="yt-timestamp" data-t="00:36:51">[00:36:51]</a>.

A promising direction is to impart "how to learn" causal knowledge to LLMs by training them on basic causal axioms <a class="yt-timestamp" data-t="00:38:55">[00:38:55]</a>. Similar to formal reasoning systems like logic, [[causal reasoning and large language models | causal reasoning]] is built on axioms (e.g., d-separation, transitivity) <a class="yt-timestamp" data-t="00:38:12">[00:38:12]</a>. By teaching LLMs these fundamental axioms, they can learn to apply and compose them at test time, potentially leading to better generalization and even the ability to deduce new theorems <a class="yt-timestamp" data-t="00:39:56">[00:39:56]</a>.

Empirical results show that training small transformer models on synthetic axiomatic data for transitivity and d-separation can lead to significant generalization to larger graphs and improved performance on causal benchmarks (like Cotoc) <a class="yt-timestamp" data-t="00:40:13">[00:40:13]</a>. While models might still find statistical shortcuts <a class="yt-timestamp" data-t="00:44:56">[00:44:56]</a>, the hope is that by scaling data and providing diverse possibilities, the space of such shortcuts can be constrained, leading to convergence on causal principles <a class="yt-timestamp" data-t="00:45:34">[00:45:34]</a>.

## Broader Context and Sociological Aspect

A significant technical challenge in [[causality_and_large_language_models | causality]] is to develop "model-free" approaches, similar to model-free reinforcement learning <a class="yt-timestamp" data-t="00:48:23">[00:48:23]</a>. This requires extracting maximum information from data sets, particularly diverse, multi-distribution data (exchangeable data), without relying on explicit world models <a class="yt-timestamp" data-t="00:48:52">[00:48:52]</a>. If data is assumed to come from many different environments and is mixed together, it unlocks significantly more possibilities for [[application of large language models LLMs in causal discovery | causal discovery]], including learning causal direction <a class="yt-timestamp" data-t="00:49:37">[00:49:37]</a>.

This challenge touches upon a "bitter lesson" for [[causality_and_large_language_models | causality]]: sometimes, the desired end-result of [[causal reasoning and large language models | causality]] can be achieved through varied means, not exclusively through principles-based causal modeling <a class="yt-timestamp" data-t="00:51:50">[00:51:50]</a>. For instance, methods like data augmentation, though not inherently causal, can contribute to achieving causal actions <a class="yt-timestamp" data-t="00:52:49">[00:52:49]</a>. The concept of "still points" in changing distributions, as seen in exchangeability and contrastive learning, highlights a fundamental cognitive ability to discern invariant causal mechanisms amidst variations <a class="yt-timestamp" data-t="00:53:08">[00:53:08]</a>.

## PY LLM: The Future of Causal Inference for Non-Experts

PY LLM is an initiative aimed at making causal inference accessible to non-experts <a class="yt-timestamp" data-t="00:29:56">[00:29:56]</a>. The vision is a system where users can input causal questions in natural language, receive text-based answers, and have advanced causal libraries working seamlessly in the background <a class="yt-timestamp" data-t="00:30:39">[00:30:39]</a>. This means a user could ask about the effect of a medicine or an intervention in schools, and the system would leverage research papers or existing data to provide causally aware insights <a class="yt-timestamp" data-t="00:30:10">[00:30:10]</a>.

This future integrates the capabilities of LLMs to process natural language and synthesize information with robust causal analysis tools <a class="yt-timestamp" data-t="00:46:57">[00:46:57]</a>. It addresses the need for systems that can reason and perform causal analysis when a natural language query demands it <a class="yt-timestamp" data-t="00:47:47">[00:47:47]</a>. This also expands the ambit of [[causality_and_large_language_models | causality]] beyond individual studies, enabling scientists to accelerate the cycle of science by automatically generating graphs from literature, planning experiments, and analyzing data <a class="yt-timestamp" data-t="01:08:58">[01:08:58]</a>.