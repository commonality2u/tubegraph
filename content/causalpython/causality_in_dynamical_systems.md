---
title: Causality in dynamical systems
videoId: UQ8j-DEkB98
---

From: [[causalpython]] <br/> 

The concept of [[causality_and_causal_models | causality]] within dynamical systems is a significant area of focus, especially concerning how [[causality_and_causal_models | causal relationships]] manifest and are understood across different time and spatio-temporal scales <a class="yt-timestamp" data-t="02:44:00">[02:44:00]</a>. This perspective suggests that [[causality_and_causal_models | causal claims]] are inherently tied to the specific scale at which a system is being considered <a class="yt-timestamp" data-t="03:02:00">[03:02:00]</a>.

## Scale Relativity of Causation
Any scientific claim or phenomenon studied is critically dependent on its time scale and more generally, spatio-temporal scale <a class="yt-timestamp" data-t="02:52:00">[02:52:00]</a>. Just as a physicist might observe different behaviors at quantum versus Newtonian scales <a class="yt-timestamp" data-t="03:06:00">[03:06:00]</a>, or a biologist differentiates between cell and systems biology <a class="yt-timestamp" data-t="03:20:00">[03:20:00]</a>, [[causality_and_causal_models | causal relationships]] also vary with scale <a class="yt-timestamp" data-t="03:24:00">[03:24:00]</a>.

This idea has not been systematically explored within the [[causality_and_causal_models | causal context]] <a class="yt-timestamp" data-t="03:30:00">[03:30:00]</a>, though it is inspired by Herbert Simon's work and a 1994 paper by Yumi Iwasaki and Herbert Simon titled "Causality and Model Abstraction" <a class="yt-timestamp" data-t="03:40:00">[03:40:00]</a>.

A simple example illustrates this point:
*   A dentist tells a patient, "Your gums bleed because you don't floss" <a class="yt-timestamp" data-t="04:16:00">[04:16:00]</a>.
*   The patient might counter, "I don't floss because it makes my gums bleed" <a class="yt-timestamp" data-t="04:13:00">[04:13:00]</a>.
These statements are not contradictory <a class="yt-timestamp" data-t="04:22:00">[04:22:00]</a>. If someone hasn't flossed and starts, their gums will bleed on that day <a class="yt-timestamp" data-t="04:25:00">[04:25:00]</a>. However, if they floss every day, after about a week, their gums won't bleed <a class="yt-timestamp" data-t="04:30:00">[04:30:00]</a>. This everyday example shows that the relationship between flossing and bleeding yields different relationships because they refer to different time scales <a class="yt-timestamp" data-t="04:40:00">[04:40:00]</a>. Any [[causality_and_causal_models | causal model]] or claim inherently builds in assumptions about the time scale being considered <a class="yt-timestamp" data-t="04:54:00">[04:54:00]</a>.

### Exogenous Variables and Time Scales
The nature of variables, especially whether they are considered "exogenous," can also depend on the time scale <a class="yt-timestamp" data-t="09:55:00">[09:55:00]</a>. An exogenous variable is one that does not depend on the values of other variables in the model <a class="yt-timestamp" data-t="10:49:00">[10:49:00]</a>.

Consider the example of crops, growth, and rainfall <a class="yt-timestamp" data-t="10:39:00">[10:39:00]</a>:
*   Rainfall is generally considered a good exogenous variable because crop growth depends on rainfall, not vice versa <a class="yt-timestamp" data-t="11:17:00">[11:17:00]</a>.
*   However, over a very long time scale (e.g., 100-200 years), agriculture can influence climate, meaning crop growth over many years can influence future rainfall <a class="yt-timestamp" data-t="11:49:00">[11:49:00]</a>.
*   A long-time scale model would reveal a feedback loop where crop growth influences rainfall <a class="yt-timestamp" data-t="12:04:00">[12:04:00]</a>.

When rainfall is initially considered exogenous, it's typically understood at shorter time scales, where the influence of crop growth on rainfall is negligible <a class="yt-timestamp" data-t="12:40:00">[12:40:00]</a>. Both representations (one with a cycle, one without) are appropriate at their respective time scales <a class="yt-timestamp" data-t="12:56:00">[12:56:00]</a>. Problems arise when extrapolating across time scales <a class="yt-timestamp" data-t="13:12:00">[13:12:00]</a>. For instance, a climate scientist studying long-term global warming effects cannot assume crop growth has no effect on rainfall <a class="yt-timestamp" data-t="13:19:00">[13:19:00]</a>.

## Causality, Emergence, and Complex Systems
The discussion of different scales naturally leads to complex systems and phenomena like emergence <a class="yt-timestamp" data-t="15:36:00">[15:36:00]</a>. A key question is whether [[causality_in_artificial_intelligence | causation in complex systems]] exists <a class="yt-timestamp" data-t="16:06:00">[16:06:00]</a>.

If [[causality_and_causal_models | causal relationships]] are indeed relative to a time scale, this helps explain much of what is discussed regarding emergence <a class="yt-timestamp" data-t="16:30:00">[16:30:00]</a>. When a system is viewed at a lower level of description, it might appear that everything depends on everything else, making [[causality_and_causal_models | causation]] seem impossible <a class="yt-timestamp" data-t="16:42:00">[16:42:00]</a>. However, at a higher level, patterns of regularity might emerge <a class="yt-timestamp" data-t="16:53:00">[16:53:00]</a>. These patterns don't come from nowhere; dynamic systems theory broadly explains how order emerges from chaos <a class="yt-timestamp" data-t="17:00:00">[17:00:00]</a>.

> [!INFO] Physics vs. Other Sciences
> Even if [[russells_view_on_causality_in_physics | Russell's view on causality in physics]] is correct regarding fundamental physics, it does not necessarily apply to other sciences like epidemiology or sociology, partly due to the existence of different scales <a class="yt-timestamp" data-t="17:19:00">[17:19:00]</a>.

The idea of "levels" in philosophy often considers parts of a higher level or orders of properties, where the same object is described differently (e.g., a physical object vs. a corkscrew) <a class="yt-timestamp" data-t="17:59:00">[17:59:00]</a>. However, when considering a system at different scales, it's akin to considering different objects entirely by zooming in and out <a class="yt-timestamp" data-t="18:45:00">[18:45:00]</a>.

An example of a thermostat-regulated room <a class="yt-timestamp" data-t="18:56:00">[18:56:00]</a>:
*   Turning on an oven might make the room hotter in 5 minutes <a class="yt-timestamp" data-t="19:02:00">[19:02:00]</a>.
*   An hour later, due to the thermostat's equilibration, the room is not hotter <a class="yt-timestamp" data-t="19:06:00">[19:06:00]</a>.
There is no contradiction or metaphysical puzzle here <a class="yt-timestamp" data-t="19:16:00">[19:16:00]</a>. Different patterns of behavior exist at different time and spatio-temporal scales in our universe <a class="yt-timestamp" data-t="20:22:00">[20:22:00]</a>. Complex systems with feedback loops and self-regulation exhibit complexity not seen by looking at only a part of the system <a class="yt-timestamp" data-t="19:42:00">[19:42:00]</a>.

## Interventions and Dynamical Systems
The definition of [[causality_and_causal_models | causality]] often revolves around interventions: understanding the difference between mere prediction and cases where one can intervene on a system to achieve a desired outcome <a class="yt-timestamp" data-t="21:15:00">[21:15:00]</a>. This is why randomized controlled trials (RCTs) are often preferred, as they can remove the influence of confounders by design <a class="yt-timestamp" data-t="06:37:00">[06:37:00]</a>. The causal methodology, especially with the use of "do-operators" in [[causality_and_causal_models | causal graphs]], allows for representing interventions <a class="yt-timestamp" data-t="21:50:00">[21:50:00]</a>.

However, the concept of intervention becomes subtle in dynamical systems, particularly when considering "intervening and letting go" <a class="yt-timestamp" data-t="40:59:00">[40:59:00]</a>.

### The Ideal Gas System Example
Consider an ideal gas in a container with thermodynamic variables like pressure, volume, and temperature, governed by the ideal gas law (pressure * volume is proportional to temperature) <a class="yt-timestamp" data-t="41:07:00">[41:07:00]</a>.

[[causality_and_causal_models | Causal models]] are relative to a system's setup <a class="yt-timestamp" data-t="41:37:00">[41:37:00]</a>:
*   **Sealed Container (Fixed Volume):** Volume and temperature cause pressure <a class="yt-timestamp" data-t="41:42:00">[41:42:00]</a>.
*   **Movable Piston (Constant Pressure):** Pressure and temperature cause volume <a class="yt-timestamp" data-t="41:54:00">[41:54:00]</a>.

A puzzle arises when considering interventions <a class="yt-timestamp" data-t="42:09:00">[42:09:00]</a>: If you start with a movable piston system and then "fix" the volume (e.g., by inserting a pin), the system behaves like a sealed container <a class="yt-timestamp" data-t="42:25:00">[42:25:00]</a>. This action looks like an intervention, but it doesn't yield the result expected by a standard "do-operator" intervention <a class="yt-timestamp" data-t="42:57:00">[42:57:00]</a>.

The key lies in the system's dynamics and time scale <a class="yt-timestamp" data-t="43:49:00">[43:49:00]</a>. A dynamical representation (away from equilibrium) shows a feedback loop between pressure and volume <a class="yt-timestamp" data-t="44:06:00">[44:06:00]</a>. When an intervention "clamps" a variable (holds it fixed indefinitely), it prevents this feedback loop from operating <a class="yt-timestamp" data-t="46:00:00">[46:00:00]</a>. The intervention destroys the system's ability to reach a certain type of equilibrium <a class="yt-timestamp" data-t="46:43:00">[46:43:00]</a>.

This demonstrates that:
*   There's no inherent basis for preferring one equilibrium model over another <a class="yt-timestamp" data-t="46:58:00">[46:58:00]</a>.
*   Each model is true for a different equilibrium state <a class="yt-timestamp" data-t="47:07:00">[47:07:00]</a>.
*   The standard structural causal model formalism has a "blind spot" regarding operations like "letting go" (removing a constraint), which is not explicitly modeled <a class="yt-timestamp" data-t="47:40:00">[47:40:00]</a>.

### Incorporating Time into Causal Models
Introducing time into [[causality_and_causal_models | causal models]] changes how interventions are conceived <a class="yt-timestamp" data-t="49:27:00">[49:27:00]</a>. When variables are repeatedly measured over time (e.g., a time series) <a class="yt-timestamp" data-t="49:42:00">[49:42:00]</a>, interventions can be distinguished between:
*   **Clamp intervention:** Holding a variable fixed indefinitely <a class="yt-timestamp" data-t="49:58:00">[49:58:00]</a>.
*   **Shock intervention:** Influencing a variable only at one time step <a class="yt-timestamp" data-t="50:06:00">[50:06:00]</a>.
*   Various types of repeated shocks at different intervals <a class="yt-timestamp" data-t="50:10:00">[50:10:00]</a>.

These distinctions are crucial for [[causality_and_causal_models | causal inference]] and understanding system outcomes <a class="yt-timestamp" data-t="50:25:00">[50:25:00]</a>. For instance, in chemotherapy, the goal is to change the body's overall state and influence its feedback loops to bring it to a new equilibrium state <a class="yt-timestamp" data-t="51:14:00">[51:14:00]</a>. This requires understanding the duration and persistence of interventions, and the time scale at which effects are expected <a class="yt-timestamp" data-t="51:51:00">[51:51:00]</a>.

## Bridging Dynamical Systems and [[causality_and_causal_models | Causal Frameworks]]
Significant work is being done to bridge the gap between differential equations, commonly used in dynamical systems, and the defined interventions in frameworks like Pearl's <a class="yt-timestamp" data-t="52:07:00">[52:07:00]</a>. Research from the UR Mo lab at the University of Amsterdam is particularly notable in this area <a class="yt-timestamp" data-t="52:32:00">[52:32:00]</a>.

Dynamic [[causality_and_causal_models | causal models]], as developed by Iwasaki and Simon, help understand the relationship between [[causality_and_causal_models | causal representations]] at equilibrium and away from equilibrium (dynamical representations) <a class="yt-timestamp" data-t="53:27:00">[53:27:00]</a>. This framework can also illuminate [[causality_and_causal_models | causal relations]] at different time scales <a class="yt-timestamp" data-t="54:02:00">[54:02:00]</a>.

The presence of time derivatives and integration operations in dynamic causal models shows that when differential equations can be written in a canonical way, a [[causality_and_causal_models | causal interpretation]] is possible <a class="yt-timestamp" data-t="54:41:00">[54:41:00]</a>. This indicates a generalization of the standard [[causality_and_causal_models | causal framework]] <a class="yt-timestamp" data-t="55:00:00">[55:00:00]</a>. While not every arbitrary dynamical system can be represented causally, this bridge exists for certain types <a class="yt-timestamp" data-t="55:06:00">[55:06:00]</a>. This continuity is also important, as Herbert Simon was a pioneer in developing [[causality_and_causal_models | causal models]] through the structural equations tradition in the 1950s <a class="yt-timestamp" data-t="55:27:00">[55:27:00]</a>.

## Causality in the Age of Big Data and AI
The "Big Data Revolution" of the early 2010s, with its promise of solving problems through massive data collection and prediction, often overlooked [[causality_and_causal_models | causality]] <a class="yt-timestamp" data-t="56:11:00">[56:11:00]</a>. However, [[causality_and_causal_models | causality]] is not about simply throwing data into a machine; it's about making assumptions to manage the complexity of the world in a local and manageable way <a class="yt-timestamp" data-t="57:06:00">[57:06:00]</a>.

> [!NOTE] Causal Modeling and Assumptions
> A key feature of a [[causality_and_causal_models | causal model]] is that missing arrows between variables imply strong assumptions (e.g., no direct causal link, no common cause) <a class="yt-timestamp" data-t="34:44:00">[34:44:00]</a>. These "background assumptions" are critical to how [[causality_and_causal_models | causal models]] function <a class="yt-timestamp" data-t="35:05:00">[35:05:00]</a>. As Nancy Cartwright stated, "no causes in, no causes out" – causal assumptions are necessary to derive causal knowledge <a class="yt-timestamp" data-t="35:17:00">[35:17:00]</a>.

While large language models might achieve impressive predictive capabilities by being trained on vast amounts of data <a class="yt-timestamp" data-t="59:03:00">[59:03:00]</a>, [[causality_in_artificial_intelligence | causal reasoning]] offers a different, arguably more human-like, approach <a class="yt-timestamp" data-t="59:37:00">[59:37:00]</a>. It caters to the need of a "limited being operating at a particular time scale" who doesn't need to know everything in the universe to predict local events <a class="yt-timestamp" data-t="58:47:00">[58:47:00]</a>.

The fear of building an incorrect [[causality_and_causal_models | causal graph]] (DAG) is legitimate <a class="yt-timestamp" data-t="35:42:00">[35:42:00]</a>. However, not building one still leaves room for error <a class="yt-timestamp" data-t="35:55:00">[35:55:00]</a>. If the goal is to make [[causality_and_causal_models | causal claims]], model intervention effects, or interpret data relationships, [[causality_and_causal_models | causal assumptions]] and models are necessary <a class="yt-timestamp" data-t="36:30:00">[36:30:00]</a>. Causal models allow for making these assumptions systematically and transparently <a class="yt-timestamp" data-t="38:09:00">[38:09:00]</a>. Building an explicit model, even if it's wrong, provides a learning opportunity to improve it, unlike implicit assumptions <a class="yt-timestamp" data-t="38:30:00">[38:30:00]</a>.

## Future of Causality
The field of [[causality_and_causal_models | causality]] is continually evolving, with significant developments in algorithmic aspects <a class="yt-timestamp" data-t="01:18:13">[01:18:13]</a>. However, there are still many unanswered questions and unexplored avenues <a class="yt-timestamp" data-t="01:18:08">[01:18:08]</a>. The focus should shift towards understanding the conditions under which [[causality_and_causal_models | causal models]] apply and why they work or break down <a class="yt-timestamp" data-t="01:19:00">[01:19:00]</a>. This includes exploring the role of time scale in making assumptions like exogeneity relative <a class="yt-timestamp" data-t="01:19:35">[01:19:35]</a>.

[[causality_in_science_and_industry | Causality in science and industry]] is not merely a transient trend <a class="yt-timestamp" data-t="01:21:01">[01:21:01]</a>. Many problems perceived as purely predictive often involve underlying needs for intervention <a class="yt-timestamp" data-t="01:21:29">[01:21:29]</a>. While correlation and statistics alone are not designed to identify invariant relationships or intervention effects <a class="yt-timestamp" data-t="01:21:52">[01:21:52]</a>, [[causality_and_machine_learning | causality]] offers the tools to address these <a class="yt-timestamp" data-t="01:21:58">[01:21:58]</a>. The challenge lies in effectively implementing and integrating [[causality_and_causal_models | causal tools]] into practice across various fields <a class="yt-timestamp" data-t="01:22:16">[01:22:16]</a>.