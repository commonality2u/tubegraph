---
title: Counterfactual reasoning in decision making
videoId: zTJFUaLjxfE
---

From: [[causalpython]] <br/> 

[[causal_analysis_and_decision_making | Counterfactual reasoning]], the third rung of Judea Pearl's Ladder of Causation <a class="yt-timestamp" data-t="01:46:00">[01:46:00]</a>, involves contemplating "what if" scenarios and imagining hypothetical worlds to estimate probabilities and build [[causal_reasoning_in_AI | causal models]] <a class="yt-timestamp" data-t="02:14:00">[02:14:00]</a>. This form of thinking is considered fundamental to human cognition, evident even in the reactions of toddlers when their expectations about cause and effect are violated <a class="yt-timestamp" data-t="03:01:00">[03:01:00]</a>. The core challenge in [[causal_inference_and_decision_theory | causal inference]] and counterfactual reasoning stems from the "fundamental problem": it is impossible to observe the outcomes of different treatments on the same individual at the same time <a class="yt-timestamp" data-t="03:51:00">[03:51:00]</a>. For instance, one cannot administer a medicine to a person and then rewind time to see what would have happened if they hadn't taken it <a class="yt-timestamp" data-t="04:01:00">[04:01:00]</a>.

## Challenges in Counterfactual Analysis

A major difficulty lies in identifying point estimates or precise probabilities for individual-level counterfactuals <a class="yt-timestamp" data-t="04:22:00">[04:22:00]</a>. While average effects can be estimated at a population level through randomized control trials <a class="yt-timestamp" data-t="04:11:00">[04:11:00]</a>, individual counterfactual probabilities remain elusive. Early work by Jin Tian and Judea Pearl established mathematical bounds on probabilities of causation, such as the probability of necessity (PN), probability of sufficiency (PS), and probability of necessity and sufficiency (PNS) <a class="yt-timestamp" data-t="04:32:00">[04:32:00]</a>. Although these bounds are "tight" (meaning no better mathematical bounds can be derived without further assumptions), they are often "too loose to be useful" for practical decision-making <a class="yt-timestamp" data-t="04:48:00">[04:48:00]</a>. The primary challenge is narrowing these bounds to a sufficient degree that allows for better understanding of underlying data-generating processes or improved decisions <a class="yt-timestamp" data-t="05:10:00">[05:10:00]</a>.

## Strategies for Narrowing Bounds

### Monotonicity Assumption

One key assumption that can significantly narrow these bounds is monotonicity <a class="yt-timestamp" data-t="06:07:00">[06:07:00]</a>. Monotonicity implies that there is no possibility of harm in a counterfactual sense <a class="yt-timestamp" data-t="06:17:00">[06:17:00]</a>. For example, if a medicine is given, monotonicity assumes it won't prevent recovery that would have occurred naturally if the medicine hadn't been taken <a class="yt-timestamp" data-t="06:36:00">[06:36:00]</a>. If the probability of such a "negative counterfactual effect" is zero, then monotonicity holds <a class="yt-timestamp" data-t="07:01:00">[07:01:00]</a>. When monotonicity is assumed, probabilities like the probability of benefit or necessity can be "point identified," meaning their exact value can be determined <a class="yt-timestamp" data-t="07:26:00">[07:26:00]</a>.

However, a practical challenge is knowing when monotonicity can be reliably assumed <a class="yt-timestamp" data-t="08:04:00">[08:04:00]</a>. Research by Scott Muller and Judea Pearl explores how data can be used to assess this assumption <a class="yt-timestamp" data-t="08:20:00">[08:20:00]</a>. While confirming monotonicity from data alone is rare and only possible in specific situations <a class="yt-timestamp" data-t="09:12:00">[09:12:00]</a>, it is more feasible to definitively disprove it from data <a class="yt-timestamp" data-t="08:56:00">[08:56:00]</a>. Even if monotonicity is violated, understanding the "limits to monotonicity" can help narrow the counterfactual probability bounds further <a class="yt-timestamp" data-t="09:57:00">[09:57:00]</a>.

## Applications in Practice

The goal of this research is to enable better decisions at both individual and population levels, such as formulating more effective policies <a class="yt-timestamp" data-t="03:30:00">[03:30:00]</a>.

### Marketing and the Unit Selection Framework

A notable application of counterfactual reasoning is in marketing, through the "unit selection framework" developed by Angley and Judea Pearl <a class="yt-timestamp" data-t="21:11:00">[021:11:00]</a>. This framework allows marketers to assign values to four distinct types of responders based on their counterfactual behavior regarding an advertisement and product purchase:

*   **Complier:** Buys the product if shown the ad, but would not if not shown the ad <a class="yt-timestamp" data-t="21:54:00">[021:54:00]</a>. Advertisers aim to target these individuals <a class="yt-timestamp" data-t="22:56:00">[022:56:00]</a>.
*   **Always Taker:** Buys the product regardless of whether they see the ad <a class="yt-timestamp" data-t="22:03:00">[022:03:00]</a>.
*   **Never Taker:** Does not buy the product regardless of whether they see the ad <a class="yt-timestamp" data-t="22:09:00">[022:09:00]</a>.
*   **Defier:** Buys the product if *not* shown the ad, but would *not* buy it if shown the ad (e.g., the ad is offensive or makes them reconsider) <a class="yt-timestamp" data-t="22:13:00">[022:13:00]</a>. These individuals are ideally avoided <a class="yt-timestamp" data-t="22:46:00">[022:46:00]</a>.

While A/B testing is a common industry practice for optimizing ad campaigns, it focuses on average performance <a class="yt-timestamp" data-t="19:37:00">[019:37:00]</a> and can be "severely suboptimal" compared to the unit selection framework <a class="yt-timestamp" data-t="24:57:00">[024:57:00]</a>. Unit selection allows for more nuanced optimization by considering the potential positive and negative values associated with advertising to each responder type, even accounting for advertising costs <a class="yt-timestamp" data-t="23:24:00">[023:24:00]</a>. Although individual-level treatment effects (and thus specific response types) cannot be identified with certainty, observational and experimental data can be used to establish bounds on the overall value of advertising to a particular subpopulation <a class="yt-timestamp" data-t="25:31:00">[025:31:00]</a>. These bounds, even if not precise point estimates, can be sufficient for making better business decisions <a class="yt-timestamp" data-t="26:19:00">[026:19:00]</a>.

## Making Research Accessible

To bridge the gap between theoretical work and practical application in [[causal_inference_in_practical_applications | causal inference]], it is crucial to develop accessible software packages and interfaces <a class="yt-timestamp" data-t="16:04:00">[016:04:00]</a>. Projects like DoWhy, which provides a consistent API akin to scikit-learn for machine learning, are vital for practitioners who may not have the time or expertise to implement models from scratch <a class="yt-timestamp" data-t="16:27:00">[016:27:00]</a>. Future developments could include user-friendly graphical user interfaces (GUIs) or integrations into existing domain-specific software (e.g., for econometrics or epidemiology), allowing non-coders with domain expertise to leverage advanced [[causal_reasoning_in_artificial_intelligence | causal reasoning]] methods <a class="yt-timestamp" data-t="17:30:00">[017:30:00]</a>. This approach would democratize access to powerful [[causal_decision_making_and_reinforcement_learning | causal decision-making]] tools, for example, enabling marketing professionals to apply advanced unit selection without deep coding knowledge <a class="yt-timestamp" data-t="18:00:00">[018:00:00]</a>.

## [[Generative AI and causal reasoning | Future of AI]] and Counterfactuals

Integrating [[abstractions_in_causal_reasoning | causal reasoning]] capabilities, particularly [[Counterfactual Explanations and Model Explainability | counterfactual thinking]], into [[causal_reasoning_in_AI | AI architectures]] is seen as a core component for achieving human-level artificial general intelligence (AGI) <a class="yt-timestamp" data-t="03:34:00">[03:34:00]</a>. While some believe that simply scaling up existing models and parameters will lead to emergent causal properties, others contend that the mathematics and science of causality must be "baked in" to machine learning models <a class="yt-timestamp" data-t="39:41:00">[39:41:00]</a>. This integration is crucial for developing AI capable of solving significant real-world problems by understanding "what caused this" and "what are the consequences of this" <a class="yt-timestamp" data-t="11:00:00">[011:00:00]</a>.