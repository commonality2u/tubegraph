---
title: Philosophical perspectives on causality
videoId: ubSFglvhBd0
---

From: [[causalpython]] <br/> 

The study of [[philosophical_aspects_of_causality | causality]] has always been of interest from a philosophical perspective <a class="yt-timestamp" data-t="00:02:29">[00:02:29]</a>. Bernhard Sch√∂lkopf, a professor at the Max Planck Institute for Intelligent Systems, found it fascinating to see how causality, traditionally a philosophical concept, could be studied using mathematical tools <a class="yt-timestamp" data-t="00:02:35">[00:02:35]</a>.

## Thinking as Acting in an Imagined Space

A core metaphor that informs the [[philosophical_aspects_of_causality | philosophical understanding of intelligence]], particularly in the context of animal behavior and [[causality_in_ai | AI]], comes from Konrad Lorenz, one of the fathers of ethology <a class="yt-timestamp" data-t="00:09:27">[00:09:27]</a>. Lorenz posited that "thinking is nothing but acting in an imagined space" <a class="yt-timestamp" data-t="00:09:32">[00:09:32]</a>.

For current [[causality_in_ai | artificial intelligence]] and [[machine_learning_and_causality | machine learning]], especially generative [[causality_in_ai | AI]], representations are primarily statistical, focusing on dependencies and pattern recognition <a class="yt-timestamp" data-t="00:09:48">[00:09:48]</a>. To move towards "acting in an imagined space," representations must incorporate notions of intervention and action <a class="yt-timestamp" data-t="00:10:37">[00:10:37]</a>. This shift moves the field closer to [[philosophical_aspects_of_causality | causality]] <a class="yt-timestamp" data-t="00:10:43">[00:10:43]</a>. Biological systems, for instance, utilize "efference copies" of actions to affect the world, contributing to the formation of internal models that simulate actions and the world <a class="yt-timestamp" data-t="00:11:18">[00:11:18]</a>.

## Correlation vs. Causality in Biological Systems

While current [[machine_learning_and_causality | AI]] demonstrates the compelling power of correlational learning with vast datasets <a class="yt-timestamp" data-t="00:13:08">[00:13:08]</a>, biological systems face finite training data and computational resources <a class="yt-timestamp" data-t="00:13:41">[00:13:41]</a>. This necessitates a more "clever" approach to learning, which involves:
*   **Modular learning** Biological systems learn in a modular fashion, reusing learned modules across multiple tasks and changing environments <a class="yt-timestamp" data-t="00:15:22">[00:15:22]</a>. For example, color consistency mechanisms in the retina apply generally, not just to specific objects <a class="yt-timestamp" data-t="00:14:55">[00:14:55]</a>.
*   **Structural similarities** There's a fascinating idea that the modules learned by biological systems might structurally resemble the modules that compose the world <a class="yt-timestamp" data-t="00:15:41">[00:15:41]</a>.
*   **Commutative Consistency** As described by physicist Heinrich Hertz, if an object is represented in the brain, thinking about its evolution should yield the same result as performing the evolution in the real world and then representing the evolved object <a class="yt-timestamp" data-t="00:16:53">[00:16:53]</a>. This "commutative diagram" captures a consistency between imagined interventions and real-world outcomes <a class="yt-timestamp" data-t="00:17:38">[00:17:38]</a>.

### The Role of Internal World Models
Internal world models are crucial for learning without constant risk to life, allowing for simulation of the world and actions within it <a class="yt-timestamp" data-t="00:20:57">[00:20:57]</a>. While many learning aspects can be done directly in the real world, complex tasks like understanding what food is safe or attributing credit for outcomes benefit from internal models <a class="yt-timestamp" data-t="00:20:22">[00:20:22]</a>. Initially explicit learning can become automatic over time, reducing the need for explicit world models <a class="yt-timestamp" data-t="00:21:33">[00:21:33]</a>.

## Physics and Causality

The book "Elements of Causal Inference" is heavily inspired by physics, co-authored by physicists and mathematicians <a class="yt-timestamp" data-t="00:22:42">[00:22:42]</a>. From a physics perspective, causal systems can be described on multiple levels:
*   **Statistical dependencies** The lowest level, common in [[machine_learning_and_causality | machine learning]] <a class="yt-timestamp" data-t="00:23:26">[00:23:26]</a>.
*   **Differential equations** The "gold standard" in physics, allowing for simulation, intervention reasoning, and understanding different initial conditions <a class="yt-timestamp" data-t="00:23:32">[00:23:32]</a>.
*   **Structural Causal Models (SCMs)** Positioned between statistical dependencies and differential equations, SCMs aim to preserve the simplicity of [[machine_learning_and_causality | machine learning]] methods while allowing for reasoning about interventions without requiring a full mechanistic understanding <a class="yt-timestamp" data-t="00:24:07">[00:24:07]</a>. SCMs are expected to be consistent with underlying physical reality <a class="yt-timestamp" data-t="00:24:36">[00:24:36]</a>.

A core tenet in the [[philosophical_aspects_of_causality | philosophical understanding of causality]] is the focus on **mechanisms** rather than just statistical dependencies <a class="yt-timestamp" data-t="00:25:16">[00:25:16]</a>. Mechanisms are viewed as underlying physical processes that give rise to statistical dependencies <a class="yt-timestamp" data-t="00:25:27">[00:25:27]</a>. This perspective aligns with how some computer scientists, like Judea Pearl, fundamentally think about [[philosophical_aspects_of_causality | causality]] <a class="yt-timestamp" data-t="00:25:36">[00:25:36]</a>. Much work, such as on independent mechanisms, seeks to capture how causal systems are physically realized in the world <a class="yt-timestamp" data-t="00:26:09">[00:26:09]</a>.