---
title: Structural causal models and causal discovery
videoId: yqKJ9pUQ6Q8
---

From: [[causalpython]] <br/> 

Professor Judea Pearl, often referred to as the "Godfather of modern causality," has significantly influenced the field, with his work, notably "The Book of Why," changing the direction of many researchers' lives <a class="yt-timestamp" data-t="01:17:53">[01:17:53]</a>, <a class="yt-timestamp" data-t="01:39:00">[01:39:00]</a>.

## Early Motivations and the Smallpox Paradox

Pearl's interest in causality began in high school with a logical paradox concerning smallpox vaccination in 1840s France <a class="yt-timestamp" data-t="02:51:00">[02:51:00]</a>. People observed that more individuals died from the vaccination itself than from smallpox, leading to protests to ban the vaccine <a class="yt-timestamp" data-t="02:58:00">[02:58:00]</a>. However, this conclusion was flawed, as the low death rate from smallpox was *due* to the vaccination's success <a class="yt-timestamp" data-t="03:22:00">[03:22:00]</a>. Pearl's teacher attempted to demonstrate how logic could rectify such paradoxes but failed <a class="yt-timestamp" data-t="03:37:00">[03:37:00]</a>. Later, when working in artificial intelligence, Pearl found that existing logical tools lacked the language to express causal statements like "died from" a specific factor <a class="yt-timestamp" data-t="04:07:00">[04:07:00]</a>, leading him to seek a more expressive language for causality <a class="yt-timestamp" data-t="04:30:00">[04:30:00]</a>.

## Evolution to [[structural_causal_models_and_graph_theory | Structural Causal Models]]

Pearl's initial work focused on Bayesian networks, which are probabilistic models <a class="yt-timestamp" data-t="05:16:00">[05:16:00]</a>. He recognized that their success stemmed from being constructed in the causal direction <a class="yt-timestamp" data-t="05:47:00">[05:47:00]</a>, but felt something was missing by 1988 <a class="yt-timestamp" data-t="06:00:00">[06:00:00]</a>. This led to the development of [[structural_causal_models_and_graph_theory | structural causal models]] (SCMs) around 1991, in collaboration with Thomas Verma <a class="yt-timestamp" data-t="06:16:00">[06:16:00]</a>. SCMs introduced a deterministic functional approach, where uncertainty arises only from uncertain boundary conditions (like error variables) <a class="yt-timestamp" data-t="06:29:00">[06:29:00]</a>. This shift from two decades of probabilistic reasoning to deterministic functions was initially a "trauma" for Pearl and Verma, requiring time to convince themselves it was the right direction <a class="yt-timestamp" data-t="06:48:00">[06:48:00]</a>.

## AI, Human Reasoning, and Biases

Considering the systematic errors and biases in human reasoning, as studied by Daniel Kahneman and Amos Tversky, the question arises whether a General Artificial Intelligence (AGI) system "should" always reason correctly in a formal sense <a class="yt-timestamp" data-t="09:12:00">[09:12:00]</a>. Humans are constrained by resources, leading to quick decision-making and built-in "shortcuts" that inevitably introduce errors and biases <a class="yt-timestamp" data-t="10:06:00">[10:06:00]</a>. Large machines, however, do not have these same computational limitations and can afford to "search deeper" for more reasoned conclusions, potentially avoiding human-like biases <a class="yt-timestamp" data-t="10:38:00">[10:38:00]</a>. Their limitations might stem from the samples they can access rather than computational resources <a class="yt-timestamp" data-t="10:58:00">[10:58:00]</a>.

## [[application_of_large_language_models_llms_in_causal_discovery | Large Language Models (LLMs)]] and Causality

The emergence of [[application_of_large_language_models_llms_in_causal_discovery | large language models (LLMs)]] has sparked debate about their ability to learn "world models" <a class="yt-timestamp" data-t="11:48:00">[11:48:00]</a>. While some argue against it based on the causal hierarchy, others like Andrew Lineman suggest LLMs can learn "active causal strategies from passive data" <a class="yt-timestamp" data-t="12:20:00">[12:20:00]</a>. Lineman distinguishes "passive data" as including not only observational data but also interventional data (describing experiments) and human interactions <a class="yt-timestamp" data-t="12:30:00">[12:30:00]</a>.

Pearl's perspective on [[application_of_large_language_models_llms_in_causal_discovery | LLMs]] is that their training data is fundamentally different <a class="yt-timestamp" data-t="13:22:00">[13:22:00]</a>. It consists of text produced by people who already possess causal models of the world <a class="yt-timestamp" data-t="13:37:00">[13:37:00]</a>. Thus, [[application_of_large_language_models_llms_in_causal_discovery | LLMs]] learn by copying or composing a "salad of associations" from these pre-existing human models, rather than directly learning a model of the world from raw environmental data <a class="yt-timestamp" data-t="13:43:00">[13:43:00]</a>. This means they are not constrained by the ladder of causation in the same way, as they access human-authored text containing causal information <a class="yt-timestamp" data-t="13:59:00">[13:59:00]</a>.

While some papers show [[application_of_large_language_models_llms_in_causal_discovery | LLMs]] performing well on certain causal benchmarks, they perform poorly on others <a class="yt-timestamp" data-t="15:11:00">[15:11:00]</a>. A notable observation is that [[application_of_large_language_models_llms_in_causal_discovery | LLMs]] perform significantly worse when asked to reason about abstract variables compared to concrete objects in stories <a class="yt-timestamp" data-t="18:21:00">[18:21:00]</a>.

### [[evaluation_and_systematic_testing_of_causal_models | Causal Benchmarks and the Firing Squad Problem]]

Pearl prefers small, well-understood causal problems with clear "ground truth" for testing [[application_of_large_language_models_llms_in_causal_discovery | LLMs]], such as the "firing squad problem" from his book <a class="yt-timestamp" data-t="15:26:00">[15:26:00]</a>. He contrasts this with large, complex benchmarks like COVID-19 data, where understanding the data is difficult and ground truth is absent <a class="yt-timestamp" data-t="15:36:00">[15:36:00]</a>. His experiments with [[application_of_large_language_models_llms_in_causal_discovery | LLMs]] on the firing squad problem showed that while initial responses might be off-topic or hesitant, with persistent and careful prompting, the model can eventually arrive at the correct causal conclusion <a class="yt-timestamp" data-t="16:16:00">[16:16:16]</a>. This highlights that [[application_of_large_language_models_llms_in_causal_discovery | LLMs]] are currently "black boxes" that require specific prompting for desired causal reasoning <a class="yt-timestamp" data-t="17:17:00">[17:17:00]</a>.

## Future Research Directions in Causality

Pearl suggests key areas for future research in causality:

*   **[[personalized_medicine_and_individualized_decision_making | Personalized medicine]] and individualized decision-making**: This involves quantifying harm and benefit for specific situations, not just populations, which has applications across medicine, political science (e.g., swing states), and business <a class="yt-timestamp" data-t="18:50:00">[18:50:00]</a>. Recent work suggests answering situation-specific questions is now feasible <a class="yt-timestamp" data-t="19:08:00">[19:08:00]</a>.
*   **Automated scientists**: Systems that can autonomously decide the best experiments to conduct next <a class="yt-timestamp" data-t="26:25:00">[26:25:00]</a>. Such systems would learn by performing local perturbations on existing models, testing intermediate variables, and exploring interventions <a class="yt-timestamp" data-t="31:03:00">[31:03:00]</a>. This requires the ability to generate hypotheses, design experiments, and understand the implications of interventions <a class="yt-timestamp" data-t="31:59:00">[31:59:00]</a>.
*   **Free will**: Understanding the computational advantage of the "illusion of free will" which appears to have survival value. If machines can learn and program this, they could act as though they have free will, fostering trust and understanding <a class="yt-timestamp" data-t="27:03:00">[27:03:00]</a>.
*   **Metaphors and reasoning by analogy**: These are powerful modes of reasoning that remain largely untackled in AI <a class="yt-timestamp" data-t="33:43:00">[33:43:00]</a>.

## [[challenges_and_opportunities_in_structural_causal_models | Challenges in Broadly Applying Causality]]

Several factors hinder the broader application of causality in industry and research:
*   **Funding, language, and training gaps**: There's a general lack of resources and education in causal inference <a class="yt-timestamp" data-t="34:28:00">[34:28:00]</a>.
*   **Attention and resistance to paradigm shifts**: Many practitioners, especially in machine learning and statistics, tend to focus on traditional methods like sampling, interpolation, and curve fitting, often assuming "everything is in the data" <a class="yt-timestamp" data-t="34:39:00">[34:39:00]</a>. This creates a significant barrier, as changing paradigms is difficult, especially when people are rewarded for working within existing ones <a class="yt-timestamp" data-t="35:10:00">[35:10:00]</a>.
*   **Misconceptions in statistics education**: While some statisticians claim causal modeling is central to their field, Pearl notes the absence of "cause" in many statistics textbooks' indices <a class="yt-timestamp" data-t="36:12:00">[36:12:00]</a>. Structural Equation Models (SEMs), though capable of representing causality, are often taught as parsimonious representations of covariance matrices, focusing on "fit to data" (e.g., using Bayesian Information Criterion) rather than causal correctness <a class="yt-timestamp" data-t="38:40:00">[38:40:00]</a>. A non-causal model might fit observational data better but will fail to generalize under distribution shifts, unlike a causally correct model <a class="yt-timestamp" data-t="37:33:00">[37:33:00]</a>. This highlights the need for a shift in education, positioning statistics as a "special branch" of [[causal_discovery_and_inference_in_ai | causal inference]] dealing with lower levels of the ladder of causation <a class="yt-timestamp" data-t="38:08:00">[38:08:08]</a>.

## The Rift in the Causal Community

A perceived "rift" exists within the causal community, with different traditions (e.g., Rubin's potential outcome framework vs. Pearl's [[structural_causal_models_and_graph_theory | structural causal models]]) sometimes speaking different languages, leading to duplicated work and misunderstandings <a class="yt-timestamp" data-t="21:44:00">[21:44:00]</a>. Pearl emphasizes that the potential outcome framework is logically equivalent to [[structural_causal_models_and_graph_theory | structural causal models]] <a class="yt-timestamp" data-t="22:52:00">[22:52:00]</a>. The main difference lies in the comfort and clarity with which one can articulate assumptions. Pearl argues that while equations for "conditional ignorability" (common in potential outcomes) can be written down, it's difficult for any human to defend such an assumption conceptually in a real-world setup <a class="yt-timestamp" data-t="23:53:00">[23:53:00]</a>. Many practitioners, even those using potential outcomes, "do graphs in their mind" because it's more intuitive <a class="yt-timestamp" data-t="23:42:00">[23:42:00]</a>.

## Limitations of Randomized Controlled Trials (RCTs)

Randomized Controlled Trials (RCTs) are often considered the "golden standard" for establishing causation, but they have limitations <a class="yt-timestamp" data-t="40:15:00">[40:15:00]</a>. One challenge is external validity: results from an RCT may not translate directly to a different environment or specific use case <a class="yt-timestamp" data-t="40:26:00">[40:26:00]</a>. This is because the population sampled for an RCT might differ significantly from the target population (e.g., participants incentivized by payment) <a class="yt-timestamp" data-t="20:15:00">[20:15:00]</a>.

Pearl notes that transferring information from one experimental environment to another, or using [[machine_learning_and_causal_inference_methodologies | data fusion]] methods, relies on "Level 2 assumptions" (graph-based assumptions) that are not always verifiable solely through randomized experiments <a class="yt-timestamp" data-t="41:00:00">[41:00:00]</a>. People often have good intuitive ideas about what makes their environment unique, and if they can formally characterize these idiosyncratic properties, information can be transferred <a class="yt-timestamp" data-t="41:47:00">[41:47:00]</a>. However, this requires familiarity with causal modeling techniques, which is often lacking <a class="yt-timestamp" data-t="42:30:00">[42:30:00]</a>.

Furthermore, RCTs can only answer interventional questions (Level 2 on the ladder of causation) <a class="yt-timestamp" data-t="43:32:00">[43:32:00]</a>. They cannot answer counterfactual questions (Level 3), such as finding the causes of an effect, distinguishing between direct and indirect causes, or assessing necessary versus sufficient causes <a class="yt-timestamp" data-t="43:50:00">[43:50:00]</a>. For example, an RCT might show a drug has no overall effect, but it cannot determine if this means it has no effect on any individual, or if it kills some and saves others <a class="yt-timestamp" data-t="44:42:00">[44:42:00]</a>. Tools that combine observational and randomized studies, however, can provide bounds on the probability of harm or benefit for individuals <a class="yt-timestamp" data-t="45:13:00">[45:13:00]</a>.

## [[causal_discovery_algorithms_and_realworld_applications | Causal Discovery]]

Regarding [[causal_discovery_algorithms_and_realworld_applications | causal discovery]] software, Pearl acknowledges its existence and improvements, particularly in making assumptions less stringent <a class="yt-timestamp" data-t="51:46:00">[51:46:00]</a>. His own work, however, has primarily focused not on *how* to discover causal structures, but rather on "if you have it, what can you do with it?" <a class="yt-timestamp" data-t="52:00:00">[52:00:00]</a>. He views this as a division of labor within the field <a class="yt-timestamp" data-t="52:44:00">[52:44:00]</a>.

The ability to establish "true causality" with fewer assumptions, even with increasing data access and more polished AI, is limited by the *kind* of data, not just its size <a class="yt-timestamp" data-t="50:39:00">[50:39:00]</a>. Even with infinite observational data, one cannot move from Level 1 (associational) to Level 2 (interventional) on the ladder of causation <a class="yt-timestamp" data-t="51:20:00">[51:20:00]</a>.

Future AGI systems are expected to learn causality similarly to humans and scientists, utilizing experiments for Level 2 and hypothesizing and imagination for Level 3 <a class="yt-timestamp" data-t="53:23:00">[53:23:23]</a>. This process will be accelerated by greater access to data and computation <a class="yt-timestamp" data-t="54:02:00">[54:02:00]</a>.

There is ongoing work on identification in Level 3, which concerns learning the functions themselves within SCMs <a class="yt-timestamp" data-t="54:49:00">[54:49:00]</a>. While not fully testable in all cases, combining experimental and observational data can provide informative bounds on Level 3 queries, sometimes collapsing into point estimates <a class="yt-timestamp" data-t="55:20:00">[55:20:00]</a>.