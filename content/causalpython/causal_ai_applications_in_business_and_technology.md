---
title: Causal AI applications in business and technology
videoId: CzL0pV6LyRk
---

From: [[causalpython]] <br/> 
## Causal AI Applications in Business and Technology

Businesses inherently seek to implement actions that produce desired outcomes, a goal fundamentally rooted in understanding problems from a causal perspective <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a>. While traditional machine learning often relies on observational data, which can contain inherent biases, [[causal AI and machine learning | causal inference]] provides methods to address these biases and enable more reliable, actionable decisions <a class="yt-timestamp" data-t="00:02:33">[00:02:33]</a>. This is particularly crucial for big tech companies, where vast amounts of data are collected observationally <a class="yt-timestamp" data-t="00:02:33">[00:02:33]</a>.

### Applications at Spotify

Spotify has achieved significant success by applying [[causal AI applications in various industries | causal inference]] beyond merely assessing the impact of product launches or new features <a class="yt-timestamp" data-t="00:00:10">[00:00:10]</a>. The focus is on leveraging [[causal AI applications in various industries | causal inference]] to drive business actions <a class="yt-timestamp" data-t="00:00:21">[00:00:21]</a>. Many of these projects are currently in late-stage testing <a class="yt-timestamp" data-t="00:00:23">[00:00:23]</a>.

One particularly useful methodology employed is **Synthetic Control** <a class="yt-timestamp" data-t="00:03:41">[00:03:41]</a>. This method is effective when an intervention occurs at a precise point in time, such as a new feature release <a class="yt-timestamp" data-t="00:03:41">[00:03:41]</a>. It allows for a clear definition of "before" and "after" treatment periods <a class="yt-timestamp" data-t="00:04:00">[00:04:00]</a>. Synthetic control constructs a "synthetic" version of the treated unit using similar, unaffected units, enabling the estimation of the intervention's impact <a class="yt-timestamp" data-t="00:04:16">[00:04:16]</a>. This is especially beneficial when identifying all potential confounders is difficult or when some confounders are hidden <a class="yt-timestamp" data-t="00:04:56">[00:04:56]</a>.

The intuitive nature of synthetic control can, however, be a double-edged sword, potentially leading to a premature belief in causal identification <a class="yt-timestamp" data-t="00:05:22">[00:05:22]</a>. To ensure [[causal AI and machine learning | causal identification]], the method can be combined with frameworks like directed acyclic graphs (DAGs) and sensitivity analysis <a class="yt-timestamp" data-t="00:05:47">[00:05:47]</a>. Historically developed by economists with semi-parametric assumptions, integrating synthetic control with a computer science perspective (like using DAGs) allows for more general, non-parametric assumptions and robust sensitivity analysis <a class="yt-timestamp" data-t="00:06:06">[00:06:06]</a>. This approach enables assessing how violations of assumptions might affect the causal estimates <a class="yt-timestamp" data-t="00:09:24">[00:09:24]</a>.

### Building Trust in Causal Models

A common concern for deep learning practitioners when encountering [[causal ai and machine learning | causal methods]] is the reliance on explicit assumptions <a class="yt-timestamp" data-t="00:10:15">[00:10:15]</a>. However, acknowledging and deeply considering these assumptions is a strength of [[causal ai and machine learning | causal inference]], as it forces a rigorous evaluation of the data generation process <a class="yt-timestamp" data-t="00:10:47">[00:10:47]</a>.

To build trust, especially regarding the control of confounders:
*   **Domain knowledge:** Leveraging expert knowledge to identify the most important confounders can help bound the potential impact of unmeasured variables <a class="yt-timestamp" data-t="00:11:27">[00:11:27]</a>.
*   **Sensitivity analysis:** This quantitatively assesses how much causal effect estimates would change if assumptions are slightly violated, providing bounds on the effect <a class="yt-timestamp" data-t="00:11:49">[00:11:49]</a>. This allows for claims even if point identification is not possible, such as demonstrating a non-zero causal effect <a class="yt-timestamp" data-t="00:12:12">[00:12:12]</a>.

### [[Challenges in integrating causal AI in business settings | Challenges in Causal Machine Learning Engineering]]

A key challenge in [[Causal Machine Learning Applications in Various Industries | causal machine learning engineering]] compared to traditional machine learning is the additional work required to prevent misspecification of variables from leading to significant changes in predictions <a class="yt-timestamp" data-t="00:13:23">[00:13:23]</a>. A naive approach might involve conditioning on all possible confounding variables <a class="yt-timestamp" data-t="00:13:37">[00:13:37]</a>. However, simply controlling for numerous variables can actually worsen estimates if some are not true confounders but rather colliders <a class="yt-timestamp" data-t="00:14:06">[00:14:06]</a>.

Therefore, a crucial prior step is to deeply analyze the causal graph (DAG) to identify genuine confounders for adjustment <a class="yt-timestamp" data-t="00:14:21">[00:14:21]</a>. The choice of methodology (e.g., double machine learning, inverse propensity weighting) should also be guided by the data's characteristics, such as noise levels and variance, to ensure confidence in the causal effect estimates <a class="yt-timestamp" data-t="00:14:34">[00:14:34]</a>. The conceptual framework preceding and following the application of these methodologies is paramount <a class="yt-timestamp" data-t="00:15:23">[00:15:23]</a>.

### [[Causal AI in medicine | Healthcare Applications]]

In healthcare, [[Causal AI in medicine | causal inference]] can be critically important for decision-making. At one startup, causal methods were applied to automate the medical triage process, which involves extracting patient symptoms and risk factors to recommend appropriate medical pathways (e.g., GP, pharmacist, emergency room) <a class="yt-timestamp" data-t="00:16:11">[00:16:11]</a>.

A disease model, represented as a probabilistic graphical model with nodes for risk factors, diseases, and symptoms, was used <a class="yt-timestamp" data-t="00:16:48">[00:16:48]</a>. The traditional diagnostic approach, dating back to the mid-1980s, involved identifying the disease with the highest posterior likelihood given observed symptoms <a class="yt-timestamp" data-t="00:17:19">[00:17:19]</a>. However, this correlational approach could lead to incorrect recommendations, such as diagnosing type two diabetes for chest pain due to shared confounders like being overweight, despite no direct causal link <a class="yt-timestamp" data-t="00:18:14">[00:18:14]</a>.

By reframing diagnosis as a **counterfactual reasoning task**, asking "what would happen to symptoms if a specific disease were cured?", the accuracy significantly improved <a class="yt-timestamp" data-t="00:19:05">[00:19:05]</a>. This counterfactual approach performed as well as the top 25% of doctors, doubling the accuracy compared to the correlational approach, which was only as good as the top 50% <a class="yt-timestamp" data-t="00:19:41">[00:19:41]</a>. Crucially, the underlying disease model remained unchanged; only the question asked of the model shifted from correlation to causation <a class="yt-timestamp" data-t="00:19:54">[00:19:54]</a>. This demonstrated the power of [[causal AI and its connection to machine learning | causal inference]] not just for quantifying effects but also for predicting future impacts.

### Surprising Learnings from Industry Application

One surprising observation when applying causal modeling in industry is that raw correlations between treatments and outcomes can be misleading regarding their relative ordering <a class="yt-timestamp" data-t="00:21:06">[00:21:06]</a>. Even after controlling for confounders, treatments with small correlations might have larger causal effects than those with strong correlations <a class="yt-timestamp" data-t="00:21:51">[00:21:51]</a>. This highlights that simply selecting the "best" treatment based on initial correlations is often incorrect, as the adjustments for confounding can differ significantly across treatments <a class="yt-timestamp" data-t="00:22:20">[00:22:20]</a>.

### The Future: Causal Representation Learning

The next frontier in [[causal ai applications in various industries | causal AI applications]] in technology is **causal representation learning** <a class="yt-timestamp" data-t="00:53:25">[00:53:25]</a>. Currently, large tech companies often learn embeddings of users and products based on co-occurrence data, which inherently captures correlations <a class="yt-timestamp" data-t="00:53:31">[00:53:31]</a>. However, when these correlation-based embeddings are used as inputs for recommendation engines – which aim to *cause* a user to enjoy content – it represents a fundamental mismatch <a class="yt-timestamp" data-t="00:53:57">[00:53:57]</a>.

By learning a [[causal ai and machine learning | causal representation]] of users and products, the goal is to identify the underlying latent factors that truly *cause* user similarity or enjoyment of content <a class="yt-timestamp" data-t="00:54:10">[00:54:10]</a>. Demonstrating the effectiveness of this approach in lower-stakes environments like music recommendations at Spotify could pave the way for its application in higher-stakes domains, such as [[Causal AI in medicine | medicine]] <a class="yt-timestamp" data-t="00:54:25">[00:54:25]</a>. This shift promises to lead to more reliable and impactful applications of [[causal AI and machine learning | AI]].