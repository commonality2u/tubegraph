---
title: Philosophical aspects of causality
videoId: rM25vt_ZmFc
---

From: [[causalpython]] <br/> 

Causality, a core concept in [[causality_in_ai | AI]], machine learning, and computer science, carries significant philosophical underpinnings. Its study often delves into foundational questions about intelligence, reality, and human understanding.

## Core Philosophical Questions
The pursuit of [[causality_in_ai | causality]] has driven researchers to ask fundamental questions:
*   What is intelligence? <a class="yt-timestamp" data-t="00:04:09">[00:04:09]</a> <a class="yt-timestamp" data-t="00:04:57">[00:04:57]</a>
*   How does one understand oneself and find meaning? <a class="yt-timestamp" data-t="00:03:40">[00:03:40]</a> <a class="yt-timestamp" data-t="00:03:45">[00:03:45]</a>
*   What constitutes a "causal variable" in the first place? <a class="yt-timestamp" data-t="00:11:47">[00:11:47]</a> <a class="yt-timestamp" data-t="00:11:50">[00:11:50]</a>

The discussion extends to whether [[causality_in_ai | causal reasoning]] is a necessary but not necessarily sufficient component for intelligence, given other complex topics like emotions. <a class="yt-timestamp" data-t="00:05:32">[00:05:32]</a> <a class="yt-timestamp" data-t="00:05:40">[00:05:40]</a>

## Causality as a Useful Abstraction
Drawing from Bertrand Russell's arguments against causality, Judea Pearl suggests that causality might be a "convenient shortcut" or a "useful form about thinking about reality." <a class="yt-timestamp" data-t="00:12:52">[00:12:52]</a> <a class="yt-timestamp" data-t="00:13:04">[00:13:04]</a>

This perspective is echoed by researchers like Bernhard Sch√∂lkopf and Dominik Janzing, who, as physicists by training, describe causality as a "useful abstraction." <a class="yt-timestamp" data-t="00:15:00">[00:15:00]</a> <a class="yt-timestamp" data-t="00:15:20">[00:15:20]</a> It simplifies complex physical systems by "kicking out all the unnecessary detail" while remaining sufficient to answer specific hypotheses. <a class="yt-timestamp" data-t="00:16:19">[00:16:19]</a> <a class="yt-timestamp" data-t="00:16:36">[00:16:36]</a>

## Abstractions in Causal Reasoning
The concept of [[abstractions_in_causal_reasoning | abstractions]] is central to philosophical inquiries in causality.
Initially, this idea was explored through "causal consistency of structural causal models," examining how models at different levels of abstraction (e.g., total cholesterol vs. HDL/LDL) can be equated while respecting interventions. <a class="yt-timestamp" data-t="00:09:40">[00:09:40]</a> <a class="yt-timestamp" data-t="00:10:09">[00:10:09]</a>

This line of questioning leads to the broader philosophical consideration of how variables themselves are defined and what constitutes a "causal variable." <a class="yt-timestamp" data-t="00:11:28">[00:11:28]</a> <a class="yt-timestamp" data-t="00:11:32">[00:11:32]</a> The ability to look at phenomena from different scopes while still capturing their characteristics is a key reason why [[abstractions_in_causal_reasoning | abstractions]] are important in [[causality_in_ai | causal reasoning]]. <a class="yt-timestamp" data-t="00:12:01">[00:12:01]</a> <a class="yt-timestamp" data-t="00:12:15">[00:12:15]</a>

## The Role of Philosophy in AI and Causality
The intersection of [[causality_in_ai | causality]] and [[philosophical_perspectives_on_causality | philosophy]] is profound:
*   **Broadening Horizons**: Research in causality needs to ask questions that might not have been considered before, looking "outside of the box" to broaden the horizon beyond existing scientific frameworks. <a class="yt-timestamp" data-t="00:07:09">[00:07:09]</a> <a class="yt-timestamp" data-t="00:07:16">[00:07:16]</a>
*   **Philosophical Grounding**: It is suggested that scientific papers, especially in [[causality_in_ai | AI]], should include statements about the authors' [[philosophical_perspectives_on_causality | philosophical grounding]] to provide transparency about their perspectives. <a class="yt-timestamp" data-t="00:18:53">[00:18:53]</a> <a class="yt-timestamp" data-t="00:19:02">[00:19:02]</a>
*   **Understanding vs. Knowing**: The debate on whether large language models "understand" or merely "know" (based on text data) touches upon fundamental philosophical questions, such as John Searle's Chinese Room argument. <a class="yt-timestamp" data-t="00:24:51">[00:24:51]</a> <a class="yt-timestamp" data-t="00:25:02">[00:25:02]</a>

## The Challenge of Formalism vs. Intuition
While intuition can provide significant insights into causation, a strong argument is made for the necessity of formalism in studying [[causality_in_ai | causality]]. <a class="yt-timestamp" data-t="01:09:50">[01:09:50]</a> <a class="yt-timestamp" data-t="01:09:56">[01:09:56]</a> This is because:
*   **Language of Assumptions**: Formalism provides a "language" to express modeling assumptions about data-generating processes and derive sound conclusions. <a class="yt-timestamp" data-t="01:08:41">[01:08:41]</a> <a class="yt-timestamp" data-t="01:09:00">[01:09:00]</a>
*   **Foundational Research**: Causality research is deeply rooted in mathematics and probability theory, requiring precise axiomatic definitions. <a class="yt-timestamp" data-t="01:09:15">[01:09:15]</a> <a class="yt-timestamp" data-t="01:09:47">[01:09:47]</a>
*   **Avoiding Wrong Intuition**: Relying solely on intuition about causation can be a "double-edged sword" as it often leads to wrong conclusions. <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a> <a class="yt-timestamp" data-t="01:09:57">[01:09:57]</a> <a class="yt-timestamp" data-t="01:10:01">[01:10:01]</a> This is highlighted by famous examples in [[philosophical_perspectives_on_causality | philosophy]]. <a class="yt-timestamp" data-t="01:10:04">[01:10:04]</a> <a class="yt-timestamp" data-t="01:10:06">[01:10:06]</a>

## White Box vs. Black Box Models
The discussion also extends to whether [[causality_in_ai | causal models]] need to be "white box" (transparent and understandable) to be useful. <a class="yt-timestamp" data-t="00:37:32">[00:37:32]</a>

While it's a common desire for systems to be white box for understanding, a white box model does not inherently mean it is explainable, especially at scale. <a class="yt-timestamp" data-t="00:42:29">[00:42:29]</a> <a class="yt-timestamp" data-t="00:42:31">[00:42:31]</a> Large linear programs, for example, are white box by nature, yet their complexity can make them incomprehensible to humans. <a class="yt-timestamp" data-t="00:43:19">[00:43:19]</a> <a class="yt-timestamp" data-t="00:44:02">[00:44:02]</a> This leads to the "hot take" that black box models can be perfectly fine, as long as they provide faithful and useful explanations. <a class="yt-timestamp" data-t="00:44:51">[00:44:51]</a> <a class="yt-timestamp" data-t="00:45:02">[00:45:02]</a>

Ultimately, the study of [[causality_in_ai | causality]] is influenced by and contributes to deep philosophical questions about intelligence, knowledge, and our understanding of the world. <a class="yt-timestamp" data-t="00:29:07">[00:29:07]</a> <a class="yt-timestamp" data-t="00:29:12">[00:29:12]</a>