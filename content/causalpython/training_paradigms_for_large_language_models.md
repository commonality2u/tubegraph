---
title: Training paradigms for large language models
videoId: relI7Q9A03g
---

From: [[causalpython]] <br/> 

Dr. Andrew Lampinen, a Senior Research Scientist at Google DeepMind, discusses his research on [[large_language_models_and_causality | large language models]] (LLMs) and their training paradigms, particularly focusing on how these models can learn causal understanding and generalize beyond their training data <a class="yt-timestamp" data-t="00:00:42">[00:00:42]</a>.

## Passive vs. Active Strategies in Training
A key distinction in Lampinen's recent paper, part of a series on [[large_language_models_and_causality | LLMs and causality]] released in the second half of 2023, is the concept of "active" and "passive" strategies <a class="yt-timestamp" data-t="00:01:16">[00:01:16]</a>. This terminology differs from traditional machine learning terms like "interventional," "observational," or "counterfactual" data, often drawn from Pearl's causal hierarchy <a class="yt-timestamp" data-t="00:01:41">[00:01:41]</a>.

While [[large_language_models_llms_learning_limitations | LLMs]] are trained passively—meaning they process language data generated by others from the internet—this data is not necessarily purely observational <a class="yt-timestamp" data-t="00:02:12">[00:02:12]</a>. For instance, language data from the internet contains interventional data, such as scientific papers describing experiments, Stack Overflow posts about debugging, or even everyday conversations where each statement is an intervention <a class="yt-timestamp" data-t="00:02:40">[00:02:40]</a>. Therefore, even though [[large_language_models_llms_learning_limitations | LLMs]] learn passively, they are exposed to interventional data <a class="yt-timestamp" data-t="00:02:57">[00:02:57]</a>.

### Impact on Generalization
This distinction is crucial because it impacts the [[large_language_models_llms_learning_limitations | models']] capabilities to generalize <a class="yt-timestamp" data-t="00:03:05">[00:03:05]</a>. Lampinen suggests two reasons why this kind of training data could lead to causal strategies and understanding that generalize beyond the training data <a class="yt-timestamp" data-t="00:03:19">[00:03:19]</a>:

1.  **Causal Strategies**: By observing others' interventions, models might discover a strategy for intervening that they can apply in new situations to uncover new causal structures for a downstream goal <a class="yt-timestamp" data-t="00:03:30">[00:03:30]</a>. Empirically, it has been shown that a generalizable strategy for intervening to determine causal structures can be discovered from passively observing someone else's interventions <a class="yt-timestamp" data-t="00:03:52">[00:03:52]</a>.

## Experimental Setup and Results
To study this, Lampinen and his team conducted controlled experiments with simpler models, training them on a data distribution featuring interventions on causal directed acyclic graphs (DAGs) <a class="yt-timestamp" data-t="00:04:31">[00:04:31]</a>. The model observed a series of interventions on a DAG and a goal (e.g., maximize a variable), along with interventions attempting to achieve that goal <a class="yt-timestamp" data-t="00:04:50">[00:04:50]</a>.

The core question was whether a model, passively observing data of interventions aimed at discovering causal structure and achieving goals, could generalize to actively intervene and discover new causal structures at test time, even on held-out causal structures <a class="yt-timestamp" data-t="00:05:04">[00:05:04]</a>.

Similar to how an [[large_language_models_llms_learning_limitations | LLM]] becomes active when deployed and interacts with a user, the research trained the system passively and tested it interactively <a class="yt-timestamp" data-t="00:05:30">[00:05:30]</a>. The results demonstrated that the system could apply passively observed causal strategies to actively intervene, discover new causal structures, and exploit them <a class="yt-timestamp" data-t="00:05:43">[00:05:43]</a>. The model significantly approximated correct [[logical_reasoning_in_large_language_models | causal reasoning]] more closely than simpler heuristic or associational strategies <a class="yt-timestamp" data-t="00:05:52">[00:05:52]</a>.

### Contradicting or Complementary to "Causal Parrots"?
The findings from Lampinen's paper challenge the hypothesis from the "Causal Parrots" paper, which suggests that [[large_language_models_llms_learning_limitations | LLMs]] learn a "meta-structural causal model" based on correlations of causal facts in training data, allowing them to "talk causality" but not "reason causally" <a class="yt-timestamp" data-t="00:06:05">[00:06:05]</a>. Lampinen's results indicate that models *are* capable of discovering [[logical_reasoning_in_large_language_models | causal reasoning]] algorithms that can be applied generally, provided a suitable training regime <a class="yt-timestamp" data-t="00:06:55">[00:06:55]</a>.

While natural data may contain more correlations and only occasional interventional data, which might lead [[large_language_models_llms_learning_limitations | models]] to learn more correlational strategies, Lampinen's work shows that if [[large_language_models_llms_learning_limitations | LLMs]] are tested on tasks requiring experimenting to discover causal structures, especially with explanations in the prompt, they can effectively learn to do so <a class="yt-timestamp" data-t="00:07:12">[00:07:12]</a>. This suggests that enough interventional data exists in [[large_language_models_llms_learning_limitations | LLM]] training distributions for them to discover some causal strategies <a class="yt-timestamp" data-t="00:08:00">[00:08:00]</a>.

## The Importance of Explanations and Training Paradigms
Explanations are emphasized as important in training <a class="yt-timestamp" data-t="00:08:19">[00:08:19]</a>. Giving a [[large_language_models_llms_learning_limitations | reinforcement learning]] agent a natural language explanation of a reward and asking it to predict those explanations can improve learning, even in confounded data <a class="yt-timestamp" data-t="00:09:12">[00:09:12]</a>.

Current [[large_language_models_llms_learning_limitations | training paradigms]] are largely driven by what works and allows for data at scale <a class="yt-timestamp" data-t="00:08:46">[00:08:46]</a>. However, designing training paradigms that leverage auxiliary tasks and incorporate known data structures can enhance learning and generalization <a class="yt-timestamp" data-t="00:09:54">[00:09:54]</a>. For instance, conditioning models on a quality signal (like a reward estimate) during training can help disentangle learning and enable the system to generate high-quality responses at test time <a class="yt-timestamp" data-t="00:10:21">[00:10:21]</a>. This is similar to techniques in offline [[large_language_models_llms_learning_limitations | reinforcement learning]], where conditioning on a signal of quality allows learning from "bad" data without replicating it at test time <a class="yt-timestamp" data-t="00:10:44">[00:10:44]</a>.

### [[large_language_models_llms_learning_limitations | LLMs]] as Agents
Lampinen views [[large_language_models_llms_learning_limitations | large language models]] as a type of agent, defining an agent as a system that takes inputs (observations) and produces output actions in a sequential decision-making problem <a class="yt-timestamp" data-t="00:11:19">[00:11:19]</a>. This covers [[large_language_models_llms_learning_limitations | LLMs]] that take natural language inputs and produce sequences of language, as well as game-playing AI <a class="yt-timestamp" data-t="00:11:39">[00:11:39]</a>.

A key distinction for agents is whether they interact with the environment during training or only at testing <a class="yt-timestamp" data-t="00:12:02">[00:12:02]</a>. Often, agents are trained partly on passive observational data (e.g., expert gameplay) before [[large_language_models_llms_learning_limitations | reinforcement learning]] fine-tuning involves active interaction and reward <a class="yt-timestamp" data-t="00:12:14">[00:12:14]</a>. This is analogous to how [[large_language_models_llms_learning_limitations | LLMs]] are first trained on passive human-generated language data, then fine-tuned, and finally subjected to a [[large_language_models_llms_learning_limitations | reinforcement learning]] step using a reward model based on human preferences for responses <a class="yt-timestamp" data-t="00:12:37">[00:12:37]</a>.

### Autoregressive Models and Error Accumulation
Autoregressive models can accumulate errors over long sequences because each subsequent token or data point is conditioned on previous generations, which may contain errors <a class="yt-timestamp" data-t="00:13:02">[00:13:02]</a>. However, state-of-the-art [[large_language_models_llms_learning_limitations | LLMs]] show some ability to self-correct using techniques like Chain of Thought <a class="yt-timestamp" data-t="00:13:58">[00:13:58]</a>. The system's architecture, including context length, impacts its ability to reconsider prior context and find errors <a class="yt-timestamp" data-t="00:14:21">[00:14:21]</a>.

Passive learning is less efficient and leads to worse out-of-distribution generalization <a class="yt-timestamp" data-t="00:14:42">[00:14:42]</a>. Simplistic passive learning strategies can cause systems to break down when they become active and move off the training data distribution <a class="yt-timestamp" data-t="00:14:52">[00:14:52]</a>. Techniques like DAgger from offline [[large_language_models_llms_learning_limitations | reinforcement learning]], which incorporate small amounts of interventional data, are important for robust systems <a class="yt-timestamp" data-t="00:15:06">[00:15:06]</a>. The active [[large_language_models_llms_learning_limitations | reinforcement learning]] done with [[large_language_models_llms_learning_limitations | LLMs]] after initial training serves a similar purpose <a class="yt-timestamp" data-t="00:15:26">[00:15:26]</a>.

### Blending Observational and Interventional Data
The idea of [[causality_and_large_language_models | causal data fusion]] or mixing observational and interventional data is analogous to improving the efficiency of inference and recovering causal structure <a class="yt-timestamp" data-t="00:16:05">[00:16:05]</a>. By combining "prior knowledge" from observational sources with interventional data, a system can efficiently determine necessary causal structures and avoid exploring irrelevant aspects <a class="yt-timestamp" data-t="00:16:42">[00:16:42]</a>.

## [[logical_reasoning_in_large_language_models | Logical Reasoning]] and Rationality in LLMs
Lampinen proposes that symbolic systems and [[logical_reasoning_in_large_language_models | logical reasoning systems]] are useful tools for an intelligent system, rather than being the essence of intelligence itself <a class="yt-timestamp" data-t="00:23:36">[00:23:36]</a>. Humans are not naturally good at [[logical_reasoning_in_large_language_models | logical]] or mathematical reasoning without extensive training <a class="yt-timestamp" data-t="00:23:46">[00:23:46]</a>. Intelligence, in Lampinen's view, often takes the form of a continuous, fuzzy reasoning system, using symbolic logic as a tool for specific constraint problems <a class="yt-timestamp" data-t="00:24:10">[00:24:10]</a>.

### Human-like vs. Superhuman Systems
The goal of building [[large_language_models_llms_learning_limitations | LLMs]] raises a fundamental question: should systems be human-like (providing a similar interaction experience) or superhuman in specific regards <a class="yt-timestamp" data-t="00:55:09">[00:55:09]</a>? Lampinen emphasizes that [[large_language_models_llms_learning_limitations | LLMs]] are currently useful as tools to assist humans, rather than as autonomous agents, due to their unreliability without human oversight <a class="yt-timestamp" data-t="00:18:10">[00:18:10]</a>.

### The "Bitter Lesson" and System Fragility
Over-constraining the reasoning processes of a system based on assumptions about how it *should* solve problems can make it fragile <a class="yt-timestamp" data-t="00:44:22">[00:44:22]</a>. If the world deviates from these assumptions, the system can break down <a class="yt-timestamp" data-t="00:44:32">[00:44:32]</a>. Lampinen advocates for "softer" solutions, such as using explanation prediction objectives to encourage desired representations without overly constraining internal computations <a class="yt-timestamp" data-t="00:44:40">[00:44:40]</a>.

This aligns with Rich Sutton's "Bitter Lesson" in machine learning, which suggests that building in specific solutions to problems might work at small scale but tends to break down when scaled up <a class="yt-timestamp" data-t="00:45:40">[00:45:40]</a>. Therefore, softer solutions like providing explanations or modifying data distributions can be more effective and scalable for complex problems like building robust [[large_language_models_llms_learning_limitations | language models]] or agents in rich virtual environments <a class="yt-timestamp" data-t="00:46:01">[00:46:01]</a>.

## Understanding and Generalization
Lampinen views "understanding" as a graded concept, not a discrete binary state <a class="yt-timestamp" data-t="00:29:02">[00:29:02]</a>. [[large_language_models_llms_learning_limitations | LLMs]] possess some degree of understanding of certain features, as demonstrated by their ability to answer questions correctly, albeit imperfectly <a class="yt-timestamp" data-t="00:29:46">[00:29:46]</a>.

For generalization, Lampinen suggests that while humans may overestimate their natural reasoning abilities, formal reasoning is largely a learned skill acquired through extensive training <a class="yt-timestamp" data-t="00:32:40">[00:32:40]</a>. This implies that [[large_language_models_llms_learning_limitations | LLMs]] might also require rigorous and interactive training to efficiently learn formal reasoning strategies <a class="yt-timestamp" data-t="00:33:06">[00:33:06]</a>. Human and [[large_language_models_llms_learning_limitations | LLM]] performance on [[logical_reasoning_in_large_language_models | logical problems]] is influenced by semantic content, performing better when content supports [[logical_reasoning_in_large_language_models | logic]] and worse when it contradicts it <a class="yt-timestamp" data-t="00:33:51">[00:33:51]</a>. This suggests that the objective of humans and [[large_language_models_llms_learning_limitations | language models]] is not necessarily to be perfectly rational reasoners, but rather to be adaptive to frequently encountered situations <a class="yt-timestamp" data-t="00:41:21">[00:41:21]</a>.

## Lessons from Psychology and Experimentation
Lampinen, with his background in cognitive psychology, highlights the importance of:
*   **Experimental Design**: Cognitive scientists have extensively considered issues like confounding and different underlying processes that explain observations <a class="yt-timestamp" data-t="00:48:13">[00:48:13]</a>.
*   **Rigorous Statistics**: Emphasizing the need for robust statistical analyses in experiments, which is often overlooked in machine learning where results are sometimes judged merely by "bigger numbers" <a class="yt-timestamp" data-t="00:48:30">[00:48:30]</a>.
*   **Abstract Thinking**: Bridging observations to more abstract models that can explain them, a valuable skill in computational cognitive science <a class="yt-timestamp" data-t="00:48:53">[00:48:53]</a>.

Working in controlled settings is vital for understanding, but real-world application acts as a "forcing function" against overfitting, ensuring that algorithms solve genuine problems rather than merely toy ones designed for them <a class="yt-timestamp" data-t="00:46:50">[00:46:50]</a>.