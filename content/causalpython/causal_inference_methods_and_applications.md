---
title: Causal inference methods and applications
videoId: 8yWKQqNFrmY
---

From: [[causalpython]] <br/> 

[[Causal inference and its applications | Causality]] is becoming increasingly important, especially as agents begin interacting in the real world or on the web with other people <a class="yt-timestamp" data-t="00:00:06">[00:00:06]</a>. Notions like action, reward, optimizing behavior, and penalizing behavior are at their core [[Causal inference and its applications | causal abstractions]] <a class="yt-timestamp" data-t="00:00:25">[00:00:25]</a>. This includes determining which agent is most useful or should be rewarded, and even notions of blame if agents are underperforming <a class="yt-timestamp" data-t="00:00:28">[00:00:28]</a>.

## Causality in AI Agents
The future of agents will bring new types of [[Causal inference and its applications | causal questions]] <a class="yt-timestamp" data-t="00:55:01">[00:55:01]</a>. While causality might not appear as much in the initial training of agents (which might rely on reinforcement learning), it becomes critical when agents interact with human agents or collaborate <a class="yt-timestamp" data-t="00:05:12">[00:05:12]</a>. Designing systems from the ground up to behave as desired is a critical area where [[Causal inference and its applications | causal thinking]] will play a role <a class="yt-timestamp" data-t="00:05:56">[00:05:56]</a>. Research, such as a paper from DeepMind's Richardson and Everett, explores how agents can learn robust world models <a class="yt-timestamp" data-t="00:00:43">[00:00:43]</a>. These models aim to help agents act correctly in a system by making the right actions to optimize outcomes <a class="yt-timestamp" data-t="00:07:22">[00:07:22]</a>.

## Causality in Large Language Models (LLMs)
LLMs have highlighted that [[Causal inference and its applications | causality]] traditionally required both domain knowledge and discovery/reasoning <a class="yt-timestamp" data-t="00:03:26">[00:03:26]</a>. While the field extensively focused on discovery and reasoning from data, domain knowledge was often left vague <a class="yt-timestamp" data-t="00:03:35">[00:03:35]</a>. LLMs have demonstrated ways to learn this domain knowledge, and in many cases, if domain knowledge is accurate, it can even enable reasoning without requiring data <a class="yt-timestamp" data-t="00:03:52">[00:03:52]</a>. This suggests that the future of [[Causal inference and its applications | causal inference]] must account for both aspects: domain knowledge and robust methods for its integration into algorithms <a class="yt-timestamp" data-t="00:04:30">[00:04:30]</a>.

There are multiple ways to achieve a causal agent <a class="yt-timestamp" data-t="00:11:06">[00:11:06]</a>:
1.  **Collecting diverse data:** With enough diverse data, systems can learn causal actions even without explicit causal models, as empirical and theoretical evidence now suggests <a class="yt-timestamp" data-t="00:09:44">[00:09:44]</a>.
2.  **Generalizing from rules and domain knowledge:** LLMs can answer [[Causal inference and its applications | causal questions]] by applying theorems and axioms learned from books, mimicking how a causal agent would act <a class="yt-timestamp" data-t="00:10:21">[00:10:21]</a>. This approach can be more attractive than models trying to discover fundamental laws from passive data <a class="yt-timestamp" data-t="00:14:06">[00:14:06]</a>.

## DoWhy Library
DoWhy is an open-source library that simplifies the process of [[Causal inference and its applications | causal inference]] <a class="yt-timestamp" data-t="02:01:01">[02:01:01]</a>. It was created to provide a unified tool for performing the steps of [[Causal inference and its applications | causal analysis]], overcoming the pain of grappling with complex books and reimplementing estimators <a class="yt-timestamp" data-t="02:17:10">[02:17:10]</a>.

DoWhy organizes the causal inference pipeline into four distinct steps:
1.  **Model:** Start with a causal graph, which can be done without data <a class="yt-timestamp" data-t="01:59:59">[01:59:59]</a>. This step is about defining domain knowledge and assumptions <a class="yt-timestamp" data-t="02:59:59">[02:59:59]</a>.
2.  **Identify:** Determine if the causal effect can be identified from the given graph, also without requiring data <a class="yt-timestamp" data-t="01:59:59">[01:59:59]</a>.
3.  **Estimate:** Compute the causal effect using various estimators, drawing from the potential outcome framework <a class="yt-timestamp" data-t="02:00:20">[02:00:20]</a>.
4.  **Refute:** Evaluate the robustness of the causal estimate. This step is crucial because causal estimates, like scientific theories, can only be disproven, not proven correct <a class="yt-timestamp" data-t="01:00:44">[01:00:44]</a>.

DoWhy incorporates robustness checks inspired by economics and biomedical literature:
*   **Sensitivity analysis:** Considers what happens if an unobserved confounder was missed <a class="yt-timestamp" data-t="02:00:05">[02:00:05]</a>.
*   **Negative controls/Placebo tests:** Involves adding random common causes, replacing treatments with placebos (random variables), or using dummy outcomes (functions of confounders but not treatment) to verify if the causal effect remains zero, as expected <a class="yt-timestamp" data-t="02:07:30">[02:07:30]</a>.
*   **Graph verification:** More recent modules by collaborators focus on verifying the causal graph itself, comparing its information to that of a random graph to assess its support by current data <a class="yt-timestamp" data-t="02:22:50">[02:22:50]</a>.

DoWhy is now part of the broader PyWhy organization, aiming to build a comprehensive causal ecosystem that makes the entire [[Causal inference and its applications | causal inference]] process seamless <a class="yt-timestamp" data-t="02:23:55">[02:23:55]</a>. This includes integrating packages like CausalLearn for discovery and EconML for estimation, allowing users to interact with a single interface to solve diverse [[Causal inference and its applications | causal problems]] <a class="yt-timestamp" data-t="02:37:12">[02:37:12]</a>.

## Challenges and Future Directions
### Verification and Graph Generation
A significant challenge in [[Causal inference and its applications | causal modeling]] is verification <a class="yt-timestamp" data-t="01:32:00">[01:32:00]</a>. For agentic frameworks, verification is the biggest missing piece, especially for vague real-world problems where human rewards are difficult to define or scale <a class="yt-timestamp" data-t="01:42:00">[01:42:00]</a>. New domains require methods to create verifiers for outcomes and even individual actions <a class="yt-timestamp" data-t="01:44:00">[01:44:00]</a>.

The biggest bottleneck for libraries like DoWhy is generating the initial causal graph, as it's hard to obtain and verify its correctness <a class="yt-timestamp" data-t="03:36:00">[03:36:00]</a>. LLMs could potentially streamline this process by generating plausible graphs that experts can critique <a class="yt-timestamp" data-t="03:08:00">[03:08:00]</a>. LLMs are also showing promise in suggesting good refutation tests <a class="yt-timestamp" data-t="03:53:00">[03:53:00]</a>.

### Making [[Causal inference and its applications | Causal Inference]] Accessible
A new library, PyWhy-LLM, aims to make [[Causal inference and its applications | causal inference]] accessible to non-experts <a class="yt-timestamp" data-t="02:52:00">[02:52:00]</a>. The vision is a system where the input is text (e.g., a question about medicine or school interventions), and the output is also text, with advanced [[Causal inference and its applications | causal inference]] tools running in the background <a class="yt-timestamp" data-t="03:09:00">[03:09:00]</a>. For questions requiring data, the system could request it, perform analysis, and provide answers <a class="yt-timestamp" data-t="03:20:00">[03:20:00]</a>.

### Using [[Causal inference and its applications | Causality]] to Enhance LLM Robustness
Beyond helping [[Causal inference and its applications | causal inference]], [[Causal inference and its applications | causality]] can make LLMs more robust <a class="yt-timestamp" data-t="03:36:00">[03:36:00]</a>. A key idea is to teach LLMs the basic axioms of [[Causal inference and its applications | causal reasoning]], rather than specific causal knowledge <a class="yt-timestamp" data-t="03:52:00">[03:52:00]</a>. For instance, training LLMs on synthetic axiomatic data for transitivity (if A causes B and B causes C, then A may cause C) and d-separation shows surprising generalization capabilities to larger graphs and improved performance on new causal tasks <a class="yt-timestamp" data-t="03:57:00">[03:57:00]</a>. This approach aims to impart reasoning capabilities that generalize, rather than relying on memorization <a class="yt-timestamp" data-t="04:13:00">[04:13:00]</a>.

### Model-Free Causality and Diverse Data
A significant technical challenge is to achieve a "model-free revolution" in [[Causal inference and its applications | causality]], similar to model-free reinforcement learning <a class="yt-timestamp" data-t="04:24:00">[04:24:00]</a>. This involves extracting maximum information from data sets without relying on explicit world models, while still providing theoretical guarantees <a class="yt-timestamp" data-t="04:26:00">[04:26:00]</a>.

The concept of **exchangeable distributions** is a promising direction <a class="yt-timestamp" data-t="04:17:00">[04:17:00]</a>. If data comes from many diverse environments (multi-distribution data) and gets mixed, it allows for much better causal discovery, potentially even learning causal direction <a class="yt-timestamp" data-t="04:49:00">[04:49:00]</a>. This approach is rooted in the idea that if a causal mechanism remains static across different environments, it provides a "still point" for understanding <a class="yt-timestamp" data-t="05:13:00">[05:13:00]</a>.

### The "Bitter Lesson" for Causality
The "bitter lesson" suggests that while [[Causal inference and its applications | causality]] is a desired end result, it may not always be the means to achieve it <a class="yt-timestamp" data-t="05:51:00">[05:51:00]</a>. Some systems, like industrial root cause analysis, perfectly align causal means and ends <a class="yt-timestamp" data-t="05:08:00">[05:08:00]</a>. However, in many other cases, the output is causal, but the methods to reach it might be varied and not necessarily principled causal approaches <a class="yt-timestamp" data-t="05:24:00">[05:24:00]</a>. This implies exploring other ways to achieve causal outputs, such as data augmentation, which may not inherently be tied to traditional causal principles <a class="yt-timestamp" data-t="05:49:00">[05:49:00]</a>.

## Conclusion
The field of [[Causal inference and its applications | causality]] is experiencing an explosion of possibilities with the advent of LLMs <a class="yt-timestamp" data-t="06:58:00">[06:58:00]</a>. It's an opportune time to explore how [[Causal inference and its applications | causality]] can make LLMs more robust and how LLMs can assist in scientific [[Causal inference and its applications | causal inference]] by generating graphs from literature and planning experiments <a class="yt-timestamp" data-t="07:09:00">[07:09:00]</a>. Community initiatives like PyWhy foster collaboration and welcome contributions from researchers and practitioners alike, especially real-world use cases <a class="yt-timestamp" data-t="07:50:00">[07:50:00]</a>.