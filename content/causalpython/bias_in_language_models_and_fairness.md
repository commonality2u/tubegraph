---
title: Bias in language models and fairness
videoId: sljBU_HFnFs
---

From: [[causalpython]] <br/> 

Abd-Rahman Zed, a PhD candidate at Mila, focuses his research on fairness in [[large_language_models_and_causality | language models]], particularly concerning bias <a class="yt-timestamp" data-t="01:49:18">[01:49:18]</a>.

## Research Motivation and Scope
His work investigates several key questions related to bias in [[large_language_models_and_causality | language models]]:
*   Why do [[large_language_models_and_causality | language models]] learn biases? <a class="yt-timestamp" data-t="01:54:57">[01:54:57]</a>
*   How can [[large_language_models_and_causality | language models]] exacerbate existing societal biases? <a class="yt-timestamp" data-t="01:57:04">[01:57:04]</a>
*   How can these biases be mitigated? <a class="yt-timestamp" data-t="01:57:04">[01:57:04]</a>
*   How can these biases be measured? <a class="yt-timestamp" data-t="01:57:04">[01:57:04]</a>

This research addresses various types of bias, including gender bias, race bias, and religion bias <a class="yt-timestamp" data-t="01:09:07">[01:09:07]</a>.

## Addressing Bias through Model Pruning
Abd-Rahman Zed co-authored a recent paper on pruning [[large_language_models_and_causality | large language models]] <a class="yt-timestamp" data-t="01:20:20">[01:20:20]</a>. This work suggests that certain parts of a [[large_language_models_and_causality | language model]] are responsible for bias <a class="yt-timestamp" data-t="01:27:00">[01:27:00]</a>. By identifying and removing these specific components, the model can become less biased <a class="yt-timestamp" data-t="01:32:00">[01:32:00]</a>.