---
title: Large Language Models and Causal Reasoning
videoId: rM25vt_ZmFc
---

From: [[causalpython]] <br/> 

[[Causality and large language models | Large language models (LLMs)]] are a significant topic in contemporary discussions within machine learning, particularly concerning their ability to engage in [[causal reasoning in language models | causal reasoning]] <a class="yt-timestamp" data-t="00:18:10">[00:18:10]</a>. Researchers are actively investigating whether these models can truly understand or only know causal relationships <a class="yt-timestamp" data-t="00:24:51">[00:24:51]</a>.

## How LLMs Approach Causality

A central conjecture regarding [[large_language_models_and_causality | LLMs]] and causality is the "correlations of causal facts" hypothesis <a class="yt-timestamp" data-t="00:21:12">[00:21:12]</a>. This hypothesis suggests that if [[large_language_models_and_causality | LLMs]] only learn correlations, their ability to correctly answer causal questions implies that such causal questions and answers were correlated within their training data <a class="yt-timestamp" data-t="00:21:48">[00:21:48]</a>. For example, if a textbook discusses the physics of how altitude affects temperature, an [[large_language_models_and_causality | LLM]] might learn this relationship by predicting the next word, where "causing" is a more probable word choice in that context <a class="yt-timestamp" data-t="00:22:20">[00:22:20]</a>.

This intuition is grounded in the idea that human-generated knowledge, such as that found in encyclopedias like Wikipedia, already contains causal information <a class="yt-timestamp" data-t="00:23:13">[00:23:13]</a>. When humans perform experiments and gain causal insights, they document this knowledge in textual representations <a class="yt-timestamp" data-t="00:24:43">[00:24:43]</a>. Therefore, [[large_language_models_and_causality | LLMs]] are inherently trained on this pre-existing causal knowledge <a class="yt-timestamp" data-t="00:24:43">[00:24:43]</a>.

### Meta Structural Causal Models (Meta SCMs)

A proposed formalism, Meta SCMs, helps conceptualize how [[large_language_models_and_causality | LLMs]] might learn causal facts <a class="yt-timestamp" data-t="00:23:48">[00:23:48]</a>. In this framework, a structural causal model (SCM) is a very general formalism used to capture relationships, including those on Pearl's ladder of causation (specifically rungs two and three, which involve interventions and counterfactuals) <a class="yt-timestamp" data-t="00:24:07">[00:24:07]</a>.

A Meta SCM operates at a higher hierarchical level, where its variables can represent statements or textual representations of causal insights and assumptions about other SCMs <a class="yt-timestamp" data-t="00:25:08">[00:25:08]</a>. For instance, an SCM might describe the causal link from altitude (variable A) to temperature (variable T) <a class="yt-timestamp" data-t="00:25:40">[00:25:40]</a>. A Meta SCM would then talk about the insight or assumption that "altitude causes temperature" <a class="yt-timestamp" data-t="00:26:08">[00:26:08]</a>. This means the L1 (observational) level of the Meta SCM could be connected to the L2 (interventional) level of a regular SCM, illustrating how LLMs can train on correlations of these causal facts found in training data <a class="yt-timestamp" data-t="00:26:32">[00:26:32]</a>.

## Performance and Limitations

Recent [[systematic_evaluation_of_language_models_in_causality | evaluations]] have shown that models like GPT-4 perform remarkably well in answering causal queries compared to previous generations like GPT-3 <a class="yt-timestamp" data-t="00:20:12">[00:20:12]</a>. Some suggest this improvement might be due to the inclusion of research papers containing causal information in their training data <a class="yt-timestamp" data-t="00:20:17">[00:20:17]</a>.

However, caution is advised when interpreting these results. For example, performance on datasets like the TÃ¼bingen dataset, which consists of bivariate pairs (e.g., X causes Y or Y causes X) <a class="yt-timestamp" data-t="00:31:31">[00:31:31]</a>, may not fully reflect true causal understanding. While some pairs like altitude and temperature are clear, others are highly obscure <a class="yt-timestamp" data-t="00:31:51">[00:31:51]</a>. Basing conclusions solely on simple metrics like accuracy on such datasets might be misleading <a class="yt-timestamp" data-t="00:32:50">[00:32:50]</a>. The importance of data quality and proper pre-processing in machine learning applies equally to [[causality and large language models | LLMs]] <a class="yt-timestamp" data-t="00:33:33">[00:33:33]</a>.

## Scaling Laws and Intelligence

The discussion around [[Large Language Models and Intelligence | LLMs]] often involves "scaling laws," which posit that increasing model size and data leads to improved capabilities <a class="yt-timestamp" data-t="00:34:29">[00:34:29]</a>. This is sometimes compared to the brain's complexity, with its billions of neurons and thousands of connections per neuron <a class="yt-timestamp" data-t="00:34:43">[00:34:43]</a>. The "scale is all you need" hypothesis suggests that simply scaling up computational resources might be sufficient for advanced AI <a class="yt-timestamp" data-t="00:35:32">[00:35:32]</a>.

However, a counter-argument, influenced by the symbolic aspects of causality, suggests that ingenuity and conceptual development are still needed <a class="yt-timestamp" data-t="00:36:09">[00:36:09]</a>. The belief is that the future of [[causality and large language models | AI]], particularly in [[causal reasoning in language models | causal reasoning]], will be a combination of both scaling and sophisticated conceptual frameworks, akin to "neuro-symbolic AI" <a class="yt-timestamp" data-t="00:36:15">[00:36:15]</a>.

## Transparency (White Box vs. Black Box)

The question of whether causal models need to be "white box" (transparent and understandable) or if "black box" models are acceptable is a deep philosophical one <a class="yt-timestamp" data-t="00:37:32">[00:37:32]</a>. While humans are generally good at causal reasoning in personal experiences (e.g., "my bike got stolen because I left it out at night"), they are not necessarily white-box reasoners themselves and can construct plausible but inaccurate causal explanations <a class="yt-timestamp" data-t="00:38:01">[00:38:01]</a>.

The desire for white-box models often stems from the expectation that they will be understandable and explainable <a class="yt-timestamp" data-t="00:41:47">[00:41:47]</a>. However, even explicitly defined "white box" systems, like large linear programs, can become uninterpretable by humans when scaled up <a class="yt-timestamp" data-t="00:43:01">[00:43:01]</a>. This suggests that explainability does not automatically follow from white-box nature <a class="yt-timestamp" data-t="00:44:19">[00:44:19]</a>. Some argue that black-box models can be acceptable as long as they provide faithful and useful explanations <a class="yt-timestamp" data-t="00:44:51">[00:44:51]</a>.

Research in this area includes developing algorithms like "structural causal explanations," which use graph structures and quantitative knowledge from causal models to provide explanations for specific outcomes <a class="yt-timestamp" data-t="00:45:17">[00:45:17]</a>. This allows for individualistic causal explanations, such as why a particular patient's mobility is poor based on factors like age and health <a class="yt-timestamp" data-t="00:48:48">[00:48:48]</a>.

## Adoption and Future Outlook

The adoption of causality in industry is progressing rapidly <a class="yt-timestamp" data-t="00:49:34">[00:49:34]</a>. While there are still many unresolved scientific and philosophical questions, progress is tremendous <a class="yt-timestamp" data-t="00:50:00">[00:50:00]</a>. A key challenge is the lack of universal benchmarks or "gold standards" for measuring causal AI, as true causal knowledge often comes from physical experiments and established laws <a class="yt-timestamp" data-t="00:51:09">[00:51:09]</a>.

Despite these challenges, causal inference is increasingly successful in practical applications, such as biomedical data analysis, where large, expert-validated causal graphs reveal surprising insights <a class="yt-timestamp" data-t="00:52:39">[00:52:39]</a>. The future envisions AI as an assistant, helping scientists make new discoveries, similar to how AI has aided in mathematical theorem proving <a class="yt-timestamp" data-t="00:53:07">[00:53:07]</a>. The integration of geometric deep learning and graph neural networks with structural causal models represents a "bridge" between hot fields in deep learning and causality, opening new research directions <a class="yt-timestamp" data-t="00:55:40">[00:55:40]</a>.

While machine learning often experiences "hype cycles" followed by "AI winters" <a class="yt-timestamp" data-t="00:57:09">[00:57:09]</a>, the current surge in AI, particularly with [[Large Language Models and Intelligence | LLMs]], seems to be continuously increasing <a class="yt-timestamp" data-t="00:58:55">[00:58:55]</a>. The scientific community should engage in open discourse and debate, rather than distancing itself from the topic of AI <a class="yt-timestamp" data-t="00:59:46">[00:59:46]</a>. Formalism in causality is considered necessary, serving as a mathematical language to discuss modeling assumptions and derive sound conclusions <a class="yt-timestamp" data-t="01:08:41">[01:08:41]</a>.