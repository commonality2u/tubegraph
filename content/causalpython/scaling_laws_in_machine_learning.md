---
title: Scaling Laws in Machine Learning
videoId: rM25vt_ZmFc
---

From: [[causalpython]] <br/> 

## Introduction to Scaling Laws
[[Large Language Models and Intelligence | Large Language Models]] (LLMs) have demonstrated impressive performance, particularly models like GPT-4, which show significant improvement in answering causal queries compared to earlier versions like GPT-3 <a class="yt-timestamp" data-t="00:20:08">[00:20:08]</a>. This phenomenon is often discussed in the context of "scaling laws" or the "scaling hypothesis" in machine learning <a class="yt-timestamp" data-t="00:19:08">[00:19:08]</a>.

## The Scaling Hypothesis
The scaling hypothesis suggests that simply increasing the size of models, the amount of data, and computational resources (e.g., GPUs) will lead to increased intelligence and performance <a class="yt-timestamp" data-t="00:35:32">[00:35:32]</a>, as encapsulated in the "GPU go" meme or the "Bitter Lesson" by Rich Sutton <a class="yt-timestamp" data-t="00:35:37">[00:35:37]</a>.

### Neuroscience Connection
The idea of scaling is partly rooted in neuroscientific observations. The human brain, for instance, contains approximately 100 billion neurons, each with an average of about 1,000 connections to other neurons <a class="yt-timestamp" data-t="00:35:01">[00:35:01]</a>. This high degree of connectivity, rather than just neuron count, is considered a crucial factor in intelligence, as some other mammals might have more neurons but are not considered more intelligent than humans <a class="yt-timestamp" data-t="00:35:09">[00:35:09]</a>. The neocortex, the outer layer of the human brain, is twisted, creating more "highways" for complex connections <a class="yt-timestamp" data-t="00:36:24">[00:36:24]</a>. The success observed in deep learning models due to increased scale further encourages this direction <a class="yt-timestamp" data-t="00:36:38">[00:36:38]</a>.

## Perspectives on Scaling Laws
Matej's personal belief is that achieving advanced intelligence in machine learning will require a *combination* of scaling and conceptual development or ingenuity <a class="yt-timestamp" data-t="00:36:12">[00:36:12]</a>. While scale is certainly necessary, as evidenced by human biology, it may not be sufficient on its own <a class="yt-timestamp" data-t="00:36:17">[00:36:17]</a>. This perspective aligns with the principles of neuro-symbolic AI, which seeks to integrate both neural networks (connectionist, data-driven approaches) and symbolic reasoning (logic-based, knowledge-driven approaches) <a class="yt-timestamp" data-t="00:36:47">[00:36:47]</a>.

## [[Large Language Models and Intelligence | LLMs]] and [[Causality and Machine Learning | Causality]]
From a [[Causality and Machine Learning | causal]] perspective, the primary question regarding [[Large Language Models and Intelligence | LLMs]] is whether they genuinely learn [[Causality and Machine Learning | causal]] relationships <a class="yt-timestamp" data-t="00:19:37">[00:19:37]</a>. While [[Large Language Models and Intelligence | LLMs]] can perform well on [[Causality and Machine Learning | causal]] queries, this could be attributed to them learning correlations of causal facts present in their training data <a class="yt-timestamp" data-t="00:21:12">[00:21:12]</a>. Knowledge, including [[Causality and Machine Learning | causal]] knowledge, is extensively recorded in textual forms (e.g., textbooks, Wikipedia), and [[Large Language Models and Intelligence | LLMs]], by learning to predict the next best word, can effectively reproduce this knowledge <a class="yt-timestamp" data-t="00:23:12">[00:23:12]</a> <a class="yt-timestamp" data-t="00:23:33">[00:23:33]</a>.

Matej suggests that the behavioral aspect—whether knowledge was learned through personal experiment or from a text—becomes indistinguishable in the model's output <a class="yt-timestamp" data-t="00:28:32">[00:28:32]</a>. This raises a philosophical question about understanding versus knowing <a class="yt-timestamp" data-t="00:24:51">[00:24:51]</a>.

### Benchmarking and Evaluation
Matej expresses skepticism about certain benchmarks used to evaluate the [[Causality and Machine Learning | causal]] capabilities of [[Large Language Models and Intelligence | LLMs]], particularly the Tubingen data set <a class="yt-timestamp" data-t="00:30:30">[00:30:30]</a>. He argues that the data set often contains very obscure pairs of variables or references to specific names (e.g., John or Mary in the earthquake diagram example), which an LLM cannot possibly know without external context <a class="yt-timestamp" data-t="00:32:07">[00:32:07]</a> <a class="yt-timestamp" data-t="00:32:52">[00:32:52]</a>. Therefore, high accuracy on such data sets might not truly indicate [[Causality and Machine Learning | causal]] reasoning <a class="yt-timestamp" data-t="00:33:11">[00:33:11]</a>. He emphasizes the critical importance of carefully considering the data on which models are evaluated <a class="yt-timestamp" data-t="00:33:32">[00:33:32]</a>, a common concern in [[Developing Effective Machine Learning Models | applied machine learning]] related to data preprocessing and cleaning <a class="yt-timestamp" data-t="00:33:40">[00:33:40]</a>.

### Hype Cycles in Machine Learning
The field of machine learning experiences "hype cycles," often characterized by early dramatic success followed by unexpected difficulties <a class="yt-timestamp" data-t="00:56:55">[00:56:55]</a>. Historically, "AI winters" have occurred approximately every 30 years, where enthusiasm wanes after initial breakthroughs fail to deliver broader results (e.g., early perceptrons, symbolic AI) <a class="yt-timestamp" data-t="00:57:34">[00:57:34]</a>. However, Matej notes that the current momentum in AI feels different and appears to be continuously increasing <a class="yt-timestamp" data-t="00:58:55">[00:58:55]</a>.

There is a debate about whether to embrace or temper down the "buzz" around AI <a class="yt-timestamp" data-t="00:59:00">[00:59:00]</a>. While some argue that any publicity is good, others advocate for scientific values like factual correctness and reproducibility, suggesting a more cautious approach <a class="yt-timestamp" data-t="00:59:09">[00:59:09]</a>. Matej believes that disengaging from the public discourse around AI, as some departments consider doing, is "wrong" because it hinders scientific debate <a class="yt-timestamp" data-t="00:59:46">[00:59:46]</a>.