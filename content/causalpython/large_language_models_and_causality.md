---
title: Large language models and causality
videoId: sljBU_HFnFs
---

From: [[causalpython]] <br/> 

This article summarizes research presented at the Triple AI conference in Vancouver, Canada, including insights from a workshop on [[workshop_on_large_language_models_and_causality | causality and large language models]] <a class="yt-timestamp" data-t="00:00:08">[00:00:08]</a>.

## Adaptive Causal Discovery Algorithms
Usman, a PhD student at CISPA Helmholtz Center for Information Security, Germany, presented work focused on developing practical causal discovery algorithms applicable in real-world scenarios <a class="yt-timestamp" data-t="00:00:21">[00:00:21]</a>, <a class="yt-timestamp" data-t="00:00:38">[00:00:38]</a>.

### Motivation and Approach
The primary motivation is to enable causal discovery algorithms to adapt to data as it arrives, rather than requiring a fully specified dataset from the beginning <a class="yt-timestamp" data-t="00:00:48">[00:00:48]</a>. This approach allows algorithms to learn as new data streams in <a class="yt-timestamp" data-t="00:01:01">[00:01:01]</a>. The underlying concept is "learning by compression" <a class="yt-timestamp" data-t="00:01:13">[00:01:13]</a>. This compression-based strategy can be used even when causal relationships change over time, allowing for the separation of different types of structures <a class="yt-timestamp" data-t="00:01:23">[00:01:23]</a>, <a class="yt-timestamp" data-t="00:01:27">[00:01:27]</a>.

### Challenges and Practical Considerations
A key challenge is distinguishing differences in data, especially when data is just starting to arrive or if each incoming data point originates from a different dynamical network <a class="yt-timestamp" data-t="00:01:46">[00:01:46]</a>, <a class="yt-timestamp" data-t="00:01:58">[00:01:58]</a>. Poor initial distinctions can negatively impact later results <a class="yt-timestamp" data-t="00:02:06">[00:02:06]</a>.

### Real-World Impact
This work could be applied in domains like healthcare for disease diagnosis, where initial data might be insufficient <a class="yt-timestamp" data-t="00:02:34">[00:02:34]</a>. The algorithms would adapt and refine their knowledge as more data becomes available, rather than starting from scratch <a class="yt-timestamp" data-t="00:02:48">[00:02:48]</a>.

### Computational Requirements
The algorithm relies on score-based causal discovery, which in the worst case can still incur exponential computational costs <a class="yt-timestamp" data-t="00:03:20">[00:03:20]</a>, <a class="yt-timestamp" data-t="00:03:23">[00:03:23]</a>. However, in practice, a greedy search approach helps mitigate this by building or discovering networks incrementally, rather than exhaustively <a class="yt-timestamp" data-t="00:03:31">[00:03:31]</a>, <a class="yt-timestamp" data-t="00:03:37">[00:03:37]</a>. This greedy approach has scaled to 500 variables in sparse networks for earlier algorithms, provided the evaluation score aligns with data assumptions <a class="yt-timestamp" data-t="00:03:53">[00:03:53]</a>, <a class="yt-timestamp" data-t="00:04:05">[00:04:05]</a>.

The main functional assumption for causal relationships is nonlinear functions with additive Gaussian noise, considered less restrictive than assuming linearity <a class="yt-timestamp" data-t="00:04:21">[00:04:21]</a>, <a class="yt-timestamp" data-t="00:04:28">[00:04:28]</a>.

## [[Large Language Models and Causal Reasoning | Large Language Models]] and [[Causality | Causality]]
Emily McMillan, an independent researcher and research scientist, presented work at the intersection of [[Causality and Large Language Models | LLMs and causality]] <a class="yt-timestamp" data-t="00:04:36">[00:04:36]</a>, <a class="yt-timestamp" data-t="00:04:39">[00:04:39]</a>.

### Motivation
The motivation stems from the hypothesis that while [[Large Language Models and Causal Reasoning | LLMs]] are trained on vast amounts of data (seemingly the "whole world's data"), it is ultimately a subsampled representation of the real world <a class="yt-timestamp" data-t="00:04:50">[00:04:50]</a>, <a class="yt-timestamp" data-t="00:04:57">[00:04:57]</a>. This sampling introduces the potential for sample selection bias, which Emily explores in different areas of interest <a class="yt-timestamp" data-t="00:05:07">[00:05:07]</a>.

### Main Insights
Key insights include the importance of considering the data generating process and avoiding the assumption that a dataset is independent and identically distributed (IID), regardless of its size <a class="yt-timestamp" data-t="00:05:17">[00:05:17]</a>, <a class="yt-timestamp" data-t="00:05:21">[00:05:21]</a>. There are many parallels between [[Large Language Models and Causal Reasoning | LLMs]] and the conditional probabilities that can be hypothesized with a causal directed acyclic graph (DAG) <a class="yt-timestamp" data-t="00:05:25">[00:05:25]</a>, <a class="yt-timestamp" data-t="00:05:31">[00:05:31]</a>. Researchers are encouraged to examine the log probabilities directly from the [[Large Language Models and Causal Reasoning | LLM]] to gain deeper insights beyond just the tokens <a class="yt-timestamp" data-t="00:05:45">[00:05:45]</a>, <a class="yt-timestamp" data-t="00:05:58">[00:05:58]</a>.

### Real-World Impact
Emily hopes her work can address the problem of missing data and sample selection bias in [[Large Language Models and Causal Reasoning | LLMs]] <a class="yt-timestamp" data-t="00:06:37">[00:06:37]</a>. It suggests that mundane or "obvious" information, often omitted from datasets because "no one's taking the time to write down what is obvious," might be precisely the common sense reasoning that [[Large Language Models and Causal Reasoning | language models]] are currently lacking <a class="yt-timestamp" data-t="00:06:47">[00:06:47]</a>, <a class="yt-timestamp" data-t="00:06:56">[00:06:56]</a>.

### Recommended Paper
Emily found the "Causal Parrots" work particularly satisfying <a class="yt-timestamp" data-t="00:07:07">[00:07:07]</a>. This 2020 NeurIPS paper explored mediation analysis within [[Large Language Models and Causal Reasoning | LLMs]], freezing some weights and drawing parallels between a causal DAG and the Transformer model to validate hypotheses <a class="yt-timestamp" data-t="00:07:18">[00:07:18]</a>, <a class="yt-timestamp" data-t="00:07:23">[00:07:23]</a>.

## Counterfactuals in Policy Making
Scott Muer, affiliated with UCLA and a PhD advisor to Judea Pearl, discussed a paper on incentivizing electric vehicle (EV) procurement <a class="yt-timestamp" data-t="00:07:37">[00:07:37]</a>, <a class="yt-timestamp" data-t="00:07:41">[00:07:41]</a>, <a class="yt-timestamp" data-t="00:08:01">[00:08:01]</a>.

### Motivation and Problem
Governments incentivize EV purchases for environmental reasons, but EVs are not always better than internal combustion engine (ICE) vehicles <a class="yt-timestamp" data-t="00:08:11">[00:08:11]</a>, <a class="yt-timestamp" data-t="00:08:23">[00:08:23]</a>. Their manufacturing greenhouse gas emissions are higher <a class="yt-timestamp" data-t="00:08:34">[00:08:34]</a>. If an incentivized EV is not driven much (e.g., as a complementary vehicle), it can have a negative environmental impact <a class="yt-timestamp" data-t="00:08:45">[00:08:45]</a>, <a class="yt-timestamp" data-t="00:08:59">[00:08:59]</a>. The core problem is how governments can incentivize the "right people" <a class="yt-timestamp" data-t="00:09:06">[00:09:06]</a>, <a class="yt-timestamp" data-t="00:09:08">[00:09:08]</a>.

### Counterfactual Nature
This problem is inherently counterfactual, involving unit selection and probabilities of necessity and sufficiency <a class="yt-timestamp" data-t="00:09:11">[00:09:11]</a>, <a class="yt-timestamp" data-t="00:09:15">[00:09:15]</a>. Households might respond negatively to incentives if they come from a disfavored political party, potentially cutting them off from EV adoption for a long time <a class="yt-timestamp" data-t="00:09:35">[00:09:35]</a>, <a class="yt-timestamp" data-t="00:09:56">[00:09:56]</a>. Conversely, an incentive could lead to a purchase and high usage, where without the incentive, the vehicle would not have been bought or driven much <a class="yt-timestamp" data-t="00:10:07">[00:10:07]</a>, <a class="yt-timestamp" data-t="00:10:14">[00:10:14]</a>. This is the notion of a "probability of benefit" <a class="yt-timestamp" data-t="00:10:19">[00:10:19]</a>.

The work incorporates weighted response types; for example, a harmful outcome (incentive leads to no purchase, but they would have bought on their own without it) might have a heavy negative weight (-10), while a beneficial outcome (incentive works perfectly) has a positive weight (2) <a class="yt-timestamp" data-t="00:10:25">[00:10:25]</a>, <a class="yt-timestamp" data-t="00:10:46">[00:10:46]</a>, <a class="yt-timestamp" data-t="00:10:54">[00:10:54]</a>.

### Main Insights and Impact
The preferences for these weights can drastically change conclusions about which groups to incentivize and what types of incentives to give <a class="yt-timestamp" data-t="00:11:11">[00:11:11]</a>, <a class="yt-timestamp" data-t="00:11:21">[00:11:21]</a>. The hope is that this work will help governments and policymakers make better, more environmentally beneficial decisions <a class="yt-timestamp" data-t="00:11:53">[00:11:53]</a>, <a class="yt-timestamp" data-t="00:11:58">[00:11:58]</a>.

### Recommended Paper
Scott mentioned a paper by Yu Chin and Adon Darwich on causal Bayesian networks and structural causal models <a class="yt-timestamp" data-t="00:12:05">[00:12:05]</a>, <a class="yt-timestamp" data-t="00:12:10">[00:12:10]</a>. This work shows that if variables are deterministic, it's possible to obtain point estimates and identify probabilities of causation or factual probabilities even without knowing the exact formulas or conditional probability tables for those deterministic variables <a class="yt-timestamp" data-t="00:12:15">[00:12:15]</a>, <a class="yt-timestamp" data-t="00:12:20">[00:12:20]</a>, <a class="yt-timestamp" data-t="00:12:30">[00:12:30]</a>.

## Understanding [[Causality and large language models | Causality in Large Language Models]]
Andrew L. (Google DeepMind) presented research on understanding what [[Causality and large language models | language models]] can learn about [[Causality | causality]] from passive training <a class="yt-timestamp" data-t="00:12:43">[00:12:43]</a>, <a class="yt-timestamp" data-t="00:12:50">[00:12:50]</a>.

### Motivation and Resolution of a Puzzle
The work aims to resolve a puzzle in the literature: systems are believed to be unable to learn about causal structures from observational data, yet [[Causality and large language models | language models]] demonstrate interesting causal-seeming tasks <a class="yt-timestamp" data-t="00:12:54">[00:12:54]</a>, <a class="yt-timestamp" data-t="00:13:03">[00:13:03]</a>. The resolution proposed is a distinction between observational data (from which [[Causality | causality]] cannot be learned) and passive data that *might contain interventions* <a class="yt-timestamp" data-t="00:13:11">[00:13:11]</a>, <a class="yt-timestamp" data-t="00:13:16">[00:13:16]</a>. They argue that internet data, which [[Causality and large language models | language models]] are trained on, is of the latter form <a class="yt-timestamp" data-t="00:13:20">[00:13:20]</a>. This means systems trained on such data *can* learn generalizable causal strategies and other forms of causal reasoning <a class="yt-timestamp" data-t="00:13:23">[00:13:23]</a>, <a class="yt-timestamp" data-t="00:13:26">[00:13:26]</a>.

### Main Insights
There are two key insights <a class="yt-timestamp" data-t="00:13:32">[00:13:32]</a>:
1.  **Distinction between Data Dimensions**: Understanding the difference between "observational" vs. "interventional" data and "passive" vs. "active" data, and how [[Causality and large language models | language models]] fit into this space due to the nature of internet text <a class="yt-timestamp" data-t="00:13:34">[00:13:34]</a>, <a class="yt-timestamp" data-t="00:13:40">[00:13:40]</a>.
2.  **Learning Strategies from Passive Data**: It is possible for systems to learn strategies that enable them to experiment and generalize in new situations purely from passive training data <a class="yt-timestamp" data-t="00:13:47">[00:13:47]</a>, <a class="yt-timestamp" data-t="00:13:53">[00:13:53]</a>. This has implications not just for [[Causality and large language models | causality and language models]] but also for the philosophy of agency <a class="yt-timestamp" data-t="00:13:58">[00:13:58]</a>, <a class="yt-timestamp" data-t="00:14:01">[00:14:01]</a>.

### Real-World Impact
The direct real-world impact might be limited to understanding what [[Causality and large language models | language models]] can and cannot do <a class="yt-timestamp" data-t="00:14:06">[00:14:06]</a>. However, it is hoped to be a fundamental scientific contribution to understanding the nature of learning [[Causality | causality]] from different data types <a class="yt-timestamp" data-t="00:14:14">[00:14:14]</a>, <a class="yt-timestamp" data-t="00:14:18">[00:14:18]</a>. This understanding could lead to building more robust learning systems and better generalization capabilities in the future <a class="yt-timestamp" data-t="00:14:23">[00:14:23]</a>, <a class="yt-timestamp" data-t="00:14:27">[00:14:27]</a>.

### Recommended Paper
Andrew found recent [[Causality | causal]] papers studying human world models to be very interesting <a class="yt-timestamp" data-t="00:14:35">[00:14:35]</a>, <a class="yt-timestamp" data-t="00:14:38">[00:14:38]</a>. Specifically, he highlighted work that examines how humans fail to fully capture certain situations in their mental simulations, sometimes struggling to discriminate between physically possible and impossible scenarios, suggesting imperfect abstractions in our own world models <a class="yt-timestamp" data-t="00:14:44">[00:14:44]</a>, <a class="yt-timestamp" data-t="00:14:50">[00:14:50]</a>, <a class="yt-timestamp" data-t="00:14:57">[00:14:57]</a>. He recommended looking for work by cognitive scientist Todd G. at NYU <a class="yt-timestamp" data-t="00:15:08">[00:15:08]</a>.

## Workshop on [[Causality and Large Language Models | Causality and Large Language Models]] Feedback

### Positive Reception
Attendees Ali (Huawei) and Abdur Rahman Zed (Mila Institute of Artificial Intelligence) praised the workshop on [[Causality and large language models | causality and large language models]], calling it "amazing and interesting" and one of the best they've attended <a class="yt-timestamp" data-t="00:15:17">[00:15:17]</a>, <a class="yt-timestamp" data-t="00:15:21">[00:15:21]</a>, <a class="yt-timestamp" data-t="00:15:30">[00:15:30]</a>, <a class="yt-timestamp" data-t="00:15:40">[00:15:40]</a>.

### Discussion on [[Causality in large language models | LLMs and Causal Relations]]
A key discussion point was whether [[Causality in large language models | large language models can learn causal relations]] <a class="yt-timestamp" data-t="00:15:47">[00:15:47]</a>. Both Ali and Abdur Rahman appreciated hearing different viewpoints and claims for and against this idea <a class="yt-timestamp" data-t="00:15:52">[00:15:52]</a>, <a class="yt-timestamp" data-t="00:16:47">[00:16:47]</a>. A notable perspective suggested it might be "too early to decide" given that large-scale [[Large Language Models and Causal Reasoning | language models]] have only been around for about five years <a class="yt-timestamp" data-t="00:16:14">[00:16:14]</a>, <a class="yt-timestamp" data-t="00:16:24">[00:16:24]</a>.

### Judea Pearl's Insights
Judea Pearl, "the Godfather of [[Causality | causal inference]]," gave a pleasureable talk that connected his work, including his book "The Book of Why," to the workshop's theme on [[Causality and large language models | LLMs and causal relationships]] <a class="yt-timestamp" data-t="00:16:52">[00:16:52]</a>, <a class="yt-timestamp" data-t="00:17:01">[00:17:01]</a>, <a class="yt-timestamp" data-t="00:17:08">[00:17:08]</a>.

### Main Insight from the Workshop
The central insight from the workshop, highlighted by Abdur Rahman, was Judea Pearl's nuanced view on [[Causality | causality]] and [[Large Language Models and Causal Reasoning | LLMs]] <a class="yt-timestamp" data-t="00:17:29">[00:17:29]</a>. While a theorem states that one cannot derive causal relationships solely from observations <a class="yt-timestamp" data-t="00:17:33">[00:17:33]</a>, Pearl suggested this might not apply to [[Large Language Models and Causal Reasoning | LLMs]] <a class="yt-timestamp" data-t="00:17:45">[00:17:45]</a>. This is because [[Large Language Models and Causal Reasoning | LLMs]] have access to an immense amount of data, including instances of *interventions* <a class="yt-timestamp" data-t="00:17:47">[00:17:47]</a>, <a class="yt-timestamp" data-t="00:17:53">[00:17:53]</a>. Unlike humans who might read a few books, [[Large Language Models and Causal Reasoning | LLMs]] are trained on the entire internet <a class="yt-timestamp" data-t="00:18:01">[00:18:01]</a>, <a class="yt-timestamp" data-t="00:18:32">[00:18:32]</a>. This extensive data access means the traditional theorem "might be proven wrong," and [[Large Language Models and Causal Reasoning | LLMs]] might indeed possess some causal understanding <a class="yt-timestamp" data-t="00:18:08">[00:18:08]</a>, <a class="yt-timestamp" data-t="00:18:13">[00:18:13]</a>. This tailored perspective for [[Large Language Models and Causal Reasoning | LLMs]] was particularly appreciated <a class="yt-timestamp" data-t="00:18:35">[00:18:35]</a>.

### Current Work
Abdur Rahman Zed's current work focuses on fairness in [[Large Language Models and Causal Reasoning | language models]], including understanding why they learn biases, how they can exacerbate societal biases, and methods for mitigating and measuring these biases (e.g., gender, race, religion) <a class="yt-timestamp" data-t="00:18:51">[00:18:51]</a>, <a class="yt-timestamp" data-t="00:19:00">[00:19:00]</a>, <a class="yt-timestamp" data-t="00:19:07">[00:19:07]</a>.

### Recommended Paper
He recommended a recent paper he co-authored on pruning [[Large Language Models and Causal Reasoning | large language models]] <a class="yt-timestamp" data-t="00:19:20">[00:19:20]</a>. The paper argues that specific parts within a [[Large Language Models and Causal Reasoning | language model]] are responsible for bias, and identifying and removing them can make the model less biased <a class="yt-timestamp" data-t="00:19:24">[00:19:24]</a>, <a class="yt-timestamp" data-t="00:19:30">[00:19:30]</a>.