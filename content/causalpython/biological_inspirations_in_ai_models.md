---
title: Biological inspirations in AI models
videoId: ubSFglvhBd0
---

From: [[causalpython]] <br/> 

Bernhard Schölkopf, a leading researcher in machine learning and causality, highlights the profound influence of biological systems on the development and understanding of advanced artificial intelligence (AI). His work often draws parallels between natural intelligence and the challenges faced in designing more capable AI models <a class="yt-timestamp" data-t="00:09:02">[00:09:02]</a>.

## The Metaphor of "Acting in an Imagined Space"

Schölkopf frequently references Konrad Lorenz, one of the fathers of ethology (the study of animal behavior). Lorenz's assertion that "thinking is nothing but acting in an imagined space" <a class="yt-timestamp" data-t="00:00:08">[00:00:08]</a> serves as a core metaphor for understanding [[AI and humanlike intelligence | intelligence]] <a class="yt-timestamp" data-t="00:09:32">[00:09:32]</a>.

Current generative AI and machine learning models are largely based on statistical representations, identifying correlations and patterns in data <a class="yt-timestamp" data-t="00:09:52">[00:09:52]</a>. While impressive, these systems primarily perform large-scale pattern recognition <a class="yt-timestamp" data-t="00:10:18">[00:10:18]</a>. To move towards thinking in an "imagined space," representations must become *interventional*, allowing the AI to act within its models and simulate consequences <a class="yt-timestamp" data-t="00:10:27">[00:10:27]</a>. This necessitates the incorporation of causality into the representations, enabling not just correlations but a notion of intervention and action <a class="yt-timestamp" data-t="00:10:40">[00:10:40]</a>.

Biological systems offer insights into this, possessing internal models that simulate the world and actions within it <a class="yt-timestamp" data-t="00:11:37">[00:11:37]</a>. This includes "efference copies," which are copies of actions produced by the brain to affect the external world <a class="yt-timestamp" data-t="00:11:18">[00:11:18]</a>.

## The Efficiency of Biological Systems

A critical difference between biological systems and many modern AI models lies in their resource constraints:
*   **Finite Training Data:** Unlike large language models that train on vast datasets like the entire internet, biological systems have finite training data and computational resources <a class="yt-timestamp" data-t="00:13:43">[00:13:43]</a>.
*   **Modular Learning:** Due to these limitations, biological systems learn in a more modular fashion, reusing learned modules across different tasks and changing conditions <a class="yt-timestamp" data-t="00:15:12">[00:15:12]</a>. For example, the human visual system adapts to changes in lighting (color consistency, gain control) without needing to relearn object recognition for every lighting condition <a class="yt-timestamp" data-t="00:14:28">[00:14:28]</a>.
*   **Structural Similarities:** This modularity suggests a fascinating bias: the modules learned by biological systems might structurally correspond to modules or mechanisms in the real world <a class="yt-timestamp" data-t="00:15:41">[00:15:41]</a>.

## Consistency and Internal World Models

The physicist Hermann von Helmholtz discussed a principle where imagining an action on an object in the brain should yield the same result as performing the action in the real world and then observing the transformed object <a class="yt-timestamp" data-t="00:16:49">[00:16:49]</a>. This "commutative diagram" captures a deep consistency in how intelligent systems represent and interact with reality <a class="yt-timestamp" data-t="00:17:38">[00:17:38]</a>.

> "I take an object I look at the object if I then move the object and look at it again I should get the same result as if I first looked closed my eyes and just imagined performing the intervention in my brain" <a class="yt-timestamp" data-t="00:17:15">[00:17:15]</a>

Schölkopf argues that an internal world model allows for learning "without having to risk your life every time" <a class="yt-timestamp" data-t="00:00:30">[00:00:30]</a>, <a class="yt-timestamp" data-t="00:20:56">[00:20:56]</a>. While some tasks become automatic through repetition and may not require explicit world models (e.g., brushing teeth) <a class="yt-timestamp" data-t="00:21:46">[00:21:46]</a>, complex problems benefit from this internal simulation for credit attribution or planning <a class="yt-timestamp" data-t="00:20:47">[00:20:47]</a>.

## The Balance Between Correlational and Causal Learning

[[AI and humanlike intelligence | Human and animal cognition]] utilize both correlational and causal learning. Correlational learning is fast and efficient for many everyday tasks, especially when quick reactions are necessary <a class="yt-timestamp" data-t="00:13:18">[00:13:18]</a>. However, [[Causal inference models and AI workshops | causal models]] become crucial for situations where understanding underlying mechanisms or making interventions is necessary, particularly with finite data <a class="yt-timestamp" data-t="00:12:28">[00:12:28]</a>.

## Physical Mechanisms and Causality

From a physics perspective, causal systems can be described at multiple levels. While machine learning often operates at the level of statistical dependencies, the "gold standard" in physics involves mechanistic understanding via differential equations that allow for simulation and reasoning about interventions <a class="yt-timestamp" data-t="00:23:32">[00:23:32]</a>. [[Trends in causal AI | Structural causal models]] provide an intermediate level, offering simplicity for learning from data without requiring full mechanistic understanding, yet still allowing reasoning about interventions <a class="yt-timestamp" data-t="00:24:07">[00:24:07]</a>.

> "A mechanism is something that from my point of view is a physical mechanism, physical process" <a class="yt-timestamp" data-t="00:25:30">[00:25:30]</a>

The concept of "independent mechanisms" in [[Interdisciplinary Approaches in AI Research | causal AI research]] <a class="yt-timestamp" data-t="00:25:57">[00:25:57]</a> attempts to capture how causal systems are physically realized in the world, further linking AI research to fundamental physical principles <a class="yt-timestamp" data-t="00:26:09">[00:26:09]</a>.

## Future Directions: Biological Inspiration for Generative AI

Schölkopf emphasizes the need for the [[Generative AI and Causal Models | causality community]] to engage with [[Generative AI and Causal Models | generative modeling]], particularly controllable generation <a class="yt-timestamp" data-t="00:32:30">[00:32:30]</a>. Many researchers in this area may not even realize the inherent connection to causality <a class="yt-timestamp" data-t="00:32:41">[00:32:41]</a>. He encourages researchers to "get our hands dirty" with training high-performance [[Generative AI and Causal Models | generative models]] using neural networks, but in a way that deeply connects to causality <a class="yt-timestamp" data-t="00:32:51">[00:32:51]</a>. This approach, heavily inspired by how biological systems learn and act, is seen as a key pathway for advancing the field.