---
title: language models and reasoning
videoId: relI7Q9A03g
---

From: [[causalpython]] <br/> 

Dr. Andrew Lampinen, a Senior Research Scientist at Google DeepMind, discusses how [[Large Language Models and Causal Reasoning | large language models]] (LLMs) engage in reasoning, particularly in the context of [[causality and large language models | causality]] and generalization. He emphasizes the distinction between how models are trained and the nature of the data they learn from <a class="yt-timestamp" data-t="02:06:00">[02:06:00]</a>.

## Passive vs. Active Strategies in Language Models

While [[large_language_models_and_causal_reasoning | language models]] are trained passively, meaning they process language data generated by others, this data is not purely observational <a class="yt-timestamp" data-t="02:12:00">[02:12:00]</a>. For instance, scientific papers, debugging posts on Stack Overflow, and everyday conversations all contain elements of intervention <a class="yt-timestamp" data-t="02:24:00">[02:24:00]</a>. Each statement in a conversation can be considered an intervention <a class="yt-timestamp" data-t="02:53:00">[02:53:00]</a>. Therefore, even through passive learning, [[causality in large language models | language models]] can absorb and learn from [[causality in large language models | interventional data]] <a class="yt-timestamp" data-t="02:57:00">[02:57:00]</a>.

This learning from interventional data can enable [[causality in large language models | causal strategies]] or understanding that generalize beyond the training data <a class="yt-timestamp" data-t="03:20:00">[03:20:00]</a>. By observing others' interventions, models can discover a strategy for intervening themselves, which they can then apply in new situations to discover new [[causal reasoning and structural causal models | causal structures]] for downstream goals <a class="yt-timestamp" data-t="03:30:00">[03:30:00]</a>.

## Experimental Setup and Insights

To study this, Dr. Lampinen and his team conducted controlled experiments with simpler models trained on a specific data distribution <a class="yt-timestamp" data-t="04:28:00">[04:28:00]</a>. The models observed a series of interventions on causal directed acyclic graphs (DAGs) and then a goal, such as maximizing a variable, along with interventions aimed at achieving that goal <a class="yt-timestamp" data-t="04:41:00">[04:41:00]</a>.

The key question was whether a model passively observing these interventions could generalize to actively intervene and discover new [[causal reasoning and structural causal models | causal structures]] at test time <a class="yt-timestamp" data-t="05:06:00">[05:06:00]</a>. Similar to how an LM becomes active when deployed with a user, the passively trained system was tested interactively <a class="yt-timestamp" data-t="05:27:00">[05:27:00]</a>. The results showed that the system was able to apply the [[causal reasoning in language models | causal strategies]] passively observed in training to actively intervene, discover new [[causal reasoning and structural causal models | causal structures]], and exploit them effectively <a class="yt-timestamp" data-t="05:40:00">[05:40:00]</a>. This performance was significantly closer to correct [[causal reasoning in language models | causal reasoning]] than simpler heuristic or associational baselines <a class="yt-timestamp" data-t="05:52:00">[05:52:00]</a>.

## Relation to "Causal Parrots" Hypothesis

Dr. Lampinen's results suggest that [[causality in large language models | language models]] are capable of discovering a [[causal reasoning in language models | causal reasoning]] algorithm that can be applied in a new, generalizable setting, given a suitable training regime <a class="yt-timestamp" data-t="06:55:00">[06:55:00]</a>. This somewhat contradicts the "Causal Parrots" hypothesis, which suggests that [[large_language_models_and_causal_reasoning | large language models]] can "talk causality" but not "reason causally" <a class="yt-timestamp" data-t="06:40:00">[06:40:40]</a>.

While natural data may contain more correlations and fewer interventions, potentially leading models to learn more correlational strategies, Dr. Lampinen's work indicates that sufficient interventional data in training can enable the discovery of [[causal reasoning in language models | causal strategies]] <a class="yt-timestamp" data-t="07:12:00">[07:12:00]</a>. When given explanations in the prompt, [[large_language_models_and_causal_reasoning | language models]] can effectively learn to experiment and discover new [[causal reasoning and structural causal models | causal structures]] not explicitly present in the prompt <a class="yt-timestamp" data-t="07:49:00">[07:49:00]</a>.

## Importance of Explanations and Training Regimes

Current training paradigms for [[large_language_models_and_causal_reasoning | language models]] are largely driven by what works at scale <a class="yt-timestamp" data-t="08:46:00">[08:46:00]</a>. However, incorporating auxiliary tasks, such as providing natural language explanations of rewards, can significantly improve learning <a class="yt-timestamp" data-t="08:58:00">[08:58:00]</a>. This can even help shape how models generalize out of distribution, even with confounded data <a class="yt-timestamp" data-t="09:27:00">[09:27:00]</a>.

Conditioning models on quality signals (e.g., reward estimates) during training is another emerging approach <a class="yt-timestamp" data-t="10:18:00">[10:18:00]</a>. This allows models to learn from a wide range of data, including lower-quality examples, but be pushed to generate high-quality responses at test time <a class="yt-timestamp" data-t="10:27:00">[10:27:00]</a>.

## Language Models as Agents

Dr. Lampinen views [[large_language_models_and_causal_reasoning | large language models]] as a form of passively trained agents <a class="yt-timestamp" data-t="11:21:00">[11:21:00]</a>. An "agent" is broadly defined as a system that takes inputs (observations) and produces output actions in a sequential decision-making problem <a class="yt-timestamp" data-t="11:27:00">[11:27:00]</a>. This includes [[large_language_models_and_causal_reasoning | language models]] (taking language inputs and producing language outputs), as well as systems for chess, Go, or video games <a class="yt-timestamp" data-t="11:39:00">[11:39:00]</a>.

A key distinction is whether interaction with the environment occurs during training or only at testing time <a class="yt-timestamp" data-t="12:02:00">[12:02:00]</a>. While agents are often trained on observational data (e.g., observing expert players), reinforcement learning (RL) training involves active interaction and rewards <a class="yt-timestamp" data-t="12:14:00">[12:14:00]</a>. This is analogous to how [[large_language_models_and_causal_reasoning | language models]] are first trained on passive human-generated language data, then fine-tuned, and finally subjected to an RL step where a reward model based on human preferences is used to guide their responses <a class="yt-timestamp" data-t="12:38:00">[12:38:00]</a>.

### Error Accumulation in Autoregressive Models

While autoregressive models can accumulate errors over long sequences, state-of-the-art [[large_language_models_and_causal_reasoning | language models]] possess some ability to self-correct within sequences, especially with techniques like Chain of Thought <a class="yt-timestamp" data-t="13:58:00">[13:58:00]</a>. The system's architecture, including its context length and ability to reconsider prior generations, impacts this <a class="yt-timestamp" data-t="14:21:00">[14:21:00]</a>.

Passive learning is less efficient and leads to poorer generalization out of distribution <a class="yt-timestamp" data-t="14:42:00">[14:42:00]</a>. When a system trained passively becomes active and moves off its training data distribution, it can break down <a class="yt-timestamp" data-t="14:58:00">[14:58:00]</a>. Techniques from offline RL, such as DAgger, which incorporate small amounts of interventional data, are crucial for robust systems <a class="yt-timestamp" data-t="15:08:00">[15:08:00]</a>. Applying RL or supervised tuning on real generations from models interacting with users provides this kind of interventional data <a class="yt-timestamp" data-t="15:24:00">[15:24:00]</a>. The exact amount of such data needed for robustness remains an open question <a class="yt-timestamp" data-t="15:40:00">[15:40:00]</a>.

## Connection to Causal Data Fusion

There is a parallel between these ideas and [[causality and large language models | causal inference]] literature, specifically "causal data fusion," which involves mixing observational and interventional data <a class="yt-timestamp" data-t="16:02:00">[16:02:00]</a>. Models can leverage "prior knowledge" from observational sources (e.g., cueing on relevant variables) to guide interventions efficiently, focusing only on the most promising variables to determine [[causal reasoning and structural causal models | causal structure]] <a class="yt-timestamp" data-t="16:42:00">[16:42:00]</a>.

## Usefulness of Sequential Decision-Making Systems

[[large_language_models_and_causal_reasoning | Language models]] are considered useful sequential decision-making systems, for tasks like writing, idea generation, and critiquing thought <a class="yt-timestamp" data-t="17:41:00">[17:41:00]</a>. However, they are not yet useful as autonomous sequential decision-making systems, as they are too unreliable without human oversight <a class="yt-timestamp" data-t="18:03:00">[18:03:00]</a>. Dr. Lampinen is more interested in AI systems as tools to assist humans (e.g., scientists in understanding complex systems or reasoning about problems) rather than fully autonomous agents in deployment <a class="yt-timestamp" data-t="18:19:00">[18:19:00]</a>.

## [[symbolic_reasoning_and_logical_deduction_in_ai | Symbolic Reasoning]] and [[Large Language Models and Intelligence | Intelligence]]

Dr. Lampinen views [[symbolic_reasoning_and_logical_deduction_in_ai | symbolic systems]] and logical reasoning as useful tools for an intelligent system, rather than intelligence in themselves <a class="yt-timestamp" data-t="23:31:00">[23:31:00]</a>. Humans, he argues, are not naturally good at logical or mathematical reasoning without extensive training <a class="yt-timestamp" data-t="23:44:00">[23:44:00]</a>. We build tools like mathematical proving systems to aid in areas where we are not inherently strong <a class="yt-timestamp" data-t="23:54:00">[23:54:00]</a>.

He believes that true [[Large Language Models and Intelligence | intelligence]] tends to manifest as a continuous, fuzzy reasoning system <a class="yt-timestamp" data-t="24:08:00">[24:08:00]</a>. [[symbolic_reasoning_and_logical_deduction_in_ai | Symbolic logic]] serves as a valuable tool for these systems to tackle specific constraint problems <a class="yt-timestamp" data-t="24:14:00">[24:14:00]</a>. The most effective and general approaches will likely be systems where fuzzy, continuous learning systems are in control, using logical systems as tools, similar to how humans do <a class="yt-timestamp" data-t="24:35:00">[24:35:00]</a>.

### Rationality of Artificial Systems

The objective of humans and [[large_language_models_and_causality | language models]] is not necessarily to be perfectly rational reasoners, but rather to be adaptive to the situations they encounter daily <a class="yt-timestamp" data-t="41:18:00">[41:18:00]</a>. Sometimes, being "less rational" in a strict logical sense can lead to better decision-making in common scenarios <a class="yt-timestamp" data-t="41:31:00">[41:31:00]</a>. Humans are adapted to be selectively irrational, which helps them navigate social and other real-world situations <a class="yt-timestamp" data-t="41:40:00">[41:40:00]</a>. Concepts like "bounded rationality" explain why humans, with limited resources, might make seemingly irrational inferences that are, in fact, optimal for accuracy in everyday contexts <a class="yt-timestamp" data-t="41:58:00">[41:58:00]</a>. This suggests that a definition of rationality based solely on formal reasoning might not be the most useful for real-world decision-making <a class="yt-timestamp" data-t="42:33:00">[42:33:00]</a>.

## Robustness and System Design

Attempting to too strongly constrain the reasoning processes of a system, for instance, by forcing it to be a formal logical reasoner, can make the system more fragile <a class="yt-timestamp" data-t="44:14:00">[44:14:00]</a>. If the real world deviates from these rigid assumptions, the system can completely break down <a class="yt-timestamp" data-t="44:30:00">[44:30:00]</a>.

A softer approach involves encouraging systems to represent important information (e.g., through explanation prediction objectives) without overly constraining their internal computations <a class="yt-timestamp" data-t="44:40:00">[44:40:00]</a>. While strong inductive biases can improve performance for well-defined, specific problems, for large-scale, complex problems like [[large_language_models_and_causality | language models]] or agents in rich virtual environments, rigid constraints can make systems brittle <a class="yt-timestamp" data-t="44:59:00">[44:59:00]</a>. This aligns with "The Bitter Lesson" from Rich Sutton, which posits that trying to build in human-conceived solutions tends to work only at small scales and breaks down when systems are scaled up <a class="yt-timestamp" data-t="45:40:00">[45:40:00]</a>. Softer solutions, such as providing explanations or altering data distributions, are often more effective and scalable <a class="yt-timestamp" data-t="46:01:00">[46:01:00]</a>.

## Understanding in Language Models

Understanding, in Dr. Lampinen's view, is a graded concept, not a discrete binary state <a class="yt-timestamp" data-t="29:02:00">[29:02:00]</a>. It refers to the richness of representation and the degree to which a system can generalize <a class="yt-timestamp" data-t="29:13:00">[29:13:00]</a>. For example, different levels of mathematical understanding exist, from an intro calculus student to a researcher <a class="yt-timestamp" data-t="29:22:00">[29:22:00]</a>.

[[large_language_models_and_intelligence | Language models]] do possess some degree of understanding of certain features, even if they cannot grasp perceptual phenomena <a class="yt-timestamp" data-t="29:46:00">[29:46:00]</a>. The fact that they are trained by predicting the next token does not negate their understanding; rather, understanding can be measured by their ability to correctly answer questions about a topic <a class="yt-timestamp" data-t="30:04:00">[30:04:00]</a>. While their understanding may be imperfect, like that of an average human, it is still present <a class="yt-timestamp" data-t="30:20:00">[30:20:00]</a>.

### Improving Performance on Abstract Tasks

To improve [[large_language_models_and_causality | language model]] performance on tasks requiring abstract structural reasoning, such as ARC tasks, a study by Taylor Webb and Keith Holyoak found that later-generation [[large_language_models_and_causal_reasoning | language models]] are increasingly proficient at analogical reasoning problems, which test abstract structure independent of superficial details <a class="yt-timestamp" data-t="31:37:00">[31:37:00]</a>.

Formal reasoning in humans requires years of rigorous education <a class="yt-timestamp" data-t="32:40:00">[32:40:00]</a>. It is possible that [[large_language_models_and_causality | language models]] also need similarly rigorous and interactive training to efficiently learn these formal reasoning strategies <a class="yt-timestamp" data-t="33:03:00">[33:03:00]</a>. Research suggests that both [[large_language_models_and_causality | language models]] and humans perform better on logical problems when the semantic content supports the logical conclusion, and worse when it contradicts it <a class="yt-timestamp" data-t="33:49:00">[33:49:00]</a>. This indicates that, like humans, [[large_language_models_and_causality | language models]] imperfectly use logic unless specifically trained for a strictly formal approach <a class="yt-timestamp" data-t="34:11:00">[34:11:00]</a>.

## Learning in Complex Fields

To learn effectively in complex fields like [[causality and large language models | causality]], reinforcement learning, or theoretical physics, Dr. Lampinen suggests playing around with concepts <a class="yt-timestamp" data-t="35:21:00">[35:21:00]</a>. This involves coding up ideas, experimenting with parameters, and observing how changes impact the system (e.g., how learning differs with more observational vs. [[causality in large language models | interventional data]]) <a class="yt-timestamp" data-t="35:30:00">[35:30:00]</a>. Such hands-on experimentation helps build fundamental intuitions <a class="yt-timestamp" data-t="35:49:00">[35:49:00]</a>.

## Discrepancy Between Lab and Reality

While controlled laboratory settings are crucial for understanding complex phenomena in [[systematic_evaluation_of_language_models_in_causality | language models]] and [[causality and large language models | causality]] research, the real world acts as a "forcing function" to prevent overfitting <a class="yt-timestamp" data-t="46:48:00">[46:48:00]</a>. When algorithms are designed and tested simultaneously in ideal, specific cases, they may not generalize to the diverse and undefined tasks encountered in reality <a class="yt-timestamp" data-t="47:09:00">[47:09:00]</a>. Working with real-world problems, such as processing any natural image or handling any natural language query, is vital for testing the true robustness and applicability of ideas <a class="yt-timestamp" data-t="47:37:00">[47:37:00]</a>.

### Lessons from Psychology

Cognitive psychology offers valuable insights for machine learning, particularly in [[causality and large language models | causality]]-related issues like confounding and distinguishing underlying processes from observations <a class="yt-timestamp" data-t="48:11:00">[48:11:00]</a>. Lessons include:
*   **Experimental Design**: Cognitive psychology emphasizes rigorous experimental design <a class="yt-timestamp" data-t="48:27:00">[48:27:00]</a>.
*   **Rigorous Statistical Analysis**: The field promotes thorough statistical analysis of experiments, a practice often less emphasized in machine learning, where simple numerical comparisons might suffice <a class="yt-timestamp" data-t="48:30:00">[48:30:00]</a>.
*   **Abstract Thinking**: Bridging from specific observations to more abstract models that can explain them is a valuable skill emphasized in computational cognitive science <a class="yt-timestamp" data-t="48:53:00">[48:53:00]</a>.

### Statistical Learning Theory

Statistical learning theory provides a framework but relies on assumptions that may be too strong or not fully understood for modern machine learning systems <a class="yt-timestamp" data-t="49:33:00">[49:33:00]</a>. Recent theoretical work attempts to bridge this gap by exploring implicit inductive biases in model architectures or learning processes (like gradient descent), which can make seemingly overparameterized models effectively less so at early training stages <a class="yt-timestamp" data-t="50:04:00">[50:04:00]</a>. This suggests that the capacity of a system might not be uniformly distributed in practice, a nuance not always captured by naive statistical learning theory <a class="yt-timestamp" data-t="50:36:00">[50:36:00]</a>.