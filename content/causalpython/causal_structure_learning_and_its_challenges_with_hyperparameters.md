---
title: Causal structure learning and its challenges with hyperparameters
videoId: vW2vB6e-Mm8
---

From: [[causalpython]] <br/> 

Causal structure learning, which involves the recovery of causal graphs from observational data, faces significant challenges, particularly concerning the choice and tuning of hyperparameters <a class="yt-timestamp" data-t="00:00:30">[00:00:30]</a>. This field aims to uncover the underlying causal relationships between variables without prior knowledge of the causal graph <a class="yt-timestamp" data-t="00:01:19">[00:01:19]</a>.

## Motivation for Research

Similar to supervised [[machine_learning_and_causal_inference_methodologies | machine learning]] where methods are highly sensitive to parameter choices, [[causal_discovery_and_learning | causal discovery and learning]] algorithms, which often rely on [[machine_learning_and_causal_inference_methodologies | machine learning]] techniques, struggle in real-world applications <a class="yt-timestamp" data-t="00:00:51">[00:00:51]</a> <a class="yt-timestamp" data-t="00:01:03">[00:01:03]</a> <a class="yt-timestamp" data-t="00:01:07">[00:01:07]</a>. A key hypothesis explored is whether these difficulties stem from the selection of hyperparameters <a class="yt-timestamp" data-t="00:01:12">[00:01:12]</a>. A fundamental [[challenges_in_developing_ai_with_causal_understanding | challenge in developing AI with causal understanding]] for structure learning is its unsupervised nature; there is no pre-existing information about the causal graphs, only the dataset <a class="yt-timestamp" data-t="00:01:19">[00:01:19]</a> <a class="yt-timestamp" data-t="00:01:22">[00:01:22]</a>.

## Illustrative Example: Bivariate Case

To demonstrate the impact of hyperparameters, a simple bivariate example with two variables, X and Y, can be used <a class="yt-timestamp" data-t="00:01:38">[00:01:38]</a>. The approach involves fitting two regression functions: X on Y and Y on X <a class="yt-timestamp" data-t="00:01:45">[00:01:45]</a> <a class="yt-timestamp" data-t="00:01:48">[00:01:48]</a>. The [[causal_discovery_and_learning | causal algorithm]] determines the direction by concluding that the regression line with the lower prediction error indicates the correct causal direction <a class="yt-timestamp" data-t="00:02:03">[00:02:03]</a> <a class="yt-timestamp" data-t="00:02:07">[00:02:07]</a>.

However, as the number of regression parameters (a hyperparameter) is changed, the inferred causal direction can also change <a class="yt-timestamp" data-t="00:02:24">[00:02:24]</a> <a class="yt-timestamp" data-t="00:02:26">[00:02:26]</a>. For certain hyperparameter values, the conclusion may be correct, while for others, it may be the opposite and incorrect <a class="yt-timestamp" data-t="00:02:36">[00:02:36]</a> <a class="yt-timestamp" data-t="00:02:41">[00:02:41]</a>. This highlights that hyperparameter choice directly influences the output and can lead to errors <a class="yt-timestamp" data-t="00:02:55">[00:02:55]</a>.

## Research Methodology

A comprehensive study was conducted using various simulated settings and a popular real-world dataset generator <a class="yt-timestamp" data-t="00:03:04">[00:03:04]</a> <a class="yt-timestamp" data-t="00:03:06">[00:03:06]</a> <a class="yt-timestamp" data-t="00:03:12">[00:03:12]</a>. The evaluation focused on three key metrics:
*   Structural Hamming distance <a class="yt-timestamp" data-t="00:03:15">[00:03:15]</a>
*   False positives of edges <a class="yt-timestamp" data-t="00:03:17">[00:03:17]</a>
*   False negatives of edges <a class="yt-timestamp" data-t="00:03:18">[00:03:18]</a>
(Lower values are better for all these metrics) <a class="yt-timestamp" data-t="00:03:21">[00:03:21]</a>.

Since robust [[hyperparameter_tuning_for_causal_machine_learning | hyperparameter tuning for causal machine learning]] methods are still experimental or don't widely exist, the study used four categories of hyperparameter choices for evaluation:
1.  **Oracle:** The best possible hyperparameter values, representing an ideal scenario <a class="yt-timestamp" data-t="00:03:48">[00:03:48]</a> <a class="yt-timestamp" data-t="00:03:52">[00:03:52]</a>.
2.  **Worst Possible:** The hyperparameter values leading to the poorest performance <a class="yt-timestamp" data-t="00:03:56">[00:03:56]</a>.
3.  **Algorithm Defaults:** Default values suggested by algorithm authors or within packages <a class="yt-timestamp" data-t="00:04:01">[00:04:01]</a>.
4.  **Derived Defaults:** Default values derived across all simulations in the study <a class="yt-timestamp" data-t="00:04:07">[00:04:07]</a>.

## Key Findings

The research yielded several significant insights into the [[benefits_and_challenges_of_causal_machine_learning | benefits and challenges of causal machine learning]] related to hyperparameters:

*   **Default Values Performance:** Surprisingly, default hyperparameter values in [[causal_discovery_and_learning | causal structure learning]] algorithms performed very close to the "Oracle" (best possible) cases <a class="yt-timestamp" data-t="00:04:27">[00:04:27]</a> <a class="yt-timestamp" data-t="00:04:30">[00:04:30]</a>. This contrasts sharply with standard [[machine_learning_and_causal_inference_methodologies | machine learning]], where default values often perform poorly <a class="yt-timestamp" data-t="00:04:33">[00:04:33]</a> <a class="yt-timestamp" data-t="00:04:35">[00:04:35]</a>.
*   **Oracle Performance & Room for Improvement:** Even when all algorithms had access to optimal "Oracle" hyperparameters, their performance still differed, indicating that algorithm selection remains crucial regardless of hyperparameter tuning <a class="yt-timestamp" data-t="00:04:41">[00:04:41]</a> <a class="yt-timestamp" data-t="00:04:50">[00:04:50]</a> <a class="yt-timestamp" data-t="00:04:53">[00:04:53]</a>. Furthermore, even with optimal parameters, there is still significant room for improvement across all algorithms <a class="yt-timestamp" data-t="00:05:01">[00:05:01]</a> <a class="yt-timestamp" data-t="00:05:05">[00:05:05]</a>.
*   **Worst-Case Scenarios and Safety:** The extent to which methods can perform poorly in worst-case hyperparameter scenarios varies <a class="yt-timestamp" data-t="00:05:14">[00:05:14]</a> <a class="yt-timestamp" data-t="00:05:21">[00:05:21]</a>. Methods with lower error values in these cases are considered safer to use <a class="yt-timestamp" data-t="00:05:30">[00:05:30]</a> <a class="yt-timestamp" data-t="00:05:37">[00:05:37]</a>. Neural network-based methods, for instance, were found to be particularly risky due to their known volatility and sensitivity to hyperparameters <a class="yt-timestamp" data-t="00:05:41">[00:05:41]</a> <a class="yt-timestamp" data-t="00:05:44">[00:05:44]</a> <a class="yt-timestamp" data-t="00:05:46">[00:05:46]</a>.
*   **False Positives and Negatives:** With "Oracle" hyperparameter cases, the number of false positives in edge detection significantly reduces, often to nearly zero <a class="yt-timestamp" data-t="00:06:05">[00:06:05]</a> <a class="yt-timestamp" data-t="00:06:07">[00:06:07]</a>. This means that while some edges might be missing (false negatives), the detected edges can be trusted as true <a class="yt-timestamp" data-t="00:06:09">[00:06:09]</a> <a class="yt-timestamp" data-t="00:06:11">[00:06:11]</a> <a class="yt-timestamp" data-t="00:06:23">[00:06:23]</a> <a class="yt-timestamp" data-t="00:06:27">[00:06:27]</a>. This is a valuable practical implication, ensuring that results are not entirely false <a class="yt-timestamp" data-t="00:06:35">[00:06:35]</a>.

## Algorithm Selection and Practical Impact

The optimal choice of algorithm varies not only with dataset characteristics (e.g., graph size, edge density) but also with the presumed quality of hyperparameter specification <a class="yt-timestamp" data-t="00:06:49">[00:06:49]</a> <a class="yt-timestamp" data-t="00:06:52">[00:06:52]</a> <a class="yt-timestamp" data-t="00:07:04">[00:07:04]</a>. Some methods are preferable if tuning can be performed effectively, while others are safer if only worst-case hyperparameter specifications are considered <a class="yt-timestamp" data-t="00:07:06">[00:07:06]</a> <a class="yt-timestamp" data-t="00:07:11">[00:07:11]</a> <a class="yt-timestamp" data-t="00:07:15">[00:07:15]</a>.

For practitioners, the main takeaway is to consider not only the dataset but also how [[hyperparameter_tuning_for_causal_machine_learning | hyperparameter tuning for causal machine learning]] impacts the optimal algorithm choice <a class="yt-timestamp" data-t="00:07:44">[00:07:44]</a> <a class="yt-timestamp" data-t="00:07:49">[00:07:49]</a>. If a case is risk-sensitive, it might be better to opt for safer methods <a class="yt-timestamp" data-t="00:08:09">[00:08:09]</a>.

## Related Work

Another area of focus for [[machine_learning_and_causality | machine learning and causality]] is [[hyperparameter_tuning_for_causal_machine_learning | hyperparameter tuning for causal machine learning]] within the double machine learning framework <a class="yt-timestamp" data-t="00:09:01">[00:09:01]</a> <a class="yt-timestamp" data-t="00:09:06">[00:09:06]</a>. This work addresses the often-overlooked aspect of tuning choices for nuisance estimations and the selection of causal models and learners <a class="yt-timestamp" data-t="00:09:34">[00:09:34]</a> <a class="yt-timestamp" data-t="00:09:41">[00:09:41]</a>. Key findings suggest that:
*   Tuning machine learning methods for nuisance estimation on full or folded data is preferred over train-test splits in small sample sizes <a class="yt-timestamp" data-t="00:10:00">[00:10:00]</a> <a class="yt-timestamp" data-t="00:10:06">[00:10:06]</a>.
*   Tracking predictive loss on the outcome variable (Y) can help in selecting the right causal model and structural assumptions, as this metric often aligns with the loss on the causal parameter <a class="yt-timestamp" data-t="00:10:19">[00:10:19]</a> <a class="yt-timestamp" data-t="00:10:29">[00:10:29]</a>.
*   A combined loss between nuisance estimates (propensity score and outcome model) can indicate appropriate parameter tuning for a small loss on the causal parameter of interest <a class="yt-timestamp" data-t="00:11:04">[00:11:04]</a> <a class="yt-timestamp" data-t="00:11:10">[00:11:10]</a> <a class="yt-timestamp" data-t="00:11:21">[00:11:21]</a>.

The choice of specific machine learning methods (e.g., Lasso, Random Forest, boosting) depends heavily on the data generating process <a class="yt-timestamp" data-t="00:11:48">[00:11:48]</a> <a class="yt-timestamp" data-t="00:11:50">[00:11:50]</a>. Automated [[machine_learning_and_causal_inference_methodologies | machine learning]] frameworks like Flamel show promise for fitting well across various data generating processes <a class="yt-timestamp" data-t="00:12:11">[00:12:11]</a> <a class="yt-timestamp" data-t="00:12:17">[00:12:17]</a> <a class="yt-timestamp" data-t="00:12:26">[00:12:26]</a>.

This research contributes to providing better guidance for tuning causal models, hyperparameters, and machine learning methods, ultimately aiming to increase the application of [[machine_learning_and_causality | causal machine learning]] in industry <a class="yt-timestamp" data-t="00:12:47">[00:12:47]</a> <a class="yt-timestamp" data-t="00:12:50">[00:12:50]</a> <a class="yt-timestamp" data-t="00:13:01">[00:13:01]</a>.