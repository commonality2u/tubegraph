---
title: Causal reasoning and structural causal models
videoId: FC-rNx5qGR0
---

From: [[causalpython]] <br/> 

The study of [[causality_and_causal_models | causality]] and its intersection with [[large_language_models_and_causal_reasoning | Large Language Models]] (LLMs) is an evolving field, with much work being done to understand how these models can interpret, learn, and apply [[causal_reasoning_in_language_models | causal reasoning]] <a class="yt-timestamp" data-t="00:02:37">[00:02:37]</a>. This area explores the potential for LLMs to go beyond mere correlations and delve into the underlying causes and effects in data.

## Historical Context and Early Research
The journey into this intersection began with position papers exploring the feasibility and opportunities of understanding [[causality_and_causal_models | causality]] using [[large_language_models_and_causal_reasoning | LLMs]] <a class="yt-timestamp" data-t="00:05:39">[00:05:39]</a>. Early work included:
*   **"Understanding Causality with Large Language Models: Feasibility and Opportunities"** (April 2023) by Changchong from Microsoft Research, which was one of the first papers to discuss the potential for this intersection <a class="yt-timestamp" data-t="00:05:32">[00:05:32]</a>.
*   Demonstrations using models like Chat GPT alongside [[causal_discovery_and_analysis | causal discovery]] and inference algorithms such as Desi to recover [[causal_discovery_and_analysis | causal graphs]] <a class="yt-timestamp" data-t="00:06:10">[00:06:10]</a>.
*   Explorations into the potential uses of [[large_language_models_and_causal_reasoning | LLMs]] for [[causal_discovery_and_analysis | causal discovery]] and [[causality_and_causal_models | causal inference]] <a class="yt-timestamp" data-t="00:06:55">[00:06:55]</a>.
*   A more recent paper co-authored by Joshua Bengio discusses [[causal_discovery_and_analysis | causal discovery]] with [[large_language_models_and_causal_reasoning | Large Language Models]] <a class="yt-timestamp" data-t="00:07:19">[00:07:19]</a>.

## Key Concepts and Models

### Structural Causal Models (SCMs)
A [[structural_causal_models_and_machine_learning | Structural Causal Model]] (SCM) is a modeling framework where systems are described with nodes representing variables and edges representing causal relationships between them <a class="yt-timestamp" data-t="00:12:23">[00:12:23]</a>. SCMs also incorporate structural information about the functions between these edges, including their parameters <a class="yt-timestamp" data-t="00:12:57">[00:12:57]</a>. An SCM generates data that can contain knowledge about [[causality_and_causal_models | causal facts]] and [[causal_discovery_and_analysis | causal structure]] <a class="yt-timestamp" data-t="00:12:38">[00:12:38]</a>.

### Causal Parrots and Meta SCMs
The paper "Causal Parrots: Large Language Models May Talk Causality But Are Not Causal" (co-authored by Morris Welling, Dave, Christian Casting) posits that LLMs can perform astonishingly well in [[causality_and_causal_models | causal tasks]], but questions whether they truly understand [[causality_and_causal_models | causality]] <a class="yt-timestamp" data-t="00:11:02">[00:11:02]</a>, <a class="yt-timestamp" data-t="00:13:10">[00:13:10]</a>. This work introduces the concept of "meta SCMs" <a class="yt-timestamp" data-t="00:13:20">[00:13:20]</a>.

The hypothesis is that LLMs learn from vast datasets, such as Wikipedia, which contain "correlations of [[causality_and_causal_models | causal facts]]" <a class="yt-timestamp" data-t="00:14:02">[00:14:02]</a>, <a class="yt-timestamp" data-t="00:13:26">[00:13:26]</a>. For example, Wikipedia articles might state that "altitude causes temperature" <a class="yt-timestamp" data-t="00:13:30">[00:13:30]</a>. If an LLM is adept at identifying these correlations within the text, it can provide causally correct answers without necessarily "understanding" [[causality_and_causal_models | causality]] in a human-like sense <a class="yt-timestamp" data-t="00:13:40">[00:13:40]</a>, <a class="yt-timestamp" data-t="00:13:52">[00:13:52]</a>. This concept suggests that [[structural_causal_models_and_machine_learning | SCMs]] can generate data that includes information about other [[structural_causal_models_and_machine_learning | SCMs]] <a class="yt-timestamp" data-t="00:17:34">[00:17:34]</a>.

## Evaluating Causal Reasoning in LLMs
Early evaluations of [[causal_reasoning_in_language_models | causal reasoning]] in [[large_language_models_and_causal_reasoning | LLMs]] showed surprising results <a class="yt-timestamp" data-t="00:08:59">[00:08:59]</a>. The paper "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality" (April/May 2023) demonstrated that GPT-4 could achieve near-human performance on a counterfactual reasoning benchmark <a class="yt-timestamp" data-t="00:09:08">[00:09:08]</a>, <a class="yt-timestamp" data-t="00:09:14">[00:09:14]</a>. However, questions arose regarding memorization versus true reasoning due to unknown training data content <a class="yt-timestamp" data-t="00:09:39">[00:09:39]</a>.

More systematic [[evaluating_causal_models_in_practice | evaluation]] methods were developed. The paper "Causal Reasoning in Language Models" by Jihong Jing and co-authors (including Bernhard Sch√∂lkopf) presented a structured approach <a class="yt-timestamp" data-t="00:22:25">[00:22:25]</a>, <a class="yt-timestamp" data-t="00:23:03">[00:23:03]</a>:
*   A "causal engine" was built to generate natural language descriptions of [[causality_and_causal_models | causal situations]] with ground truths <a class="yt-timestamp" data-t="00:23:11">[00:23:11]</a>.
*   Models were tasked with answering correlational, interventional, and counterfactual queries <a class="yt-timestamp" data-t="00:23:26">[00:23:26]</a>.
*   Initial results showed that most models performed at a random level <a class="yt-timestamp" data-t="00:23:41">[00:23:41]</a>.
*   A "causal Chain of Thought" strategy, inspired by Chain of Thought prompting, significantly improved performance, especially for later models like GPT-4, though still far from early enthusiastic results <a class="yt-timestamp" data-t="00:24:01">[00:24:01]</a>, <a class="yt-timestamp" data-t="00:24:12">[00:24:12]</a>.
*   The paper also introduced "CADDi," a large public dataset of causal pairs with ground truths <a class="yt-timestamp" data-t="00:24:32">[00:24:32]</a>.

### World Models and Causality in Vision
The discussion extends to how models might construct "world models" and whether [[causality_and_causal_models | causality]] exists within different modalities, such as images <a class="yt-timestamp" data-t="00:14:53">[00:14:53]</a>. For instance, Open AI's Sora model, which generates realistic videos, has been claimed to build a world model <a class="yt-timestamp" data-t="00:16:02">[00:16:02]</a>. However, it's suggested that Sora instead builds many local approximations of a world model, producing realistic but sometimes inconsistent visuals <a class="yt-timestamp" data-t="00:16:15">[00:16:15]</a>, <a class="yt-timestamp" data-t="00:16:29">[00:16:29]</a>. This raises questions about whether Sora functions as a true simulator or merely a "soft approximation" of one <a class="yt-timestamp" data-t="00:16:39">[00:16:39]</a>.

The concept of "correlations of [[causality_and_causal_models | causal effects]]" is considered general, potentially applicable to any domain or representation, whether text or images <a class="yt-timestamp" data-t="00:17:26">[00:17:26]</a>, <a class="yt-timestamp" data-t="00:17:30">[00:17:30]</a>. This raises a philosophical question: Is there [[causality_and_causal_models | causality]] within static images, or is video (which includes time) always necessary? <a class="yt-timestamp" data-t="00:17:48">[00:17:48]</a>. The nature of [[causality_and_causal_models | causal variables]] itself, as explored by Sander Becka, becomes a deeply important question for advancing the field <a class="yt-timestamp" data-t="00:18:43">[00:18:43]</a>, <a class="yt-timestamp" data-t="00:19:02">[00:19:02]</a>.

## Generalization of Causal Structures
Further research, such as "Passive Learning of Active Causal Strategies in Agents and Language Models" by Andrew Lampinen from Google DeepMind, has shown that [[large_language_models_and_causal_reasoning | language models]] can generalize [[causal_discovery_and_analysis | causal structures]] under specific conditions <a class="yt-timestamp" data-t="00:19:16">[00:19:16]</a>, <a class="yt-timestamp" data-t="00:19:33">[00:19:33]</a>. This highlights the importance of not limiting thinking to only the Pearlian framework of [[causality_and_causal_models | causality]], but also considering cognitive science, physics, and common sense perspectives <a class="yt-timestamp" data-t="00:20:17">[00:20:17]</a>, <a class="yt-timestamp" data-t="00:20:21">[00:20:21]</a>, <a class="yt-timestamp" data-t="00:20:29">[00:20:29]</a>. The field benefits from cross-pollination of ideas from different domains <a class="yt-timestamp" data-t="00:22:22">[00:22:22]</a>.

## Ongoing Discussions and Future Directions
The understanding of [[causality_and_causal_models | causality]] and [[large_language_models_and_causal_reasoning | LLMs]] remains unsettled <a class="yt-timestamp" data-t="00:21:15">[00:21:15]</a>. Key questions revolve around whether [[large_language_models_and_causal_reasoning | LLMs]] truly reason and how they do so, and whether their capabilities align with [[structural_causal_models_and_machine_learning | SCMs]] <a class="yt-timestamp" data-t="00:27:46">[00:27:46]</a>. This requires open discussion and engagement from smart individuals to collectively figure out solutions <a class="yt-timestamp" data-t="00:27:58">[00:27:58]</a>, <a class="yt-timestamp" data-t="00:28:04">[00:28:04]</a>.

As Judea Pearl famously stated, "As x-rays are to the surgeon, graphs are for causation," highlighting the essential role of graphical models in [[causality_and_causal_models | causal reasoning]] <a class="yt-timestamp" data-t="00:29:11">[00:29:11]</a>, <a class="yt-timestamp" data-t="00:29:40">[00:29:40]</a>.