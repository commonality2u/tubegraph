---
title: Challenges in Evaluating Causal Models
videoId: w9Dy4xqn7mA
---

From: [[causalpython]] <br/> 

Evaluating causal models presents unique and significant [[challenges_of_implementing_causality_in_research_and_industry | challenges]] compared to traditional predictive machine learning models <a class="yt-timestamp" data-t="00:16:42">[00:16:42]</a>. While machine learning often focuses on demonstrating *that* something works, the field of causal inference demands a deeper understanding of *why* and *when* methods are effective, especially in safety-critical applications <a class="yt-timestamp" data-t="00:02:38">[00:02:38]</a><a class="yt-timestamp" data-t="00:13:30">[00:13:30]</a>. A key aim in research is to build intuition about when and why models are expected to work <a class="yt-timestamp" data-t="00:14:06">[00:14:06]</a>.

## Core Technical Challenges in Causal Machine Learning

Within the context of Conditional Average Treatment Effect (CATE) estimation, two primary technical [[challenges_in_causal_machine_learning_compared_to_traditional_methods | challenges]] stand out:

*   **Covariate Shift due to Treatment Assignment Bias** When the group receiving treatment differs significantly from the control group, a "covariate shift" occurs <a class="yt-timestamp" data-t="00:17:00">[00:17:00]</a>. This imbalance between the two groups complicates the reliable fitting of models <a class="yt-timestamp" data-t="00:17:11">[00:17:11]</a>.
*   **Unobserved Labels** The true label for CATE estimation, which is the difference between an individual's outcome if treated versus if not treated, is fundamentally unobservable <a class="yt-timestamp" data-t="00:17:27">[00:17:27]</a>. An individual can only ever be observed in one of the two states (treated or untreated), making learning the counterfactual challenging and interesting <a class="yt-timestamp" data-t="00:17:41">[00:17:41]</a><a class="yt-timestamp" data-t="00:17:49">[00:17:49]</a>.

## Practical Evaluation Hurdles

Beyond the technical aspects, [[evaluating_and_explaining_causal_models_in_industry | evaluating]] and deploying causal models in practice faces significant hurdles:

*   **Untestable Assumptions** The assumptions underlying causal inference are often untestable <a class="yt-timestamp" data-t="00:18:21">[00:18:21]</a>. This makes it challenging to validate whether a heterogeneous treatment effect system will perform well when deployed <a class="yt-timestamp" data-t="00:18:25">[00:18:25]</a>. The inability to directly observe both potential outcomes further compounds this problem <a class="yt-timestamp" data-t="00:18:17">[00:18:17]</a>.
*   **Benchmarking Practices** A critical look at current benchmarking practices in the field of CATE estimation reveals issues similar to those in causal discovery <a class="yt-timestamp" data-t="00:19:46">[00:19:46]</a><a class="yt-timestamp" data-t="00:19:58">[00:19:58]</a>. Because of the missing data problem, reliance on simulated datasets is common <a class="yt-timestamp" data-t="00:20:03">[00:20:03]</a>. However, these simulated datasets often encode problem characteristics that heavily favor specific estimators, even if these characteristics are not necessarily realistic reflections of real-world data generating processes <a class="yt-timestamp" data-t="00:20:19">[00:20:19]</a><a class="yt-timestamp" data-t="00:20:54">[00:20:54]</a>. This highlights a need for more authoritative statements on likely real-world data generating processes to create better benchmarking testbeds <a class="yt-timestamp" data-t="00:21:01">[00:21:01]</a>.

## Insights from Research

Research sheds light on additional complexities in [[evaluation_and_systematic_testing_of_causal_models | evaluating]] causal models:

*   **Outcome Prediction vs. Treatment Effect Prediction** Models that perform best at predicting outcomes are not necessarily the ones that perform best at predicting treatment effects <a class="yt-timestamp" data-t="00:23:09">[00:23:09]</a>. In low data regimes, there can be a trade-off between accurately fitting the complex regression surface for potential outcomes and accurately fitting the treatment effect itself <a class="yt-timestamp" data-t="00:23:22">[00:23:22]</a>. Errors in potential outcome predictions can either cancel out or add up when calculating the difference (the treatment effect), which lacks testable implications <a class="yt-timestamp" data-t="00:23:37">[00:23:37]</a><a class="yt-timestamp" data-t="00:24:27">[00:24:27]</a>. While strong performance in outcome prediction is a good first step, it doesn't guarantee accuracy in treatment effect estimation <a class="yt-timestamp" data-t="00:24:01">[00:24:01]</a>.
*   **Addressing Multi-Dimensional Complexity** Real-world problems often have multiple layers of complexity, including time, different outcomes, multiple treatments or combinations, missingness, and informative sampling <a class="yt-timestamp" data-t="00:50:03">[00:50:03]</a>. The current focus in the machine learning literature on treatment effect estimation is often on a very small part of these real problems, typically a single binary treatment and a simple continuous outcome in static data <a class="yt-timestamp" data-t="00:49:33">[00:49:33]</a>.

## The Unifying "Missingness Problem"

Interestingly, many additional complexities encountered in causal problems—such as survival analysis, competing risks, censoring, and informative sampling—can ultimately be framed as a "missingness problem" <a class="yt-timestamp" data-t="00:50:45">[00:50:45]</a>. The more layers of complexity added, the more unobserved information there is, leading to a sparsity problem where much less is observed than desired <a class="yt-timestamp" data-t="00:51:07">[00:51:07]</a>. This unified perspective, similar to the work of Donald Rubin, suggests that these are fundamentally similar problems, despite slight differences in assumed causal structures or identification needs <a class="yt-timestamp" data-t="00:52:09">[00:52:09]</a><a class="yt-timestamp" data-t="00:52:21">[00:52:21]</a>. This unifying perspective offers an interesting opportunity to tackle these missingness problems jointly <a class="yt-timestamp" data-t="00:51:34">[00:51:34]</a>.

## Cultural Barriers in Machine Learning Research

A significant [[challenges_in_implementing_causal_analysis_in_practice | challenge]] in advancing causal inference research, particularly in machine learning, stems from the differing cultures of publication:

*   **Focus on Novelty over Understanding** The machine learning community often prioritizes methodological novelty and achieving "state-of-the-art" results, where new architectures are presented as superior by beating existing benchmarks <a class="yt-timestamp" data-t="00:54:40">[00:54:40]</a><a class="yt-timestamp" data-t="00:54:47">[00:54:47]</a>. This contrasts with fields like statistics and econometrics, which place a greater emphasis on understanding the underlying structure of a problem and *why* specific solutions work <a class="yt-timestamp" data-t="00:55:06">[00:55:06]</a>.
*   **Lack of "Sources of Gain" Analysis** There is a lack of rigorous analysis into the "sources of gain" when a new method outperforms others <a class="yt-timestamp" data-t="00:56:55">[00:56:55]</a>. If a new architecture changes multiple things, without ablating these changes to understand which elements contributed to the improvement, little is truly learned <a class="yt-timestamp" data-t="00:57:05">[00:57:05]</a>. This focus on methodological novelty, rather than understanding problems better, is seen as a significant problem, as it can lead to an accumulation of methods with the same fundamental failure modes <a class="yt-timestamp" data-t="00:56:11">[00:56:11]</a><a class="yt-timestamp" data-t="00:56:17">[00:56:17]</a>.

## Future Directions and Desired Solutions

The biggest [[challenges and future directions in causal inference | challenge]] to solve for causal models is developing better ways to [[evaluation_and_systematic_testing_of_causal_models | evaluate]] their effectiveness in practice <a class="yt-timestamp" data-t="00:33:02">[00:33:02]</a>. This includes:

*   **Validating Untestable Assumptions** A crucial need is for methods to validate untestable assumptions, perhaps by mapping domain expertise into quantifiable assurances that assumptions are likely to hold <a class="yt-timestamp" data-t="00:33:33">[00:33:33]</a><a class="yt-timestamp" data-t="00:33:46">[00:33:46]</a>.
*   **Sensitivity Analysis** [[Challenges and Opportunities in Structural Causal Models | Sensitivity analysis]] offers a promising tool to perturb assumptions and observe the impact on results <a class="yt-timestamp" data-t="00:47:04">[00:47:04]</a>. This could involve placing bounds on the certainty of treatment effects <a class="yt-timestamp" data-t="00:34:49">[00:34:49]</a>. Recent work uses modern machine learning ideas like conformal prediction intervals to bound the effects of unobserved confounding <a class="yt-timestamp" data-t="00:35:31">[00:35:31]</a>. Sensitivity models could also be applied to other complex aspects of real problems, such as censoring, missingness, or informative sampling <a class="yt-timestamp" data-t="00:48:06">[00:48:06]</a>.
*   **Error Propagation Over Time** A pertinent question for future research is understanding how errors propagate and compound over multiple time points when assumptions are not perfectly met <a class="yt-timestamp" data-t="00:49:03">[00:49:03]</a>.
*   **Learning from Other Fields** Insights from fields like biostatistics and semi-parametric statistics, which formalize problems in terms of functionals of statistical models rather than rigid parametric assumptions, can be highly beneficial for the causal community <a class="yt-timestamp" data-t="00:39:26">[00:39:26]</a><a class="yt-timestamp" data-t="00:40:01">[00:40:01]</a>.
*   **Asking "Why"** The continued emphasis on asking "why" models work and finding the root causes of their performance is crucial for advancing the field of causal machine learning <a class="yt-timestamp" data-t="00:59:20">[00:59:20]</a><a class="yt-timestamp" data-t="00:59:43">[00:59:43]</a>.