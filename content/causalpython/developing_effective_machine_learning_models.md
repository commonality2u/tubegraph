---
title: Developing Effective Machine Learning Models
videoId: _mJclm_aJlc
---

From: [[causalpython]] <br/> 

Professor Stefan Feuerriegel, a researcher and head of the Institute for AI and Management at LMU, emphasizes the critical role of making a distinctive impact in practice when developing machine learning models <a class="yt-timestamp" data-t="01:45:06">[01:45:06]</a>. His work, consistently accepted at top machine learning conferences, focuses on translating complex algorithms into actionable insights for decision-makers <a class="yt-timestamp" data-t="02:00:23">[02:00:23]</a>.

## The Drive for Impact and Translation

A primary motivation for Feuerriegel's research is to improve people's lives, particularly through the application of [[Causality and Machine Learning | causal machine learning]] in medicine <a class="yt-timestamp" data-t="01:52:03">[01:52:03]</a>. While some researchers are interested in causal machine learning purely as a statistical challenge, Feuerriegel is driven by its potential for profound real-world impact <a class="yt-timestamp" data-t="01:38:00">[01:38:00]</a>.

### Bridging the Communication Gap
A significant challenge in the adoption of advanced machine learning methods is the communication gap between researchers and practitioners <a class="yt-timestamp" data-t="02:05:00">[02:05:00]</a>. Feuerriegel stresses the need for the causal community to "move to their field and speak their language" rather than expecting practitioners to understand complex technical jargon <a class="yt-timestamp" data-t="03:07:00">[03:07:00]</a>. His experience at McKinsey reinforced the importance of translating complex thoughts and mathematics into language that end-users and decision-makers can readily grasp and utilize <a class="yt-timestamp" data-t="04:01:00">[04:01:00]</a>.

He highlights the difficulty in explaining abstract concepts like causal machine learning methods to those without a strong technical background, such as medical professionals <a class="yt-timestamp" data-t="05:05:00">[05:05:00]</a>. His "crystal ball" analogy helps managers understand that [[Causality and Machine Learning | causal machine learning]] isn't about predicting the future, but about predicting the outcome of different *decisions* to allow for better choices <a class="yt-timestamp" data-t="01:41:00">[01:41:00]</a>.

## Key Pillars for Improving Machine Learning Models

Stefan Feuerriegel identifies three crucial areas for enhancing machine learning models to make reliable inferences in practical settings:

1.  **Moving Beyond Stylized Settings**: Traditional computer science research often focuses on simplified settings to rigorously demonstrate methods <a class="yt-timestamp" data-t="07:47:00">[07:47:00]</a>. However, real-world decision-making settings (e.g., medicine) are rarely so stylized. There is a need for methods that can handle:
    *   Time series data <a class="yt-timestamp" data-t="08:22:00">[08:22:00]</a>.
    *   Multiple continuous treatments, such as drug combinations <a class="yt-timestamp" data-t="08:30:00">[08:30:00]</a>.
    *   Complex treatments <a class="yt-timestamp" data-t="08:42:00">[08:42:00]</a>.

2.  **Uncertainty Quantification**: This is "hugely important" for reliable decision-making, especially in medicine <a class="yt-timestamp" data-t="08:49:00">[08:49:00]</a>. Unlike medicine, where reporting uncertainty intervals is standard practice, many machine learning methods for [[Causality and Machine Learning | causal inference]] and treatment effect estimation are not yet robust enough for reliable [[Machine Learning and Explainability | uncertainty quantification]] <a class="yt-timestamp" data-t="09:14:00">[09:14:00]</a>. Without this, medical practice will not adopt methods that rely solely on point estimates for treatment recommendations <a class="yt-timestamp" data-t="09:50:00">[09:50:00]</a>.

3.  **Education**: Educating practitioners and decision-makers on the "why" and "how" of [[Causality and Machine Learning | causal machine learning]] is vital <a class="yt-timestamp" data-t="15:48:00">[15:48:00]</a>. This includes providing accessible resources, role model papers illustrating best practices, and guidance on [[Role of Assumptions in Machine Learning Models | robustness checks]] <a class="yt-timestamp" data-t="16:37:00">[16:37:00]</a>.

## Real-World Impact and Applications

Feuerriegel provides compelling examples demonstrating the practical applicability of [[Causality and Machine Learning | causal machine learning]]:

*   **ABB Hitachi**: A rigorous field experiment showed that a causal machine learning tool could reduce yield loss in semiconductor fabrication by almost 50% (49%) <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a>, <a class="yt-timestamp" data-t="16:01:00">[16:01:00]</a>.
*   **Booking.com**: This company runs [[Causal Machine Learning Applications in Various Industries | causal machine learning algorithms]] at a large scale <a class="yt-timestamp" data-t="00:18:00">[00:18:00]</a>, <a class="yt-timestamp" data-t="15:09:00">[15:09:00]</a>.
*   **Health Insurance in the Middle East**: A two-stage causal machine learning model was developed to identify patients who would benefit most from diabetes prevention programs, allowing the health insurance company to allocate limited resources cost-effectively <a class="yt-timestamp" data-t="12:03:00">[12:03:03]</a>.
*   **Large Media Company**: Causal inference was used to optimize the front page of a newspaper, providing decision support systems that empowered editors to make better choices and drive long-term impact <a class="yt-timestamp" data-t="13:15:00">[13:15:00]</a>.

These examples directly counter the criticism that [[Causality and Machine Learning | causal machine learning]] is impossible to implement in practice <a class="yt-timestamp" data-t="13:05:00">[13:05:00]</a>. The key is to augment human decision-makers rather than replace them, fostering trust and driving project implementation from top management <a class="yt-timestamp" data-t="15:28:00">[15:28:00]</a>.

## Building a Successful Team and Approach

Feuerriegel attributes his team's success to having a "great team" <a class="yt-timestamp" data-t="19:52:00">[19:52:00]</a>. He emphasizes the importance of diversity, not just in culture or gender, but also in academic backgrounds (mathematics, data science, statistics, computer science, economics) <a class="yt-timestamp" data-t="21:12:00">[21:12:00]</a>. This diversity brings different perspectives and strengths to problem-solving <a class="yt-timestamp" data-t="21:30:00">[21:30:00]</a>.

His team's vision is to "redevelop, implement, and evaluate new AI algorithms to improve decision-making" <a class="yt-timestamp" data-t="23:08:00">[23:08:00]</a>. This encompasses not just theoretical development but also practical application with companies and practitioners <a class="yt-timestamp" data-t="23:18:00">[23:18:00]</a>.

## Lessons Learned for Effective Model Development

From his entrepreneurial background and research, Feuerriegel shares valuable lessons:

*   **Timing is Crucial**: Being "too early" for a market, requiring extensive customer education and trust-building for AI systems, can be a major hurdle <a class="yt-timestamp" data-t="28:21:00">[28:21:00]</a>.
*   **Avoid Over-Customization**: Products that require extensive customization can lead to a complex and inefficient sales funnel <a class="yt-timestamp" data-t="28:35:00">[28:35:00]</a>.
*   **Test Ideas Early and Detach**: Don't get too attached to initial ideas <a class="yt-timestamp" data-t="28:48:00">[28:48:00]</a>. For traditional machine learning, he suggests a "one-day rule": if a linear regression or random forest doesn't yield satisfying performance on the first day, the dataset might be poor or the task might not be solvable by machine learning <a class="yt-timestamp" data-t="29:57:00">[29:57:00]</a>. Giving up early can prevent wasted investment and improve project success <a class="yt-timestamp" data-t="30:34:00">[30:34:00]</a>.
*   **Simplicity Often Prevails**: In practice, especially with data sparsity (common in small and medium enterprises), simpler models can be more robust and effective than complex ones like neural networks <a class="yt-timestamp" data-t="26:50:00">[26:50:00]</a>. For example, a generalized propensity score with polynomial regression outperformed complex approaches like generative adversarial networks in a development aid allocation project <a class="yt-timestamp" data-t="27:04:00">[27:04:00]</a>.

## Recommendations for the Community

For those starting to learn about [[Causality and Machine Learning | causal machine learning]], Feuerriegel recommends resources like their Nature Medicine paper or the KDD 2024 paper on development aid, which offer accessible introductions <a class="yt-timestamp" data-t="24:23:00">[24:23:00]</a>.

He calls for more Python packages, similar to `econML` and `doubleML`, and tools that cover other parts of the causal inference pipeline, such as partial identification <a class="yt-timestamp" data-t="31:08:00">[31:08:00]</a>. These libraries could significantly aid the uptake of causal ideas <a class="yt-timestamp" data-t="31:41:00">[31:41:00]</a>.

To managers and entrepreneurs, he advises testing ideas to identify effective use cases <a class="yt-timestamp" data-t="32:54:00">[32:54:00]</a>. It's important to recognize that sometimes a simple traditional machine learning method (like an S-learner) is sufficient, while other situations demand the rigor of a causal forest or a doubly robust learner <a class="yt-timestamp" data-t="33:04:00">[33:04:00]</a>. The key is to test what works and is sufficiently robust in practice, especially given the challenge of observing counterfactual values <a class="yt-timestamp" data-t="33:37:00">[33:37:00]</a>.