---
title: Role of causality in agentbased systems
videoId: 8yWKQqNFrmY
---

From: [[causalpython]] <br/> 

The role of [[causality_in_artificial_intelligence | causality in Artificial Intelligence]] is becoming increasingly significant, especially for agents that interact with the real world or other entities online <a class="yt-timestamp" data-t="00:00:09">[00:00:09]</a>. These interactions necessitate a deeper understanding of cause-and-effect relationships for agents to behave optimally and be held accountable.

## Why Causality is Crucial for Agents

When agents begin interacting in the real world or web environments, fundamental concepts like action, reward, desired behavior, and penalized behavior become inherently causal abstractions <a class="yt-timestamp" data-t="00:00:18">[00:00:18]</a>. Questions arise such as determining which agent is most useful or deserving of reward, or establishing notions of blame for agents performing poorly <a class="yt-timestamp" data-t="00:00:29">[00:00:29]</a>. This emphasizes the need for [[causality_and_causal_models | causal models]] in agent design and deployment <a class="yt-timestamp" data-t="00:00:41">[00:00:41]</a>.

## New Causal Questions

As agents become more prevalent, new types of causal questions are expected to emerge <a class="yt-timestamp" data-t="00:01:05">[00:01:05]</a>. These questions may resemble those in social sciences but will focus more on designing systems from the ground up to ensure desired agent behaviors <a class="yt-timestamp" data-t="00:05:51">[00:05:51]</a>. This includes considerations for how agents should learn robust world models <a class="yt-timestamp" data-t="00:06:22">[00:06:22]</a>. While causality might not be as apparent in the initial training of agents (which might rely on reinforcement learning), it becomes critical once agents begin collaborating or interacting with human agents <a class="yt-timestamp" data-t="00:05:12">[00:05:12]</a>.

Key areas where causal thinking will be vital for agents:
*   **Action and Reward Optimization**: Identifying which actions lead to desirable outcomes and how to optimize or penalize behavior <a class="yt-timestamp" data-t="00:00:18">[00:00:18]</a>.
*   **Attribution and Blame**: Determining responsibility for outcomes, such as identifying useful agents or removing underperforming ones <a class="yt-timestamp" data-t="00:00:30">[00:00:30]</a>.
*   **Robust World Models**: Learning how agents can acquire robust models of their environment <a class="yt-timestamp" data-t="00:00:50">[00:00:50]</a>. Work by Richardson and Everett from Google DeepMind explores how agents can learn robust world models <a class="yt-timestamp" data-t="00:06:17">[00:06:17]</a>.

## Learning and Robustness in Agent Systems

One perspective suggests that agents can learn correct actions in a system even without explicit causal knowledge, provided they are exposed to diverse environments <a class="yt-timestamp" data-t="00:07:27">[00:07:27]</a>. If an agent's objective is simply to perform well across varied environments, it may eventually mimic the actions of a true [[causality_and_causal_models | causal agent]] that possesses a world model <a class="yt-timestamp" data-t="00:07:31">[00:07:31]</a>. It is even theoretically possible to recover an agent's world model by merely observing its actions <a class="yt-timestamp" data-t="00:07:59">[00:07:59]</a>. This highlights the power of diverse data in imparting [[causality_in_artificial_intelligence | causality]] <a class="yt-timestamp" data-t="00:08:14">[00:08:14]</a>.

Another way agents can achieve causal actions is by building foundations on existing domain knowledge, theorems, and axioms <a class="yt-timestamp" data-t="00:10:56">[00:10:56]</a>. This is analogous to how large language models (LLMs) answer causal questions by having learned theorems (e.g., in physics) from vast amounts of text <a class="yt-timestamp" data-t="00:10:21">[00:10:21]</a>. This "cheating" method, as it is sometimes called, allows agents to apply known rules rather than discovering them from first principles, thereby reaching correct causal answers more efficiently <a class="yt-timestamp" data-t="00:10:46">[00:10:46]</a>. This approach may also be more attractive for training, as it structures initial knowledge to enable easier adaptation <a class="yt-timestamp" data-t="00:14:18">[00:14:18]</a>.

There are multiple pathways to achieving causal agents, beyond the traditional focus on explicit causal mechanisms, graphs, and inductive biases <a class="yt-timestamp" data-t="00:09:17">[00:09:17]</a>. Gathering extensive diverse data or building upon established domain knowledge are alternative effective strategies <a class="yt-timestamp" data-t="00:09:43">[00:09:43]</a>.

## Verification and Feedback for Agents

A significant challenge in contemporary agentic frameworks is **verification** <a class="yt-timestamp" data-t="00:14:42">[00:14:42]</a>. While agents excel in well-controlled areas like coding (where compilers act as verifiers) or API calls (where clear failure signals exist), applying them to vague real-world problems is difficult <a class="yt-timestamp" data-t="00:14:50">[00:14:50]</a>. It is challenging to provide good rewards for agents in such complex scenarios, and human feedback is not scalable <a class="yt-timestamp" data-t="00:15:34">[00:15:34]</a>.

The solution requires creating verifiers for new domains, initially for outcomes, and then for individual actions taken by agents <a class="yt-timestamp" data-t="00:15:42">[00:15:42]</a>. For example, in summarizing a meeting, verification might involve assessing precision, recall of important points, or relevance to specific individuals <a class="yt-timestamp" data-t="00:16:11">[00:16:11]</a>. For scientific agents suggesting experiments, feedback mechanisms are needed to guide them toward designing better experiments <a class="yt-timestamp" data-t="00:16:43">[00:16:43]</a>. This emphasizes the need for holistic verification systems, which involve both art and science in defining verifiable properties and creating [[causality_and_causal_models | causal systems]] to prove necessary properties within a given domain <a class="yt-timestamp" data-t="00:17:11">[00:17:11]</a>.

## Future Directions and Challenges

A main challenge in [[causality_in_artificial_intelligence | causality for AI]] agents is the difficulty of obtaining and verifying [[causality_and_causal_models | causal graphs]] for complex systems <a class="yt-timestamp" data-t="00:32:56">[00:32:56]</a>. Large Language Models (LLMs) show promise in generating plausible graphs, which experts can then refine <a class="yt-timestamp" data-t="00:33:08">[00:33:08]</a>. Additionally, LLMs can assist in suggesting robust reputation tests, such as identifying "dummy outcomes" or "placebo variables" that can be used to validate causal models <a class="yt-timestamp" data-t="00:33:52">[00:33:52]</a>.

One research direction explores how [[causality_in_artificial_intelligence | causality]] can make LLMs more robust <a class="yt-timestamp" data-t="00:36:02">[00:36:02]</a>. Instead of directly imparting complex causal knowledge, the focus is on teaching LLMs *how to learn* causal knowledge by training them on fundamental axioms of causal reasoning, such as transitivity and d-separation <a class="yt-timestamp" data-t="00:38:55">[00:38:55]</a>. Initial results indicate that small transformer models trained on synthetic axiomatic data can generalize their causal reasoning abilities to larger graphs and even improve performance on complex causal tasks, like constructing graphs from correlational statements <a class="yt-timestamp" data-t="00:40:13">[00:40:13]</a>. This suggests a powerful new way of imparting reasoning to models without relying solely on memorization <a class="yt-timestamp" data-t="00:42:30">[00:42:30]</a>.

While there's a risk of models learning "statistical shortcuts" instead of true causal principles, the hope is that by scaling diverse axiomatic data, the space of such shortcuts can be constrained, leading models to converge on genuine causal principles <a class="yt-timestamp" data-t="00:45:56">[00:45:56]</a>. This aligns with the idea that if systems are designed to interact in natural language and answer complex reasoning questions, they need to either parse natural language into specific API calls or embed reasoning capabilities directly within the language model <a class="yt-timestamp" data-t="00:46:57">[00:46:57]</a>.

A major technical challenge for [[causality_in_artificial_intelligence | causality in AI]] is to achieve a "model-free revolution," similar to what occurred in reinforcement learning <a class="yt-timestamp" data-t="00:48:23">[00:48:23]</a>. This means developing methods that extract maximum information from diverse datasets and provide the same guarantees as [[causality_and_causal_models | graphical models]], even without an explicit causal model <a class="yt-timestamp" data-t="00:48:49">[00:48:49]</a>. Concepts like exchangeable distributions and data from multiple environments are promising in this regard <a class="yt-timestamp" data-t="00:49:15">[00:49:15]</a>. If we can leverage diverse, multi-distributional data, the identification of causal relationships could become significantly richer <a class="yt-timestamp" data-t="00:50:29">[00:50:29]</a>.

Ultimately, the future of [[causality_in_artificial_intelligence | causality in AI]] for agents suggests a broader ambition: enabling scientists to accelerate their research cycles by using AI to generate graphs from literature, plan experiments, and generally make better use of their data <a class="yt-timestamp" data-t="01:08:58">[01:08:58]</a>.