---
title: Symbolic reasoning and logical deduction in AI
videoId: zFeAtV7AN0A
---

From: [[causalpython]] <br/> 

## Introduction
The topic of [[causal_reasoning_in_language_models | causal reasoning in language models]] and their capacity for symbolic reasoning and logical deduction was a significant focus at the post-AI, post-Triple-AI live stream workshop <a class="yt-timestamp" data-t="00:00:24">[00:00:24]</a>. Discussions highlighted the capabilities and limitations of [[language_models_and_reasoning | large language models]] (LLMs) in performing complex logical tasks and the theoretical underpinnings of these abilities <a class="yt-timestamp" data-t="00:17:02">[00:17:02]</a>.

## Can LLMs Perform Logical Reasoning?
A central question explored was whether [[language_models_and_reasoning | large language models]] can perform logical reasoning <a class="yt-timestamp" data-t="00:17:20">[00:17:20]</a>. Humans are capable of transitive logical reasoning, such as inferring that if A causes B and B causes C, then A should cause C <a class="yt-timestamp" data-t="00:17:26">[00:17:26]</a>. Research presented at the workshop, specifically a talk by Guy Van den Broeck from UCLA, showed that LLMs can perform well in certain logical reasoning tasks <a class="yt-timestamp" data-t="00:17:40">[00:17:40]</a>.

However, a critical finding was that while LLMs might initially perform near 100% accuracy on a simple logic benchmark, their performance significantly drops if the benchmark is slightly modified <a class="yt-timestamp" data-t="00:17:54">[00:17:54]</a>. This suggests that LLMs may not consistently follow the rules of logical deduction <a class="yt-timestamp" data-t="00:17:47">[00:17:47]</a>.

## The Theorem of Ground Truth Reasoning Functions
A significant highlight was a theorem presented by Guy Van den Broeck, showing that "there exist Transformer parameters that can compute the ground truth reasoning function" <a class="yt-timestamp" data-t="00:18:15">[00:18:15]</a>. This implies that it is theoretically possible to accurately set the weights of a Transformer architecture so that they mirror the data generating process, allowing for pure reasoning <a class="yt-timestamp" data-t="00:18:40">[00:18:40]</a>, <a class="yt-timestamp" data-t="00:18:44">[00:18:44]</a>, <a class="yt-timestamp" data-t="00:18:52">[00:18:52]</a>.

### The Paradox of Unlearning
Despite this theoretical possibility, a perplexing paradox emerged: if a model starts with these theoretically "correct" weights (representing the exact solution) and is then fine-tuned on empirical, regular training data, it will unlearn the correct solution and instead learn a shortcut <a class="yt-timestamp" data-t="00:19:00">[00:19:00]</a>, <a class="yt-timestamp" data-t="00:19:13">[00:19:13]</a>. This phenomenon, known as [[causality_and_ai_challenges_and_opportunities | shortcut learning]], is a significant focus in machine learning <a class="yt-timestamp" data-t="00:19:27">[00:19:27]</a>. This finding has profound implications for how we train and evaluate [[language_models_and_reasoning | language models]] to achieve true [[causality_in_artificial_intelligence | causal reasoning]].

## Abstraction and Reasoning Tasks
Further discussions on reasoning capabilities included a talk by Alessandro Palmarini, co-authored with Melanie Mitchell, comparing human, GPT-4, and GPT-4V performance on abstraction and reasoning tasks <a class="yt-timestamp" data-t="00:20:11">[00:20:11]</a>. The results indicated that GPT-4 models largely fail on these tasks, showing around 25% accuracy on the ARC Benchmark <a class="yt-timestamp" data-t="00:20:41">[00:20:41]</a>, <a class="yt-timestamp" data-t="00:20:42">[00:20:42]</a>, <a class="yt-timestamp" data-t="00:20:48">[00:20:48]</a>. Even extensive prompting was unable to significantly improve these results <a class="yt-timestamp" data-t="00:21:03">[00:21:03]</a>. This benchmark could potentially be modified into a [[benchmarking_causal_reasoning_in_ai | causal benchmark]] <a class="yt-timestamp" data-t="00:21:14">[00:21:14]</a>.

## Community Skepticism and Future Directions
A survey conducted among workshop participants before and after the event revealed shifts in perception regarding LLMs' causal reasoning capabilities <a class="yt-timestamp" data-t="00:27:06">[00:27:06]</a>. Initially, opinions were almost evenly split on whether LLMs can reason causally (50/50) <a class="yt-timestamp" data-t="00:27:14">[00:27:14]</a>. After the workshop, this shifted to a 2/3 to 1/3 split, with more people believing LLMs could reason causally <a class="yt-timestamp" data-t="00:27:21">[00:27:21]</a>, <a class="yt-timestamp" data-t="00:27:30">[00:27:30]</a>.

However, when asked if implicit causal world models are part of what LLMs have learned, 75-76% initially said yes <a class="yt-timestamp" data-t="00:27:40">[00:27:40]</a>, <a class="yt-timestamp" data-t="00:27:46">[00:27:46]</a>. After the workshop, this dropped to 63% believing yes, and 36% saying no (up from 23% before) <a class="yt-timestamp" data-t="00:27:51">[00:27:51]</a>, <a class="yt-timestamp" data-t="00:27:58">[00:27:58]</a>, <a class="yt-timestamp" data-t="00:28:03">[00:28:03]</a>. This suggests that the workshop made participants more skeptical about the inherent causal understanding within LLMs <a class="yt-timestamp" data-t="00:28:08">[00:28:08]</a>. This skepticism is seen as positive, driving further research into how [[language_models_and_reasoning | language models]] may talk [[causality_in_artificial_intelligence | causality]] but not necessarily *be* causal <a class="yt-timestamp" data-t="00:28:18">[00:28:18]</a>, <a class="yt-timestamp" data-t="00:29:09">[00:29:09]</a>.