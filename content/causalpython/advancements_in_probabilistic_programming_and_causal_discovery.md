---
title: Advancements in probabilistic programming and causal discovery
videoId: gazCIKYEv44
---

From: [[causalpython]] <br/> 
Dr. Robert Ness, a Senior Researcher at Microsoft Research, is a distinguished guest on the Causal Bandits podcast, which focuses on causality and machine learning. His work spans economics, statistics, computation, and [[causal_discovery_and_inference_in_AI | causal inference]], driven by a desire to solve applied problems in the real world using quantitative methods <a class="yt-timestamp" data-t="01:26:00">[01:26:00]</a> <a class="yt-timestamp" data-t="01:38:00">[01:38:00]</a>.

### Journey into Causality

Dr. Ness found a natural aptitude for statistics, describing it as a "superpower" compared to other quantitative subjects <a class="yt-timestamp" data-t="01:57:00">[01:57:00]</a> <a class="yt-timestamp" data-t="02:07:00">[02:07:00]</a>. His attraction to the field stemmed from the process of modeling data, data science, and building models <a class="yt-timestamp" data-t="02:55:00">[02:55:00]</a>.

His doctoral research led him to systems biology, a field he preferred over financial engineering due to the presence of a "ground truth" in natural sciences <a class="yt-timestamp" data-t="04:00:00">[04:00:00]</a> <a class="yt-timestamp" data-t="04:13:00">[04:13:00]</a>. In systems biology, he was introduced to the task of reconstructing biological pathways, such as signal transduction pathways, from single-cell data using [[causal_discovery_algorithms_and_realworld_applications | causal discovery algorithms]] (also known as structure learning) <a class="yt-timestamp" data-t="05:04:00">[05:04:00]</a> <a class="yt-timestamp" data-t="05:10:00">[05:10:00]</a>.

### Challenges in Causal Discovery and Uncertainty Quantification

A primary challenge in applying [[causal_discovery_algorithms_and_realworld_applications | causal discovery algorithms]] to biological data was building practically useful tools for laboratory analysts, rather than just generating a Directed Acyclic Graph (DAG) <a class="yt-timestamp" data-t="05:41:00">[05:41:00]</a> <a class="yt-timestamp" data-t="07:01:00">[07:01:00]</a>. Biologists often aim to achieve downstream goals like discovering biomarkers or new signaling pathways <a class="yt-timestamp" data-t="07:10:00">[07:10:00]</a>. This need led Dr. Ness to adapt causal discovery methods towards experimental design, using Bayesian experimental design and sequential workflows to manage uncertainty and inform optimal measurement strategies <a class="yt-timestamp" data-t="07:41:00">[07:41:00]</a> <a class="yt-timestamp" data-t="08:03:00">[08:03:00]</a>.

Traditional methods for evaluating learned causal structures involved comparing them to known ground truth graphs using metrics like precision, recall, or Hamming distance <a class="yt-timestamp" data-t="08:44:00">[08:44:00]</a> <a class="yt-timestamp" data-t="09:08:00">[09:08:00]</a>. However, when ground truth is unknown, a Bayesian approach incorporating prior knowledge and modeling uncertainty allows for guarantees of moving towards the correct answer with more data, even if the exact truth is unobservable <a class="yt-timestamp" data-t="09:18:00">[09:18:00]</a> <a class="yt-timestamp" data-t="10:00:00">[10:00:00]</a>.

### Causal Decision Making and Reinforcement Learning

Dr. Ness highlights that [[machine_learning_and_causal_inference_methodologies | causal decision theory]] contrasts with empirical decision theory, emphasizing that decision-making agents should attend to the consequences of actions to maximize expected utility under intervention <a class="yt-timestamp" data-t="11:16:00">[11:16:00]</a> <a class="yt-timestamp" data-t="11:23:00">[11:23:00]</a>. Examples like Elias Bareinblat's work on causal bandits demonstrate applying causal knowledge to optimization in systems with unknown or confounding causal factors <a class="yt-timestamp" data-t="12:06:00">[12:06:00]</a> <a class="yt-timestamp" data-t="12:34:00">[12:34:00]</a>.

[[advancements_in_causal_modeling_and_AI | Causal reinforcement learning]] (RL) is another area gaining traction, where causal assumptions can lead to greater sample efficiency, especially in high-dimensional settings, and improve credit assignment (understanding why a policy led to an outcome) through methods like attribution and root cause analysis <a class="yt-timestamp" data-t="12:58:00">[12:58:00]</a> <a class="yt-timestamp" data-t="13:21:00">[13:21:00]</a>.

While traditional deep reinforcement learning often ignores causal nuances by folding everything into a "state" variable, [[advancements_in_causal_modeling_and_AI | causal RL]] is crucial when actions must maximize outcomes under circumstances not seen in training data, or when the optimal action changes based on how actions alter the environment <a class="yt-timestamp" data-t="17:56:00">[17:56:00]</a> <a class="yt-timestamp" data-t="19:00:00">[19:00:00]</a>. Dr. Ness stresses that although traditional RL might suffice for many problems, [[structural_causal_models_and_causal_discovery | causal models]] are necessary in specific high-value scenarios where conditioning on certain data can bias results (e.g., colliders or mediators) <a class="yt-timestamp" data-t="20:28:00">[20:28:00]</a> <a class="yt-timestamp" data-t="21:12:00">[21:12:00]</a>. The challenge lies in identifying practical, high-value scenarios where causal modeling is indispensable, moving beyond mere "toy problems" <a class="yt-timestamp" data-t="21:48:00">[21:48:00]</a> <a class="yt-timestamp" data-t="25:50:00">[25:50:00]</a>.

### Causality and Artificial General Intelligence (AGI)

From a causal perspective, an Artificial General Intelligence (AGI) model would need to reason about cause and effect <a class="yt-timestamp" data-t="29:56:00">[29:56:00]</a>. This involves drawing on insights from computational psychology, where human reasoning abilities are modeled and algorithms are designed to align with human responses <a class="yt-timestamp" data-t="30:10:00">[30:10:00]</a> <a class="yt-timestamp" data-t="31:07:00">[31:07:00]</a>. Causal models are essential here for specifying the assumptions and inductive biases that enable conclusions beyond what observed data alone can provide <a class="yt-timestamp" data-t="31:16:00">[31:16:00]</a> <a class="yt-timestamp" data-t="31:29:00">[31:29:00]</a>.

A key distinction arises between seeking "objective truth" (typical in traditional causal inference, e.g., "does smoking cause cancer?") and "alignment" with human reasoning (crucial for AGI, even if human reasoning exhibits cognitive biases or inaccurate heuristics) <a class="yt-timestamp" data-t="33:24:00">[33:24:00]</a> <a class="yt-timestamp" data-t="34:09:00">[34:09:00]</a>. Dr. Ness gives the example of humans' reduced engagement in counterfactual simulation when considering the absence of actions or events, which could be algorithmically remedied <a class="yt-timestamp" data-t="35:55:00">[35:55:00]</a> <a class="yt-timestamp" data-t="37:15:00">[37:15:00]</a>.

### Probabilistic Programming and Causal Inference

A paper co-authored by Dr. Ness demonstrated that if a causal quantity is identifiable using the do-calculus, implementing the system in a probabilistic programming framework (like PyMC or Pyro) guarantees unbiased estimation of the causal effect <a class="yt-timestamp" data-t="39:19:00">[39:19:00]</a> <a class="yt-timestamp" data-t="40:30:00">[40:30:00]</a>. This result opens the world of causal reasoning to researchers familiar with latent variable models and graphical modeling, provided they can prove the validity of their approach using graphical identification procedures (which can be algorithmic, e.g., using libraries like `y0` in Python) <a class="yt-timestamp" data-t="40:59:00">[40:59:00]</a> <a class="yt-timestamp" data-t="41:15:00">[41:15:00]</a>.

He also discussed Cairo, a new library built on Pyro, designed to abstract away difficult causal inference abstractions (e.g., handling deterministic variables and intractable likelihoods in [[structural_causal_models_and_causal_discovery | structural causal models]]) <a class="yt-timestamp" data-t="47:11:00">[47:11:00]</a> <a class="yt-timestamp" data-t="47:56:00">[47:56:00]</a>. Cairo promotes the philosophy that causal uncertainty and Bayesian uncertainty are deeply interconnected, making it a valuable tool for those bridging probabilistic machine learning and causality <a class="yt-timestamp" data-t="48:11:00">[48:11:00]</a> <a class="yt-timestamp" data-t="48:58:00">[48:58:00]</a>.

### Future Directions in Causality: Causal Representation Learning and LLMs

Dr. Ness identifies **causal representation learning** as a significant future development <a class="yt-timestamp" data-t="49:48:00">[49:48:00]</a>. This involves learning latent representations that correspond to true causes in the data-generating process, using causal principles to identify how these causes should behave under intervention (e.g., modularity, invariance) <a class="yt-timestamp" data-t="50:07:00">[50:07:00]</a>.

A practical example is in generative AI (like Midjourney or Stable Diffusion), where users struggle to make specific, localized causal edits (e.g., changing glasses color without altering other image features) because the models lack explicit causal representations <a class="yt-timestamp" data-t="51:02:00">[51:02:00]</a> <a class="yt-timestamp" data-t="52:24:00">[52:24:00]</a>. If models could operate semantically on learned causal abstractions, adjusting images would become much easier and more intuitive <a class="yt-timestamp" data-t="52:40:00">[52:40:00]</a> <a class="yt-timestamp" data-t="54:08:00">[54:08:00]</a>.

Regarding large language models (LLMs) and causality, the question is whether LLMs "learn a world model" (i.e., a [[structural_causal_models_and_causal_discovery | Causal Model]]) <a class="yt-timestamp" data-t="55:00:00">[55:00:00]</a>. The Causal Hierarchy Theorem (or Pearl's Causal Ladder) states that answering interventional (Level 2) or counterfactual (Level 3) questions requires assumptions at that same level <a class="yt-timestamp" data-t="55:50:00">[55:50:00]</a> <a class="yt-timestamp" data-t="56:31:00">[56:31:00]</a>. While LLMs can empirically answer causal and counterfactual questions, their "hallucinations" (generating false information) highlight a lack of reliable causal reasoning <a class="yt-timestamp" data-t="57:13:00">[57:13:00]</a> <a class="yt-timestamp" data-t="57:44:00">[57:44:00]</a>.

Dr. Ness is exploring ways to incorporate causal information directly into Transformer architectures to provide theoretical guarantees, similar to those achieved with latent variable models <a class="yt-timestamp" data-t="01:00:08:00">[01:00:08:00]</a>. This would allow LLMs to reliably answer causal queries and provide "knobs" for creators to make semantically meaningful adjustments, augmenting human creative processes <a class="yt-timestamp" data-t="01:01:53:00">[01:01:53:00]</a> <a class="yt-timestamp" data-t="01:02:21:00">[01:02:21:00]</a>. This approach focuses on leveraging existing data structure and depth, rather than merely scaling models with more data, to achieve more powerful and aligned AI systems <a class="yt-timestamp" data-t="01:03:18:00">[01:03:18:00]</a> <a class="yt-timestamp" data-t="01:05:22:00">[01:05:22:00]</a>.

Ultimately, Dr. Ness believes that causal questions are enduring and will not be made obsolete by new deep learning architectures <a class="yt-timestamp" data-t="01:16:11:00">[01:16:11:00]</a>.

### Recommended Readings

*   *Computational Systems Biology* by Darren Wilkinson <a class="yt-timestamp" data-t="01:05:45:00">[01:05:45:00]</a>
*   A book on the R language by Hadley Wickham (influenced by functional programming paradigms like SICP) <a class="yt-timestamp" data-t="01:07:32:00">[01:07:32:00]</a>

### Advice for Aspiring Causal Researchers

*   Find a good book and work through it to understand the field's landscape <a class="yt-timestamp" data-t="01:13:29:00">[01:13:29:00]</a>.
*   Identify influential researchers, attend their workshops, listen to podcasts, and read their papers to grasp the cutting edge <a class="yt-timestamp" data-t="01:13:56:00">[01:13:56:00]</a>.
*   Connect with people in the field during one's training (e.g., PhD), aiming to be in proximity to that network <a class="yt-timestamp" data-t="01:14:26:00">[01:14:26:00]</a>. Prioritize personal connections and shared interests over seeking mere institutional prestige <a class="yt-timestamp" data-t="01:15:15:00">[01:15:15:00]</a>.