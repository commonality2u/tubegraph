---
title: challenges and future directions in causal inference
videoId: 8yWKQqNFrmY
---

From: [[causalpython]] <br/> 

Dr. Amit Sharma, Principal Researcher at Microsoft Research India and the creator of DoWhy, believes that data tells stories, and his research aims to uncover causal narratives within that data <a class="yt-timestamp" data-t="01:47:56">[01:47:56]</a>. He views causality as a fundamental goal, even when the path to achieving it remains unclear <a class="yt-timestamp" data-t="01:52:16">[01:52:16]</a>. The *Causal Bandits Podcast*, a platform for discussing causality and AI, frequently delves into these complex topics <a class="yt-timestamp" data-t="01:43:40">[01:43:40]</a>.

## The Rising Importance of Causality in AI Agents

Causality is becoming increasingly critical for artificial intelligence (AI) agents, especially as they interact with the real world and other entities, whether human or digital <a class="yt-timestamp" data-t="00:06:06">[00:06:06]</a>, <a class="yt-timestamp" data-t="04:53:00">[04:53:00]</a>. Fundamental concepts such as action, reward, optimizing or penalizing behavior, and accountability are inherently [[challenges_in_developing_ai_with_causal_understanding | causal abstractions]] <a class="yt-timestamp" data-t="00:18:21">[00:18:21]</a>, <a class="yt-timestamp" data-t="05:06:07">[05:06:07]</a>.

While causality might not be overtly present in the initial training of agents (e.g., in simple reinforcement learning), its significance emerges as agents begin to collaborate and interact <a class="yt-timestamp" data-t="05:12:02">[05:12:02]</a>, <a class="yt-timestamp" data-t="05:21:07">[05:21:07]</a>. This leads to new causal questions, such as determining which agent is most effective, how to allocate rewards, or applying counterfactual notions of blame to identify underperforming agents <a class="yt-timestamp" data-t="05:27:00">[05:27:00]</a>, <a class="yt-timestamp" data-t="05:47:00">[05:47:00]</a>. Designing systems with causal thinking from the ground up will be crucial for agents to behave as desired <a class="yt-timestamp" data-t="05:56:00">[05:56:00]</a>.

Researchers at Google DeepMind, such as Richardson and Everett, have contributed to this area by proposing theories on how agents can learn robust world models <a class="yt-timestamp" data-t="00:43:43">[00:43:43]</a>, <a class="yt-timestamp" data-t="06:17:00">[06:17:00]</a>. Their work suggests that agents operating in diverse environments can learn to mimic the actions of an ideal causal agent, even without an explicit world model, and can even recover the world model by observing actions <a class="yt-timestamp" data-t="07:15:00">[07:15:00]</a>, <a class="yt-timestamp" data-t="08:00:00">[08:00:00]</a>. This highlights the power of diverse data in imparting causal understanding <a class="yt-timestamp" data-t="08:14:00">[08:14:00]</a>.

## Large Language Models (LLMs) and the Shift in Causal Inference

The rapid advancements in Large Language Models (LLMs) have reshaped the understanding of causality. Traditionally, causal inference focused heavily on discovery and reasoning from data, often neglecting the crucial aspect of domain knowledge <a class="yt-timestamp" data-t="03:35:00">[03:35:00]</a>, <a class="yt-timestamp" data-t="03:46:00">[03:46:00]</a>. LLMs have demonstrated the capability to learn this domain knowledge, which can even enable reasoning in situations without explicit data <a class="yt-timestamp" data-t="03:52:00">[03:52:00]</a>, <a class="yt-timestamp" data-t="04:08:00">[04:08:00]</a>.

This implies that the future of causality must integrate both domain knowledge and causal discovery/reasoning methods <a class="yt-timestamp" data-t="04:26:00">[04:26:00]</a>, <a class="yt-timestamp" data-t="04:30:00">[04:30:00]</a>. LLMs can appear to perform causal reasoning due to their vast learned domain knowledge and powerful generalization abilities <a class="yt-timestamp" data-t="04:15:00">[04:15:00]</a>. This indicates that there are multiple pathways to achieving "causal actions" <a class="yt-timestamp" data-t="09:17:00">[09:17:00]</a>.

One such way is through extensive and diverse data collection <a class="yt-timestamp" data-t="09:44:00">[09:44:00]</a>. Empirical and theoretical evidence suggests that with enough diverse data, models can generalize and learn causal actions even without understanding the underlying causal model <a class="yt-timestamp" data-t="09:57:00">[09:57:00]</a>, <a class="yt-timestamp" data-t="10:06:00">[10:06:00]</a>. Another approach involves building foundational knowledge from domain rules, theorems, and axioms <a class="yt-timestamp" data-t="10:58:00">[10:58:00]</a>, <a class="yt-timestamp" data-t="11:04:00">[11:04:00]</a>. LLMs, having processed vast amounts of text including scientific literature, can apply these rules (e.g., physics theorems) to answer causal questions, mimicking a causal agent <a class="yt-timestamp" data-t="10:21:00">[10:21:00]</a>, <a class="yt-timestamp" data-t="11:12:00">[11:12:00]</a>. This is akin to humans learning from books or observing behavior, speeding up the learning process for valid strategies <a class="yt-timestamp" data-t="11:53:00">[11:53:00]</a>.

## Verification and Evaluation in Causal Models

A significant [[challenges_in_evaluating_causal_models | challenge in causal modeling]] is verification, particularly in the context of agentic frameworks <a class="yt-timestamp" data-t="14:42:00">[14:42:00]</a>. Current agentic frameworks are often applied in controlled environments (e.g., coding, API calls) where clear verifiers exist <a class="yt-timestamp" data-t="14:48:00">[14:48:00]</a>. However, applying them to vague, real-world problems where human feedback is insufficient or unscalable is difficult <a class="yt-timestamp" data-t="15:25:00">[15:25:00]</a>, <a class="yt-timestamp" data-t="15:38:00">[15:38:00]</a>.

The need to create robust verifiers for new domains is crucial, not just for outcomes but also for evaluating individual actions an agent takes <a class="yt-timestamp" data-t="15:42:00">[15:42:00]</a>, <a class="yt-timestamp" data-t="15:50:00">[15:50:00]</a>. For example, in tasks like summarizing a meeting, verification involves assessing precision, recall of important points, and relevance to different stakeholders <a class="yt-timestamp" data-t="16:01:00">[16:01:00]</a>. In scientific discovery, agents might suggest experiments, requiring feedback mechanisms for designing better experiments <a class="yt-timestamp" data-t="16:35:00">[16:35:00]</a>.

### DoWhy and its Refutation Methods

The DoWhy library, a pioneering computational library for causal inference, addresses the verification challenge by offering various refutation methods <a class="yt-timestamp" data-t="17:27:00">[17:27:00]</a>, <a class="yt-timestamp" data-t="18:03:00">[18:03:00]</a>. In causal inference, obtaining estimates from observational data without a randomized control trial makes verification difficult <a class="yt-timestamp" data-t="18:10:00">[18:10:00]</a>. DoWhy's design was inspired by practices in economics and social sciences, where studies undergo rigorous feedback and robustness checks <a class="yt-timestamp" data-t="19:08:00">[19:08:00]</a>, <a class="yt-timestamp" data-t="19:51:00">[19:51:00]</a>.

DoWhy's refutation methods include:
*   **Sensitivity analysis**: Checking what happens if an unobserved confounder is missed <a class="yt-timestamp" data-t="20:05:00">[20:05:00]</a>.
*   **Negative controls**: Inspired by biomedical literature, this involves removing non-causal factors or replacing treatment with a placebo to ensure the effect is due to the intended cause <a class="yt-timestamp" data-t="20:27:00">[20:27:00]</a>, <a class="yt-timestamp" data-t="20:44:00">[20:44:00]</a>.
*   **Dummy outcome**: Creating an outcome variable that is a function of confounders but not of the treatment; a causal effect of zero should be observed <a class="yt-timestamp" data-t="21:31:00">[21:31:00]</a>, <a class="yt-timestamp" data-t="21:54:00">[21:54:00]</a>.
*   **Overlap of treatment across confounding variables**: A test that assesses where estimates can be trusted based on data overlap, contributed by Michael Oust <a class="yt-timestamp" data-t="22:20:00">[22:20:00]</a>, <a class="yt-timestamp" data-t="22:32:00">[22:32:00]</a>.
*   **Graph verification**: More recent additions by collaborators at Amazon focus on verifying the underlying causal graph by comparing its information to a random graph, providing confidence in its informativeness and data support <a class="yt-timestamp" data-t="22:50:00">[22:50:00]</a>, <a class="yt-timestamp" data-t="23:51:00">[23:51:00]</a>.

DoWhy was developed out of a need for a unified tool that simplifies the complex, multi-step process of causal inference, integrating graphical models and potential outcomes frameworks <a class="yt-timestamp" data-t="27:02:00">[27:02:00]</a>. The four core steps in DoWhy reflect this logical progression: modeling the graph, identifying the effect, estimating it, and refuting the estimate <a class="yt-timestamp" data-t="59:13:00">[59:13:00]</a>, <a class="yt-timestamp" data-t="01:00:14">[01:00:14]</a>.

## New Approaches and Challenges in Causal Machine Learning

### Model-Free Causality and Diverse Data

A significant [[challenges_in_causal_discovery_and_uncertainty_quantification | technical challenge]] in causality is to figure out how to extract maximum information from datasets in a model-free manner, similar to model-free reinforcement learning <a class="yt-timestamp" data-t="04:54:00">[04:54:00]</a>, <a class="yt-timestamp" data-t="04:57:00">[04:57:00]</a>. This approach is inherently scalable, but currently lacks sufficient theoretical backing and theorems <a class="yt-timestamp" data-t="04:31:00">[04:31:00]</a>, <a class="yt-timestamp" data-t="04:40:00">[04:40:00]</a>. Developing methods that can extract maximal information from data while still providing guarantees, akin to those from graphical models, would be a major breakthrough <a class="yt-timestamp" data-t="04:46:00">[04:46:00]</a>, <a class="yt-timestamp" data-t="04:56:00">[04:56:00]</a>.

Work by Shchur's group on exchangeable distributions highlights this direction <a class="yt-timestamp" data-t="04:18:00">[04:18:00]</a>, <a class="yt-timestamp" data-t="04:58:00">[04:58:00]</a>. It suggests that while i.i.d. (independent and identically distributed) data offers limited causal discovery, data from many diverse environments (exchangeable data) can significantly improve the ability to learn causal directions <a class="yt-timestamp" data-t="04:24:00">[04:24:00]</a>, <a class="yt-timestamp" data-t="04:28:00">[04:28:00]</a>, <a class="yt-timestamp" data-t="04:49:00">[04:49:00]</a>. This unexplored area could lead to richer identification results <a class="yt-timestamp" data-t="04:58:00">[04:58:00]</a>, <a class="yt-timestamp" data-t="05:27:00">[05:27:00]</a>, <a class="yt-timestamp" data-t="05:30:00">[05:30:00]</a>.

This concept of "still points in a turning world" – identifying static causal mechanisms amidst changing environments – is central to both data augmentation and contrastive learning <a class="yt-timestamp" data-t="05:11:00">[05:11:00]</a>, <a class="yt-timestamp" data-t="05:35:00">[05:35:00]</a>, <a class="yt-timestamp" data-t="05:51:00">[05:51:00]</a>. These methods aim to learn invariant properties without explicit causal models, guided by causal constraints or testable implications <a class="yt-timestamp" data-t="05:57:00">[05:57:00]</a>, <a class="yt-timestamp" data-t="06:09:00">[06:09:00]</a>.

### Integrating LLMs and Axiomatic Training for Causal Understanding

[[advances_in_causal_machine_learning_research | Advances in causal machine learning research]] involve integrating LLMs with causal inference to address bottlenecks, particularly in generating credible causal graphs <a class="yt-timestamp" data-t="32:36:00">[32:36:00]</a>, <a class="yt-timestamp" data-t="33:05:00">[33:05:00]</a>. LLMs can also assist in suggesting relevant refutation tests, such as identifying variables that can serve as dummy outcomes <a class="yt-timestamp" data-t="33:17:00">[33:17:00]</a>, <a class="yt-timestamp" data-t="33:53:00">[33:53:00]</a>.

A key research direction is how causality can enhance the robustness of LLMs. This involves imparting not causal knowledge itself, but rather *how to learn* causal knowledge <a class="yt-timestamp" data-t="38:51:00">[38:51:00]</a>, <a class="yt-timestamp" data-t="38:55:00">[38:55:00]</a>. The idea is to train LLMs on basic causal axioms (e.g., transitivity, d-separation) to enable them to apply and compose these axioms for better generalization <a class="yt-timestamp" data-t="39:06:00">[39:06:00]</a>, <a class="yt-timestamp" data-t="39:56:00">[39:56:00]</a>.

Early results show promising [[challenges_in_causal_machine_learning_compared_to_traditional_methods | advancements in causal machine learning compared to traditional methods]]. Models trained on synthetic axiomatic data for small graphs can generalize to larger graphs, and even improve performance on external causal benchmarks like CottoC <a class="yt-timestamp" data-t="40:13:00">[40:13:00]</a>, <a class="yt-timestamp" data-t="41:12:00">[41:12:00]</a>, <a class="yt-timestamp" data-t="41:42:00">[41:42:00]</a>. This approach, learning causal reasoning from "passive data" or demonstrations of axioms, suggests a different way to impart reasoning without memorization constraints <a class="yt-timestamp" data-t="43:27:00">[43:27:00]</a>, <a class="yt-timestamp" data-t="43:41:00">[43:41:00]</a>. While models might still find statistical shortcuts, the hope is that scaling diverse data can constrain these shortcuts and lead to convergence on causal principles <a class="yt-timestamp" data-t="44:51:00">[44:51:00]</a>, <a class="yt-timestamp" data-t="45:49:00">[45:49:00]</a>.

## Sociological and Practical Challenges in Causal Inference

A broader [[communication_challenges_in_causal_inference | sociological challenge]] is why causality is not more widely adopted <a class="yt-timestamp" data-t="50:52:00">[50:52:00]</a>, <a class="yt-timestamp" data-t="50:57:00">[50:57:00]</a>. Historically, the field has leaned towards principled, model-based methods <a class="yt-timestamp" data-t="51:24:00">[51:24:00]</a>. However, the "bitter lesson" for causality suggests that while causality might be the desired end result, it may not always be the only means to achieve it <a class="yt-timestamp" data-t="51:56:00">[51:56:00]</a>. This implies exploring alternative ways to achieve causal outcomes, which may not initially appear "causal" (e.g., data augmentation) <a class="yt-timestamp" data-t="52:41:00">[52:41:00]</a>, <a class="yt-timestamp" data-t="52:49:00">[52:49:00]</a>.

## Future Directions: Accessibility and Broader Impact

The future vision for causal inference involves creating a seamless causal ecosystem that makes the process accessible to both experts and non-experts <a class="yt-timestamp" data-t="28:22:00">[28:22:00]</a>, <a class="yt-timestamp" data-t="29:56:00">[29:56:00]</a>.

Key directions include:
*   **Unified Libraries**: Consolidating tools like DoWhy (for effect estimation), CausalPy (for root cause analysis), and CausalLearn (for causal discovery) under a common framework, such as the `Py` organization, to streamline the causal analysis pipeline <a class="yt-timestamp" data-t="28:20:00">[28:20:00]</a>, <a class="yt-timestamp" data-t="29:11:00">[29:11:00]</a>.
*   **LLM Integration (PyLLM)**: Developing higher-level abstractions like PyLLM, where natural language inputs can lead to causal analyses, with outputs also in text form <a class="yt-timestamp" data-t="29:33:00">[29:33:00]</a>, <a class="yt-timestamp" data-t="30:36:00">[30:36:00]</a>. This aims to empower citizens to engage with causal questions, such as understanding the impact of interventions in schools <a class="yt-timestamp" data-t="29:58:00">[29:58:00]</a>, <a class="yt-timestamp" data-t="30:20:00">[30:20:00]</a>.
*   **Scientific Acceleration**: Utilizing LLMs to assist scientists by generating literature reviews, constructing graphs from scientific texts, and proposing experiments <a class="yt-timestamp" data-t="01:08:01">[01:08:01]</a>, <a class="yt-timestamp" data-t="01:08:32">[01:08:32]</a>, <a class="yt-timestamp" data-t="01:08:58">[01:08:58]</a>. The goal is to accelerate the scientific cycle by providing tools that help scientists make better use of data and literature <a class="yt-timestamp" data-t="01:08:14">[01:08:14]</a>, <a class="yt-timestamp" data-t="01:08:45">[01:08:45]</a>.
*   **"Shoe Leather" Causality**: Emphasizing that obtaining causal insights often requires "shoe leather" – actively engaging with the real world through experiments and observations, rather than solely relying on existing datasets <a class="yt-timestamp" data-t="01:05:52">[01:05:52]</a>, <a class="yt-timestamp" data-t="01:06:14">[01:06:14]</a>. Tools like DoWhy's refutation tests aim to bring this "shoe leather" into the computational realm by simulating robust checks <a class="yt-timestamp" data-t="01:06:26">[01:06:26]</a>.

This expansive view of causality, moving beyond single studies to enabling broader scientific and societal impact, is a driving force for the community <a class="yt-timestamp" data-t="01:08:53">[01:08:53]</a>. The `Py` organization encourages researchers and practitioners to contribute to its open-source projects, sharing methods, feedback, and real-world use cases to demonstrate the widespread applicability of causality <a class="yt-timestamp" data-t="01:09:19">[01:09:19]</a>, <a class="yt-timestamp" data-t="01:10:47">[01:10:47]</a>.