---
title: Reinforcement learning and causal structures
videoId: relI7Q9A03g
---

From: [[causalpython]] <br/> 

Dr. Andrew Lampinen, a Senior Research Scientist at Google DeepMind, explores the intersection of [[machine_learning_and_causality | causality and machine learning]], particularly focusing on large language models (LLMs) and their ability to acquire causal understanding through different learning strategies <a class="yt-timestamp" data-t="00:41:40">[00:41:40]</a>. His background spans mathematics, physics, and cognitive psychology, providing a unique perspective on the computational and cognitive aspects of intelligence <a class="yt-timestamp" data-t="00:30:30">[00:30:30]</a> <a class="yt-timestamp" data-t="00:37:37">[00:37:37]</a>.

## Passive vs. Active Strategies in LLMs

In a 2023 paper series on large language models and [[machine_learning_and_causality | causality]], Lampinen discusses "active and passive strategies," a terminology chosen to highlight a key distinction from traditional [[machine_learning_and_causal_inference_methodologies | machine learning and causal inference methodologies]] <a class="yt-timestamp" data-t="01:12:00">[01:12:00]</a> <a class="yt-timestamp" data-t="01:30:00">[01:30:00]</a>. While common approaches might label models or training regimes as interventional, observational, or counterfactual, Lampinen's work emphasizes that LLMs, despite being trained passively (processing language data generated by others) <a class="yt-timestamp" data-t="02:12:00">[02:12:00]</a>, can still learn from interventional data <a class="yt-timestamp" data-t="02:29:00">[02:29:00]</a>.

He argues that language data from the internet contains interventional information, such as scientific papers, debugging posts, and even conversations where each statement is an intervention <a class="yt-timestamp" data-t="02:40:00">[02:40:00]</a>. This means LLMs can learn [[causal_discovery_and_learning | causal information]] even when passively absorbing data <a class="yt-timestamp" data-t="03:00:00">[03:00:00]</a>.

### Learning Causal Strategies

The research explores whether this passive training impacts LLM capabilities to generalize beyond their training data <a class="yt-timestamp" data-t="03:05:00">[03:05:00]</a>. Lampinen suggests two reasons:
1.  **Causal Strategies**: Models can discover a strategy for intervening by learning from others' interventions <a class="yt-timestamp" data-t="03:30:00">[03:30:00]</a>. This strategy can then be applied in new situations to discover and utilize new [[causal_discovery_and_learning | causal structures]] for downstream goals <a class="yt-timestamp" data-t="03:39:00">[03:39:00]</a>.
    *   Empirical evidence shows that a generalizable strategy for intervening to determine causal structures can be discovered by passively observing someone else's interventions, which a system can then deploy <a class="yt-timestamp" data-t="03:49:00">[03:49:00]</a>.
2.  **Generalization**: The paper shows that models trained passively on interventional data can generalize to actively intervene and discover novel causal structures at test time <a class="yt-timestamp" data-t="05:20:00">[05:20:00]</a>. This is tested by training a simpler model on a distribution of data showing interventions on causal directed acyclic graphs (DAGs) and then testing it interactively <a class="yt-timestamp" data-t="04:38:00">[04:38:00]</a> <a class="yt-timestamp" data-t="05:40:00">[05:40:00]</a>. The model's approach is closer to correct causal reasoning than heuristic or associational strategies <a class="yt-timestamp" data-t="05:52:00">[05:52:00]</a>.

#### Contradicting "Causal Parrots" Hypothesis

While the "Causal Parrots" paper suggests LLMs learn a "Meta-Structural Causal Model" based on correlations of causal facts, implying they "talk causality but do not reason causally" <a class="yt-timestamp" data-t="06:03:00">[06:03:00]</a>, Lampinen's results partially contradict this <a class="yt-timestamp" data-t="06:52:00">[06:52:00]</a>. He argues that LLMs are capable of discovering and applying causal reasoning algorithms in a generalizable way, especially with effective training regimes <a class="yt-timestamp" data-t="06:57:00">[06:57:00]</a>. When given explanations in the prompt, LLMs can learn to discover new causal structures effectively <a class="yt-timestamp" data-t="07:51:00">[07:51:00]</a>.

## LLMs as Agents in Sequential Decision-Making

Lampinen views LLMs as a type of agent, defined as a system that takes inputs (observations) and produces output actions in a sequential decision-making problem <a class="yt-timestamp" data-t="11:19:00">[11:19:00]</a>. This broad definition includes game-playing engines and systems that interact with users <a class="yt-timestamp" data-t="11:40:00">[11:40:00]</a>.

The key distinction for agents is whether they interact with the environment during training or only at test time <a class="yt-timestamp" data-t="12:02:00">[12:02:00]</a>. Often, agents are trained partly on passive observational data (e.g., expert demonstrations) and then fine-tuned with [[causal_decision_making_and_reinforcement_learning | reinforcement learning]] steps where they actively interact and receive rewards <a class="yt-timestamp" data-t="12:14:00">[12:14:00]</a>. This hybrid training paradigm is similar to how LLMs are first trained on passive human-generated language data and then fine-tuned with [[causal_decision_making_and_reinforcement_learning | reinforcement learning]] based on human preferences <a class="yt-timestamp" data-t="12:38:00">[12:38:00]</a>.

### Improving Autoregressive Models with Interventions

For autoregressive models, which can accumulate errors over long sequences <a class="yt-timestamp" data-t="13:08:00">[13:08:00]</a>, interventions can help decrease error. While current models show some error correction ability (e.g., Chain of Thought prompting) <a class="yt-timestamp" data-t="14:01:00">[14:01:00]</a>, passive learning is less efficient and leads to worse generalization out of distribution <a class="yt-timestamp" data-t="14:40:00">[14:40:00]</a>. Strategies from offline [[causal_decision_making_and_reinforcement_learning | reinforcement learning]], like DAgger, which incorporate interventional data, are important for robust systems <a class="yt-timestamp" data-t="15:06:00">[15:06:00]</a>. Active [[causal_decision_making_and_reinforcement_learning | reinforcement learning]] or supervised tuning on real generations from user interactions can provide this necessary interventional data <a class="yt-timestamp" data-t="15:24:00">[15:24:00]</a>.

### Rethinking Rationality and System Design

Lampinen argues against constraining reasoning processes too strongly in AI systems <a class="yt-timestamp" data-t="44:20:00">[44:20:00]</a>. Over-constraining makes a system fragile and prone to breaking down when faced with unexpected situations <a class="yt-timestamp" data-t="44:28:00">[44:28:00]</a>. Instead, he advocates for systems that learn constraints from data <a class="yt-timestamp" data-t="44:03:00">[44:03:00]</a>.

His approach uses techniques like explanation prediction objectives to encourage systems to represent important information without overly constraining internal computations <a class="yt-timestamp" data-t="44:43:00">[44:44:00]</a>. This "softer" influence allows for greater flexibility and scalability, aligning with "The Bitter Lesson" from Rich Sutton, which suggests that building in assumed "right" ways to solve problems tends to fail at scale <a class="yt-timestamp" data-t="45:40:00">[45:40:00]</a>.

Regarding rationality, Lampinen believes the objective of humans and LLMs is not to be perfectly rational reasoners, but rather to be adaptive to encountered situations <a class="yt-timestamp" data-t="41:21:00">[41:21:00]</a>. This might mean being "less rational" in a strict logical sense but performing better on common daily situations <a class="yt-timestamp" data-t="41:31:00">[41:31:00]</a>. This aligns with concepts of "bounded rationality," where limitations or adaptive biases lead to more effective behavior in real-world contexts <a class="yt-timestamp" data-t="41:59:00">[41:59:00]</a>.

### Bridging the Gap: Lab vs. Reality

Working in controlled lab settings is crucial for understanding system behavior <a class="yt-timestamp" data-t="46:50:00">[46:50:00]</a>. However, the real world acts as a "forcing function" against overfitting <a class="yt-timestamp" data-t="47:01:00">[47:01:00]</a>. Designing an algorithm and simultaneously designing its test can lead to algorithms that only work perfectly for that specific, tailor-made case <a class="yt-timestamp" data-t="47:09:00">[47:09:00]</a>. Testing ideas in the real world ensures they solve actual problems, not just toy ones <a class="yt-timestamp" data-t="47:37:00">[47:37:00]</a>.

## Future of [[Machine Learning and Causality | Causality and Machine Learning]]

Lampinen envisions a future where [[machine_learning_and_causality | causality and machine learning]] converge to create more holistic systems <a class="yt-timestamp" data-t="53:35:00">[53:35:00]</a>. He advocates for systems capable of bridging understanding across multiple levels of abstraction:
*   **Integrating data and causal abstractions**: Treating raw data, causal abstractions, and even counterfactual questions in a homogeneous way <a class="yt-timestamp" data-t="36:08:00">[36:08:00]</a>. This enables reasoning like learning causal strategies <a class="yt-timestamp" data-t="36:26:00">[36:26:00]</a>.
*   **Soft learning with symbolic tools**: He believes that the most general and effective AI approaches will be through "fuzzy learning continuous systems" that use symbolic logic as a tool for specific constraint problems (e.g., in mathematics or physics) <a class="yt-timestamp" data-t="37:19:00">[37:19:00]</a>. This contrasts with viewing symbolic systems as inherently intelligent, seeing them instead as tools that aid an intelligent, continuous reasoning system <a class="yt-timestamp" data-t="38:00:00">[38:00:00]</a>.
*   **Causal Data Fusion**: Combining observational and interventional data to recover [[causal_structure_learning_and_its_challenges_with_hyperparameters | causal structures]] and maximize inference efficiency <a class="yt-timestamp" data-t="16:05:00">[16:05:00]</a>. Systems can use "prior knowledge" from observational sources to efficiently determine which variables are relevant to intervene on <a class="yt-timestamp" data-t="16:42:00">[16:42:00]</a>.

Lampinen's work, particularly his metalearning regime where systems learn "how to learn" by intervening, distinguishes itself from approaches focusing on passive imitation in a single instance <a class="yt-timestamp" data-t="37:33:00">[37:33:00]</a>. He emphasizes that learning *how to intervene* can be acquired passively through observation and then deployed for general [[causal_representation_learning | causal understanding]] in new situations <a class="yt-timestamp" data-t="38:09:00">[38:09:00]</a>.