---
title: AI and humanlike intelligence
videoId: relI7Q9A03g
---

From: [[causalpython]] <br/> 

Dr. Andrew Lampinen, a Senior Research Scientist at Google DeepMind, discusses the capabilities of [[large_language_models_and_intelligence | large language models]] (LLMs) and agents, emphasizing their potential for generalization and their relationship to human intelligence and rationality. He shares insights from his research into [[causality_in_artificial_intelligence | causality in artificial intelligence]] and the nuances of training AI systems for real-world application.

## Passive vs. Active Learning Strategies in LLMs

Dr. Lampinen's recent research, part of a series of papers on [[large_language_models_and_intelligence | large language models]] and [[causality_in_artificial_intelligence | causality]] published in the second half of 2023, introduces the terms "active" and "passive strategies" [01:30:30]. This terminology deviates from traditional [[causal_ai_and_machine_learning | machine learning]] discourse which often uses terms like "interventional," "observational," or "counterfactual" based on Pearl's causal hierarchy [01:43:0].

A key distinction highlighted in his paper is that while [[large_language_models_and_intelligence | language models]] are trained passively—processing language data generated by others from the internet—this data is not necessarily purely observational [02:12:0]. For example, scientific papers, Stack Overflow posts where debugging experiments are discussed, and even everyday conversations, contain "interventional data" [02:24:0]. Each statement in a conversation can be considered an intervention [02:53:0]. This means that even passively trained [[large_language_models_and_intelligence | language models]] can learn from data that is inherently interventional [02:57:0].

This distinction impacts their generalization capabilities [03:05:0]. The paper suggests two reasons why this type of training data could lead to [[causality_in_artificial_intelligence | causal strategies]] or understanding that generalizes beyond the training data [03:19:0]:
1.  **Causal Strategies**: By observing others' interventions, models might discover a strategy for intervening that they can apply in new situations to uncover new causal structures and use them for a downstream goal [03:30:0].
2.  **Generalizability**: The research formally and empirically shows that a generalizable strategy for intervening to determine causal structures can be discovered from purely passively observing someone else's interventions [03:52:0].

To study this, Dr. Lampinen and his team conducted controlled experiments by training a simpler model on a data distribution that explicitly showed interventions on causal directed acyclic graphs (DAGs) [04:29:0]. The model observed a series of interventions on a DAG and a goal, along with interventions to achieve that goal [04:50:0]. The core question was whether a model, passively observing interventions on a set of DAGs (with some causal structures held out from training), could actively intervene at test time to discover and exploit new causal structures [05:14:0].

The results demonstrated that the system, when deployed interactively, was able to apply the [[causality_in_artificial_intelligence | causal strategies]] passively observed during training [05:41:0]. It could actively intervene at test time, discover new causal structures, and exploit them [05:49:0]. Comparisons with heuristic or associational strategies showed that the model approximated correct [[causality_in_artificial_intelligence | causal reasoning]] much more closely than simpler baselines [05:52:0].

### Contradiction or Complement? The "Causal Parrots" Hypothesis
Dr. Lampinen's findings appear to contradict, to some extent, the "Causal Parrots" hypothesis proposed by Matej Zecevic and others, which suggests that [[large_language_models_and_intelligence | causal models]] can "talk [[causality_in_artificial_intelligence | causality]]" but do not "reason causally" [06:05:0]. Dr. Lampinen believes his results suggest models *are* capable of discovering a [[causality_in_artificial_intelligence | causal reasoning]] algorithm applicable in new, generalizable settings, given a good training regime [06:55:0].

He acknowledges that natural language data might contain more correlations and only occasional interventional data, which influences what models learn [07:12:0]. However, he notes that when tested on tasks requiring experimentation to discover causal structures (similar to their simpler experiments), and especially when provided with explanations in the prompt, [[large_language_models_and_intelligence | language models]] can learn to do this effectively, discovering new causal structures not explicitly included in the prompt [07:36:0]. This implies enough interventional data exists in standard training distributions for them to acquire some of these strategies [08:00:0].

## The Role of Explanations and Training Paradigms

Explanations are crucial for structuring the training regime [08:19:0]. Dr. Lampinen suggests that leveraging auxiliary tasks, such as providing natural language explanations of why a reinforcement learning agent receives a reward, and asking the model to predict those explanations, can improve learning [09:01:0]. This can even help shape how a model generalizes out of distribution when data is confounded [09:27:0].

However, understanding the structure of data on the internet is challenging [09:48:0]. While controlled experiments provide insight, applying those findings to the vast and unstructured internet data is complex [09:54:0]. The field of training [[large_language_models_and_intelligence | large language models]] is relatively new, and researchers are just beginning to explore more interesting ways to structure the training process [10:07:0].

One such advancement is conditioning models on a quality signal or reward estimate during training [10:18:0]. This helps to disentangle training, allowing the system to learn from lower-quality data while still being able to generate high-quality responses at test time [10:29:0]. This parallels techniques in offline reinforcement learning, like Decision Transformer or Upside-Down RL, where conditioning on a "goodness" signal allows learning from suboptimal data without replicating it [10:44:0].

## Large Language Models as Agents

Dr. Lampinen considers [[large_language_models_and_intelligence | large language models]] to be a form of "passively trained agents" [11:21:0]. He defines an agent as a system that takes environmental observations as input and produces output actions in a sequential decision-making problem [11:27:0]. This broad definition includes [[large_language_models_and_intelligence | language models]] (taking natural language inputs and producing sequences of language) [11:39:0], as well as chess and Go engines, and video game-playing systems [11:47:0].

A key distinction for agents is whether they interact with the environment during training or only at testing time [12:02:0]. Often, agents are trained partly on passive data (e.g., observing expert Starcraft or Go players) before undergoing reinforcement learning (e.g., playing against other agents and receiving rewards) [12:14:0]. This multi-stage training paradigm is similar to how [[large_language_models_and_intelligence | language models]] are currently developed: initial training on vast passive human-generated text, followed by fine-tuning, and then a reinforcement learning step where a reward model is trained based on human preference for language responses [12:38:0].

Regarding autoregressive models and error accumulation in long sequences (e.g., text or video generation), Dr. Lampinen notes that state-of-the-art [[large_language_models_and_intelligence | language models]] *do* have some ability to error-correct within sequences, as seen in Chain of Thought prompting where models correct their own mistakes [13:58:0]. The architecture and context length of the system affect its ability to reconsider prior context and find errors [14:21:0].

Passive learning is known to be less efficient and can lead to poorer generalization out of distribution [14:42:0]. Systems trained passively may break down when they become active and move off their training data distribution because they haven't encountered such situations before [14:56:0]. Techniques from offline reinforcement learning, like DAGGER, which incorporate interventional data to help systems recover to the data distribution, are crucial for robust systems [15:09:0]. The active reinforcement learning done with [[large_language_models_and_intelligence | language models]] at the end of training, or supervised tuning on real generations from user interaction, serve as forms of interventional data [15:24:0]. However, quantifying the amount of interventional data needed for a robust model remains an open question [15:40:0].

Dr. Lampinen sees parallels between this and [[causal_ai_and_machine_learning | causal inference]] literature on data fusion and mixing observational and interventional data to recover causal structures and maximize inference efficiency [16:05:0]. He suggests that combining observational data could efficiently guide interventions or aid the learning process more generally [17:04:0].

## The Utility and Future of AI Systems

Dr. Lampinen views [[large_language_models_and_intelligence | language models]] as "useful sequential decision-making systems" [17:59:0]. While not yet autonomous, they are valuable when a human is in the loop, assisting with tasks like writing, idea generation, or critiquing thinking [18:03:0]. He is more interested in AI systems as tools to augment humans, particularly in scientific research, rather than as fully autonomous agents [18:19:0].

His work is primarily driven by curiosity and the hope that understanding these systems will lead to improvements that can be deployed downstream in scientific or other applications [19:11:0].

## Interdisciplinary Background and Generalization

Dr. Lampinen's background in physics and mathematics provided strong quantitative and programming foundations, which have been invaluable across his career [20:18:0]. He credits the people he's interacted with, especially supportive colleagues and advisors from diverse backgrounds, as the main resource for his learning [20:47:0].

Regarding whether [[causality_in_artificial_intelligence | causality]] is necessary for generalization, Dr. Lampinen states it's a useful tool [21:24:0]. While he sees mathematics as fundamentally about equivalences without causal direction [21:46:0], for real-world and agentic systems, [[causality_in_artificial_intelligence | causality]] offers crucial intuitions for understanding generalization and predicting system breakdowns [21:54:0].

### Symbolic vs. Fuzzy Systems and Rationality
Dr. Lampinen believes combining representation learning (soft, differentiable) with [[symbolic_reasoning_and_logical_deduction_in_ai | symbolic systems]] (discrete) has a "bright future" [23:07:0]. However, he views [[symbolic_reasoning_and_logical_deduction_in_ai | symbolic systems]] and [[symbolic_reasoning_and_logical_deduction_in_ai | logical reasoning]] not as intelligence itself, but as useful tools *for* an intelligent system to employ [23:33:0]. He points out that humans are not naturally good at [[symbolic_reasoning_and_logical_deduction_in_ai | logical]] or mathematical reasoning without extensive training, hence the development of tools like mathematical proving systems [23:44:0]. He suggests that core intelligence is more akin to a "continuous fuzzy sort of reasoning system," which then uses [[symbolic_reasoning_and_logical_deduction_in_ai | symbolic logic]] for specific constrained problems [24:10:0].

This perspective extends to the concept of rationality in artificial systems. The objective of humans and [[large_language_models_and_intelligence | language models]] is not necessarily to be purely rational reasoners, but to be adaptive to the situations they encounter daily [41:18:0]. Humans and [[large_language_models_and_intelligence | language models]] perform better in logical reasoning problems when the semantic content supports their logical reasoning and worse when it contradicts it [33:51:0]. This suggests that "it's better to be less rational but to do better on the kinds of situations that you tend to encounter every day" [41:31:0]. This aligns with concepts of "bounded rationality," where irrationality is sometimes beneficial given limited resources or to enhance accuracy in common situations [41:59:0].

### The "Bitter Lesson" and Robustness
Over-constraining complex, dynamical systems, like formalizing human reasoning, can lead to fragility [43:02:0]. Dr. Lampinen argues against building fixed constraints into a system; instead, he advocates for systems to learn constraints from data [44:03:0]. If one overly constrains a system's reasoning processes based on assumptions (e.g., it *should* be a formal [[symbolic_reasoning_and_logical_deduction_in_ai | logical Reasoner]]), it becomes fragile and can "totally break down" when it encounters unexpected situations [44:20:0].

He prefers "softer" approaches like explanation prediction objectives that encourage representation of important concepts without overly constraining internal computations [44:40:0]. While strong inductive biases can improve performance for well-defined problems, for large-scale, complex problems like developing better [[large_language_models_and_intelligence | language models]] or agents in rich virtual environments, we don't know the "right solution" [45:01:0]. Attempting to impose strong, specific structures often leads to brittle systems [45:27:0].

This aligns with Rich Sutton's "Bitter Lesson" in machine learning, which posits that attempts to hard-code presumed solutions, while effective at small scales, tend to break down when systems are scaled up [45:40:0]. Therefore, softer solutions like providing explanations or modifying data distribution are often more effective and scalable [46:01:0].

## Bridging Lab and Reality
Dr. Lampinen emphasizes the importance of controlled experimental settings to understand underlying phenomena, even if they are "toy" problems [46:50:0]. However, he views the "real world" as a crucial "forcing function" to prevent overfitting [47:01:0]. Designing an algorithm and simultaneously designing its test can lead to testing it on the "perfect case" for which it works, rather than its generalizability [47:09:0]. Working with real-world problems (e.g., natural images, complex language queries) helps validate ideas and ensure they solve the actual problem, not just a simplified version [47:37:0].

## Lessons from Psychology and Statistical Learning Theory

From his studies in psychology, Dr. Lampinen learned the importance of experimental design and rigorous statistical analysis, an area where [[causal_ai_and_machine_learning | machine learning]] often falls short [48:11:0]. He also highlights the skill of thinking at an abstract level and bridging observations to more abstract models, a key emphasis in computational cognitive science [48:50:0].

Regarding statistical learning theory (SLT), Dr. Lampinen notes that SLT's assumptions can be too strong for modern [[causal_ai_and_machine_learning | machine learning]] systems [49:47:0]. He suggests that phenomena like implicit inductive biases in model architectures or training processes (e.g., gradient descent) mean that models are not as overparameterized as naive SLT might suggest [50:08:0]. This nuance is often not captured in simpler SLT versions that assume uniform capacity distribution [50:38:0].

Dr. Lampinen firmly believes the future is [[causality_in_artificial_intelligence | causal]] because the world is [[causality_in_artificial_intelligence | causal]] [50:50:0]. He encourages the [[causal_ai_and_machine_learning | causal]] community to build "more holistic systems" that bridge understanding across multiple levels, from raw data to [[causality_in_artificial_intelligence | causal abstractions]] and counterfactual questions [59:59:0].