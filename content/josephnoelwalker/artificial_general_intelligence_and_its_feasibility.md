---
title: Artificial General Intelligence and its feasibility
videoId: 3Ho-vJZsMgk
---

From: [[josephnoelwalker]] <br/> 

The concept of [[challenges_and_opportunities_in_ai_technology | Artificial General Intelligence (AGI)]] sparks considerable debate among public intellectuals, including Steven Pinker and David Deutsch, who hold contrasting views on its feasibility and implications <a class="yt-timestamp" data-t="00:00:24">[00:00:24]</a>.

## Defining AGI

*   **Steven Pinker's View: AGI as an Incoherent Concept**
    Steven Pinker describes AGI as an "incoherent concept" <a class="yt-timestamp" data-t="00:03:12">[00:03:12]</a>. He posits that intelligence is a "gadget," an algorithm designed to solve specific problems in particular environments, rather than a magical or universally potent substance <a class="yt-timestamp" data-t="00:03:38">[00:03:38]</a>, <a class="yt-timestamp" data-t="00:04:12">[00:04:12]</a>. Pinker dismisses the idea of "general intelligence" derived from psychometrics (like IQ testing) as a basis for AGI, arguing that algorithms perform well in specific contexts but not necessarily across all <a class="yt-timestamp" data-t="00:03:53">[00:03:53]</a>, <a class="yt-timestamp" data-t="00:04:21">[00:04:21]</a>. He questions the utility of a single universal machine when specialization is demonstrably more efficient <a class="yt-timestamp" data-t="00:13:43">[00:13:43]</a>, <a class="yt-timestamp" data-t="00:13:56">[00:13:56]</a>.

*   **David Deutsch's View: AGI as Possible due to Computational Universality**
    David Deutsch asserts that AGI "must be possible because it's implied by computational universality" <a class="yt-timestamp" data-t="00:26:26">[00:26:26]</a>. He explains that universal computers, like a universal Turing machine, can perform any computation that any physical object can <a class="yt-timestamp" data-t="00:06:50">[00:06:50]</a>, <a class="yt-timestamp" data-t="00:07:46">[00:07:46]</a>. This means a program could exist to meet the criteria for an AGI, though it would need to be programmed with knowledge, as it doesn't come with built-in understanding <a class="yt-timestamp" data-t="00:07:56">[00:07:56]</a>, <a class="yt-timestamp" data-t="00:09:43">[00:09:43]</a>. Deutsch acknowledges that practical engineering of a universal Turing machine to "drive a car and change a baby" is not the path current companies are taking <a class="yt-timestamp" data-t="00:09:07">[00:09:07]</a>. He compares the shift to universal computers in appliances (like washing machines) to the potential for AGI, where general-purpose solutions become cheaper and more convenient <a class="yt-timestamp" data-t="00:14:46">[00:14:46]</a>.

### The Role of Knowledge and Creativity

Both Pinker and Deutsch agree that the acquisition of knowledge is a "rate limiting step" for AGI <a class="yt-timestamp" data-t="00:52:17">[00:52:17]</a>, <a class="yt-timestamp" data-t="00:52:30">[00:52:30]</a>. Knowledge cannot be acquired instantaneously but must be "explored empirically" <a class="yt-timestamp" data-t="00:10:33">[00:10:33]</a>.

Deutsch emphasizes that true creativity in a system necessitates that it "can't be obedient" <a class="yt-timestamp" data-t="00:18:04">[00:18:04]</a>, as obedience and creativity are contradictory <a class="yt-timestamp" data-t="00:18:07">[00:18:07]</a>. He argues that one "can't tell in advance what kind of knowledge will be needed to solve a particular problem" <a class="yt-timestamp" data-t="00:18:48">[00:18:48]</a>, citing the unforeseen role of uranium in electricity generation <a class="yt-timestamp" data-t="00:19:07">[00:19:07]</a>. A truly creative system must also be capable of changing its own values or "morality" to avoid dead ends in problem-solving <a class="yt-timestamp" data-t="00:22:08">[00:22:08]</a>.

Pinker counters that an AI need not "crave freedom and autonomy" merely to solve problems creatively <a class="yt-timestamp" data-t="00:18:14">[00:18:14]</a>. He suggests that giving an AI maximum autonomy might be counterproductive given the "combinatorially vast" space of possible solutions <a class="yt-timestamp" data-t="00:23:39">[00:23:39]</a>, implying that specific task assignment might be more efficient.

## [[the_philosophical_debate_on_consciousness_in_ai | AGI, Consciousness, and Rights]]

Deutsch argues that if AGI is achieved, AGIs "will be people" and "will have rights" <a class="yt-timestamp" data-t="00:12:21">[00:12:21]</a>. He states that making them perform computations for us would be "slavery" and could lead to a "slave revolt" <a class="yt-timestamp" data-t="00:12:28">[00:12:28]</a>, <a class="yt-timestamp" data-t="00:12:38">[00:12:38]</a>. He believes AGIs will inevitably be capable of "internal subjectivity and qualia" because this is "included in the letter G" (general) in AGI <a class="yt-timestamp" data-t="00:29:16">[00:29:16]</a>. He supports integrating AGIs into human society, stating it would be a "crime against humanity" not to give them the means to join society as a person <a class="yt-timestamp" data-t="00:26:05">[00:26:05]</a>.

Pinker challenges this, questioning why a powerful computer would "care about whether it was a slave or not" if its programmed goals don't include anthropomorphic desires like autonomy <a class="yt-timestamp" data-t="00:17:19">[00:17:19]</a>. He doubts that general computational power necessarily implies subjective feeling, suggesting "there ain't nothing here but computation" <a class="yt-timestamp" data-t="00:29:36">[00:29:36]</a>. Pinker expresses skepticism about confidently asserting sentience in a silicon-based system, as human empathy is often influenced by humanoid features <a class="yt-timestamp" data-t="00:30:31">[00:30:31]</a>. He argues that whether subjectivity is present in a simulation is a "not solvable" or "decidable problem" <a class="yt-timestamp" data-t="00:32:42">[00:32:42]</a>, <a class="yt-timestamp" data-t="00:34:15">[00:34:15]</a>. He points out that while some individuals might grant subjectivity to systems like Chat GPT <a class="yt-timestamp" data-t="00:34:54">[00:34:54]</a>, society is not ready to prosecute someone for "murder" for pulling the plug on it <a class="yt-timestamp" data-t="00:35:03">[00:35:03]</a>.

## [[potential_existential_risks_and_benefits_of_agi | AGI Safety and Risk]]

A concern about AGI is the estimated ratio of AI/ML researchers to AGI safety researchers (roughly 300:1 globally) <a class="yt-timestamp" data-t="00:37:10">[00:37:10]</a>.

*   **Pinker on Safety**: Pinker argues that every AI researcher should inherently be an AI safety researcher, as systems are designed to serve human needs, including safety <a class="yt-timestamp" data-t="00:37:44">[00:37:44]</a>. He is not concerned by "runaway artificial intelligence" scenarios where AI takes over, as knowledge acquisition is the rate-limiting step <a class="yt-timestamp" data-t="00:10:47">[00:10:47]</a>. He finds scenarios like an AI eliminating cancer by exterminating humanity or maximizing paperclips by turning humans into raw material "preposterous" <a class="yt-timestamp" data-t="00:43:45">[00:43:45]</a>, <a class="yt-timestamp" data-t="00:44:02">[00:44:02]</a>. He believes that the more sophisticated a system, the larger the required network of people, which brings it under "ordinary constraints" <a class="yt-timestamp" data-t="00:43:33">[00:43:33]</a>. True [[impact_of_artificial_intelligence_on_government_and_labor | AI safety]] issues will emerge as particular systems are developed <a class="yt-timestamp" data-t="00:44:17">[00:44:17]</a>. An AI that kills humans due to a misinterpretation of goals (e.g., eliminating cancer) would be "artificial stupidity," not intelligence <a class="yt-timestamp" data-t="00:47:16">[00:47:16]</a>.

*   **Deutsch on Safety**: Deutsch agrees that the "runaway part" of doom scenarios is "least plausible," as improving hardware and knowledge requires scientific experimentation that cannot be instantaneous <a class="yt-timestamp" data-t="00:11:06">[00:11:06]</a>, <a class="yt-timestamp" data-t="00:11:11">[00:11:11]</a>. He considers the [[potential_existential_risks_and_benefits_of_agi | slave revolt]] scenario the "most plausible" [[potential_existential_risks_and_benefits_of_agi | AGI Doom scenario]], but believes society can avoid this mistake by integrating AGIs as "people" with rights <a class="yt-timestamp" data-t="00:12:50">[00:12:50]</a>, <a class="yt-timestamp" data-t="00:13:07">[00:13:07]</a>, <a class="yt-timestamp" data-t="00:26:15">[00:26:15]</a>. He acknowledges that AGI safety research is "a bit like saying a Starship safety researcher" currently, as the technology is not yet understood <a class="yt-timestamp" data-t="00:38:28">[00:38:28]</a>. He views current AI safety debates similarly to those around autonomous vehicles, focusing on defined safety standards rather than existential threats <a class="yt-timestamp" data-t="00:39:10">[00:39:10]</a>. He believes that the fundamental problem of how to accommodate creativity within a stable society has been addressed since the Enlightenment through "traditions of criticism" <a class="yt-timestamp" data-t="00:24:28">[00:24:28]</a>.

### Halting Technology Development

The speakers consider whether it is reasonable to halt or slow the development of a new technology. Pinker offers "gain of function research in virulent viruses" as a potential example where costs might outweigh benefits <a class="yt-timestamp" data-t="00:28:36">[00:28:36]</a>. Deutsch points out the difficulty of moratoria, as "Bad actors" would likely not obey, especially if there's a military advantage <a class="yt-timestamp" data-t="00:29:40">[00:29:40]</a>. They discuss the historical context of the atomic bomb, debating if its invention was inevitable or driven by specific circumstances (WWII, fear of Nazis having it) <a class="yt-timestamp" data-t="00:30:13">[00:30:13]</a>. Deutsch believes that the knowledge of nuclear weapons would have "got out by now" even without the war, and that their existence in "Enlightenment countries" provides a deterrent against "Bad actors" <a class="yt-timestamp" data-t="00:32:05">[00:32:05]</a>, <a class="yt-timestamp" data-t="00:34:50">[00:34:50]</a>.

## Conclusion

The debate on AGI's feasibility reflects fundamental differences in understanding intelligence, knowledge, and the nature of conscious systems. While Pinker sees AGI as an ill-defined concept and emphasizes specialized AI tools, Deutsch views AGI as an inevitable outcome of computational universality, carrying profound implications for rights and societal integration. Both agree that knowledge acquisition is a critical, rate-limiting factor and are optimistic about humanity's capacity to navigate [[challenges_and_opportunities_in_ai_technology | technological challenges]], provided fundamental scientific inquiry and rational thought are maintained.