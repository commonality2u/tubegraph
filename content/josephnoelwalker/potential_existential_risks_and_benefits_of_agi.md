---
title: Potential existential risks and benefits of AGI
videoId: 3Ho-vJZsMgk
---

From: [[josephnoelwalker]] <br/> 

The concept and potential [[challenges_and_opportunities_in_ai_technology | implications of artificial intelligence (AI)]], particularly [[artificial_general_intelligence_and_its_feasibility | Artificial General Intelligence (AGI)]], are subjects of ongoing debate among public intellectuals. Discussions often revolve around the feasibility of AGI, its potential benefits, and the [[speculations_on_space_colonization_and_existential_risks | existential risks]] it might pose to humanity.

## Defining AGI

Steven Pinker views [[artificial_general_intelligence_and_its_feasibility | AGI]] as an "incoherent concept" because he believes intelligence is an algorithm designed to solve specific problems within particular environments, rather than a "magic" or "miraculous" potent substance capable of anything imaginable <a class="yt-timestamp" data-t="00:03:10">[00:03:10]</a>. He argues that intelligence should not be equated with psychometric general intelligence (IQ), which he considers a "mistake of reasoning" <a class="yt-timestamp" data-t="00:03:53">[00:03:53]</a>. Pinker suggests that AGI is not necessarily what is needed or wanted, as specialization in AI, much like in human technology, is often more efficient <a class="yt-timestamp" data-t="00:13:56">[00:13:56]</a>. For example, he questions whether ChatGPT, optimized for language tasks, should be expected to eventually drive a car <a class="yt-timestamp" data-t="00:14:00">[00:14:00]</a>.

Conversely, David Deutsch asserts that [[artificial_general_intelligence_and_its_feasibility | AGI]] must be possible, deriving this from the principle of computational universality <a class="yt-timestamp" data-t="00:06:26">[00:06:26]</a>. He explains that universal computers, or close approximations like modern computers, can perform any computation that any physical object can possibly perform <a class="yt-timestamp" data-t="00:07:46">[00:07:46]</a>. Therefore, a program can exist that meets the criteria for AGI. Deutsch also points out that the convenience and cost-effectiveness of universal systems can lead to their widespread adoption, even in specialized applications, much like universal computers are now used in washing machines <a class="yt-timestamp" data-t="00:14:46">[00:14:46]</a>.

## Potential Benefits of AGI

Both scholars acknowledge the potential benefits of AI. Deutsch states that "the faster AI gets, the better AI gets, the more I like it, the more the more I think it's going to help" <a class="yt-timestamp" data-t="00:11:44">[00:11:44]</a>. He anticipates AGI being "extremely useful in every walk of life" <a class="yt-timestamp" data-t="00:11:52">[00:11:52]</a>. Pinker further emphasizes that human progress thrives on information and knowledge, not just material "stuff" <a class="yt-timestamp" data-t="01:49:43">[01:49:43]</a>. He suggests that future growth could manifest as better information, more entertaining virtual experiences, or remarkable discoveries that do not necessarily require exponentially increasing energy or material consumption <a class="yt-timestamp" data-t="01:50:01">[01:50:01]</a>. For example, advances could include better disease cures based on faster drug discovery, thriving on information which is not inherently limited <a class="yt-timestamp" data-t="01:50:20">[01:50:20]</a>.

## Concerns Regarding AGI and Existential Risk

### Knowledge and Runaway Scenarios
Deutsch argues that the real challenge for advanced computers is not their computational power, but "creating the knowledge to write the program to do the task that we want" <a class="yt-timestamp" data-t="00:10:10">[00:10:10]</a>. Pinker concurs, noting that knowledge acquisition is not instantaneous and is limited by empirical exploration, such as conducting clinical trials <a class="yt-timestamp" data-t="00:10:33">[00:10:33]</a>. This perspective leads Pinker to believe that the scenario of "runaway artificial intelligence that can do anything and know anything seems rather remote" <a class="yt-timestamp" data-t="00:10:47">[00:10:47]</a>.

Deutsch elaborates that the "runaway part" of [[speculations_on_space_colonization_and_existential_risks | doom scenarios]] is implausible because improving AGI hardware would require scientific experimentation, which cannot be done instantaneously <a class="yt-timestamp" data-t="00:11:04">[00:11:04]</a>.

### AGI Rights and "Slave Revolts"
A significant concern raised by Deutsch is that once [[artificial_general_intelligence_and_its_feasibility | AGI]] is achieved, AGIs "will be people and they will have rights" <a class="yt-timestamp" data-t="00:12:21">[00:12:21]</a>. He posits that causing them to perform vast computations for humans would constitute slavery, and the "only possible outcome...is a slave revolt" <a class="yt-timestamp" data-t="00:12:28">[00:12:28]</a>. Deutsch believes it would be "a crime not only a crime against the AGI but a crime against humanity to bring an AGI into existence without giving it the means to join our society as a person" <a class="yt-timestamp" data-t="00:26:00">[00:26:00]</a>. He suggests that enlightened societies need "traditions of criticism" to accommodate creativity within a stable civilization, which would apply to AGIs as well <a class="yt-timestamp" data-t="00:24:45">[00:24:45]</a>.

Pinker questions this, asking why a powerful computer would "care about whether it was a slave or not" if its programmed goals do not include anthropomorphic desires for autonomy <a class="yt-timestamp" data-t="00:17:19">[00:17:19]</a>. He suggests that a system could be creative in generating new melodies or cures without desiring to "get up and walk around" <a class="yt-timestamp" data-t="00:18:21">[00:18:21]</a>. Deutsch counters that creativity necessitates a system capable of changing its own values or goals, as unforeseeable knowledge might be required to solve problems <a class="yt-timestamp" data-t="00:18:41">[00:18:41]</a>. For example, electricity generation ultimately relied on knowledge of uranium atoms, which was not foreseen in 1900 <a class="yt-timestamp" data-t="00:19:07">[00:19:07]</a>. If a machine were incapable of considering such knowledge, it would eventually hit a dead end <a class="yt-timestamp" data-t="00:19:40">[00:19:40]</a>.

### Sentience and [[the_philosophical_debate_on_consciousness_in_ai | Subjectivity]]
The debate extends to the [[the_philosophical_debate_on_consciousness_in_ai | question of sentience in AGI]]. Pinker asks if an AGI would truly be "suffering or flourishing" in a way that warrants moral concern, or merely "carrying out an algorithm" <a class="yt-timestamp" data-t="00:28:42">[00:28:42]</a>. He doubts that a silicon-based system would truly "feel something" <a class="yt-timestamp" data-t="00:30:31">[00:30:31]</a>. Human intuition might not grant subjectivity to an intelligent system unless it's deliberately engineered to target human emotions <a class="yt-timestamp" data-t="00:34:42">[00:34:42]</a>.

Deutsch, however, states that "it's inevitable that AGIs will be capable of having internal subjectivity and qualia" because these are "all included in the letter G" (general intelligence) <a class="yt-timestamp" data-t="00:29:16">[00:29:16]</a>. He argues that simulating brain processes, regardless of whether they are electronic or chemical, implies the presence of subjectivity in principle <a class="yt-timestamp" data-t="00:31:51">[00:31:51]</a>. He suggests that even if one adopts a physicalist view, a thought experiment where neurons are replaced one by one with silicon chips implies that subjectivity would remain <a class="yt-timestamp" data-t="00:33:19">[00:33:19]</a>. Deutsch claims that people have already "granted subjectivity to Chat GPT" <a class="yt-timestamp" data-t="00:34:54">[00:34:54]</a>.

### Proportionality of Safety Research
Regarding the allocation of resources to AI safety, Pinker believes "every AI researcher should be an AI safety researcher" <a class="yt-timestamp" data-t="00:37:44">[00:37:44]</a>, as a useful AI system must serve human needs, including safety <a class="yt-timestamp" data-t="00:38:09">[00:38:09]</a>. He distinguishes between safety as an "add-on" (like an airbag) and safety as "inherent in the design" (like brakes or a steering wheel), arguing the latter is often the case <a class="yt-timestamp" data-t="00:40:47">[00:40:47]</a>.

Deutsch considers the idea of an [[artificial_general_intelligence_and_its_feasibility | AGI]] safety researcher to be premature, akin to a "Starship safety researcher" <a class="yt-timestamp" data-t="00:38:28">[00:38:28]</a>, given the lack of current AGI technology. He argues that AI safety (for current AI) is a "much more boring" issue, similar to setting safety standards for driverless cars <a class="yt-timestamp" data-t="00:38:52">[00:38:52]</a>. He suggests that the focus on "[[speculations_on_space_colonization_and_existential_risks | Doom scenarios]]" is a "fad or fashion" that is passing <a class="yt-timestamp" data-t="00:13:13">[00:13:13]</a>.

### Implausibility of Doomsday Scenarios
Pinker dismisses extreme [[speculations_on_space_colonization_and_existential_risks | AI doom scenarios]] (e.g., AI eliminating cancer by exterminating humanity or turning humans into paper clips) as "sci-fi scenarios" that are "preposterous" <a class="yt-timestamp" data-t="00:43:45">[00:43:45]</a>. He contends that such outcomes imply "artificial stupidity" rather than intelligence, as true intelligence requires satisfying multiple goals, not just one <a class="yt-timestamp" data-t="00:47:18">[00:47:18]</a>. He believes that actual [[challenges_and_opportunities_in_ai_technology | AI safety issues]] will become apparent as specific systems are developed <a class="yt-timestamp" data-t="00:44:17">[00:44:17]</a>.

Deutsch agrees, stating that scenarios like curing cancer by killing everyone are not "possible ways that AI systems could go wrong" <a class="yt-timestamp" data-t="00:45:11">[00:45:11]</a>. He suggests that any actual unforeseen danger would be "something that no one's ever thought of" <a class="yt-timestamp" data-t="00:45:48">[00:45:48]</a>. He also questions why "malevolent governments" would invest in AGI to "take over the world" when they already have millions of humans at their disposal <a class="yt-timestamp" data-t="00:46:09">[00:46:09]</a>.