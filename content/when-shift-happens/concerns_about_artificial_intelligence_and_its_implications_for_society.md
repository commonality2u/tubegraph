---
title: Concerns about artificial intelligence and its implications for society
videoId: 0P5A9CtcL08
---

From: [[when-shift-happens]] <br/> 

Concerns surrounding [[impact_of_ai_on_human_society | Artificial Intelligence]] (AI) and its potential implications for society have been a subject of discussion among experts for many years, intensifying with recent rapid advancements <a class="yt-timestamp" data-t="00:18:24">[00:18:24]</a>.

## Categories of Concern

### Job Displacement and Economic Inequality
A significant concern revolves around the potential for widespread job displacement as AI technology becomes increasingly capable <a class="yt-timestamp" data-t="00:37:39">[00:37:39]</a>. While some believe that technology's advancement could make human interaction less relevant, others express worry that many individuals could be left jobless <a class="yt-timestamp" data-t="00:34:33">[00:34:33]</a>.

AI's ability to serve as a near-perfect substitute for labor could lead to a significant decrease in the value of human labor <a class="yt-timestamp" data-t="00:32:32">[00:32:32]</a>. This could exacerbate [[the_implications_of_ai_agents_on_traditional_market_dynamics | economic inequalities]], creating a larger gap between those with capital and those who rely on their labor <a class="yt-timestamp" data-t="00:35:46">[00:35:46]</a>. Although AI might raise living standards by making many goods and services cheaper, the potential for increased inequality remains a concern <a class="yt-timestamp" data-t="00:38:50">[00:38:50]</a>.

### Loss of Human Control and Existential Risk
A more profound concern is the possibility of AI systems achieving intelligence levels where human beings are comparatively insignificant <a class="yt-timestamp" data-t="00:27:08">[00:27:08]</a>. This could lead to a future where humans lose control over the trajectory of humanity, with the future being determined by AI systems <a class="yt-timestamp" data-t="00:29:28">[00:29:28]</a>. The growth curve of AI progress suggests that such a risk could become significant within three to five years <a class="yt-timestamp" data-t="00:46:17">[00:46:17]</a>.

This existential risk has been a topic of consideration since at least 2009-2010 <a class="yt-timestamp" data-t="00:30:17">[00:30:17]</a>. Notable figures in the AI community, including Yoshua Bengio and Geoffrey Hinton (two of the three Turing Award winners for deep learning), have voiced their concerns about threats to humanity from AI <a class="yt-timestamp" data-t="00:31:04">[00:31:04]</a>. However, a lack of concrete plans for building powerful yet safe AI systems remains a challenge <a class="yt-timestamp" data-t="00:34:25">[00:34:25]</a>.

## Prevention and Regulation
The primary challenge in addressing [[challenges_and_opportunities_in_ai_development | AI existential risk]] is convincing people that a problem exists <a class="yt-timestamp" data-t="00:32:00">[00:32:00]</a>. Public understanding and advocacy are considered crucial for mitigating these risks <a class="yt-timestamp" data-t="00:34:33">[00:34:33]</a>.

The ability of governments to regulate AI is also questioned, given their perceived lack of understanding of simpler technologies like social media <a class="yt-timestamp" data-t="00:30:08">[00:30:08]</a>. Private companies, with their focus on short-term profits, might also prioritize immediate gains over long-term consequences <a class="yt-timestamp" data-t="00:31:00">[00:31:00]</a>.

Despite these challenges, the short-term nature of AI risks (5-10 years) means that everyone is incentivized to find solutions, unlike issues like global warming where the benefits of inaction might appear to outweigh the long-term costs <a class="yt-timestamp" data-t="00:39:10">[00:39:10]</a>.

## The Technological Singularity
The concept of a "technological singularity" describes a point where technological progress accelerates so rapidly that it fundamentally transforms society and the universe in a very short period <a class="yt-timestamp" data-t="00:41:00">[00:41:00]</a>. Historically, technological progress has been slow, but in recent decades, it has accelerated significantly <a class="yt-timestamp" data-t="00:42:00">[00:42:00]</a>. A positive singularity would align with human values, leading to a thriving and generally happy human existence <a class="yt-timestamp" data-t="00:42:00">[00:42:00]</a>.

However, even a positive singularity presents philosophical questions. Once technology can accomplish anything, the pursuit of fulfillment through achievement may become obsolete <a class="yt-timestamp" data-t="00:43:00">[00:43:00]</a>. In such a future, where building and accomplishing things lose their meaning, individuals might have to find purpose solely in experiences for their own sake <a class="yt-timestamp" data-t="00:44:00">[00:44:00]</a>.

## Philosophical Implications
The implications of advanced AI touch upon fundamental questions about the meaning of life <a class="yt-timestamp" data-t="00:46:00">[00:46:00]</a>. Humanity seeks neither purely internal experiences (like a hallucination machine) nor purely external states of the world, but rather a real experience of external goals being achieved <a class="yt-timestamp" data-t="00:47:00">[00:47:00]</a>. This creates paradoxes that make fundamental human satisfaction difficult to achieve <a class="yt-timestamp" data-t="00:48:00">[00:48:00]</a>.

The discussion also extends to human morality. Moral philosophy suggests that morality is a type of preference, possibly shaped by evolution, rather than an external, objective system <a class="yt-timestamp" data-t="00:59:55">[00:59:55]</a>. While unethical behavior can occur in business, the idea that it's always an advantage to be immoral is unlikely, and those who act unethically often hurt themselves in the long run through damaged reputation and other consequences <a class="yt-timestamp" data-t="01:02:47">[01:02:47]</a>. The preference not to betray, for example, is evolutionarily stable, as it prevents cognitive overload and maintains a good reputation <a class="yt-timestamp" data-t="01:04:11">[01:04:11]</a>.