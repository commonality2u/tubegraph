---
title: nuclear arms race and cold war strategies
videoId: mScpHTIi-kM
---

From: [[veritasium]] <br/> 

The [[game theory and the prisoners dilemma | prisoner's dilemma]], a foundational problem in [[game theory and the prisoners dilemma | game theory]], provides insights into various conflicts, from interpersonal disputes to international relations, including the [[historical_context_of_world_war_ii_and_nuclear_arms_race | nuclear arms race]] <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a>. Understanding the optimal strategy in such scenarios can influence outcomes of global significance <a class="yt-timestamp" data-t="00:00:13">[00:00:13]</a>.

## The Dawn of the Nuclear Arms Race

On September 3, 1949, an American weather monitoring plane detected radioactive material in air samples collected over Japan <a class="yt-timestamp" data-t="00:00:34">[00:00:34]</a>. Subsequent testing of rainwater samples globally confirmed the presence of Cerium-141 and Yttrium-91, isotopes with short half-lives, indicating a recent [[development of the atomic bomb | nuclear explosion]] <a class="yt-timestamp" data-t="00:00:46">[00:00:46]</a>. Since the US had not conducted any tests that year, the inescapable conclusion was that the Soviet Union had successfully developed a [[development of the atomic bomb | nuclear bomb]] <a class="yt-timestamp" data-t="00:01:09">[00:01:09]</a>.

This revelation was met with dread in the United States, as it marked the rapid erosion of their military supremacy, which had been established through the [[manhattan_project_and_los_alamos_laboratory | Manhattan Project]] <a class="yt-timestamp" data-t="00:01:20">[00:01:20]</a>. Some officials, like Navy Secretary Matthews, advocated for a preemptive [[ethical_implications_and_aftermath_of_nuclear_weapons | nuclear strike]] against the Soviets while the US still held an advantage, arguing to become "aggressors for peace" <a class="yt-timestamp" data-t="00:01:37">[00:01:37]</a>. John von Neumann, a founder of [[game theory and the prisoners dilemma | game theory]], also suggested immediate action: "If you say why not bomb them tomorrow, I say, why not bomb them today? If you say today at five o'clock, I say why not at one o'clock?" <a class="yt-timestamp" data-t="00:01:51">[00:01:51]</a>.

## [[game theory and the prisoners dilemma | Game Theory]] and the Cold War Conflict

In 1950, the RAND Corporation, a US-based think tank, began studying the issue of [[ethical_implications_and_aftermath_of_nuclear_weapons | nuclear weapons]], turning to [[game theory and the prisoners dilemma | game theory]] for insights <a class="yt-timestamp" data-t="00:02:11">[00:02:11]</a>. During this research, two mathematicians at RAND developed a new game, later known as the [[game theory and the prisoners dilemma | prisoner's dilemma]], which strikingly resembled the US-Soviet conflict <a class="yt-timestamp" data-t="00:02:21">[00:02:21]</a>.

### The Prisoner's Dilemma Explained

In the [[game theory and the prisoners dilemma | prisoner's dilemma]], players choose between two actions: "cooperate" or "defect" <a class="yt-timestamp" data-t="00:02:42">[00:02:42]</a>.
*   If both cooperate, they each receive a moderate reward (e.g., three coins) <a class="yt-timestamp" data-t="00:02:46">[00:02:46]</a>.
*   If one cooperates and the other defects, the defector receives a large reward (e.g., five coins), and the cooperater gets nothing <a class="yt-timestamp" data-t="00:02:50">[00:02:50]</a>.
*   If both defect, they each receive a small reward (e.g., one coin) <a class="yt-timestamp" data-t="00:02:57">[00:02:57]</a>.

A rational player, regardless of the opponent's choice, will always find it advantageous to defect <a class="yt-timestamp" data-t="00:03:09">[00:03:09]</a>. If the opponent cooperates, defecting yields five coins instead of three <a class="yt-timestamp" data-t="00:03:09">[00:03:09]</a>. If the opponent defects, defecting yields one coin instead of zero <a class="yt-timestamp" data-t="00:03:24">[00:03:24]</a>. Consequently, if both players are rational, they both defect, resulting in a suboptimal outcome where they each get one coin, rather than the three they could have received through mutual cooperation <a class="yt-timestamp" data-t="00:03:44">[00:03:44]</a>.

### Application to the Arms Race

This dilemma mirrored the US-Soviet situation, leading both nations to amass vast [[historical_context_of_world_war_ii_and_nuclear_arms_race | nuclear arsenals]] of tens of thousands of weapons, capable of destroying each other many times over <a class="yt-timestamp" data-t="00:03:54">[00:03:54]</a>. While this massive build-up created a deterrent (neither could use them without assured destruction), both countries collectively spent approximately $10 trillion on these weapons <a class="yt-timestamp" data-t="00:04:07">[00:04:07]</a>. They would have been better off cooperating by agreeing not to develop the technology further, but acting in their individual best interests led to a worse outcome for both <a class="yt-timestamp" data-t="00:04:16">[00:04:16]</a>.

## Robert Axelrod's Tournaments and Winning Strategies

In 1980, political scientist Robert Axelrod conducted a computer tournament to determine the best strategy in a *repeated* [[game theory and the prisoners dilemma | prisoner's dilemma]] <a class="yt-timestamp" data-t="00:06:18">[00:06:18]</a>. Unlike a single interaction, repeated interactions allow for consequences in future rounds <a class="yt-timestamp" data-t="00:06:02">[00:06:02]</a>. Game theorists submitted computer programs, or "strategies," to play against each other over 200 rounds <a class="yt-timestamp" data-t="00:06:26">[00:06:26]</a>.

### First Tournament Results

The surprising winner of the first tournament was the simplest program: "Tit for Tat" <a class="yt-timestamp" data-t="00:08:21">[00:08:21]</a>.
*   **Tit for Tat**: Starts by cooperating, then simply copies the opponent's last move <a class="yt-timestamp" data-t="00:08:28">[00:08:28]</a>. It retaliates for defection but returns to cooperation if the opponent does <a class="yt-timestamp" data-t="00:08:35">[00:08:35]</a>.
*   **Friedman**: Cooperates initially but defects permanently if the opponent defects even once <a class="yt-timestamp" data-t="00:07:32">[00:07:32]</a>.
*   **Joss**: Copies the opponent's last move but defects randomly 10% of the time <a class="yt-timestamp" data-t="00:07:42">[00:07:42]</a>.
*   **Graaskamp**: Similar to Joss but defects on the 50th round to probe <a class="yt-timestamp" data-t="00:07:55">[00:07:55]</a>.

Axelrod identified two key qualities of the top-performing strategies:
1.  **Nice**: Strategies that are never the first to defect <a class="yt-timestamp" data-t="00:10:15">[00:10:15]</a>. Eight out of 15 strategies were nice, and the top eight performers were all nice <a class="yt-timestamp" data-t="00:10:32">[00:10:32]</a>.
2.  **Forgiving**: Strategies that retaliate but do not hold a grudge, returning to cooperation if the opponent does <a class="yt-timestamp" data-t="00:10:46">[00:10:46]</a>. Tit for Tat is forgiving, while Friedman is maximally unforgiving <a class="yt-timestamp" data-t="00:10:54">[00:10:54]</a>.

The finding that "nice guys finished first" challenged many experts who tried to implement complex, sneaky strategies <a class="yt-timestamp" data-t="00:11:23">[00:11:23]</a>. Axelrod later found that a slightly more forgiving strategy, "Tit for Two Tats" (which defects only after two consecutive opponent defections), would have won the first tournament had it been submitted <a class="yt-timestamp" data-t="00:11:44">[00:11:44]</a>.

### Second Tournament and Additional Qualities

A second tournament was held, where the number of rounds was not fixed (an average of 200, but randomized to prevent end-game defection) <a class="yt-timestamp" data-t="00:12:28">[00:12:28]</a>. Contestants knew the results of the first tournament <a class="yt-timestamp" data-t="00:12:27">[00:12:27]</a>. Despite some attempting to exploit anticipated niceness, Tit for Tat won again <a class="yt-timestamp" data-t="00:14:16">[00:14:16]</a>.

Axelrod identified two more qualities for successful strategies:
3.  **Retaliatory (or Provokable)**: Strategies that immediately strike back if the opponent defects, avoiding being a "pushover" <a class="yt-timestamp" data-t="00:14:38">[00:14:38]</a>. Tit for Tat is very hard to take advantage of <a class="yt-timestamp" data-t="00:14:52">[00:14:52]</a>.
4.  **Clear**: Strategies that are transparent and easy for opponents to understand and predict, fostering trust <a class="yt-timestamp" data-t="00:14:57">[00:14:57]</a>. Overly complicated or random strategies prevent the establishment of trust <a class="yt-timestamp" data-t="00:15:01">[00:15:01]</a>.

These four principles—being nice, forgiving, provokable, and clear—resemble an "eye for an eye" morality <a class="yt-timestamp" data-t="00:15:31">[00:15:31]</a>.

### Ecological Simulation and the Emergence of Cooperation

Axelrod demonstrated that there is no single "best" strategy, as performance depends on the environment of other strategies <a class="yt-timestamp" data-t="00:16:03">[00:16:03]</a>. He ran a simulation where successful strategies multiplied and unsuccessful ones declined, akin to ecological evolution <a class="yt-timestamp" data-t="00:16:35">[00:16:35]</a>. In this simulation, nasty strategies initially grew but then dwindled as the exploitable strategies went extinct <a class="yt-timestamp" data-t="00:16:51">[00:16:51]</a>. After 1,000 generations, only nice strategies survived, with Tit for Tat remaining dominant <a class="yt-timestamp" data-t="00:17:11">[00:17:11]</a>.

This simulation showed that even in a harsh environment populated by defectors, a small "cluster of tit-for-tat players" could emerge, cooperate, and eventually take over the population, demonstrating how cooperation can arise among self-interested entities without altruism <a class="yt-timestamp" data-t="00:17:39">[00:17:39]</a>. This concept suggests how cooperation might have emerged and flourished in nature, from impalas grooming each other to fish cleaning sharks <a class="yt-timestamp" data-t="00:18:38">[00:18:38]</a>. The strategy can be encoded in DNA, allowing successful cooperative behaviors to spread if they outperform others <a class="yt-timestamp" data-t="00:19:11">[00:19:11]</a>.

## The Impact of Noise and Error

Axelrod's original tournaments did not account for "noise" or random error <a class="yt-timestamp" data-t="00:19:36">[00:19:36]</a>. In real-world scenarios, an intended cooperation might be perceived as a defection <a class="yt-timestamp" data-t="00:19:42">[00:19:42]</a>. For example, in 1983, a Soviet early warning system mistakenly detected a US missile launch due to sunlight reflecting off clouds <a class="yt-timestamp" data-t="00:19:50">[00:19:50]</a>. This highlights the severe costs of signal errors in contexts like the Cold War, where such mistakes could lead to global annihilation <a class="yt-timestamp" data-t="00:20:07">[00:20:07]</a>.

In a noisy environment, Tit for Tat playing against itself can lead to a chain of alternating retaliations if a single cooperation is misperceived as a defection <a class="yt-timestamp" data-t="00:20:47">[00:20:47]</a>. This can quickly degrade into constant mutual defection, drastically reducing payoffs <a class="yt-timestamp" data-t="00:21:02">[00:21:02]</a>. To counter this, a strategy needs a reliable way to break out of these echo effects <a class="yt-timestamp" data-t="00:21:21">[00:21:21]</a>. One solution is "Generous Tit for Tat," which plays Tit for Tat but includes about 10% more forgiveness, retaliating only nine out of ten times <a class="yt-timestamp" data-t="00:21:25">[00:21:25]</a>. This allows for breaking echo effects while remaining retaliatory enough to avoid exploitation <a class="yt-timestamp" data-t="00:21:38">[00:21:38]</a>.

## Cold War Strategies and Beyond

The insights from [[game theory and the prisoners dilemma | game theory]] proved crucial for the [[historical_context_of_world_war_ii_and_nuclear_arms_race | nuclear arms race]]. From 1950 to 1986, the US and Soviet Union struggled to cooperate, continuing to develop [[ethical_implications_and_aftermath_of_nuclear_weapons | nuclear weapons]] <a class="yt-timestamp" data-t="00:23:08">[00:23:08]</a>. However, from the late 1980s onwards, they began to reduce their nuclear stockpiles, indicating they had learned to resolve conflict <a class="yt-timestamp" data-t="00:23:15">[00:23:15]</a>. Instead of attempting a single agreement to abolish all [[ethical_implications_and_aftermath_of_nuclear_weapons | nuclear arms]] (which would resemble a single [[game theory and the prisoners dilemma | prisoner's dilemma]] where defection is rational), they opted for a gradual approach: disarming a small number of nukes each year, with mutual verification, and repeating the process <a class="yt-timestamp" data-t="00:23:24">[00:23:24]</a>. This established a repeated interaction that allowed for cooperation to emerge.

Overall, Axelrod's key takeaways remain relevant: in repeated interactions, it pays to be nice, forgiving, but not a pushover <a class="yt-timestamp" data-t="00:24:09">[00:24:09]</a>. While "winning" in many contexts is seen as beating an opponent, in non-zero-sum games like the [[game theory and the prisoners dilemma | prisoner's dilemma]], collective rewards can be gained by cooperating with the other player, benefiting both <a class="yt-timestamp" data-t="00:22:27">[00:22:27]</a>. This emphasizes finding "win-win" situations and working together to unlock greater rewards from the environment <a class="yt-timestamp" data-t="00:22:57">[00:22:57]</a>.