---
title: the prisoners dilemma
videoId: mScpHTIi-kM
---

From: [[veritasium]] <br/> 

The Prisoner's Dilemma is described as the most famous problem in [[game theory and life | game theory]] <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a>. Problems of this type are ubiquitous, appearing in scenarios from international conflicts to roommate disputes and even game shows <a class="yt-timestamp" data-t="00:00:03">[00:00:03]</a>. Understanding the optimal strategy can determine outcomes such as "life and death, war and peace, flourishing and the destruction of the planet" <a class="yt-timestamp" data-t="00:00:16">[00:00:16]</a>. The mechanics of this game may even explain the emergence of [[evolutionary biology and cooperation | cooperation]] in nature <a class="yt-timestamp" data-t="00:00:22">[00:00:22]</a>.

## Origin of the Dilemma

The origins of the game are rooted in Cold War anxieties. On September 3, 1949, an American weather monitoring plane detected radioactive material in air samples over Japan <a class="yt-timestamp" data-t="00:00:34">[00:00:34]</a>. Subsequent tests of rainwater samples worldwide confirmed the presence of Cerium-141 and Yttrium-91, isotopes with short half-lives, indicating a recent nuclear explosion <a class="yt-timestamp" data-t="00:00:46">[00:00:46]</a>. As the US had conducted no tests, the only conclusion was that the Soviet Union had developed a nuclear bomb <a class="yt-timestamp" data-t="00:01:09">[00:01:09]</a>.

This news signaled the fading of American military supremacy gained through the Manhattan Project <a class="yt-timestamp" data-t="00:01:20">[00:01:20]</a>. Some officials, like Navy Secretary Matthews, advocated for a preemptive nuclear strike against the Soviets while the US still held an advantage, arguing to become "aggressors for peace" <a class="yt-timestamp" data-t="00:01:37">[00:01:37]</a>. John von Neumann, a founder of [[game theory and life | game theory]], famously stated, "If you say why not bomb them tomorrow, I say, why not bomb them today? If you say today at five o'clock, I say why not at one o'clock?" <a class="yt-timestamp" data-t="00:01:51">[00:01:51]</a>.

In 1950, the RAND Corporation, a US think tank, was tasked with studying the nuclear weapons dilemma <a class="yt-timestamp" data-t="00:02:11">[00:02:11]</a>. As part of this research, two mathematicians at RAND invented a new game that mirrored the US-Soviet conflict <a class="yt-timestamp" data-t="00:02:21">[00:02:21]</a>. This game is now known as the Prisoner's Dilemma <a class="yt-timestamp" data-t="00:02:31">[00:02:31]</a>.

## How the Game Works

The game involves two players, each presented with two choices: to **cooperate** or to **defect** <a class="yt-timestamp" data-t="00:02:42">[00:02:42]</a>. The objective is to maximize one's own gain (coins) <a class="yt-timestamp" data-t="00:03:01">[00:03:01]</a>. The payoffs are structured as follows:

*   **Both Cooperate:** Each player receives three coins <a class="yt-timestamp" data-t="00:02:46">[00:02:46]</a>.
*   **One Cooperates, the Other Defects:** The defector receives five coins, while the cooperator receives nothing <a class="yt-timestamp" data-t="00:02:50">[00:02:50]</a>.
*   **Both Defect:** Each player receives one coin <a class="yt-timestamp" data-t="00:02:57">[00:02:57]</a>.

### The Dilemma

The dilemma arises from individual rationality. Regardless of what the other player chooses, defecting always yields a better outcome for the individual <a class="yt-timestamp" data-t="00:03:31">[00:03:31]</a>:
*   If the opponent cooperates, defecting yields 5 coins (vs. 3 for cooperating) <a class="yt-timestamp" data-t="00:03:09">[00:03:09]</a>.
*   If the opponent defects, defecting yields 1 coin (vs. 0 for cooperating) <a class="yt-timestamp" data-t="00:03:21">[00:03:21]</a>.

If both players act rationally, they will both choose to defect <a class="yt-timestamp" data-t="00:03:37">[00:03:37]</a>. This results in a suboptimal outcome where each player receives only one coin <a class="yt-timestamp" data-t="00:03:46">[00:03:46]</a>, when mutual cooperation could have yielded three coins each <a class="yt-timestamp" data-t="00:03:51">[00:03:51]</a>.

#### Cold War Example

In the context of the US and Soviet Union, this led to an arms race where both countries developed tens of thousands of nuclear weapons <a class="yt-timestamp" data-t="00:03:54">[00:03:54]</a>. While neither country could use the weapons due to mutual assured destruction, they collectively spent around $10 trillion on their development <a class="yt-timestamp" data-t="00:04:07">[00:04:07]</a>. Both would have been better off cooperating by agreeing not to develop the technology further, but their individual rational choices led to a worse outcome for everyone <a class="yt-timestamp" data-t="00:04:16">[00:04:16]</a>.

## Repeated Prisoner's Dilemma

The Prisoner's Dilemma is one of the most famous games in [[game theory and life | game theory]], with thousands of papers published on its variations <a class="yt-timestamp" data-t="00:04:30">[00:04:30]</a>. It appears in many natural phenomena, such as impalas grooming each other to remove ticks <a class="yt-timestamp" data-t="00:04:44">[00:04:44]</a>. Impalas need help grooming hard-to-reach spots, but grooming another impala comes at a cost (saliva, electrolytes, time, attention) <a class="yt-timestamp" data-t="00:05:01">[00:05:01]</a>. If impalas only interact once, the rational choice is always to defect (not groom) <a class="yt-timestamp" data-t="00:05:34">[00:05:34]</a>.

However, many real-world problems are not single instances but [[strategies in repeated games | repeated games]] <a class="yt-timestamp" data-t="00:05:43">[00:05:43]</a>. Impalas interact daily, and the same situation recurs <a class="yt-timestamp" data-t="00:05:48">[00:05:48]</a>. This changes the problem, as a defection in one round can influence future interactions <a class="yt-timestamp" data-t="00:06:02">[00:06:02]</a>.

### Axelrod's Tournaments

To determine the best strategy in this [[strategies in repeated games | repeated game]], political scientist Robert Axelrod held a computer tournament in 1980 <a class="yt-timestamp" data-t="00:06:18">[00:06:18]</a>. He invited leading game theorists to submit computer programs (strategies) that would play against each other <a class="yt-timestamp" data-t="00:06:26">[00:06:26]</a>.

**Tournament Rules:**
*   Each strategy played against every other strategy and a copy of itself <a class="yt-timestamp" data-t="00:06:38">[00:06:38]</a>.
*   Each match lasted 200 rounds <a class="yt-timestamp" data-t="00:06:43">[00:06:43]</a>.
*   Payoffs were the same as the coin game, but in points <a class="yt-timestamp" data-t="00:06:49">[00:06:49]</a>.
*   The goal was to win as many points as possible <a class="yt-timestamp" data-t="00:06:53">[00:06:53]</a>.
*   The entire tournament was repeated five times <a class="yt-timestamp" data-t="00:06:58">[00:06:58]</a>.

Axelrod received 14 strategies and added a 15th, "Random," which cooperated or defected 50% of the time <a class="yt-timestamp" data-t="00:07:16">[00:07:16]</a>.
*   **Friedman:** Cooperates initially, but defects for the rest of the game if the opponent defects once <a class="yt-timestamp" data-t="00:07:32">[00:07:32]</a>.
*   **Joss:** Starts by cooperating, then copies the opponent's last move, but occasionally defects (around 10% of the time) <a class="yt-timestamp" data-t="00:07:42">[00:07:42]</a>.
*   **Graaskamp:** Similar to Joss, but defects in the 50th round to probe for weaknesses <a class="yt-timestamp" data-t="00:07:55">[00:07:55]</a>.
*   **Name Withheld:** The most elaborate strategy, with 77 lines of code <a class="yt-timestamp" data-t="00:08:10">[00:08:10]</a>.

#### Tournament Results: Tit for Tat

The simplest program, "Tit for Tat," won the tournament <a class="yt-timestamp" data-t="00:08:21">[00:08:21]</a>.
*   **Tit for Tat's Logic:** Starts by cooperating, then perfectly copies the opponent's last move <a class="yt-timestamp" data-t="00:08:28">[00:08:28]</a>. It cooperates if the opponent cooperated, and defects if the opponent defected, but immediately returns to cooperation if the opponent does <a class="yt-timestamp" data-t="00:08:35">[00:08:35]</a>.
*   **Against Friedman:** Both cooperated throughout, achieving perfect scores <a class="yt-timestamp" data-t="00:08:45">[00:08:45]</a>.
*   **Against Joss:** They started cooperating, but when Joss defected on the sixth move, it triggered an "echo effect" of back-and-forth defections <a class="yt-timestamp" data-t="00:08:54">[00:08:54]</a>. This mutual retaliation led to poor scores for both, but Tit for Tat still won due to successful cooperation with other strategies <a class="yt-timestamp" data-t="00:09:31">[00:09:31]</a>.

### Qualities of Successful Strategies

Axelrod identified four key qualities shared by the best-performing strategies, including Tit for Tat <a class="yt-timestamp" data-t="00:10:08">[00:10:08]</a>:

1.  **Nice:** They are never the first to defect <a class="yt-timestamp" data-t="00:10:15">[00:10:15]</a>. Tit for Tat is nice because it only defects in retaliation <a class="yt-timestamp" data-t="00:10:20">[00:10:20]</a>. Nasty strategies, like Joss, defect first <a class="yt-timestamp" data-t="00:10:28">[00:10:28]</a>. All top eight strategies in the tournament were nice, significantly outperforming nasty ones <a class="yt-timestamp" data-t="00:10:37">[00:10:37]</a>.
2.  **Forgiving:** They retaliate but do not hold grudges <a class="yt-timestamp" data-t="00:10:46">[00:10:46]</a>. Tit for Tat is forgiving as it doesn't let past defections (before the last round) influence current decisions <a class="yt-timestamp" data-t="00:10:54">[00:10:54]</a>. Friedman, conversely, is maximally unforgiving, defecting for the rest of the game after a single opponent defection <a class="yt-timestamp" data-t="00:11:05">[00:11:05]</a>. This conclusion shocked experts, as many had designed tricky, nasty strategies that failed, proving that "nice guys finished first" <a class="yt-timestamp" data-t="00:11:23">[00:11:23]</a>. Axelrod's sample strategy, "Tit for Two Tats" (which only defects after two consecutive defections from an opponent) is even more forgiving and would have won the first tournament if submitted <a class="yt-timestamp" data-t="00:11:40">[00:11:40]</a>.

### Second Tournament

Axelrod held a second tournament, with the only change being that the number of rounds per game was not precisely 200, but rather on average 200, with a random number generator determining the exact end <a class="yt-timestamp" data-t="00:12:22">[00:12:22]</a>. This is crucial, as knowing the exact end allows rational players to defect in the last round, leading to a chain reaction of defections backward to the first round <a class="yt-timestamp" data-t="00:12:32">[00:12:32]</a>. Uncertainty about the end encourages continued cooperation <a class="yt-timestamp" data-t="00:13:13">[00:13:13]</a>.

Contestants had access to the first tournament's results. Some submitted nice and forgiving strategies (including Tit for Two Tats), while others submitted nasty strategies, like "Tester," to exploit extra-forgiving opponents <a class="yt-timestamp" data-t="00:13:33">[00:13:33]</a>. Tester would defect on the first move; if the opponent retaliated, Tester would play Tit for Tat; if not, Tester would defect every other move <a class="yt-timestamp" data-t="00:13:56">[00:13:56]</a>.

Despite the new strategies, being nasty again did not pay <a class="yt-timestamp" data-t="00:14:13">[00:14:13]</a>. Tit for Tat was once again the most effective <a class="yt-timestamp" data-t="00:14:16">[00:14:16]</a>. In the top 15 strategies, only one was not nice, and similarly, in the bottom 15, only one was not nasty <a class="yt-timestamp" data-t="00:14:21">[00:14:21]</a>.

Axelrod identified two more qualities after the second tournament:

3.  **Retaliatory (Provokable):** If an opponent defects, strike back immediately; don't be a pushover <a class="yt-timestamp" data-t="00:14:38">[00:14:38]</a>. "Always Cooperate" is easily exploited, while Tit for Tat is not <a class="yt-timestamp" data-t="00:14:47">[00:14:47]</a>.
4.  **Clear:** Strategies that are opaque or too complex are difficult for other programs to understand, hindering the establishment of trust <a class="yt-timestamp" data-t="00:14:57">[00:14:57]</a>.

These four principles—being nice, forgiving, provokable, and clear—remarkably resemble a form of evolved morality, often summarized as "an eye for an eye," distinct from turning the other cheek <a class="yt-timestamp" data-t="00:15:31">[00:15:31]</a>.

### The Dynamic Nature of Strategies

It's important to note that "Tit for Two Tats," which would have won the first tournament, placed only 24th in the second <a class="yt-timestamp" data-t="00:15:57">[00:15:57]</a>. This highlights that there is no single best strategy in the repeated Prisoner's Dilemma; the optimal strategy depends on the other strategies present in the environment <a class="yt-timestamp" data-t="00:16:03">[00:16:03]</a>. For instance, in an environment of only "Always Defect" strategies, Tit for Tat performs poorly <a class="yt-timestamp" data-t="00:16:15">[00:16:15]</a>.

Axelrod conducted a simulation where successful strategies increased in number over generations, and unsuccessful ones declined <a class="yt-timestamp" data-t="00:16:35">[00:16:35]</a>. Nasty strategies, like Harrington, initially grew by preying on others but quickly declined as their prey went extinct <a class="yt-timestamp" data-t="00:16:51">[00:16:51]</a>. After a thousand generations, the population stabilized, with only nice strategies surviving, and Tit for Tat remained dominant, representing 14.5% of the total <a class="yt-timestamp" data-t="00:17:11">[00:17:11]</a>.

This simulation, which is an ecological simulation rather than evolutionary due to the lack of mutations, demonstrates how cooperation can emerge and spread even in a population of self-interested players <a class="yt-timestamp" data-t="00:17:27">[00:17:27]</a>. A small cluster of cooperators can emerge and eventually take over the population, showing that altruism isn't necessary for cooperation to flourish <a class="yt-timestamp" data-t="00:17:39">[00:17:39]</a>.

## Noise in the System

Axelrod's insights were applied to [[evolutionary biology and cooperation | evolutionary biology]] and international conflicts <a class="yt-timestamp" data-t="00:19:27">[00:19:27]</a>. However, his original tournaments didn't account for "noise" or random errors in the game <a class="yt-timestamp" data-t="00:19:36">[00:19:36]</a>. For example, a player trying to cooperate might be perceived as defecting <a class="yt-timestamp" data-t="00:19:42">[00:19:42]</a>. Real-world examples include the 1983 incident where a Soviet early warning system mistook sunlight reflecting off clouds for US ballistic missiles, thankfully dismissed by officer Stanislav Petrov <a class="yt-timestamp" data-t="00:19:50">[00:19:50]</a>.

When Tit for Tat plays against itself in a noisy environment, a single perceived defection can trigger a chain of alternating retaliations, leading to constant mutual defection and significantly reduced scores <a class="yt-timestamp" data-t="00:20:47">[00:20:47]</a>. To solve this, a strategy needs a reliable way to break out of these echo effects <a class="yt-timestamp" data-t="00:21:21">[00:21:21]</a>. One solution is "generous Tit for Tat," which plays Tit for Tat but with about 10% more forgiveness, retaliating only nine out of ten times <a class="yt-timestamp" data-t="00:21:25">[00:21:25]</a>. This allows breaking echoes while remaining sufficiently retaliatory <a class="yt-timestamp" data-t="00:21:38">[00:21:38]</a>.

### Winning in Real Life

It's notable that Tit for Tat, by design, can never score higher than its opponent in a single interaction; it can only lose or draw <a class="yt-timestamp" data-t="00:22:01">[00:22:01]</a>. Yet, it consistently comes out ahead when all interactions are tallied <a class="yt-timestamp" data-t="00:22:10">[00:22:10]</a>. Conversely, "Always Defect" can never lose a game but performs extremely poorly overall <a class="yt-timestamp" data-t="00:22:17">[00:22:17]</a>.

This highlights a common misconception: winning does not always mean beating the other person <a class="yt-timestamp" data-t="00:22:27">[00:22:27]</a>. Games like chess or poker are zero-sum, where one person's gain is another's loss <a class="yt-timestamp" data-t="00:22:34">[00:22:34]</a>. However, "most of life is not zero sum" <a class="yt-timestamp" data-t="00:22:43">[00:22:43]</a>. In the Prisoner's Dilemma, rewards come from an external source (the "banker," or the "world" in real life), not from the other player <a class="yt-timestamp" data-t="00:22:46">[00:22:46]</a>. This means finding "win-win situations" and cooperating to unlock mutual rewards is key <a class="yt-timestamp" data-t="00:22:57">[00:22:57]</a>. Cooperation pays even among rivals <a class="yt-timestamp" data-t="00:23:04">[00:23:04]</a>.

From the late 1980s, the US and Soviet Union began reducing their nuclear stockpiles, learning to resolve conflict by slowly disarming and mutually checking each other's compliance over time <a class="yt-timestamp" data-t="00:23:10">[00:23:10]</a>. This approach avoided turning disarmament into a single Prisoner's Dilemma by making it a [[strategies in repeated games | repeated game]] <a class="yt-timestamp" data-t="00:23:24">[00:23:24]</a>.

For over 40 years since Axelrod's tournaments, researchers have continued to study optimal strategies in varying environments, including different payoff structures, errors, and even mutations <a class="yt-timestamp" data-t="00:23:47">[00:23:47]</a>. While Tit for Tat or generous Tit for Tat doesn't always win, Axelrod's core takeaways remain: "be nice, forgiving, but don't be a pushover" <a class="yt-timestamp" data-t="00:24:01">[00:24:01]</a>.

Life involves making choices that impact not only one's own future but also the future of those interacted with <a class="yt-timestamp" data-t="00:24:52">[00:24:52]</a>. In the short term, the environment shapes the player, but in the long run, players shape the environment <a class="yt-timestamp" data-t="00:25:02">[00:25:02]</a>. Choosing wisely is crucial, as the impact can extend further than one might think <a class="yt-timestamp" data-t="00:25:17">[00:25:17]</a>.