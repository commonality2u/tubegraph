---
title: Analog computers history and resurgence
videoId: GVsUOuSjvcg
---

From: [[veritasium]] <br/> 

For centuries, [[Analog computers history and resurgence | analog computers]] were considered the most powerful computing devices globally <a class="yt-timestamp" data-t="00:00:01">[00:00:01]</a>. They were used for complex calculations like predicting eclipses and [[historical_use_of_analog_computers_for_tide_prediction | tides]] <a class="yt-timestamp" data-t="00:00:05">[00:00:05]</a>, and even for [[analog_computers_in_military_applications_during_world_war_ii | guiding anti-aircraft guns]] during World War II <a class="yt-timestamp" data-t="00:00:05">[00:00:05]</a>.

With the advent of solid-state transistors, [[digital computers]] began to dominate <a class="yt-timestamp" data-t="00:00:09">[00:00:09]</a>, leading to a period where virtually all computers became digital <a class="yt-timestamp" data-t="00:00:14">[00:00:14]</a>. However, a convergence of factors is now setting the stage for a [[the_resurgence_and_potential_of_modern_analog_computing | resurgence of analog technology]] <a class="yt-timestamp" data-t="00:00:18">[00:00:18]</a>.

## How Analog Computers Work

Unlike [[digital computers]] that rely on zeros and ones, [[differences_between_analog_and_digital_computers | analog computers]] operate without discrete binary states <a class="yt-timestamp" data-t="00:01:00">[00:01:00]</a>. Instead, they use physical quantities, such as voltage, to represent variables <a class="yt-timestamp" data-t="00:01:03">[00:01:03]</a>. The electrical circuitry acts as a direct analog for the physical problem being solved <a class="yt-timestamp" data-t="00:01:10">[00:01:10]</a>.

By connecting wires in specific ways, an analog computer can be programmed to solve a range of differential equations <a class="yt-timestamp" data-t="00:00:27">[00:00:27]</a>. For instance, a setup can simulate a damped mass oscillating on a spring, where a voltage oscillates exactly like the physical mass <a class="yt-timestamp" data-t="00:00:34">[00:00:34]</a>. Parameters like damping, spring constant, or mass can be varied to observe changes in oscillation amplitude and duration <a class="yt-timestamp" data-t="00:00:45">[00:00:45]</a>. Changing electrical connections allows the computer to solve other equations, such as the Lorenz system, a basic model of atmospheric convection known for exhibiting chaos <a class="yt-timestamp" data-t="00:01:16">[00:01:16]</a>.

### Advantages of Analog Computing
[[Analog computers]] offer significant advantages:
*   **Power and Speed** They are powerful computing devices capable of completing many computations quickly <a class="yt-timestamp" data-t="00:01:50">[00:01:50]</a>.
*   **Energy Efficiency** They require little power <a class="yt-timestamp" data-t="00:01:56">[00:01:56]</a>. For example, adding two eight-bit numbers on a digital computer requires about 50 transistors, while an analog computer can add two currents by simply connecting two wires <a class="yt-timestamp" data-t="00:02:01">[00:02:01]</a>. Multiplying two numbers on a digital computer needs around 1,000 transistors, whereas an analog computer can achieve this by passing a current through a resistor, where the voltage across it (I times R) effectively performs the multiplication <a class="yt-timestamp" data-t="00:02:15">[00:02:15]</a>.

### Drawbacks of Analog Computing
Despite their advantages, [[challenges_and_potential_of_analog_computing_in_modern_technology | analog computers]] have notable drawbacks:
*   **Single-Purpose** They are not general-purpose computing devices; they cannot run diverse software like Microsoft Word <a class="yt-timestamp" data-t="00:02:43">[00:02:43]</a>.
*   **Inexactness** Inputs and outputs are continuous, making it impossible to input exact values or achieve perfectly repeatable calculations <a class="yt-timestamp" data-t="00:02:49">[00:02:49]</a>.
*   **Manufacturing Variability** Variations in component values (like resistors or capacitors) during manufacturing typically lead to about a 1% error <a class="yt-timestamp" data-t="00:03:03">[00:03:03]</a>.

These limitations—being single-purpose, non-repeatable, and inexact—were major reasons why [[analog computers]] fell out of favor once [[digital computers]] became viable <a class="yt-timestamp" data-t="00:03:25">[00:03:25]</a>.

## The Rise of Digital and AI's Evolution

The dominance of digital computing, fueled by advancements in solid-state transistors <a class="yt-timestamp" data-t="00:00:09">[00:00:09]</a>, shifted the computing landscape. However, the current [[the_resurgence_and_potential_of_modern_analog_computing | resurgence of analog computing]] is closely tied to the demands of Artificial Intelligence (AI) <a class="yt-timestamp" data-t="00:03:43">[00:03:43]</a>.

### Early AI and the Perceptron
AI, a term coined in 1956 <a class="yt-timestamp" data-t="00:03:51">[00:03:51]</a>, saw early development with Frank Rosenblatt's perceptron in 1958 <a class="yt-timestamp" data-t="00:03:55">[00:03:55]</a>. Designed to mimic brain neurons <a class="yt-timestamp" data-t="00:04:01">[00:04:01]</a>, the perceptron received input from 400 photocells representing a 20x20 pixel image <a class="yt-timestamp" data-t="00:04:53">[00:04:53]</a>. These inputs were connected to a single output neuron via adjustable weights, performing a vector dot product to determine if the output neuron would fire <a class="yt-timestamp" data-t="00:05:15">[00:05:15]</a>.

The perceptron was trained to distinguish between images (e.g., rectangles and circles) by adjusting its weights <a class="yt-timestamp" data-t="00:05:38">[00:05:38]</a>. If the output was wrong, input activations were added to or subtracted from the weights <a class="yt-timestamp" data-t="00:06:21">[00:06:21]</a>. This algorithm was proven to converge if the categories were distinct <a class="yt-timestamp" data-t="00:06:52">[00:06:52]</a>. While capable of distinguishing shapes and letters <a class="yt-timestamp" data-t="00:07:02">[00:07:02]</a>, Rosenblatt's claims of "original thought" and distinguishing cats from dogs were overstated <a class="yt-timestamp" data-t="00:07:14">[00:07:14]</a>. Criticisms in a 1969 book by Minsky and Papert led to the "first AI winter," a period of stagnation for artificial neural networks <a class="yt-timestamp" data-t="00:07:52">[00:07:52]</a>.

### AI Resurgence and Deep Learning
The 1980s saw an AI resurgence, exemplified by Carnegie Mellon's ALVINN, one of the first self-driving cars <a class="yt-timestamp" data-t="00:08:25">[00:08:25]</a>. ALVINN was an artificial neural network with a hidden layer of neurons between input (30x32 pixel images of the road) and output (32 neurons determining steering angle) <a class="yt-timestamp" data-t="00:08:32">[00:08:32]</a>. Transitions between layers involved matrix multiplication <a class="yt-timestamp" data-t="00:09:01">[00:09:01]</a>. Training involved a human driver providing correct steering angles, with weights adjusted through backpropagation <a class="yt-timestamp" data-t="00:09:15">[00:09:15]</a>. The vehicle's speed was limited by the computer's matrix multiplication speed <a class="yt-timestamp" data-t="00:10:06">[00:10:06]</a>.

Despite these advancements, neural networks still struggled with tasks like identifying cats and dogs, leading to a second AI lull in the 1990s <a class="yt-timestamp" data-t="00:10:12">[00:10:12]</a>. Fei-Fei Li proposed that the problem might be a lack of training data <a class="yt-timestamp" data-t="00:10:43">[00:10:43]</a>. From 2006 to 2009, she created ImageNet, a database of 1.2 million human-labeled images <a class="yt-timestamp" data-t="00:10:56">[00:10:56]</a>. The ImageNet Large Scale Visual Recognition Challenge ran from 2010 to 2017, where software programs competed to classify images into 1,000 categories <a class="yt-timestamp" data-t="00:11:06">[00:11:06]</a>.

In 2012, AlexNet, an artificial neural network from the University of Toronto, significantly reduced the top-5 error rate from 25.8% to 16.4% <a class="yt-timestamp" data-t="00:12:12">[00:12:12]</a>. AlexNet's success was due to its unprecedented size and depth: eight layers and 500,000 neurons <a class="yt-timestamp" data-t="00:12:22">[00:12:22]</a>. Training involved adjusting 60 million weights and biases, requiring 700 million math operations per image <a class="yt-timestamp" data-t="00:12:30">[00:12:30]</a>. This was achieved by pioneering the use of Graphical Processing Units (GPUs) for fast parallel computations <a class="yt-timestamp" data-t="00:12:48">[00:12:48]</a>. The AlexNet paper, cited over 100,000 times, established that the scale of the neural network was key to its performance <a class="yt-timestamp" data-t="00:13:00">[00:13:00]</a>. By 2015, neural networks achieved a top-5 error rate of 3.6%, surpassing human performance <a class="yt-timestamp" data-t="00:13:20">[00:13:20]</a>, with the best network having 100 layers of neurons <a class="yt-timestamp" data-t="00:13:31">[00:13:31]</a>.

## The Case for Analog Resurgence

The increasing demand for larger neural networks presents several challenges for [[digital computers]]:
*   **Energy Consumption** Training a neural network can consume electricity equivalent to the yearly consumption of three households <a class="yt-timestamp" data-t="00:13:43">[00:13:43]</a>.
*   **Von Neumann Bottleneck** In modern digital computers, the majority of time and energy during large matrix multiplications in neural networks is spent fetching weight values from memory rather than performing the computation <a class="yt-timestamp" data-t="00:13:50">[00:13:50]</a>.
*   **Moore's Law Limitations** The long-standing trend of doubling transistors on a chip every two years is approaching fundamental physical limits as transistor size nears atomic scale <a class="yt-timestamp" data-t="00:14:10">[00:14:10]</a>.

This "perfect storm" creates an ideal environment for [[the_resurgence_and_potential_of_modern_analog_computing | analog computers]] <a class="yt-timestamp" data-t="00:14:26">[00:14:26]</a>. While [[digital computers]] face limits, neural networks are exploding in popularity, and their core operation is often matrix multiplication <a class="yt-timestamp" data-t="00:14:30">[00:14:30]</a>. Crucially, neural networks do not require the high precision of digital computers; slight variability in components or conditions can be tolerated because exact confidence levels (e.g., 96% vs. 98% for a chicken) do not fundamentally alter the outcome <a class="yt-timestamp" data-t="00:14:41">[00:14:41]</a>. This tolerance for imprecision directly addresses a key drawback of traditional analog computing.

## Modern Analog Computing for AI

Companies like Mythic AI are at the forefront of the [[applications_of_analog_computers_in_ai | application of analog computing in AI]] <a class="yt-timestamp" data-t="00:14:58">[00:14:58]</a>. They are developing analog chips specifically designed to run neural networks <a class="yt-timestamp" data-t="00:15:03">[00:15:03]</a>.

### Mythic AI's Approach
Mythic AI repurposes digital flash storage cells, traditionally used to store binary ones or zeros <a class="yt-timestamp" data-t="00:15:54">[00:15:54]</a>. Instead of on/off switches, these cells are used as variable resistors <a class="yt-timestamp" data-t="00:16:33">[00:16:33]</a>. By precisely controlling the number of electrons on a cell's floating gate, its resistance can be varied <a class="yt-timestamp" data-t="00:16:40">[00:16:40]</a>. When a small voltage is applied, the resulting current (V/R or voltage * conductance) effectively multiplies two values <a class="yt-timestamp" data-t="00:16:51">[00:16:51]</a>.

To run a neural network, Mythic AI writes all the network's weights to the flash cells as their conductance values <a class="yt-timestamp" data-t="00:17:09">[00:17:09]</a>. Input activation values are then applied as voltage to the cells. The resulting current, which is the product of voltage and conductance (activation times weight), is then added together from multiple cells, completing the necessary matrix multiplication <a class="yt-timestamp" data-t="00:17:16">[00:17:16]</a>.

Mythic AI's first product can perform 25 trillion math operations per second while consuming only about three watts of power <a class="yt-timestamp" data-t="00:17:39">[00:17:39]</a>. This contrasts with newer digital systems that can achieve 25 to 100 trillion operations per second but are larger, more expensive, and consume 50 to 100 watts of power <a class="yt-timestamp" data-t="00:17:54">[00:17:54]</a>. While not a direct "apples-to-apples" comparison (as digital GPUs are still needed for training AI algorithms), these analog chips are highly efficient for deploying AI workloads in various applications, such as security cameras, autonomous systems, and manufacturing inspection equipment <a class="yt-timestamp" data-t="00:18:10">[00:18:10]</a>.

### Hybrid Analog-Digital Approaches
For complex neural networks involving many sequential matrix multiplications, a purely analog approach can lead to signal distortion <a class="yt-timestamp" data-t="00:18:56">[00:18:56]</a>. Therefore, modern implementations often involve converting signals from analog to digital after each processing block, then back to analog for the next, to preserve signal integrity <a class="yt-timestamp" data-t="00:19:10">[00:19:10]</a>.

## Conclusion

The potential of [[analog computers]] in the modern era, particularly for AI, is significant. Frank Rosenblatt himself, frustrated by the slowness of early digital computers, built a custom analog computer for his perceptron <a class="yt-timestamp" data-t="00:19:21">[00:19:21]</a>. His core idea of neural networks proved correct, and perhaps his inclination towards analog might also prove prescient <a class="yt-timestamp" data-t="00:19:35">[00:19:35]</a>.

While it's uncertain if [[analog computers]] will experience the same scale of adoption as [[digital computers]] did last century <a class="yt-timestamp" data-t="00:19:43">[00:19:43]</a>, they appear better suited for many of today's computational tasks, especially those requiring fast, energy-efficient matrix multiplications without absolute precision <a class="yt-timestamp" data-t="00:19:48">[00:19:48]</a>. The human brain, both digital in its neuron firing and analog in its distributed "thinking everywhere, all at once," suggests that a blend of both approaches might be necessary to achieve true artificial intelligence <a class="yt-timestamp" data-t="00:20:17">[00:20:17]</a>.