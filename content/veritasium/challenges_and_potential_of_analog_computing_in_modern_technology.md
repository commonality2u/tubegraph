---
title: Challenges and potential of analog computing in modern technology
videoId: GVsUOuSjvcg
---

From: [[veritasium]] <br/> 

For hundreds of years, [[analog_computers_history_and_resurgence | analog computers]] were considered the most powerful computers globally, capable of [[the_antikythera_mechanism_as_an_early_analog_computer | predicting eclipses]], [[historical_use_of_analog_computers_for_tide_prediction | tides]], and guiding [[analog_computers_in_military_applications_during_world_war_ii | anti-aircraft guns]] <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a>. However, with the advent of solid-state transistors, [[differences_between_analog_and_digital_computers | digital computers]] rapidly advanced, leading to the near-universal use of [[differences_between_analog_and_digital_computers | digital computers]] today <a class="yt-timestamp" data-t="00:00:09">[00:00:09]</a>. Despite this, a combination of factors is now setting the scene for a [[the_resurgence_and_potential_of_modern_analog_computing | resurgence of analog technology]] <a class="yt-timestamp" data-t="00:00:18">[00:00:18]</a>.

## How Analog Computers Work

An [[analog_computers_history_and_resurgence | analog computer]] can be programmed to solve differential equations by connecting wires in specific ways <a class="yt-timestamp" data-t="00:00:27">[00:00:27]</a>. For instance, a setup can simulate a damped mass oscillating on a spring, allowing real-time variation of parameters like damping, spring constant, or mass to observe changes in amplitude and duration of oscillations <a class="yt-timestamp" data-t="00:00:34">[00:00:34]</a>.

The core principle of an [[analog_computers_history_and_resurgence | analog computer]] is that it operates without zeros and ones <a class="yt-timestamp" data-t="00:00:57">[00:00:57]</a>. Instead, it uses a voltage that oscillates, mirroring the physical problem being simulated, but at a much faster rate <a class="yt-timestamp" data-t="00:01:03">[00:01:03]</a>. By changing electrical connections, the computer can solve other differential equations, such as the Lorenz system, a basic model of atmospheric convection famous for being an early example of chaos <a class="yt-timestamp" data-t="00:01:16">[00:01:16]</a>. This allows for real-time observation of effects when parameters are changed <a class="yt-timestamp" data-t="00:01:38">[00:01:38]</a>.

## Advantages of Analog Computing

[[analog_computers_history_and_resurgence | Analog computers]] offer several benefits:
*   **Power and Speed** They are powerful computing devices capable of completing many computations quickly <a class="yt-timestamp" data-t="00:01:50">[00:01:50]</a>.
*   **Energy Efficiency** They require significantly less power <a class="yt-timestamp" data-t="00:01:56">[00:01:56]</a>. For example, adding two eight-bit numbers on a [[differences_between_analog_and_digital_computers | digital computer]] needs about 50 transistors, while an [[analog_computers_history_and_resurgence | analog computer]] can add two currents by simply connecting two wires <a class="yt-timestamp" data-t="00:02:01">[00:02:01]</a>. Similarly, multiplying two numbers digitally might require around 1,000 transistors, whereas an [[analog_computers_history_and_resurgence | analog computer]] can achieve this by passing a current through a resistor, where the resulting voltage is the product (I times R) <a class="yt-timestamp" data-t="00:02:15">[00:02:15]</a>.

## Drawbacks of Analog Computing

Despite their advantages, [[analog_computers_history_and_resurgence | analog computers]] have notable drawbacks:
*   **Lack of Generality** They are not general-purpose computing devices; they cannot run complex software like word processors <a class="yt-timestamp" data-t="00:02:40">[00:02:40]</a>.
*   **Imprecision and Non-Repeatability** Since inputs and outputs are continuous, exact values cannot be input, leading to non-repeatable results when repeating calculations <a class="yt-timestamp" data-t="00:02:49">[00:02:49]</a>.
*   **Manufacturing Variability** Manufacturing variations in components like resistors or capacitors lead to inherent inaccuracies, typically around a 1% error <a class="yt-timestamp" data-t="00:03:01">[00:03:01]</a>.

In summary, [[analog_computers_history_and_resurgence | analog computers]] are powerful, fast, and energy-efficient, but they are single-purpose, non-repeatable, and inexact <a class="yt-timestamp" data-t="00:03:15">[00:03:15]</a>. These limitations were major reasons why [[analog_computers_history_and_resurgence | analog computers]] fell out of favor once [[differences_between_analog_and_digital_computers | digital computers]] became viable <a class="yt-timestamp" data-t="00:03:30">[00:03:30]</a>.

## The Resurgence: A "Perfect Storm" for Analog

A "perfect storm" of factors is contributing to the potential comeback of [[analog_computers_history_and_resurgence | analog computers]] <a class="yt-timestamp" data-t="00:03:36">[00:03:36]</a>, primarily driven by the demands of [[applications_of_analog_computers_in_ai | artificial intelligence]] <a class="yt-timestamp" data-t="00:03:43">[00:03:43]</a>.

### The Rise of Neural Networks

[[applications_of_analog_computers_in_ai | AI]] has a long history, with the term coined in 1956 <a class="yt-timestamp" data-t="00:03:51">[00:03:51]</a>. Frank Rosenblatt built the perceptron in 1958, designed to mimic how neurons fire in the brain <a class="yt-timestamp" data-t="00:03:55">[00:03:55]</a>. Neurons activate (1) or not (0), with inputs from other neurons weighted by connection strength <a class="yt-timestamp" data-t="00:04:05">[00:04:05]</a>. The perceptron used 400 photocells for input, connecting to a single output neuron via adjustable weights <a class="yt-timestamp" data-t="00:04:53">[00:04:53]</a>. The goal was to distinguish images like rectangles and circles, achieved by training and adjusting weights based on correct or incorrect outputs <a class="yt-timestamp" data-t="00:05:38">[00:05:38]</a>. Rosenblatt's perceptron could distinguish shapes and even letters <a class="yt-timestamp" data-t="00:07:02">[00:07:02]</a>. However, its limitations, highlighted by Minsky and Papert in 1969, led to the first "AI winter" <a class="yt-timestamp" data-t="00:07:52">[00:08:00]</a>.

The 1980s saw an [[applications_of_analog_computers_in_ai | AI]] [[the_resurgence_and_potential_of_modern_analog_computing | resurgence]] with the creation of ALVINN, one of the first self-driving cars, which used a [[advancements_in_neural_networks_and_their_computational_needs | neural network]] with a hidden layer <a class="yt-timestamp" data-t="00:08:25">[00:08:25]</a>. ALVINN processed road images, performing matrix multiplication to determine steering angles, but its speed was limited by the [[differences_between_analog_and_digital_computers | computer's]] ability to perform these calculations <a class="yt-timestamp" data-t="00:08:43">[00:10:00]</a>. Despite these [[advancements_in_neural_networks_and_their_computational_needs | advancements]], [[applications_of_analog_computers_in_ai | artificial neural networks]] still struggled with seemingly simple tasks, leading to another lull in the 1990s <a class="yt-timestamp" data-t="00:10:12">[00:10:12]</a>.

### The Data Revolution and Computational Demands

By the mid-2000s, researcher Fei-Fei Li proposed that [[applications_of_analog_computers_in_ai | artificial neural networks]] needed more training data <a class="yt-timestamp" data-t="00:10:43">[00:10:43]</a>. This led to ImageNet, a database of 1.2 million human-labeled images created between 2006 and 2009 <a class="yt-timestamp" data-t="00:10:52">[00:10:52]</a>. The ImageNet competition challenged software programs to classify images into 1,000 categories <a class="yt-timestamp" data-t="00:11:06">[00:11:06]</a>.

In 2012, AlexNet, an [[advancements_in_neural_networks_and_their_computational_needs | artificial neural network]] from the University of Toronto, achieved a breakthrough with a top-5 error rate of just 16.4%, significantly outperforming previous systems <a class="yt-timestamp" data-t="00:12:12">[00:12:12]</a>. AlexNet's success was attributed to its scale and depth: eight layers, 500,000 neurons, and 60 million weights and biases <a class="yt-timestamp" data-t="00:12:22">[00:12:22]</a>. Processing a single image required 700 million math operations <a class="yt-timestamp" data-t="00:12:40">[00:12:40]</a>, making training computationally intensive. The team pioneered the use of GPUs (graphical processing units) for this, which are specialized for fast parallel computations <a class="yt-timestamp" data-t="00:12:48">[00:12:48]</a>. The importance of scale in [[advancements_in_neural_networks_and_their_computational_needs | neural network]] performance became clear, with error rates plummeting to 3.6% by 2015, surpassing human performance <a class="yt-timestamp" data-t="00:13:07">[00:13:07]</a>.

This success highlights an increasing demand for ever-larger [[advancements_in_neural_networks_and_their_computational_needs | neural networks]], which poses several problems for [[differences_between_analog_and_digital_computers | digital computers]]:
*   **Energy Consumption** Training a [[advancements_in_neural_networks_and_their_computational_needs | neural network]] can require electricity similar to the yearly consumption of three households <a class="yt-timestamp" data-t="00:13:43">[00:13:43]</a>.
*   **Von Neumann Bottleneck** In [[differences_between_analog_and_digital_computers | modern digital computers]], much time and energy are spent fetching weight values from memory over a bus, rather than performing computations, especially in the huge matrix multiplications required by deep [[advancements_in_neural_networks_and_their_computational_needs | neural networks]] <a class="yt-timestamp" data-t="00:13:50">[00:13:50]</a>.
*   **Limits of Moore's Law** The long-standing trend of doubling transistor count on a chip every two years is facing fundamental physical challenges as transistors approach atomic sizes <a class="yt-timestamp" data-t="00:14:10">[00:14:10]</a>. This represents a significant point of [[technological_obsolescence_and_innovation | technological obsolescence and innovation]].

This situation creates a "perfect storm" for [[analog_computers_history_and_resurgence | analog computers]] <a class="yt-timestamp" data-t="00:14:26">[00:14:26]</a>. As [[differences_between_analog_and_digital_computers | digital computers]] reach their limits and [[advancements_in_neural_networks_and_their_computational_needs | neural networks]] explode in popularity, the primary task of [[advancements_in_neural_networks_and_their_computational_needs | neural networks]]—matrix multiplication—is well-suited for [[analog_computers_history_and_resurgence | analog computing]] <a class="yt-timestamp" data-t="00:14:30">[00:14:30]</a>. Crucially, [[advancements_in_neural_networks_and_their_computational_needs | neural networks]] do not require the high precision of [[differences_between_analog_and_digital_computers | digital computers]], tolerating slight variability in components or conditions <a class="yt-timestamp" data-t="00:14:41">[00:14:41]</a>.

## Modern Analog Computing Applications

Companies like Mythic AI are developing [[the_resurgence_and_potential_of_modern_analog_computing | analog chips]] specifically designed to run [[advancements_in_neural_networks_and_their_computational_needs | neural networks]] <a class="yt-timestamp" data-t="00:14:58">[00:15:01]</a>. These chips perform matrix multiplication in the analog domain by repurposing [[differences_between_analog_and_digital_computers | digital]] flash storage cells <a class="yt-timestamp" data-t="00:15:47">[00:15:47]</a>. Instead of storing just a one or a zero, these cells are set to a specific number of electrons on their floating gate, making them act as variable resistors <a class="yt-timestamp" data-t="00:16:33">[00:16:33]</a>. The current that flows through the cell when a small voltage is applied is equal to voltage times conductance (the reciprocal of resistance), effectively multiplying two values <a class="yt-timestamp" data-t="00:16:51">[00:16:51]</a>. By wiring cells together, the currents add up, completing the matrix multiplication required for [[advancements_in_neural_networks_and_their_computational_needs | neural networks]] <a class="yt-timestamp" data-t="00:17:28">[00:17:28]</a>.

Mythic AI's first product can perform 25 trillion math operations per second while consuming only about three watts of power <a class="yt-timestamp" data-t="00:17:40">[00:17:40]</a>. While newer [[differences_between_analog_and_digital_computers | digital systems]] can achieve 25 to 100 trillion operations per second, they are typically large, expensive systems consuming 50 to 100 watts <a class="yt-timestamp" data-t="00:17:54">[00:17:54]</a>. This makes [[the_resurgence_and_potential_of_modern_analog_computing | analog chips]] ideal for deploying [[applications_of_analog_computers_in_ai | AI]] workloads in edge devices, security cameras, autonomous systems, and manufacturing inspection equipment <a class="yt-timestamp" data-t="00:18:17">[00:18:17]</a>.

Another proposed application is using [[analog_computers_history_and_resurgence | analog circuitry]] in smart home speakers specifically to listen for wake words like "Alexa" or "Siri" <a class="yt-timestamp" data-t="00:18:40">[00:18:40]</a>. This would significantly reduce power consumption and quickly activate the device's [[differences_between_analog_and_digital_computers | digital circuitry]] <a class="yt-timestamp" data-t="00:18:47">[00:18:47]</a>.

### Hybrid Analog-Digital Approaches

Despite the benefits, purely [[analog_computers_history_and_resurgence | analog systems]] still face challenges, particularly with signal distortion over multiple layers of computation <a class="yt-timestamp" data-t="00:18:53">[00:19:00]</a>. To mitigate this, a hybrid approach is often employed: converting from the analog domain to the [[differences_between_analog_and_digital_computers | digital domain]] after each processing block, then back to analog for the next, to preserve the signal <a class="yt-timestamp" data-t="00:19:10">[00:19:10]</a>.

## Future Outlook

Historically, Frank Rosenblatt initially used a [[differences_between_analog_and_digital_computers | digital]] IBM computer for his perceptron but found it too slow, leading him to build a custom [[analog_computers_history_and_resurgence | analog computer]] <a class="yt-timestamp" data-t="00:19:21">[00:19:21]</a>. While his idea of [[advancements_in_neural_networks_and_their_computational_needs | neural networks]] proved correct, perhaps his intuition about [[analog_computers_history_and_resurgence | analog]] was also accurate <a class="yt-timestamp" data-t="00:19:35">[00:19:35]</a>.

It is uncertain whether [[analog_computers_history_and_resurgence | analog computers]] will take off in the same way [[differences_between_analog_and_digital_computers | digital]] ones did, but they appear to be better suited for many tasks demanded of computers today <a class="yt-timestamp" data-t="00:19:43">[00:19:43]</a>. While music, pictures, and video have all transitioned to [[differences_between_analog_and_digital_computers | digital]], the future of information processing might not view [[differences_between_analog_and_digital_computers | digital]] as the end point <a class="yt-timestamp" data-t="00:20:01">[00:20:01]</a>. Human brains, for example, are both [[differences_between_analog_and_digital_computers | digital]] (neurons fire or don't) and [[analog_computers_history_and_resurgence | analog]] (thinking occurs everywhere at once) <a class="yt-timestamp" data-t="00:20:17">[00:20:17]</a>. Achieving true [[applications_of_analog_computers_in_ai | artificial intelligence]] that thinks like humans may require harnessing the power of [[analog_computers_history_and_resurgence | analog computing]] <a class="yt-timestamp" data-t="00:20:28">[00:20:28]</a>.