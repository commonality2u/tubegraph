---
title: evolutionary biology and cooperation
videoId: mScpHTIi-kM
---

From: [[veritasium]] <br/> 

[[game theory and life | Game theory]] provides a framework for understanding conflicts and cooperation, appearing in various situations from international relations to animal interactions <a class="yt-timestamp" data-t="00:00:03">[00:00:03]</a>. A central concept in [[game theory and life | game theory]] is [[the prisoners dilemma | the Prisoner's Dilemma]], which can shed light on the unexpected emergence of cooperation in nature <a class="yt-timestamp" data-t="00:00:22">[00:00:22]</a>.

## The Prisoner's Dilemma

Invented by two mathematicians at the RAND Corporation in 1950, [[the prisoners dilemma | the Prisoner's Dilemma]] is a game where two players choose to either "cooperate" or "defect" <a class="yt-timestamp" data-t="00:02:21">[00:02:21]</a>. The goal is to maximize individual gain <a class="yt-timestamp" data-t="00:03:01">[00:03:01]</a>.

The payoff structure is as follows:
*   If both cooperate, each gets three coins <a class="yt-timestamp" data-t="00:02:46">[00:02:46]</a>.
*   If one cooperates and the other defects, the defector gets five coins, and the cooperator gets nothing <a class="yt-timestamp" data-t="00:02:50">[00:02:50]</a>.
*   If both defect, each gets one coin <a class="yt-timestamp" data-t="00:02:57">[00:02:57]</a>.

In a single round of [[the prisoners dilemma | the Prisoner's Dilemma]], the rational choice for each player, regardless of the opponent's action, is always to defect <a class="yt-timestamp" data-t="00:03:31">[00:03:31]</a>. This leads to a suboptimal outcome where both players receive only one coin, rather than the three they could have received through mutual cooperation <a class="yt-timestamp" data-t="00:03:44">[00:03:44]</a>.

### Case Study: The Nuclear Arms Race

The US-Soviet [[nuclear arms race and cooperation | conflict]] served as a real-world parallel to [[the prisoners dilemma | the Prisoner's Dilemma]] <a class="yt-timestamp" data-t="00:02:27">[00:02:27]</a>. In 1949, the US detected radioactive material, confirming the Soviet Union had developed a nuclear bomb <a class="yt-timestamp" data-t="00:00:34">[00:00:34]</a>. This revelation led to concerns about war and the suggestion by some, including Navy Secretary Matthews, to launch a preemptive strike <a class="yt-timestamp" data-t="00:01:37">[00:01:37]</a>.

Nobel Prize winner John von Neuman, a founder of [[game theory and life | game theory]], famously advocated for immediate action:
> "If you say why not bomb them tomorrow, I say, why not bomb them today? If you say today at five o'clock, I say why not at one o'clock?" <a class="yt-timestamp" data-t="00:01:53">[00:01:53]</a>

Applying the logic of [[the prisoners dilemma | the Prisoner's Dilemma]], both the US and the Soviet Union acted in their self-interest by developing massive nuclear arsenals, costing around $10 trillion <a class="yt-timestamp" data-t="00:03:54">[00:03:54]</a>. Although neither could use the weapons, both would have been better off if they had cooperated and halted development <a class="yt-timestamp" data-t="00:04:07">[00:04:07]</a>. This illustrates how individual rational actions can lead to a collectively worse outcome <a class="yt-timestamp" data-t="00:04:22">[00:04:22]</a>.

## Cooperation in Nature

The [[the prisoners dilemma | Prisoner's Dilemma]] can be observed in nature. Impalas, for example, groom each other to remove disease-carrying ticks <a class="yt-timestamp" data-t="00:04:44">[00:04:44]</a>. While grooming another impala incurs costs (saliva, electrolytes, time), impalas cannot reach all parts of their own bodies, requiring mutual grooming <a class="yt-timestamp" data-t="00:04:58">[00:04:58]</a>. If impalas only interacted once, the rational choice would be to defect (not groom) <a class="yt-timestamp" data-t="00:05:34">[00:05:34]</a>. However, many interactions in nature, like impalas seeing each other daily, are repeated games <a class="yt-timestamp" data-t="00:05:43">[00:05:43]</a>.

## Axelrod's Tournaments

To understand the best strategies in repeated [[the prisoners dilemma | Prisoner's Dilemma]] scenarios, political scientist Robert Axelrod held computer tournaments in 1980 <a class="yt-timestamp" data-t="00:06:18">[00:06:18]</a>. Leading [[game theory and life | game theorists]] submitted computer programs, or "strategies," to play against each other over 200 rounds <a class="yt-timestamp" data-t="00:06:26">[00:06:26]</a>.

### Tournament Results

Despite complex submissions like "Graaskamp" and "Name Withheld," the simplest program, **Tit for Tat**, emerged victorious <a class="yt-timestamp" data-t="00:08:10">[00:08:10]</a>.

#### Tit for Tat
Tit for Tat starts by cooperating and then simply copies its opponent's last move <a class="yt-timestamp" data-t="00:08:28">[00:08:28]</a>. If its opponent cooperates, Tit for Tat cooperates. If its opponent defects, Tit for Tat defects in the next round, but then reverts to cooperation if the opponent does <a class="yt-timestamp" data-t="00:08:35">[00:08:35]</a>.

*   **Against Friedman (unforgiving):** Both started cooperating and maintained perfect scores <a class="yt-timestamp" data-t="00:08:45">[00:08:45]</a>.
*   **Against Joss (nasty, probabilistic defector):** Joss's defection sparked a series of back-and-forth retaliations, resulting in poor scores for both <a class="yt-timestamp" data-t="00:08:54">[00:08:54]</a>.

### Qualities of Successful Strategies

Axelrod identified four key qualities shared by the best-performing strategies, including Tit for Tat <a class="yt-timestamp" data-t="00:10:08">[00:10:08]</a>:

1.  **Nice:** Strategies that are never the first to defect <a class="yt-timestamp" data-t="00:10:15">[00:10:15]</a>. The top eight strategies in the tournament were all nice <a class="yt-timestamp" data-t="00:10:37">[00:10:37]</a>.
2.  **Forgiving:** Strategies that retaliate but do not hold a grudge, meaning past defections (before the last round) don't influence current decisions <a class="yt-timestamp" data-t="00:10:49">[00:10:49]</a>. Friedman, for example, was maximally unforgiving, defecting for the rest of the game after a single opponent defection <a class="yt-timestamp" data-t="00:11:05">[00:11:05]</a>.
3.  **Retaliatory:** Strategies that strike back immediately if an opponent defects, avoiding being a pushover <a class="yt-timestamp" data-t="00:14:38">[00:14:38]</a>. "Always cooperate" is easily exploited, whereas Tit for Tat is difficult to take advantage of <a class="yt-timestamp" data-t="00:14:47">[00:14:47]</a>.
4.  **Clear:** Strategies that are easy for other programs to understand and predict, fostering patterns of trust <a class="yt-timestamp" data-t="00:14:59">[00:14:59]</a>.

These principles—being nice, forgiving, provokable (retaliatory), and clear—are remarkably similar to certain moral codes summarized as "an eye for an eye" <a class="yt-timestamp" data-t="00:15:31">[00:15:31]</a>.

### The Importance of Uncertainty

A crucial aspect of Axelrod's tournaments was that players did not know the exact number of rounds <a class="yt-timestamp" data-t="00:13:00">[00:13:00]</a>. If the last round is known, rational players will defect in that round, which then unravels cooperation all the way back to the first round <a class="yt-timestamp" data-t="00:12:32">[00:12:32]</a>. Uncertainty about the end time encourages continued cooperation <a class="yt-timestamp" data-t="00:13:13">[00:13:13]</a>.

### Introducing Noise

Axelrod later explored the impact of "noise" or random error, where a cooperative move might be misinterpreted as a defection <a class="yt-timestamp" data-t="00:19:36">[00:19:36]</a>. An example is the 1983 Soviet false alarm of a US missile launch due to sunlight reflecting off clouds <a class="yt-timestamp" data-t="00:19:50">[00:19:50]</a>.

In a noisy environment, Tit for Tat can fall into alternating retaliations, leading to constant mutual defection and significantly reduced scores <a class="yt-timestamp" data-t="00:20:47">[00:20:47]</a>. The solution is a more forgiving strategy, such as **Generous Tit for Tat**, which cooperates even after a perceived defection about 10% of the time, allowing it to break out of echo effects while still being retaliatory <a class="yt-timestamp" data-t="00:21:21">[00:21:21]</a>.

## Evolutionary Implications

Axelrod's work has significant implications for [[evolutionary biology and cooperation | evolutionary biology and cooperation]]. He ran simulations where successful strategies multiplied and unsuccessful ones diminished, mimicking a form of natural selection, though referred to as an "ecological simulation" since it didn't involve mutations <a class="yt-timestamp" data-t="00:16:35">[00:16:35]</a>.

In these simulations, nasty strategies initially thrived by preying on more cooperative ones, but eventually dwindled as their exploited counterparts went extinct <a class="yt-timestamp" data-t="00:16:51">[00:16:51]</a>. After many generations, only nice strategies survived, with Tit for Tat often emerging as dominant <a class="yt-timestamp" data-t="00:17:11">[00:17:11]</a>.

This demonstrates how cooperation can emerge and spread even in a population of self-interested individuals <a class="yt-timestamp" data-t="00:18:02">[00:18:02]</a>. A small "island of cooperation" can form and expand, eventually taking over the population, without requiring altruism <a class="yt-timestamp" data-t="00:17:46">[00:17:46]</a>. The strategies, encoded in DNA, simply spread if they perform better than others <a class="yt-timestamp" data-t="00:19:11">[00:19:11]</a>. This offers a possible explanation for how cooperation flourished in a world of initially selfish organisms <a class="yt-timestamp" data-t="00:18:38">[00:18:38]</a>.

### Win-Win Situations

A common misconception is that winning requires beating an opponent. However, most of [[game theory and life | life]] is not a zero-sum game, where one's gain is another's loss <a class="yt-timestamp" data-t="00:22:41">[00:22:41]</a>. In many scenarios, rewards can be gained from a shared "banker" (the world), making cooperation a path to mutual benefit <a class="yt-timestamp" data-t="00:22:46">[00:22:46]</a>.

For example, the US and Soviet Union, after years of mutual nuclear development, began reducing their stockpiles from the late 1980s onward <a class="yt-timestamp" data-t="00:23:08">[00:23:08]</a>. They achieved this by disarming slowly and checking each other's compliance, turning a single [[the prisoners dilemma | Prisoner's Dilemma]] into a series of repeated interactions where cooperation could be built <a class="yt-timestamp" data-t="00:23:24">[00:23:24]</a>.

Ultimately, while the optimal strategy can vary based on the environment, Axelrod's key takeaways remain: be nice, forgiving, and retaliatory, but don't be a pushover <a class="yt-timestamp" data-t="00:24:09">[00:24:09]</a>. These insights underscore that in the long run, players shape their environment, and making wise choices can have far-reaching impacts <a class="yt-timestamp" data-t="00:25:08">[00:25:08]</a>.