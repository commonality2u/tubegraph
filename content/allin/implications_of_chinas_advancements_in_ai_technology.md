---
title: Implications of Chinas advancements in AI technology
videoId: 8RkgkOqWs0s
---

From: [[allin]] <br/> 

The release of DeepSeek's R1 language model has highlighted the accelerating pace of [[ai_advancements_and_the_impact_on_technology_and_society | AI advancements]] in China, creating significant discussion regarding global competition and innovation <a class="yt-timestamp" data-t="00:15:40">[00:15:40]</a>.

## The DeepSeek R1 Model

DeepSeek, a Chinese [[artificial_intelligence_and_its_economic_impact | AI]] startup, released its R1 language model, which is considered on par with some of the best models produced in the West, such as [[openai_and_ai_advancements | OpenAI's]] 01 model <a class="yt-timestamp" data-t="00:15:47">[00:15:47]</a>. This release was surprising to many, as it suggested that China was closer to the AI frontier than previously thought, potentially shifting the perceived gap from six to twelve months down to three to six months <a class="yt-timestamp" data-t="00:21:36">[00:21:36]</a>.

### Cost Claims and Compute Resources
DeepSeek claimed to have trained R1 for just $6 million using only 2,000 GPUs <a class="yt-timestamp" data-t="00:16:02">[00:16:02]</a>. This contrasts sharply with [[openai_and_ai_advancements | OpenAI's]] reported $800 million expenditure for GPT-4 and projected $1 billion for GPT-5 <a class="yt-timestamp" data-t="00:16:06">[00:16:06]</a>. However, this $6 million claim is largely debunked by experts, who argue that this figure likely only refers to the final training run <a class="yt-timestamp" data-t="00:22:04">[00:22:04]</a>. The fully loaded cost, including R&D and the substantial compute cluster, is estimated to be over a billion dollars <a class="yt-timestamp" data-t="00:25:25">[00:25:25]</a>. DeepSeek is believed to possess a cluster of about 50,000 Hoppers, including 10,000 H100s, 10,000 H800s, and 30,000 H20s, which were likely acquired before export controls were fully implemented <a class="yt-timestamp" data-t="00:24:31">[00:24:31]</a>.

### Distillation and Open Source
There is strong evidence, including self-identification by DeepSeek's V3 model as ChatGPT, suggesting that DeepSeek's models have undergone "distillation," meaning they were trained using output from larger models like [[openai_and_ai_advancements | OpenAI's]] GPT-4 <a class="yt-timestamp" data-t="00:35:08">[00:35:08]</a>. This process involves a smaller model learning from the responses of a larger, more complex model <a class="yt-timestamp" data-t="00:31:22">[00:31:22]</a>. While the exact method of acquisition (e.g., web crawling public output or API access) is debated, [[openai_and_ai_advancements | OpenAI]] itself has indicated improper distillation <a class="yt-timestamp" data-t="00:36:08">[00:36:08]</a>.

A key aspect of DeepSeek's R1 release is its open-source nature, offering API access at a significantly lower cost compared to Western counterparts <a class="yt-timestamp" data-t="00:21:21">[00:21:21]</a>. This move challenges the closed-source approach taken by many leading [[ai_advancements_and_the_impact_on_technology_and_society | AI]] companies in the West, and some view it as a realization of the original open-source mission that [[openai_and_ai_advancements | OpenAI]] was initially founded upon <a class="yt-timestamp" data-t="00:40:04">[00:40:04]</a>.

## China's Innovation Under Constraint

China's approach to [[ai_advancements_and_the_impact_on_technology_and_society | AI]] development demonstrates "necessity as the mother of invention" <a class="yt-timestamp" data-t="00:26:51">[00:26:51]</a>. Despite export restrictions on advanced GPUs, Chinese firms are finding innovative ways to develop competitive models. This includes:
*   **Novel Algorithms:** DeepSeek developed a new reinforcement learning algorithm (GRPO) that uses less computer memory and is highly performant, differing from the previously dominant PPO algorithm used in the West <a class="yt-timestamp" data-t="00:27:17">[00:27:17]</a>.
*   **Circumventing Proprietary Languages:** Chinese developers have worked around Nvidia's proprietary CUDA language, using PTX to directly access the bare metal of the chips, demonstrating a high level of technical ingenuity <a class="yt-timestamp" data-t="00:28:01">[00:28:01]</a>.
*   **Rapid Copying and Innovation:** Based on observations from Uber's operations in China, Chinese companies have an "epic" ability to rapidly copy and then innovate upon existing technologies, leading to new solutions not seen elsewhere <a class="yt-timestamp" data-t="00:51:25">[00:51:25]</a>. An example is the widespread use of smart lockers in Chinese office buildings for food and package delivery, which optimizes courier efficiency <a class="yt-timestamp" data-t="00:53:56">[00:53:56]</a>.

## Geopolitical and Economic Implications

The competition in [[artificial_intelligence_and_its_economic_impact | AI]] is a significant aspect of the broader US-China rivalry <a class="yt-timestamp" data-t="00:18:09">[00:18:09]</a>.

### Export Controls and Self-Reliance
The US has implemented export restrictions on Nvidia H100 GPUs to China <a class="yt-timestamp" data-t="00:22:21">[00:22:21]</a>, aiming to curb China's [[ai_advancements_and_the_impact_on_technology_and_society | AI]] progress. However, there are concerns that these controls may be futile, with chips potentially being rerouted through places like Singapore, where a significant portion of Nvidia's revenue is directed <a class="yt-timestamp" data-t="00:57:05">[00:57:05]</a>. Furthermore, cutting off access could force China to develop its own chip manufacturing capabilities and design chips that circumvent the most complex Western technologies, leveraging [[artificial_intelligence_and_its_economic_impact | AI]] to design simpler, yet effective, chips <a class="yt-timestamp" data-t="00:59:21">[00:59:21]</a>.

### Commoditization and Value Shift in AI
The rapid progress and open-source nature of models like R1 suggest that [[artificial_intelligence_and_its_economic_impact | AI]] models themselves could become commoditized much faster than anticipated <a class="yt-timestamp" data-t="00:29:21">[00:29:21]</a>. This shifts the point of value creation in the [[impact_of_ai_on_different_industries_and_technical_fields | AI]] value chain away from base models to the application layer, similar to how YouTube was built on storage or Uber on GPS <a class="yt-timestamp" data-t="00:43:42">[00:43:42]</a>. The argument is that the fastest depreciating asset in the world is a large language model <a class="yt-timestamp" data-t="00:44:04">[00:44:04]</a>.

### The Role of Government and Capitalism
In China, a central authority might bear the capital expenditure for developing advanced [[artificial_intelligence_and_its_economic_impact | AI]] models, which can then be freely used by Chinese companies, effectively operating with a "golden vote" and a seat on their boards <a class="yt-timestamp" data-t="01:01:13">[01:01:13]</a>. This contrasts with the Western model, where private companies seek venture capital, potentially leading to overcapitalization and bureaucracy, which could hinder agile innovation compared to constraint-driven development <a class="yt-timestamp" data-t="01:04:48">[01:04:48]</a>.

### [[future_of_ai_in_technology_and_society | Future of AI]] and Economic Impact
The decreasing cost of [[artificial_intelligence_and_its_economic_impact | AI]] (cheap AI) is expected to significantly increase its usage across various applications and industries, similar to Jevons' Paradox, where the efficiency of resource use leads to increased demand <a class="yt-timestamp" data-t="00:45:23">[00:45:23]</a>. This will lead to more specialized [[artificial_intelligence_and_its_economic_impact | AI]] models for specific tasks (e.g., investor AI, autonomous car AI) <a class="yt-timestamp" data-t="00:46:06">[00:46:06]</a>. The true competitive advantage in [[ai_advancements_and_the_impact_on_technology_and_society | AI]] may not lie in owning the largest data center networks, but in proprietary data and content, and the ability to leverage this data to build continuously improving products <a class="yt-timestamp" data-t="01:09:07">[01:09:07]</a>.