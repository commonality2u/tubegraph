---
title: Monetization of online data and content licensing deals
videoId: -Am0vMW3fA0
---

From: [[allin]] <br/> 

The emergence of [[AI Developments and Business Models | AI]] has led to a new paradigm for content monetization, with major tech companies entering into significant licensing deals for training data. This shift is being described as "Traffic Acquisition Cost (TAC) 2.0," evolving from previous models of traffic acquisition. <a class="yt-timestamp" data-t="00:42:01">[00:42:01]</a>

## Traffic Acquisition Cost (TAC) 1.0 vs. TAC 2.0

Historically, Google's "Traffic Acquisition Cost" (TAC 1.0) model involved paying partners to feature Google search, which would then generate revenue through ads. <a class="yt-timestamp" data-t="00:42:03">[00:42:03]</a> This model scaled from small companies to large ones like Apple, with Google paying Apple between $18 billion and $20 billion annually to be the default search engine on iPhones. <a class="yt-timestamp" data-t="00:43:05">[00:43:05]</a>

The new "TAC 2.0" framework sees companies like [[Google earnings and tech monopolies | Google]] paying for data to train their [[AI Developments and Business Models | AI]] models, rather than solely for search distribution. <a class="yt-timestamp" data-t="00:43:19">[00:43:19]</a> This represents a significant revenue stream for businesses with unique and high-quality data. <a class="yt-timestamp" data-t="00:43:34">[00:43:34]</a>

## Key Licensing Deals and Their Implications

Recent examples of these deals highlight the growing trend:
*   **[[Reddit IPO analysis and business growth challenges | Reddit]]**: [[Google earnings and tech monopolies | Google]] reportedly made a $60 million annual deal with [[Reddit IPO analysis and business growth challenges | Reddit]] for its data. <a class="yt-timestamp" data-t="00:40:26">[00:40:26]</a> [[Reddit IPO analysis and business growth challenges | Reddit]]'s S-1 filing indicates they have closed $200 million worth of [[AI Developments and Business Models | AI]] licensing deals over the next two to three years. <a class="yt-timestamp" data-t="00:40:43">[00:40:43]</a>
*   **Stack Overflow**: This platform is now using its API to train [[Google earnings and tech monopolies | Google]]'s Gemini [[AI Developments and Business Models | AI]]. <a class="yt-timestamp" data-t="00:40:33">[00:40:33]</a>
*   **Axel Springer**: Partnered with [[openais_revenue_growth_and_business_model | OpenAI]] for content licensing. <a class="yt-timestamp" data-t="00:41:00">[00:41:00]</a>
*   **CNN, Fox, Time**: [[openais_revenue_growth_and_business_model | OpenAI]] is reportedly in talks with these media companies to license their content. <a class="yt-timestamp" data-t="00:41:06">[00:41:06]</a>

These deals are in response to lawsuits, such as the New York Times' case against [[openais_revenue_growth_and_business_model | OpenAI]] over [[role_of_ai_in_content_generation_and_copyrights | copyright infringement]]. <a class="yt-timestamp" data-t="00:41:11">[00:41:11]</a> Both [[openais_revenue_growth_and_business_model | OpenAI]] and [[Google earnings and tech monopolies | Google]]'s Gemini are actively "guardrailing" their systems to prevent [[role_of_ai_in_content_generation_and_copyrights | copyright infringement]]. <a class="yt-timestamp" data-t="00:41:40">[00:41:40]</a>

## Value of Data and Content

The value of content for [[AI Developments and Business Models | AI]] training depends on several factors:
*   **Uniqueness and Proprietary Nature**: Data that is unique and proprietary (like user-generated content from [[Reddit IPO analysis and business growth challenges | Reddit]]) can command high prices. <a class="yt-timestamp" data-t="00:44:06">[00:44:06]</a>
*   **Freshness**: For certain types of content, such as news, data can quickly become stale, reducing its value over time. <a class="yt-timestamp" data-t="00:47:24">[00:47:24]</a>
*   **Attribution**: A challenge is determining how much incremental value a model derives from specific datasets, especially for smaller websites. <a class="yt-timestamp" data-t="00:45:21">[00:45:21]</a>

The market for this type of content licensing is compared to the [[streaming_service_dynamics_competition_and_sustainability | content licensing deals]] seen in the entertainment industry (e.g., Netflix paying studios). <a class="yt-timestamp" data-t="00:46:44">[00:46:44]</a>

### Data Volume and Future Trends
Currently, there are approximately one million petabytes of data on the internet, with humans generating about 2,500 petabytes of new data daily. <a class="yt-timestamp" data-t="00:47:41">[00:47:41]</a> However, about half of all generated data is never used, and the majority of public domain data is not on the internet. <a class="yt-timestamp" data-t="00:47:57">[00:47:57]</a> The rate of data generation is continuously increasing, potentially making older data less valuable over time. <a class="yt-timestamp" data-t="00:49:11">[00:49:11]</a>

## Challenges and Opportunities for Content Creators

*   **Small Websites**: While large platforms like [[Reddit IPO analysis and business growth challenges | Reddit]] can secure multi-million dollar deals, the monetization potential for smaller websites remains uncertain. <a class="yt-timestamp" data-t="00:44:46">[00:44:46]</a>
*   **Exclusivity**: Current deals, like [[Reddit IPO analysis and business growth challenges | Reddit]]'s with [[Google earnings and tech monopolies | Google]], appear to be non-exclusive, meaning content providers can license their data to multiple [[AI Developments and Business Models | AI]] developers. <a class="yt-timestamp" data-t="00:50:00">[00:50:00]</a> Some argue that major [[AI Developments and Business Models | AI]] companies should pursue exclusive deals to block competitors. <a class="yt-timestamp" data-t="00:50:08">[00:50:08]</a>
*   **Unionization/Federation**: The idea of content creators, particularly news organizations, forming a federation to collectively bargain with tech giants for better terms is suggested, akin to the music industry's successful efforts in licensing. <a class="yt-timestamp" data-t="00:52:26">[00:52:26]</a> This could prevent a situation where a large number of content suppliers are competing for a limited number of buyers, driving down prices. <a class="yt-timestamp" data-t="00:52:50">[00:52:50]</a>