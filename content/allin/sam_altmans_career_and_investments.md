---
title: Sam Altmans career and investments
videoId: nSM0xd8xHUM
---

From: [[allin]] <br/> 

Sam Altman is a prominent figure in technology, known for his work in venture capital, startups, and artificial intelligence.

## Early Career and Investments

Altman first met one of the podcast hosts almost 20 years ago when he was working on a local mobile app called Looped <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a>. Both were backed by Sequoia Capital and were part of the first class of Sequoia Scouts <a class="yt-timestamp" data-t="00:00:07">[00:00:07]</a>. Through this program, Altman invested in the fintech company Stripe <a class="yt-timestamp" data-t="00:00:14">[00:00:14]</a>. The Sequoia Scouts experimental fund achieved Sequoia's highest multiple return, turning a couple of low-digit millions into over $200 million <a class="yt-timestamp" data-t="00:00:43">[00:00:43]</a>.

## Y Combinator

From 2014 to 2019, Altman served as the president of Y Combinator <a class="yt-timestamp" data-t="00:00:57">[00:00:57]</a>.

## OpenAI Leadership and Development

In 2016, Altman co-founded OpenAI with the mission to ensure that Artificial General Intelligence (AGI) benefits all of humanity <a class="yt-timestamp" data-t="00:01:01">[00:01:01]</a>. He left Y Combinator in 2019 to become OpenAI's full-time CEO <a class="yt-timestamp" data-t="00:01:09">[00:01:09]</a>.

### ChatGPT Launch and Growth
On November 30, 2022, OpenAI launched ChatGPT <a class="yt-timestamp" data-t="00:01:16">[00:01:16]</a>. This launch contributed to a significant increase in public awareness and interest in AI, as many people did not consider AI to be important or happening before this event <a class="yt-timestamp" data-t="00:10:14">[00:10:14]</a>.

ChatGPT quickly became the fastest product in history to reach 100 million users, achieving this milestone in just two months <a class="yt-timestamp" data-t="00:02:10">[00:02:10]</a>. In January 2023, Microsoft invested $10 billion in OpenAI <a class="yt-timestamp" data-t="00:01:21">[00:01:21]</a>. OpenAI reportedly hit $2 billion in Annual Recurring Revenue (ARR) last year <a class="yt-timestamp" data-t="00:02:17">[00:02:17]</a>.

### November 2023 Board Dispute and Return
In November 2023, Altman experienced a turbulent five-day period during which he was fired from OpenAI <a class="yt-timestamp" data-t="00:01:25">[00:01:25]</a>. At one point, it was speculated that the entire team would move to Microsoft <a class="yt-timestamp" data-t="00:01:29">[00:01:29]</a>. Altman was confused by the firing, which he learned about via text message and a phone call while in Las Vegas <a class="yt-timestamp" data-t="00:54:07">[00:54:07]</a>. He realized he "really loved OpenAI and the people" <a class="yt-timestamp" data-t="00:53:14">[00:53:14]</a> and ultimately returned as CEO a few days later <a class="yt-timestamp" data-t="00:01:42">[00:01:42]</a>.

Altman stated that the board at the time, which was a non-profit board, made the decision to fire him <a class="yt-timestamp" data-t="00:55:57">[00:55:57]</a>. While he disagreed with their decision-making, he never doubted their integrity or commitment to the shared mission of safe and beneficial AGI <a class="yt-timestamp" data-t="00:56:56">[00:56:56]</a>. He also clarified that various projects attributed to him, such as device companies or chip fabrication companies, are part of OpenAI, not personal ventures <a class="yt-timestamp" data-t="01:01:27">[01:01:27]</a>.

## Future Visions and Investments

Altman is reportedly looking to raise significant capital for AI chip projects, though he stated the reported "$7 trillion" figure is inaccurate and he doesn't know where it came from <a class="yt-timestamp" data-t="01:01:14">[01:01:14]</a>. He believes the world needs "a lot more AI infrastructure" than currently planned <a class="yt-timestamp" data-t="01:02:21">[01:02:21]</a>. Previously, there were reports he aimed to raise $1 billion for an "iPhone killer" project with Jony Ive <a class="yt-timestamp" data-t="00:01:54">[00:01:54]</a>.

### AI Model Development
Altman indicates that OpenAI takes its time with major model releases and may release future versions differently, potentially not even calling the next one "GPT-5" <a class="yt-timestamp" data-t="00:02:45">[00:02:45]</a>. He suggests a future where AI systems continuously improve rather than having distinct version numbers <a class="yt-timestamp" data-t="00:03:27">[00:03:27]</a>.

A key goal for OpenAI is to make advanced technology available to free users, despite the high cost of running models like GPT-4 <a class="yt-timestamp" data-t="00:05:04">[00:05:04]</a>. Altman believes that the cost and latency of AI models will dramatically decrease due to ongoing scientific development and engineering advancements <a class="yt-timestamp" data-t="00:06:27">[00:06:27]</a>.

### Open Source vs. Closed Source
Altman acknowledges the roles for both open-source and closed-source AI models <a class="yt-timestamp" data-t="00:07:10">[00:07:10]</a>. OpenAI's primary mission is to build towards AGI and widely distribute its benefits <a class="yt-timestamp" data-t="00:07:17">[00:07:17]</a>. He expressed personal interest in an open-source model capable of running effectively on a phone <a class="yt-timestamp" data-t="00:07:41">[00:07:41]</a>. He believes that the value of models will eventually shift beyond raw accuracy to proprietary training data, leading to an "arms race for data" <a class="yt-timestamp" data-t="00:12:15">[00:12:15]</a>. However, he later stated that once models are smart enough, it shouldn't be about more data for training, but rather for making them useful <a class="yt-timestamp" data-t="00:12:36">[00:12:36]</a>.

### AI Interface and Applications
Altman is interested in new form factors for computing enabled by AI <a class="yt-timestamp" data-t="00:15:47">[00:15:47]</a>. While he considers the iPhone the "greatest piece of technology Humanity has ever made" <a class="yt-timestamp" data-t="00:16:10">[00:16:10]</a>, he believes the next big innovation needs a "really different interaction paradigm" that technology enables <a class="yt-timestamp" data-t="00:17:09">[00:17:09]</a>. He sees voice interaction as a hint for the next computing paradigm <a class="yt-timestamp" data-t="00:17:34">[00:17:34]</a>.

He envisions an "always-on, super low friction" AI assistant that helps throughout the day, with as much context as possible, acting as a "world's greatest assistant" <a class="yt-timestamp" data-t="00:19:24">[00:19:24]</a>. This assistant would be more like a "great senior employee" than an extension of oneself, capable of pushing back and reasoning <a class="yt-timestamp" data-t="00:20:25">[00:20:25]</a>. He aims to design a world "equally usable by humans and by AIs" <a class="yt-timestamp" data-t="00:22:37">[00:22:37]</a>, with shared interfaces rather than AI-specific ones <a class="yt-timestamp" data-t="00:23:19">[00:23:19]</a>.

Areas where he sees significant potential for AI include:
*   **AI Tutors:** A "monorail-level reinvention for how people learn" <a class="yt-timestamp" data-t="00:25:48">[00:25:48]</a>.
*   **Coding:** Tools like Devin show a cool vision for the future <a class="yt-timestamp" data-t="00:26:20">[00:26:20]</a>.
*   **Healthcare:** Believes it "should be pretty transformed" by AI <a class="yt-timestamp" data-t="00:26:26">[00:26:26]</a>.
*   **Scientific Discovery:** Personally most excited about AI enabling faster and better scientific discovery <a class="yt-timestamp" data-t="00:26:34">[00:26:34]</a>.

He highlights the importance of "reasoning" in models, suggesting that once models can reason, they can be connected to various simulators (e.g., chemistry) to address complex problems <a class="yt-timestamp" data-t="00:27:25">[00:27:25]</a>. He believes generalized reasoning could be a "fast unlock" for new problem domains, similar to how humans generalize <a class="yt-timestamp" data-t="00:31:49">[00:31:49]</a>.

## AI and Copyright/Fair Use

Altman acknowledges that the issue of AI training data and fair use is complex <a class="yt-timestamp" data-t="00:34:10">[00:34:10]</a>. While his legal position on fair use is "reasonable under the current law," he believes AI is so different that "for things like art, we'll need to think about them in different ways" <a class="yt-timestamp" data-t="00:34:14">[00:34:14]</a>. He distinguishes between generalized human knowledge (e.g., math) and art, especially systems generating art "in the style or the likeness of another artist" <a class="yt-timestamp" data-t="00:34:50">[00:34:50]</a>.

He anticipates that the debate will shift from training data to what happens at "inference time" as training data becomes less valuable <a class="yt-timestamp" data-t="00:35:36">[00:35:36]</a>. For instance, if an AI generates a song "in the style of Taylor Swift," even if not trained on her songs directly, questions arise about whether this should be allowed and how the artist should be compensated <a class="yt-timestamp" data-t="00:36:08">[00:36:08]</a>. OpenAI has currently decided not to pursue music generation partly due to these difficult questions <a class="yt-timestamp" data-t="00:38:27">[00:38:27]</a>. OpenAI also has internal policies preventing the generation of specific IP, such as Darth Vader <a class="yt-timestamp" data-t="00:40:25">[00:40:25]</a>.

Altman agrees that there is "something beautiful about human creativity and human artistic expression" <a class="yt-timestamp" data-t="00:39:53">[00:39:53]</a>, and while AI will be a tool leading to "greater creative heights," it should be done in a way that "preserves the spirit" of human artistry <a class="yt-timestamp" data-t="00:40:10">[00:40:10]</a>.

## AI Regulation

Altman expresses concern about various proposed AI regulations, especially those at the state level <a class="yt-timestamp" data-t="00:42:15">[00:42:15]</a>. He believes that in the not-too-distant future, "frontier AI systems" will be "capable of causing significant global harm" <a class="yt-timestamp" data-t="00:42:55">[00:42:55]</a>. For such systems, he advocates for an international agency to provide global oversight, similar to regulations for nuclear weapons or synthetic biology <a class="yt-timestamp" data-t="00:43:06">[00:43:06]</a>. This agency would ensure "reasonable safety testing" to prevent scenarios like recursive self-improvement or autonomous bioweapon deployment <a class="yt-timestamp" data-t="00:48:51">[00:48:51]</a>.

He supports safety testing on the *outputs* of powerful models, akin to certifying an airplane's safety, rather than auditing source code or weights, which he views as "crazy proposals" that would quickly become outdated <a class="yt-timestamp" data-t="00:46:47">[00:46:47]</a>. He acknowledges concerns about regulatory capture and the burden on startups, suggesting that regulation could apply only to models trained on extremely expensive computational resources (e.g., >$10 billion) <a class="yt-timestamp" data-t="00:43:49">[00:43:49]</a>. Altman emphasizes the need for policymakers to avoid "regulatory overreach" while also doing enough to manage potential risks <a class="yt-timestamp" data-t="00:44:30">[00:44:30]</a>.

## Universal Basic Income (UBI) / Universal Basic Compute (UBC)

Altman's interest in Universal Basic Income (UBI) dates back to 2016, around the same time he started taking AI seriously <a class="yt-timestamp" data-t="00:50:22">[00:50:22]</a>. He theorized that the magnitude of change AI would bring to society, jobs, and the economy necessitated exploring new social arrangements <a class="yt-timestamp" data-t="00:50:31">[00:50:31]</a>. He believes that giving people money could enable them to make good decisions, potentially solving some problems and lifting people out of poverty more effectively than traditional government policies <a class="yt-timestamp" data-t="00:51:01">[00:51:01]</a>.

However, with the way AI is developing, Altman now wonders if "Universal Basic Compute" (UBC) might be a more fitting future model than UBI <a class="yt-timestamp" data-t="00:52:03">[00:52:03]</a>. In this model, individuals would receive a "slice of GPT-7 compute" that they could use, resell, or donate for purposes like cancer research <a class="yt-timestamp" data-t="00:52:09">[00:52:09]</a>.

## OpenAI's Structure and Altman's Compensation

OpenAI's unique structure, starting as a non-profit, influenced Altman's compensation. He does not hold equity in OpenAI's for-profit arm <a class="yt-timestamp" data-t="00:59:12">[00:59:12]</a>. He explains that this was partly due to the non-profit's requirement for a majority of disinterested directors on its board <a class="yt-timestamp" data-t="00:59:45">[00:59:45]</a>. He notes that people find it "unimaginable" that he doesn't need more money, leading to "conspiracy theories" about his motivations <a class="yt-timestamp" data-t="01:00:07">[01:00:07]</a>. He jokingly states it would save "a lot of conspiracy theories" if he were just trying to make a trillion dollars with OpenAI <a class="yt-timestamp" data-t="01:00:26">[01:00:26]</a>.

## Key Takeaways from Interviewers

The interviewers noted that OpenAI is poised to be one of the four major companies in the AI space <a class="yt-timestamp" data-t="01:05:47">[01:05:47]</a>. They interpret Altman's comments to mean that while AI models will become largely similar in capability, the economic value will reside in the "scaffolding" and infrastructure built around these models, much like open-source software running on cloud platforms <a class="yt-timestamp" data-t="01:06:08">[01:06:08]</a>. They also highlighted Altman's focus on "reasoning" capabilities in AI, suggesting that AI models will need to integrate multi-dimensional inputs to achieve intelligent reasoning, moving beyond just language models <a class="yt-timestamp" data-t="01:06:48">[01:06:48]</a>. There is also an acknowledgement that there are many developments outside of Large Language Models (LLMs) that deserve attention <a class="yt-timestamp" data-t="01:07:30">[01:07:30]</a>.