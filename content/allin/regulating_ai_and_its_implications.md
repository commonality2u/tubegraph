---
title: Regulating AI and its implications
videoId: nSM0xd8xHUM
---

From: [[allin]] <br/> 

The discussion around [[ai_regulation_debate | AI regulation]] encompasses a wide range of viewpoints and challenges, from intellectual property rights to the fundamental structure of future AI systems and their societal impact <a class="yt-timestamp" data-t="00:42:27">[00:42:27]</a>.

## Defining AI Regulation

When people discuss regulating AI, they do not mean one single thing <a class="yt-timestamp" data-t="00:42:27">[00:42:27]</a>. Some suggest banning the entire technology, while others advocate for requirements related to open-source or closed-source models <a class="yt-timestamp" data-t="00:42:31">[00:42:31]</a>. There is a general [[ai_regulation_debate | concern about regulatory overreach]] and the possibility of getting regulation wrong by either doing too much or too little <a class="yt-timestamp" data-t="00:44:33">[00:44:33]</a>.

## Future of AI Systems and Oversight

There is an expectation that in the not-too-distant future, "frontier AI systems" will be capable of causing significant global harm <a class="yt-timestamp" data-t="00:42:55">[00:42:55]</a>. For such powerful systems, there is an interest in developing an international agency similar to those overseeing nuclear weapons or synthetic biology <a class="yt-timestamp" data-t="00:43:06">[00:43:06]</a>. This agency would focus on ensuring reasonable safety testing and preventing scenarios like AIs escaping and recursively self-improving, or autonomously designing and deploying bioweapons <a class="yt-timestamp" data-t="00:43:24">[00:43:24]</a>, <a class="yt-timestamp" data-t="00:48:51">[00:48:51]</a>.

### Challenges of Regulatory Implementation
Current legislative proposals, such as those from California and some federal initiatives, raise [[impact_of_ai_regulation_on_innovation_and_global_competitiveness | concerns about requiring government auditing]] of AI models, source code, parameters, and weightings before deployment for commercial or public use <a class="yt-timestamp" data-t="00:45:31">[00:45:31]</a>. Such requirements are viewed as potentially leading to a "dark ages type of era" by [[impact_of_ai_regulation_on_innovation_and_global_competitiveness | restricting the growth of these incredible technologies]] <a class="yt-timestamp" data-t="00:47:34">[00:47:34]</a>.

The rapid pace of AI development means that laws written today could be obsolete in 12 to 24 months <a class="yt-timestamp" data-t="00:46:18">[00:46:18]</a>. An agency-based approach, rather than rigid laws, is preferred for large-scale oversight because of the technology's rapid evolution <a class="yt-timestamp" data-t="00:46:23">[00:46:23]</a>. This approach would be similar to how airplanes undergo safety tests and certification, focusing on reviewing the output of the model rather than its internal code <a class="yt-timestamp" data-t="00:47:03">[00:47:03]</a>.

### Regulatory Capture Concerns
There are [[impact_of_ai_regulation_on_innovation_and_global_competitiveness | criticisms that large companies]] with significant resources can lobby and influence politicians, potentially leading to regulatory capture that disadvantages startups <a class="yt-timestamp" data-t="00:43:32">[00:43:32]</a>. However, a potential solution suggested is to only apply regulation to models trained on extremely expensive computing resources (e.g., costing more than $10 billion), thereby not burdening startups <a class="yt-timestamp" data-t="00:44:01">[00:44:01]</a>.

## Intellectual Property and Fair Use

The conversation around AI and intellectual property has historically focused on training data, but it is expected to increasingly shift to "inference time" â€“ what the system does when generating output <a class="yt-timestamp" data-t="00:35:36">[00:35:36]</a>.

There's a spectrum of how different types of content should be treated:
*   **Generalized human knowledge:** Learning from publicly available information (e.g., math theorems) is generally seen as unobjectionable <a class="yt-timestamp" data-t="00:34:29">[00:34:29]</a>.
*   **Artistic Style/Likeness:** Generating art in the style or likeness of another artist is considered the "furthest end" of the spectrum in terms of potential issues <a class="yt-timestamp" data-t="00:35:16">[00:35:16]</a>. If an AI generates a song in the style of a specific artist, even if not trained on their original works, questions arise about how the artist should be compensated <a class="yt-timestamp" data-t="00:36:11">[00:36:11]</a>.
*   **Corporate IP:** Companies like OpenAI are making decisions to prevent their models from directly generating specific intellectual property, such as "Darth Vader" characters <a class="yt-timestamp" data-t="00:40:27">[00:40:27]</a>. However, there are complexities when cultural elements, like "Sith Lord" or "Jedi," become part of the broader culture <a class="yt-timestamp" data-t="00:41:18">[00:41:18]</a>.

Analogy to music sampling is suggested as an interesting historical perspective, though AI generation is not exactly the same <a class="yt-timestamp" data-t="00:36:50">[00:36:50]</a>. The fundamental difference between an AI learning music structure from a vast corpus and a human doing the same is questioned <a class="yt-timestamp" data-t="00:37:01">[00:37:01]</a>. However, directly prompting an AI to create content in the style of a specific artist (e.g., "song in the style of Taylor Swift") presents a distinct case <a class="yt-timestamp" data-t="00:37:51">[00:37:51]</a>.

## Societal and Economic [[future_of_ai_in_technology_and_society | Implications]]

The development of AI, particularly [[artificial_intelligence_and_its_economic_impact | Artificial General Intelligence (AGI)]], is seen as unavoidable and potentially tremendously beneficial, but requires careful navigation due to the significant changes it will bring <a class="yt-timestamp" data-t="00:58:35">[00:58:35]</a>. There are [[implications_of_ai_on_traditional_tech_jobs_and_growth | concerns about AI's impact on jobs and the economy]] <a class="yt-timestamp" data-t="00:50:31">[00:50:31]</a>.

### Universal Basic Income (UBI) vs. Universal Basic Compute (UBC)
The idea of Universal Basic Income (UBI) has been explored since 2016, stemming from the anticipation of large-scale changes to society, jobs, and the social contract due to AI <a class="yt-timestamp" data-t="00:50:22">[00:50:22]</a>. The goal is to provide people with money to make good decisions, lift up the floor, and eliminate poverty <a class="yt-timestamp" data-t="00:51:06">[00:51:06]</a>.

However, with the way AI is developing, a different concept, Universal Basic Compute (UBC), might be more appropriate for the future <a class="yt-timestamp" data-t="00:52:05">[00:52:05]</a>. Under UBC, everyone would receive a "slice" of future AI compute (e.g., GPT-7 compute) that they could use, resell, or donate for purposes like cancer research <a class="yt-timestamp" data-t="00:52:07">[00:52:07]</a>. This represents owning a part of the productivity generated by AI <a class="yt-timestamp" data-t="00:52:22">[00:52:22]</a>.

### AI as a "Senior Employee" Assistant
A key vision for AI's role in daily life is an "always on, super low friction thing" that acts as the "world's greatest assistant" <a class="yt-timestamp" data-t="00:19:24">[00:19:24]</a>. This AI would possess significant context and work to make the user better <a class="yt-timestamp" data-t="00:19:40">[00:19:40]</a>.

Two conceptual approaches to AI assistance are:
1.  **Extension of self/Alter Ego:** An AI that is "more me" and acts on one's behalf, even responding to emails without direct instruction <a class="yt-timestamp" data-t="00:20:04">[00:20:04]</a>.
2.  **Great Senior Employee:** An AI that knows the user well, can be delegated tasks (e.g., email access with constraints), but is considered a separate entity <a class="yt-timestamp" data-t="00:20:25">[00:20:25]</a>. This type of AI would be able to push back, offer alternative perspectives, and reason rather than blindly follow instructions <a class="yt-timestamp" data-t="00:21:26">[00:21:26]</a>. This is the preferred approach, aiming for an "always available, always great, super capable assistant executive agent" <a class="yt-timestamp" data-t="00:20:51">[00:20:51]</a>.

This vision implies a shift in how apps and infrastructure might work, with AI agents interfacing with existing services via APIs or even visually, allowing for human oversight <a class="yt-timestamp" data-t="00:22:55">[00:22:55]</a>. The world would be designed to be equally usable by humans and AIs <a class="yt-timestamp" data-t="00:22:40">[00:22:40]</a>. While voice interaction is seen as a hint for the next computing paradigm, visual user interfaces are expected to remain important for many tasks due to their efficiency in conveying information <a class="yt-timestamp" data-t="00:24:02">[00:24:02]</a>.

## AI and Scientific Discovery

A significant area of excitement is the acceleration of scientific discovery through AI <a class="yt-timestamp" data-t="00:26:34">[00:26:34]</a>. While current models like GPT-4 contribute to scientist productivity, the real breakthrough is anticipated with models capable of "reasoning" <a class="yt-timestamp" data-t="00:27:25">[00:27:25]</a>. The ability to connect reasoning capabilities with specialized tools (like chemistry simulators) or access external information in real-time, without retraining entire models, is crucial <a class="yt-timestamp" data-t="00:30:30">[00:30:30]</a>.

There's an intuition that if the core of generalized reasoning can be figured out, connecting it to new problem domains will be a "fast unlock" <a class="yt-timestamp" data-t="00:31:49">[00:31:49]</a>. This contrasts with the current approach where specific models (like Sora for video generation) are built from scratch, customized for their domain <a class="yt-timestamp" data-t="00:32:13">[00:32:13]</a>. The question remains whether specialized startups will die out as generalist models become capable of acquiring necessary data and solving problems on their own <a class="yt-timestamp" data-t="00:29:29">[00:29:29]</a>.

> [!NOTE] Example: AlphaFold 3
> Google's AlphaFold 3, which predicts the 3D structure of proteins and their interactions with small molecules, is a "breathtaking moment" for biology, bioengineering, and human health <a class="yt-timestamp" data-t="01:30:22">[01:30:22]</a>. This capability allows for the in-silico modeling of drug interactions, predicting off-target effects and accelerating drug discovery <a class="yt-timestamp" data-t="01:32:51">[01:32:51]</a>. Unlike some other AI developments, Google has largely kept AlphaFold 3's intellectual property within its subsidiary, Isomorphic Labs, limiting public access to a non-commercial web-based viewer <a class="yt-timestamp" data-t="01:33:49">[01:33:49]</a>. This highlights a trend where companies that are ahead might prioritize closed-source monetization over open-source contributions <a class="yt-timestamp" data-t="01:38:10">[01:38:10]</a>.

## Conclusion

The future of [[regulation_and_oversight_of_ai | AI regulation]] is still being shaped, with debates over its scope, implementation, and impact on innovation and global competitiveness. The evolving nature of AI means that regulatory frameworks must be flexible and adaptable, focusing on outcomes and safety rather than rigid rules, to avoid stifling progress while addressing potential societal risks.