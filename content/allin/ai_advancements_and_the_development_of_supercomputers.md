---
title: AI advancements and the development of supercomputers
videoId: K2xfW3hgxb4
---

From: [[allin]] <br/> 

The development of large-scale supercomputers is a critical factor in the advancement of [[ai_advancements_and_the_impact_on_technology_and_society | AI]] capabilities and the broader [[ai_developments_and_economic_impact | AI industry]].

## The Colossus Supercomputer

Elon Musk's xAI has constructed what is reported to be the world's largest supercomputer, with plans to expand it tenfold <a class="yt-timestamp" data-t="00:50:00">[00:50:00]</a>. This development marks a significant moment for the entire [[ai_investment_and_technology | AI investment and technology]] sector <a class="yt-timestamp" data-t="00:50:00">[00:50:00]</a>.

Key features and achievements:
*   **Scale** It was previously believed impossible to connect more than 25,000 to 32,000 Nvidia Hopper GPUs in a "coherent" manner <a class="yt-timestamp" data-t="00:50:08">[00:50:08]</a>. Coherence in a training cluster means each GPU is aware of what every other GPU is processing, which necessitates robust networking <a class="yt-timestamp" data-t="00:50:10">[00:50:10]</a>.
*   **Innovative Design** By applying a first-principles approach, Elon Musk devised a unique data center design that successfully made over 100,000 GPUs coherent <a class="yt-timestamp" data-t="00:52:44">[00:52:44]</a>. This feat was considered unachievable by engineers at major tech companies like Meta and Google <a class="yt-timestamp" data-t="00:53:20">[00:53:20]</a>.
*   **Industry Recognition** Jensen Huang, CEO of Nvidia, described Musk's accomplishment as "superhuman" <a class="yt-timestamp" data-t="00:53:50">[00:53:50]</a>. This success also helped Nvidia by spurring demand for Hoppers during a delay in their Blackwell chip release <a class="yt-timestamp" data-t="00:53:50">[00:53:50]</a>.
*   **Location and Energy** The supercomputer is housed in a former Electrolux factory in Memphis, utilizing natural gas and Tesla Mega Packs for power <a class="yt-timestamp" data-t="00:54:27">[00:54:27]</a>.

## Scaling Laws and AI Model Performance

The development of such large-scale computing infrastructure is crucial for testing and advancing "scaling laws" in [[ai_as_a_computing_platform_and_its_potential_disruption | AI as a computing platform]].
*   **Impact of Compute** Scaling laws suggest that increasing the amount of compute used to train a model significantly enhances its intelligence and capabilities, often leading to "emergent properties" or higher IQ <a class="yt-timestamp" data-t="00:49:40">[00:49:40]</a>.
*   **Grock 3** xAI's Grock 3 model will be the first major test of these scaling laws since GPT-4 <a class="yt-timestamp" data-t="00:54:50">[00:54:50]</a>. If the scaling laws hold, Grock 3 is anticipated to represent a significant advancement in the state-of-the-art for [[developments_in_technology_and_ai_for_2024 | AI developments]] <a class="yt-timestamp" data-t="00:55:00">[00:55:00]</a>.
*   **Networking Technology** The coherence of GPUs is enabled by advanced networking technologies such as NVLink, NV Switch, and Infiniband <a class="yt-timestamp" data-t="00:51:57">[00:51:57]</a>. There is also interest in Ethernet for large-scale [[openai_and_ai_advancements | OpenAI and AI advancements]] <a class="yt-timestamp" data-t="00:50:41">[00:50:41]</a>.
*   **Future Scaling** The current plan for xAI is to scale to 200,000 Hoppers and eventually to a million GPUs <a class="yt-timestamp" data-t="00:57:28">[00:57:28]</a>.

## Beyond Core Compute

Even if the traditional scaling laws for training models encounter limitations, other dimensions of [[ai_developments_and_business_models | AI developments]] offer continued innovation:
*   **Models of Models** Already, applications are being built by chaining together multiple [[ai_advancements_and_the_impact_on_technology_and_society | AI models]], often starting with a cheaper model and validating its output with a more expensive one <a class="yt-timestamp" data-t="00:56:24">[00:56:24]</a>.
*   **Test Time Compute / Inference Scaling** Allowing models more "think time" for complex questions can dramatically improve their "IQ" <a class="yt-timestamp" data-t="00:58:29">[00:58:29]</a>. This is a new scaling law at its beginning <a class="yt-timestamp" data-t="00:58:38">[00:58:38]</a>.
*   **Context Window** The "context window" refers to the amount of information (tokens, or essentially words) that can be input into a conversation with a large language model <a class="yt-timestamp" data-t="00:59:54">[00:59:54]</a>. Expanding this and improving the speed of processing within it is another area of advancement <a class="yt-timestamp" data-t="00:59:54">[00:59:54]</a>.
*   **Architectural Efficiency** Significant research is focused on re-engineering the [[ai_as_a_computing_platform_and_its_potential_disruption | AI]] stack to reduce energy consumption and other resource demands, leading to better performance through more refined design <a class="yt-timestamp" data-t="00:59:22">[00:59:22]</a>. Newer chips like the H200s are 50% more power-efficient and offer more compute power and memory <a class="yt-timestamp" data-t="01:02:00">[01:02:00]</a>.

## Impact on Business and Productivity

The [[ai_advancements_and_their_impact_on_productivity_and_economy | ROI on AI]] investment has been "very positive thus far," with public companies spending heavily on GPUs showing vertical returns on invested capital <a class="yt-timestamp" data-t="01:04:30">[01:04:30]</a>.
*   **Productivity** [[ai_advancements_and_their_impact_on_productivity_and_economy | AI's impact on productivity]] is evident, particularly in startups, which are employing significantly fewer people today for a given size than they would have three years ago (reportedly 50% less) <a class="yt-timestamp" data-t="01:06:25">[01:06:25]</a>.
*   **Software Development** Tools like Cursor and Notion AI are demonstrating the impressive impact of [[ai_advancements_and_the_impact_on_technology_and_society | AI on workplace productivity]]. Individuals are building and deploying software tools from scratch without prior experience <a class="yt-timestamp" data-t="01:19:33">[01:19:33]</a>. The ability to articulate an app idea and have [[developments_in_technology_and_ai_for_2024 | AI]] build, test, and refine it is rapidly improving <a class="yt-timestamp" data-t="01:20:29">[01:20:29]</a>. Human language is expected to become the dominant programming language <a class="yt-timestamp" data-t="01:31:37">[01:31:37]</a>.
*   **Market Dynamics** Companies are in a "prisoners' dilemma" where they believe that whoever achieves artificial superintelligence first will create trillions of dollars in value, and losing the race puts their company at "mortal risk," driving continued [[ai_investment_and_technology | AI investment]] regardless of short-term ROI <a class="yt-timestamp" data-t="01:07:03">[01:07:03]</a>.