---
title: Controversial AI regulation and government response
videoId: 6SKXkcCgBN0
---

From: [[sachinandadam]] <br/> 

The discussion around [[ai_and_its_implications_for_society | AI]] has moved beyond its future impact to whether it will destroy or help create it <a class="yt-timestamp" data-t="00:00:32">[00:00:32]</a>. While [[ai_and_its_implications_for_society | AI]] can be a force for good, it requires safety measures <a class="yt-timestamp" data-t="00:00:09">[00:00:09]</a>. California lawmakers have passed a controversial [[balancing_innovation_with_ai_safety_measures | AI safety]] Bill SB 1047 <a class="yt-timestamp" data-t="00:00:14">[00:00:14]</a>.

## Driving Concerns for [[balancing_innovation_with_ai_safety_measures | AI Safety]]
Concerns about [[balancing_innovation_with_ai_safety_measures | AI safety]] are leading some of the creators of [[ai_and_its_implications_for_society | AI]] to leave their positions, citing [[challenges_faced_by_ai_developers_regarding_safety_concerns | safety concerns]] <a class="yt-timestamp" data-t="00:00:46">[00:00:46]</a>. Noteworthy departures include:
*   **Ilya Sutskever**: Chief Scientist and co-founder of OpenAI, who left citing [[challenges_faced_by_ai_developers_regarding_safety_concerns | safety concerns]] <a class="yt-timestamp" data-t="00:00:53">[00:00:53]</a>.
*   **Jan Leike**: A senior member who led OpenAI's Superalignment division (their Safety Division), also left with similar concerns <a class="yt-timestamp" data-t="00:00:58">[00:00:58]</a>.
*   **DeepMind Founder**: One of the founders of DeepMind, a seminal [[ai_and_its_implications_for_society | AI]] company, has also raised concerns <a class="yt-timestamp" data-t="00:01:07">[00:01:07]</a>.

The central question is whether developers were so preoccupied with what they *could* build that they didn't consider if they *should* <a class="yt-timestamp" data-t="00:01:16">[00:01:16]</a>. S. Por, co-founder of Harmony [[ai_and_its_implications_for_society | AI]] and a leading [[balancing_innovation_with_ai_safety_measures | AI safety]] expert, highlights the potential for [[ai_and_its_implications_for_society | AI]] to either lead to incredible things or to catastrophic and even [[artificial_intelligence_and_existential_risks | existential risks]] for humanity <a class="yt-timestamp" data-t="00:04:16">[00:04:16]</a>.

## Defining [[balancing_innovation_with_ai_safety_measures | AI Safety]]
S. Por's Harmony [[ai_and_its_implications_for_society | AI]] is an [[balancing_innovation_with_ai_safety_measures | AI Safety]] Company focused on defensive technologies <a class="yt-timestamp" data-t="00:03:04">[00:03:04]</a>. For Harmony [[ai_and_its_implications_for_society | AI]], [[balancing_innovation_with_ai_safety_measures | AI safety]] means ensuring that powerful [[ai_and_its_implications_for_society | AI]] systems reshape the world, organizations, and people's lives in positive ways <a class="yt-timestamp" data-t="00:05:08">[00:05:08]</a>. This includes preventing bad actors from causing harm and ensuring [[ai_and_its_implications_for_society | AI]] systems themselves do not cause societal harm <a class="yt-timestamp" data-t="00:05:25">[00:05:25]</a>.

The focus is on societal scale threats from [[ai_and_its_implications_for_society | AI]], such as:
*   Election manipulation <a class="yt-timestamp" data-t="00:05:42">[00:05:42]</a>
*   Cyber attacks <a class="yt-timestamp" data-t="00:05:51">[00:05:51]</a>
*   Terrorism <a class="yt-timestamp" data-t="00:05:51">[00:05:51]</a>
*   Unwanted warfare <a class="yt-timestamp" data-t="00:05:54">[00:05:54]</a>

Harmony [[ai_and_its_implications_for_society | AI]] specifically works on measuring and understanding these risks with [[ai_and_its_implications_for_society | AI]] labs and governments to get ahead of them <a class="yt-timestamp" data-t="00:05:58">[00:05:58]</a>.

## Government Response to [[ai_and_its_implications_for_society | AI]] Threats
Governments and organizations are grappling with immediate [[ai_and_its_implications_for_society | AI]] threats:
*   **Deepfakes and Election Manipulation**: Governments have largely struggled to act effectively, with current prevention relying on public skepticism <a class="yt-timestamp" data-t="00:07:01">[00:07:01]</a>.
*   **[[ai_and_its_implications_for_society | AI]] Porn**: Legislation is being floated in Australia and the UK to make this illegal, though enforcement remains a challenge <a class="yt-timestamp" data-t="00:06:40">[00:06:40]</a>.

Currently, governments' existing infrastructure is too slow to catch up with the rapid pace of [[ai_and_its_implications_for_society | AI]] development <a class="yt-timestamp" data-t="00:07:44">[00:07:44]</a>. While companies like Anthropic, OpenAI, and DeepMind have shown thought and care in their releases, open-source developers like Meta also need to be responsible <a class="yt-timestamp" data-t="00:08:00">[00:08:00]</a>.

To address this, S. Por advocates for building world-class government agencies staffed with top [[ai_and_its_implications_for_society | AI]] talent <a class="yt-timestamp" data-t="00:08:25">[00:08:25]</a>. The UK's [[balancing_innovation_with_ai_safety_measures | AI Safety]] Institute is an example, but most other places, like Australia, lack such agencies or do not take the risks seriously enough <a class="yt-timestamp" data-t="00:08:35">[00:08:35]</a>.

## [[balancing_innovation_with_ai_safety_measures | Balancing Innovation with AI Safety Measures]]
There's a debate about whether regulation stifles innovation <a class="yt-timestamp" data-t="00:09:07">[00:09:07]</a>. While regulation can impede positive developments, S. Por believes it's not a hard trade-off <a class="yt-timestamp" data-t="00:09:12">[00:09:12]</a>. Successful economies, including Silicon Valley, have historically relied on a careful balance of bottom-up innovation from the private sector and guiding principles, investment, regulation, and institutions from the public sector <a class="yt-timestamp" data-t="00:09:26">[00:09:26]</a>. Without effective safety measures, the benefits of innovation could be lost due to catastrophic or [[artificial_intelligence_and_existential_risks | existential risks]] <a class="yt-timestamp" data-t="00:09:56">[00:09:56]</a>.

### The [[ai_and_its_implications_for_society | AI]] Culture Wars
The "[[ai_and_its_implications_for_society | AI]] culture wars" pit "[[ai_and_its_implications_for_society | AI]] accelerationists" (e.g., Marc Andreessen), who advocate for rapid development, against "[[balancing_innovation_with_ai_safety_measures | AI safety]] doomers" <a class="yt-timestamp" data-t="00:10:18">[00:10:18]</a>. S. Por references Vitalik Buterin's concept of "defensive accelerationism," which argues for thoughtful invention, some regulation, and heavy investment in defensive technologies <a class="yt-timestamp" data-t="00:10:50">[00:10:50]</a>. This includes defensive cybersecurity, protecting democracies from misinformation, and good content moderation systems <a class="yt-timestamp" data-t="00:11:43">[00:11:43]</a>. The goal is to protect innovation while ensuring it provides actual societal benefits <a class="yt-timestamp" data-t="00:12:03">[00:12:03]</a>.

## [[geopolitical_implications_of_ai_development | Geopolitical Implications]] of [[balancing_innovation_with_ai_safety_measures | AI Safety]]
A crucial [[geopolitical_implications_of_ai_development | geopolitical implication]] is the balance between national defense and international coordination on [[ai_and_its_implications_for_society | AI]] development <a class="yt-timestamp" data-t="00:12:22">[00:12:22]</a>. While democratic nations need to invest in [[ai_and_its_implications_for_society | AI]] for defense, taking this too far could lead to a failure of coordination where all nations lose <a class="yt-timestamp" data-t="00:12:59">[00:12:59]</a>. The ultimate concern is not which country has the biggest [[ai_and_its_implications_for_society | AI]], but whether [[ai_and_its_implications_for_society | AI]] systems remain under humanity's control <a class="yt-timestamp" data-t="00:13:15">[00:13:15]</a>. If effective international coordination to prevent risks fails, then all nations, regardless of their advancement (e.g., China, US), stand to lose <a class="yt-timestamp" data-t="00:13:31">[00:13:31]</a>. This means the challenge is not just country vs. country, but potentially humans vs. [[ai_and_its_implications_for_society | AI]], a concern that could manifest within decades <a class="yt-timestamp" data-t="00:13:42">[00:13:42]</a>.

## Perspectives on Key [[ai_and_its_implications_for_society | AI]] Companies Regarding Safety
S. Por offered brief takes on several prominent [[ai_and_its_implications_for_society | AI]] entities regarding their approach to safety:
*   **Anthropic**: Considered "probably the most responsible, thoughtful, and careful investor in [[ai_and_its_implications_for_society | AI]] systems" <a class="yt-timestamp" data-t="00:14:18">[00:14:18]</a>.
*   **Elon Musk's xAI**: Shows "some good signals around safety," but needs to maintain investment to avoid deprioritizing it <a class="yt-timestamp" data-t="00:14:27">[00:14:27]</a>.
*   **OpenAI**: Continues to do good work but has experienced "two steps forward, one step back" with team losses and diminished trust due to some actions, requiring them to rebuild <a class="yt-timestamp" data-t="00:14:37">[00:14:37]</a>.
*   **Mistral**: Viewed as "too focused on techno-optimism," an overly positive view that "everything is good, all tech is good," which S. Por believes is not reality <a class="yt-timestamp" data-t="00:14:51">[00:14:51]</a>.
*   **GPT Wrappers**: S. Por is "bullish" on them, arguing that not all software companies rely on technology as their sole "moat" (competitive advantage); brand and network effects can also serve this purpose <a class="yt-timestamp" data-t="00:15:02">[00:15:02]</a>.

Ultimately, society should pressure major companies and governments to prioritize [[balancing_innovation_with_ai_safety_measures | AI safety]] <a class="yt-timestamp" data-t="00:15:43">[00:15:43]</a>. This involves investing in companies building solutions in the [[balancing_innovation_with_ai_safety_measures | AI safety]] space, rather than solely trusting governments to develop these technologies <a class="yt-timestamp" data-t="00:15:51">[00:15:51]</a>.