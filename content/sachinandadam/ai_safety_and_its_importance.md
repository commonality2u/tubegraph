---
title: AI safety and its importance
videoId: 6SKXkcCgBN0
---

From: [[sachinandadam]] <br/> 

Artificial Intelligence (AI) is recognized as a powerful force for good, but it requires significant safety measures to prevent potential harm <a class="yt-timestamp" data-t="00:00:09">[00:00:09]</a>. The rapid advancement of AI has prompted critical questions about its future impact: will it destroy or help create our future? <a class="yt-timestamp" data-t="00:00:32">[00:00:32]</a> For a long time, there have been more questions than answers regarding [[AI safety and its implications for society | AI safety]] <a class="yt-timestamp" data-t="00:00:38">[00:00:38]</a>.

## Growing Concerns from AI Developers

Despite AI's pervasive presence, some of its key builders are leaving the field due to [[challenges_faced_by_ai_developers_regarding_safety_concerns | safety concerns]] <a class="yt-timestamp" data-t="00:00:48">[00:00:48]</a>. For example, Ilya Sutskever, the chief scientist and co-founder of OpenAI, departed citing safety concerns <a class="yt-timestamp" data-t="00:00:53">[00:00:53]</a>. Jan Leike, who led OpenAI's Superalignment (Safety) division, also left with similar concerns <a class="yt-timestamp" data-t="00:01:01">[00:01:01]</a>. Concerns about AI safety have also been raised by one of the founders of DeepMind, a seminal AI company <a class="yt-timestamp" data-t="00:01:07">[00:01:07]</a>. This raises the question of whether scientists have been too preoccupied with whether they *could* build AI, without considering if they *should* <a class="yt-timestamp" data-t="00:01:16">[00:01:16]</a>.

## Defining AI Safety: Harmony AI's Mission

Sos Preuss, co-founder of Harmony AI and a leading [[AI safety and its implications for society | AI safety expert]], defines AI safety as ensuring that AI systems, which can reshape the world, organizations, and lives, do so in a positive direction <a class="yt-timestamp" data-t="00:05:08">[00:05:08]</a>. This includes preventing bad actors from causing harm with AI, and at some point, ensuring AI systems themselves don't cause societal harm <a class="yt-timestamp" data-t="00:05:25">[00:05:25]</a>. Harmony Intelligence is an [[challenges_faced_by_ai_developers_regarding_safety_concerns | AI safety]] company focused on defensive technologies, aiming to ensure humanity flourishes and remains safe as powerful AI systems are built <a class="yt-timestamp" data-t="00:03:06">[00:03:06]</a>. Preuss's career, spanning fintech (Plaid), food sustainability (Vow), and now AI safety, is driven by a desire to apply his skills to increasingly significant problems that make the world a better place <a class="yt-timestamp" data-t="00:03:50">[00:03:50]</a>. He believes AI will be the most impactful trend of the 21st century <a class="yt-timestamp" data-t="00:04:12">[00:04:12]</a>, with the potential for incredible benefits if directed correctly, or catastrophic and even [[artificial_intelligence_and_existential_risks | existential risks]] if it goes wrong <a class="yt-timestamp" data-t="00:04:18">[00:04:18]</a>.

### Societal-Scale Threats from AI

Harmony AI focuses on societal-scale threats posed by AI, such as:
*   Election manipulation <a class="yt-timestamp" data-t="00:05:42">[00:05:42]</a>, including deepfakes <a class="yt-timestamp" data-t="00:06:16">[00:06:16]</a>.
*   [[AI security and data permissions challenges | Cyberattacks]] <a class="yt-timestamp" data-t="00:05:51">[00:05:51]</a>.
*   Terrorism <a class="yt-timestamp" data-t="00:05:51">[00:05:51]</a>.
*   Unwanted warfare <a class="yt-timestamp" data-t="00:05:54">[00:05:54]</a>.
*   AI porn, where faces are used to create pornographic material <a class="yt-timestamp" data-t="00:06:20">[00:06:20]</a>.

The company's initial focus is on measuring and understanding these risks, working with AI labs and governments to assess system capabilities and anticipate dangers <a class="yt-timestamp" data-t="00:05:57">[00:05:57]</a>.

## Governmental and Organizational Responses

Governments are attempting to tackle some AI-related issues; for instance, legislation to make AI porn illegal has been floated in Australia and the UK, though enforcement remains challenging <a class="yt-timestamp" data-t="00:06:40">[00:06:40]</a>. However, for issues like election manipulation, governments have not yet found effective solutions beyond encouraging public skepticism toward news <a class="yt-timestamp" data-t="00:07:01">[00:07:01]</a>.

The current governmental infrastructure is not equipped to move quickly enough to keep pace with rapid AI development <a class="yt-timestamp" data-t="00:07:44">[00:07:44]</a>. While individuals and companies like Anthropic, OpenAI, and DeepMind are taking steps to release AI responsibly <a class="yt-timestamp" data-t="00:07:51">[00:07:51]</a>, and open-source players like Meta also need to be responsible <a class="yt-timestamp" data-t="00:08:14">[00:08:14]</a>, a key solution is to build world-class government agencies staffed with top AI talent <a class="yt-timestamp" data-t="00:08:25">[00:08:25]</a>. The UK's AI Safety Institute is a positive example, but it's an exception, with many places not taking the risks seriously enough <a class="yt-timestamp" data-t="00:08:35">[00:08:35]</a>.

## Regulation vs. Innovation and Geopolitical Considerations

There's a debate about the trade-off between regulation and innovation <a class="yt-timestamp" data-t="00:09:58">[00:09:58]</a>. While over-regulation can stifle positive developments, Preuss argues against a hard trade-off <a class="yt-timestamp" data-t="00:09:12">[00:09:12]</a>. Successful economies balance private sector innovation with public sector guiding principles, including investment, regulation, institutions, and norms <a class="yt-timestamp" data-t="00:09:26">[00:09:26]</a>. Innovation is moot if a catastrophe or [[artificial_intelligence_and_existential_risks | existential threat]] occurs <a class="yt-timestamp" data-t="00:09:56">[00:09:56]</a>.

The "AI accelerationist" vs. "AI safety doomers" debate highlights this tension <a class="yt-timestamp" data-t="00:10:18">[00:10:18]</a>. Vitalik Buterin's concept of "defensive accelerationism" offers a middle ground <a class="yt-timestamp" data-t="00:10:50">[00:10:50]</a>. It suggests that while there are risks from unbridled AI and dangerous centralization, the solution involves thoughtful invention, some regulation, and heavy investment in defensive technologies <a class="yt-timestamp" data-t="00:11:03">[00:11:03]</a>. This includes defensive cybersecurity, protecting democracies from misinformation, good content moderation, and safety fine-tuning for open-weight models <a class="yt-timestamp" data-t="00:11:43">[00:11:43]</a>. The goal is to [[balancing_innovation_with_ai_safety_measures | protect innovation]] while ensuring it provides actual societal benefits <a class="yt-timestamp" data-t="00:12:03">[00:12:03]</a>.

Geopolitically, while democratic nations need to invest in AI for defense, taking this argument too far could lead to a "failure of coordination where we all lose" <a class="yt-timestamp" data-t="00:12:22">[00:12:22]</a>. The long-term concern is not which country has the biggest AI, but whether AI systems remain under humanity's control at all <a class="yt-timestamp" data-t="00:13:15">[00:13:15]</a>. Effective international coordination on risks is crucial to prevent losing control to powerful AI systems, a concern that could materialize within decades, not centuries <a class="yt-timestamp" data-t="00:13:26">[00:13:26]</a>. This implies a human-versus-AI dynamic at the limit <a class="yt-timestamp" data-t="00:13:47">[00:13:47]</a>.

## Industry Perspectives on AI Safety

Sos Preuss offered quick takes on various AI companies and their approaches to safety:
*   **Anthropic:** Considered perhaps the most responsible, thoughtful, and careful investor in AI systems <a class="yt-timestamp" data-t="00:14:18">[00:14:18]</a>.
*   **Elon Musk's xAI:** Shows some good signals around safety, but needs to continue investing to avoid deprioritizing it <a class="yt-timestamp" data-t="00:14:26">[00:14:26]</a>.
*   **OpenAI:** Continues good work but has experienced setbacks with team losses and trust issues due to certain actions <a class="yt-timestamp" data-t="00:14:37">[00:14:37]</a>.
*   **Mistral:** Viewed as too focused on techno-optimism, assuming "all tech is good," which Preuss believes is not realistic <a class="yt-timestamp" data-t="00:14:51">[00:14:51]</a>.
*   **GPT Wrappers:** Preuss is bullish, arguing that software companies don't solely rely on technology as a moat, and other moats like brand or network effects can be equally valid <a class="yt-timestamp" data-t="00:15:02">[00:15:02]</a>.

## Conclusion

It is imperative for society to pressure major companies and governments to take [[AI safety and its implications for society | AI safety]] more seriously <a class="yt-timestamp" data-t="00:15:40">[00:15:40]</a>. A key way to achieve this is by investing in the growing number of companies dedicated to AI safety <a class="yt-timestamp" data-t="00:15:51">[00:15:51]</a>. Relying solely on governments to develop these technologies is insufficient; instead, there's fortunate progress being made by smart individuals and companies building solutions to these critical safety problems <a class="yt-timestamp" data-t="00:15:56">[00:15:56]</a>.