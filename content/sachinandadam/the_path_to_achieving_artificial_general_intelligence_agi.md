---
title: The path to achieving Artificial General Intelligence AGI
videoId: iGa-6tpLBXA
---

From: [[sachinandadam]] <br/> 

The current sentiment among leading AI developers is that there is a clear understanding of how to achieve Artificial General Intelligence (AGI), with the primary challenge being execution <a class="yt-timestamp" data-t="00:00:01">[00:00:01]</a>, <a class="yt-timestamp" data-t="00:44:59">[00:44:59]</a>. This path involves dedicated work on reinforcement learning (RL) and providing models with the appropriate data <a class="yt-timestamp" data-t="00:45:08">[00:45:08]</a>.

## Reinforcement Learning (RL) as a Key Component
Recent advancements in [[role_of_reinforcement_learning_in_ai_advancements | reinforcement learning]] are proving to be "much more effective" than previously observed <a class="yt-timestamp" data-t="00:21:12">[00:21:12]</a>. A notable example is the DeepSeek announcement, which demonstrated [[role_of_reinforcement_learning_in_ai_advancements | reinforcement learning]] at scale in an unprecedented way <a class="yt-timestamp" data-t="00:20:29">[00:20:29]</a>.

### Data for Reinforcement Learning
From a data perspective, major AI labs are actively creating large datasets to train [[role_of_reinforcement_learning_in_ai_advancements | reinforcement learning]] models <a class="yt-timestamp" data-t="00:20:36">[00:20:36]</a>. This effort is initially focused on software engineers, who act as "teachers" for these models to enhance their capabilities in areas like coding benchmarks <a class="yt-timestamp" data-t="00:20:50">[00:20:50]</a>.

### The DeepSeek Breakthrough
The significance of DeepSeek's achievement lies in its method of operating without direct human intervention in the feedback loop <a class="yt-timestamp" data-t="00:22:20">[00:22:20]</a>.
Traditionally, [[role_of_reinforcement_learning_in_ai_advancements | reinforcement learning]] models rely on human feedback to assess output quality, similar to humans judging a recipe <a class="yt-timestamp" data-t="00:22:10">[00:22:10]</a>. However, DeepSeek has developed mechanisms that allow the model to "check its own homework" and independently determine if its output meets the criteria for correctness <a class="yt-timestamp" data-t="00:22:50">[00:22:50]</a>.

> [!NOTE] Impact of DeepSeek's Approach
> Cutting humans out of the data creation process means that models could theoretically improve more rapidly without human intervention, leading to faster capability enhancements <a class="yt-timestamp" data-t="00:23:08">[00:23:08]</a>. Furthermore, current observations indicate no "tapering off" in performance gains as more data is fed into these models, suggesting significant potential for future improvements <a class="yt-timestamp" data-t="00:23:33">[00:23:33]</a>.

## Implications for AI Development and Costs
The increased accessibility of [[role_of_reinforcement_learning_in_ai_advancements | reinforcement learning]] implies that better models can be developed at a lower cost <a class="yt-timestamp" data-t="00:23:50">[00:23:50]</a>. While the DeepSeek paper's quoted cost did not include all associated expenses like initial compute acquisition or multiple training runs <a class="yt-timestamp" data-t="00:24:14">[00:24:14]</a>, the trajectory suggests that costs are indeed coming down significantly <a class="yt-timestamp" data-t="00:24:52">[00:24:52]</a>.

This development could lead to the acceleration of domain-specific [[Artificial Intelligence and its implications for society | AI]] use cases that were previously difficult to achieve <a class="yt-timestamp" data-t="00:25:16">[00:25:16]</a>. Tools like Anthropic's computer use API and OpenAI's operator tool allow for collecting data from actual user workflows, which is crucial for training new versions of models that are domain-specific and capable of executing particular tasks <a class="yt-timestamp" data-t="00:25:29">[00:25:29]</a>.

## Previous Beliefs and Shifting Perspectives
Previously, there was uncertainty about whether merely scaling models was the sole path to AGI, or if different architectures or techniques were necessary <a class="yt-timestamp" data-t="00:44:50">[00:44:50]</a>. However, the current consensus among researchers indicates a strong conviction that the existing approach, focusing on RL and data, is the way forward <a class="yt-timestamp" data-t="00:44:59">[00:44:59]</a>.

> [!QUOTE] The Mood Shift
> "Now it feels like the mood is we actually feel like we have a very good idea of how to get to AGI but we just need to kind of execute on this like RL work and feeding it with the right kind of data." <a class="yt-timestamp" data-t="00:44:59">[00:44:59]</a>