---
title: Control methods and sensors in Apple Vision Pro
videoId: dtp6b76pMak
---

From: [[mkbhd]] <br/> 

The [[apple_vision_pro_features_and_design | Apple Vision Pro]] is described as one of Apple's most radical products, featuring numerous new technologies, including a new operating system, infrared eye tracking, and virtual reconstructions of the user <a class="yt-timestamp" data-t="00:00:33">[00:00:33]</a>. It functions as a high-end virtual reality (VR) headset <a class="yt-timestamp" data-t="00:01:30">[00:01:30]</a>.

## Components and Physical Design

The [[apple_vision_pro_features_and_design | Vision Pro]] is constructed from metal and glass, contributing to its high-quality feel but also its relative weight <a class="yt-timestamp" data-t="00:01:51">[00:01:51]</a>. Key physical features include:
*   **Aluminum Frame**: A precisely machined aluminum frame surrounds the outside <a class="yt-timestamp" data-t="00:02:00">[00:02:00]</a>.
*   **Fans**: Intakes for fans are located at the bottom, with vents at the top, to manage heat <a class="yt-timestamp" data-t="00:02:04">[00:02:04]</a>.
*   **Digital Crown**: Situated on the right side, it can be pressed or turned <a class="yt-timestamp" data-t="00:02:09">[00:02:09]</a>.
*   **Button**: A single larger button is on the other side, similar to an [[apple_vision_pro_features_and_design | Apple Watch]] <a class="yt-timestamp" data-t="00:02:13">[00:02:13]</a>.
*   **Speakers**: Pods with downward-facing grills near the ears function as surprisingly effective speakers, though they allow some sound bleed <a class="yt-timestamp" data-t="00:02:20">[00:02:20]</a>.
*   **Front Glass**: An "enormous" piece of glass covers the front, prone to fingerprints and smudges <a class="yt-timestamp" data-t="00:02:41">[00:02:41]</a>.
*   **Outward-Facing Display**: Behind the glass, an OLED display shows a representation of the user's eyes to those outside the headset <a class="yt-timestamp" data-t="00:02:47">[00:02:47]</a>.

## Internal Processing and Sensors

The [[apple_vision_pro_features_and_specifications | Vision Pro]] houses two main chips:
*   **M2 Chip**: Processes general data and operations <a class="yt-timestamp" data-t="00:03:03">[00:03:03]</a>.
*   **R1 Chip**: Specifically handles sensor data, ensuring low latency for passthrough (under 12 milliseconds) <a class="yt-timestamp" data-t="00:03:05">[00:03:05]</a>.

A comprehensive array of sensors facilitates the device's functionality:
*   **Outward-Facing Sensors**: A "bunch of sensors" are positioned around the front of the headset, including:
    *   Forward-facing cameras <a class="yt-timestamp" data-t="00:02:52">[00:02:52]</a>
    *   Sideways-facing cameras <a class="yt-timestamp" data-t="00:02:54">[00:02:54]</a>
    *   Downward-facing cameras <a class="yt-timestamp" data-t="00:02:54">[00:02:54]</a>
    *   Depth sensors <a class="yt-timestamp" data-t="00:02:57">[00:02:57]</a>
    *   Infrared illuminators <a class="yt-timestamp" data-t="00:02:58">[00:02:58]</a>
    *   LiDAR scanners <a class="yt-timestamp" data-t="00:02:59">[00:02:59]</a>
    *   Regular RGB cameras <a class="yt-timestamp" data-t="00:03:01">[00:03:01]</a>
*   **Inward-Facing Sensors**: Inside the headset, more sensors track the user's eyes in real-time for control and to display eye representations externally <a class="yt-timestamp" data-t="00:03:09">[00:03:09]</a>. These sensors track at 90 frames per second <a class="yt-timestamp" data-t="00:24:57">[00:24:57]</a>.

## Primary Control Methods

The [[user_experience_and_eye_tracking_with_apple_vision_pro | Vision Pro]]'s primary input methods rely on the user's eyes and hands, without physical controllers <a class="yt-timestamp" data-t="00:06:32">[00:06:32]</a>.

### Initial Calibration

[[setting_up_the_apple_vision_pro_for_the_first_time | The first time the headset is worn]], a calibration process occurs:
1.  **Lens Adjustment**: The lenses physically move to match the distance between the user's eyes <a class="yt-timestamp" data-t="00:06:48">[00:06:48]</a>.
2.  **Hand Scan**: A hand scan helps the device understand the user's hands <a class="yt-timestamp" data-t="00:06:55">[00:06:55]</a>.
3.  **Eye-Tap Calibration**: The user looks at dots around the screen and taps their fingers together to select them <a class="yt-timestamp" data-t="00:06:57">[00:06:57]</a>.

### Eye Tracking and Hand Gestures

*   **Eye Control**: Users select items by simply looking at them <a class="yt-timestamp" data-t="00:08:41">[00:08:41]</a>. A key adjustment for users is controlling *only* what they are directly looking at <a class="yt-timestamp" data-t="00:09:05">[00:09:05]</a>.
*   **Pinching Gesture**: A pinch gesture (touching fingers together) acts as a "click" or selection method <a class="yt-timestamp" data-t="00:07:02">[00:07:02]</a>. This can be performed with hands anywhere the headset's wide-angle sensors can see, including in front of the user or in their lap <a class="yt-timestamp" data-t="00:07:09">[00:07:09]</a>.
*   **Scrolling**: Users pinch and grab in the air, then pull as if on a string <a class="yt-timestamp" data-t="00:08:50">[00:08:50]</a>.
*   **Window Manipulation**:
    *   Windows lock into place in 3D space <a class="yt-timestamp" data-t="00:09:37">[00:09:37]</a>.
    *   A bar at the bottom of a window allows users to drag it anywhere in X, Y, and Z space by looking at the bar and pinching <a class="yt-timestamp" data-t="00:09:59">[00:09:59]</a>.
    *   Resizing is done by looking at either bottom corner <a class="yt-timestamp" data-t="00:10:13">[00:10:13]</a>.
    *   Closing an app is done by selecting a small "X" at the bottom <a class="yt-timestamp" data-t="00:10:18">[00:10:18]</a>.

### Digital Crown and Control Center

*   **App Drawer**: A single press of the digital crown brings up the app drawer <a class="yt-timestamp" data-t="00:07:39">[00:07:39]</a>.
*   **Immersion**: Turning the digital crown clockwise slowly dials in an immersive environment until it fully surrounds the user <a class="yt-timestamp" data-t="00:12:45">[00:12:45]</a>.
*   **Control Center**: To access Control Center, users must physically turn their head up and look at an arrow that appears above them <a class="yt-timestamp" data-t="00:13:32">[00:13:32]</a>.

### Text Input

Text input without physical controllers has three methods:
1.  **Hunt and Peck**: Poking virtual keys on a floating keyboard with pointer fingers <a class="yt-timestamp" data-t="00:14:12">[00:14:12]</a>. This method is slow and doesn't allow for fast typing <a class="yt-timestamp" data-t="00:14:22">[00:14:22]</a>.
2.  **Look and Pinch**: Looking at a key and pinching to select it <a class="yt-timestamp" data-t="00:14:29">[00:14:29]</a>. This method can be faster and provides haptic feedback from fingers tapping together <a class="yt-timestamp" data-t="00:14:47">[00:14:47]</a>.
3.  **Voice Input**: Looking at the microphone icon and saying the URL or text aloud <a class="yt-timestamp" data-t="00:14:53">[00:14:53]</a>.

## Passthrough Technology

The [[user_experience_and_interface_of_apple_vision_pro | Vision Pro]] defaults to passthrough mode, allowing users to see their surroundings via the headset's cameras <a class="yt-timestamp" data-t="00:10:29">[00:10:29]</a>. This gives the impression of augmented reality <a class="yt-timestamp" data-t="00:10:42">[00:10:42]</a>.

> "This is the best passthrough of any VR headset I've ever used, and it's not even that close." <a class="yt-timestamp" data-t="00:10:59">[00:10:59]</a>

The R1 chip processes all passthrough data, adjusting shutter speed for lighting conditions and maintaining latency under 12 milliseconds <a class="yt-timestamp" data-t="00:11:45">[00:11:45]</a>. This, combined with impressive color and brightness fidelity, makes the passthrough feel "kind of real" <a class="yt-timestamp" data-t="00:11:57">[00:11:57]</a>. While close-up objects and fine text can be blurry, users can still text or read notifications <a class="yt-timestamp" data-t="00:12:08">[00:12:08]</a>.

## Personas

Personas are digital representations of the user's face, created by scanning the user with the headset's external cameras <a class="yt-timestamp" data-t="00:27:16">[00:27:16]</a>. This feature is currently in beta <a class="yt-timestamp" data-t="00:25:11">[00:25:11]</a>.

### Persona Creation Process

1.  **Remove Headset**: The user removes the [[apple_vision_pro_features_and_design | Apple Vision Pro]] <a class="yt-timestamp" data-t="00:28:08">[00:28:08]</a>.
2.  **Hold at Eye Level**: Hold the headset at eye level, aligning the entire face within the frame <a class="yt-timestamp" data-t="00:28:15">[00:28:15]</a>.
3.  **Head Turns**: Slowly turn the head right, then left, then tilt up and down <a class="yt-timestamp" data-t="00:28:25">[00:28:25]</a>.
4.  **Facial Expressions**: Capture expressions such as a closed-mouth smile, a big smile with teeth, raised eyebrows, and closed eyes <a class="yt-timestamp" data-t="00:28:46">[00:28:46]</a>.
5.  **Customization**: After capture, users can adjust lighting, skin tone, and add virtual glasses <a class="yt-timestamp" data-t="00:29:35">[00:29:35]</a>.

### Persona Functionality

*   **Eye Display**: The external OLED screen displays a reconstructed representation of the user's eyes based on internal sensor data <a class="yt-timestamp" data-t="00:24:48">[00:24:48]</a>. In passthrough mode, eyes shine through to indicate the wearer can see others <a class="yt-timestamp" data-t="00:25:32">[00:25:32]</a>. In immersive mode, a blue/purple glow covers the eyes, indicating the wearer cannot see outside <a class="yt-timestamp" data-t="00:25:43">[00:25:43]</a>. If someone approaches while immersed, their image can appear through a "fog," and the wearer's eyes will partially show through the glow <a class="yt-timestamp" data-t="00:26:02">[00:26:02]</a>.
*   **Optic ID**: The internal eye-tracking sensors power Optic ID, which is used for logging into the headset and security <a class="yt-timestamp" data-t="00:25:00">[00:25:00]</a>.
*   **FaceTime Integration**: Personas are used as the camera feed for apps requiring a front-facing camera, like FaceTime <a class="yt-timestamp" data-t="00:31:54">[00:31:54]</a>. On a FaceTime call:
    *   Windows appear as glassy panes with people looking through them <a class="yt-timestamp" data-t="00:32:36">[00:32:36]</a>.
    *   The angle of eye contact matches the recipient's view <a class="yt-timestamp" data-t="00:32:46">[00:32:46]</a>.
    *   Hand gestures made in front of the user are tracked by cameras and appear at the correct angle towards the intended recipient <a class="yt-timestamp" data-t="00:33:17">[00:33:17]</a>.
    *   [[User experience and eye tracking with Apple Vision Pro | Spatial audio]] makes voices come from the direction of the virtual window, and moving the window changes the perceived sound origin <a class="yt-timestamp" data-t="00:33:43">[00:33:43]</a>.

## Mac Virtual Display

The [[apple_vision_pro_features_and_specifications | Vision Pro]] can connect to a Mac, turning the Mac's display into a 4K virtual monitor within the headset <a class="yt-timestamp" data-t="00:21:40">[00:21:40]</a>. This feature relies on the Mac's keyboard and trackpad for control, allowing seamless interaction between the Mac display and other [[applications_and_ecosystem_of_apple_vision_pro | Vision Pro apps]] <a class="yt-timestamp" data-t="00:22:02">[00:22:02]</a>.