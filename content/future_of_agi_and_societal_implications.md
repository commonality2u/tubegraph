---
title: Future of AGI and societal implications
videoId: a42key59cZQ
---

From: [[dwarkesh | The Dwarkesh Podcast]]

This article summarizes [[gwern_branwen_and_anonymity_in_writing | Gwern Branwen's]] perspectives on the future of Artificial General Intelligence (AGI), its potential characteristics, and the societal and individual implications of its development, based on a podcast interview.

## Foundational Views on Intelligence and AGI

Branwen posits that the success of scaling points to a specific understanding of intelligence:
*   **Intelligence as Search:** All intelligence is essentially a search over Turing machines. Learning and scaling involve searching over more and longer Turing machines and applying them to specific cases. There isn't a "master algorithm" or a special "intelligence fluid," but rather a vast number of special cases learned and encoded.
*   **Rarity of Human-Level General Intelligence:** This view helps explain why human-level general intelligence is rare. For many organisms in static niches or with short lifespans, evolving specific, genetically hard-coded solutions is more adaptive and less costly than a general-purpose, expensive, and glitchy search process like human intelligence.

## The Trajectory and Timelines for AGI

*   **Accelerating Timelines:** Branwen's personal AGI timelines significantly shortened following breakthroughs like AlexNet and DanNet, dropping at a rate of roughly two years per calendar year.
*   **Planning Horizon:** He considers Anthropic's 2028 AGI timeline a "good personal planning starting point." He even suggests that AI capable of writing Gwern-quality essays might be possible in 2-3 years, particularly if fine-tuned on his existing corpus.

## Automation of Companies

Branwen expects corporate automation to occur primarily from the bottom up:
*   **Bottom-Up Automation:** It is more palatable and practical to replace roles starting from the bottom and working upwards. This could lead to firms where human executives oversee a workforce of AIs.
*   **The Human CEO's Role:** If humans retain an advantage, it will likely be in long-term vision and "taste," areas where AI might be too myopic. This envisions a Steve Jobs-type CEO guiding an AI corporation, making high-level strategic and qualitative judgments. Such human-led firms might outcompete entirely AI-run firms that make myopic long-term choices.

## Characteristics of Future AI Systems

### Unit of Selection in AI Firms
*   Once individual AI models can be perfectly replicated, the unit of selection can move to larger groups or "packages of minds."
*   These packages could comprise complementary AI types (e.g., programmer, manager, financial, legal AIs) forming default units that can be copied and evolved through random variations. Selection would favor groups that work well together globally, even if the reasons are not easily attributable to specific interactions.

### Cognitive Diversity
*   Branwen argues that, excluding capability, AI models are *already* more cognitively diverse than humans.
*   Different architectures like LLMs, GANs, and VAEs operate in fundamentally distinct ways, with varying latent spaces and error patterns, especially in smaller or less-developed models.
*   He cites the example of GANs being incentivized to "hide" problematic features (like hands), whereas diffusion models might render them in "monstrous, Cthulhu-esque abortions."
*   While there's been a recent loss of diversity *within* LLMs due to similar training data and techniques, deep learning in general has produced a wide range of "minds."

### Development of AI Agency
*   Current AI agency is largely an "accidental byproduct" of training on data, rather than a result of direct selection for agency.
*   It's "miraculous" that current LLMs exhibit any coherent action, given they are primarily trained on text scrapes rather than proper reinforcement learning (RL) setups with state-environment-reward sequences.
*   The current trend is to train LLMs and use as little RL as possible, rather than directly training for agency in a comprehensive RL manner.

## Societal and Individual Implications and Adaptations

### The Evolving Role of Humans
*   Branwen foresees his own "last keystroke" as being the "Steve Jobs-thing of choosing." His AI minions would bring him outputs (like essays), and his role would be to select the best one or suggest refinements.
*   He emphasizes focusing on uniquely human contributions: recording preferences, desires, evaluations, and judgments. An AI "cannot eat ice cream for you" or decide your preferences.

### The Imperative to Record Human Values and Preferences
*   **Writing for the LLM Corpus:** Branwen describes the present as a "vital hinge-y time to write." Writing is a way of "voting on the future of the Shoggoth" (LLM) using tokens it has to predict.
*   If personal values, likes, and wants are not expressed in text online, they "don't exist" for the AI, which is "dangerously close to won't exist."
*   Individuals are also creating their future "persona" for LLMs. He cites how Kevin Roose's interactions with Sydney (Bing) led to LLMs generally mistreating Roose's persona. This implies one can also write to mold a desired future self in the eyes of AI.

### Gwern Branwen's Personal Strategy for the Near Future
Given the short AGI timelines, Branwen is adapting his work:
1.  **Enjoyment:** Doing things he wants to do, regardless of future AI capabilities, simply for personal enjoyment or amusement.
2.  **Human-Centric Tasks:** Focusing on the "human part" of projects, such as sketching out proposals, defining what is wanted and why it's good, for future AGIs to potentially implement. This avoids spending scarce time on implementation that AGI could do better soon.
3.  **Recording the Ephemeral:** Writing about unrecorded preferences, desires, evaluations, and judgmentsâ€”things an AI cannot replace and that are unique to his personal experience. This helps future AI versions of himself.
Projects that don't fall into these categories may have little lasting value if AGI can perform them in a few years.

## AGI and Unanswered Scientific Questions

Branwen hopes that by 2050, with the aid of AGI, humanity might finally answer fundamental questions that have long resisted definitive answers, such as the nature of sleep and dreams, human aging, the existence of sexual reproduction, reasons for human individual differences, the slow development of technological civilization, the Fermi paradox, and the reasons for the brain's oversized nature compared to ANNs.

By realizing the potential of AGI, it could substantially impact both technology and society, ultimately transforming approaches to solving complex scientific and societal challenges [[impact_of_ai_on_future_technology_and_society]].