---
title: Comparisons between atomic bomb development and modern AI advancements
videoId: tMdMiYsfHKo
---

From: [[dwarkesh | The Dwarkesh Podcast]]

The development of the atomic bomb, as detailed in Richard Rhodes's book *The Making of the Atomic Bomb*, has drawn significant comparisons to the rapid advancements currently being made in the field of Artificial Intelligence (AI). This article outlines the parallels discussed in a podcast episode featuring Richard Rhodes, based solely on the information presented therein.

## Cult Following in the AI Community

Richard Rhodes's book *The Making of the Atomic Bomb* has reportedly become a "cult classic" within the AI community [[impact_of_ai_on_future_technology_and_society | Impact of AI on future technology and society]]. Individuals working in AI are described as "huge fans" of Rhodes's work, recommending it because they perceive strong parallels between the historical progression of nuclear technology and the current trajectory of AI development [[the_making_and_impact_of_the_atomic_bomb | The making and impact of the atomic bomb]], [[artificial_intelligence_vs_human_intelligence | Artificial Intelligence vs Human Intelligence]].

## Parallels in Scientific Development

Several aspects of the atomic bomb's development are seen as analogous to contemporary AI research:

### Initial Scientific Hints and Key Discoveries
The narrative structure of *The Making of the Atomic Bomb*, which begins with initial scientific hints, resonates with AI researchers [[ai_alignment_and_potential_risks | AI alignment and potential risks]].
*   The advent of deep learning, where a system can teach itself any function, is likened to Leó Szilárd's crucial insight into the nuclear chain reaction [[the_role_of_deep_learning_and_discrete_program_search_in_ai_development | The role of deep learning and discrete program search in AI development]].
*   Ilya Sutskever, Chief Scientist at OpenAI, is highlighted as a modern counterpart to figures like Szilárd. Sutskever was instrumental in training the ImageNet neural network and, a decade ago, foresaw the potential to achieve human-level intelligence by scaling up such models [[ai_trajectory_and_scaling_hypothesis | AI trajectory and scaling hypothesis]].

### Scaling Laws and Extrapolation
The concept of "scaling laws" in AI, where increasing a model's size leads to significant improvements in reasoning and text prediction, is a key parallel [[ai_scalability_and_breakthroughs | AI scalability and breakthroughs]]. By extrapolating these trends, researchers anticipate that a few more orders of magnitude in model size could result in capabilities resembling human-level intelligence [[forecasting_ai_progress_and_the_intelligence_explosion | Forecasting AI progress and the intelligence explosion]].

### Sense of Historical Pursuit and Urgency
AI researchers reportedly see many analogies between their current work and the timeline of the Manhattan Project, feeling that, in terms of progress, they are mentally at a stage comparable to "page 400" of the bomb's development story [[historical_influences_on_leadership_and_innovation | Historical influences on leadership and innovation]]. They view themselves as an "eclectic group of people... engaged in this historical pursuit," aware of its significance and the early indications of its transformative potential [[exploring_the_future_of_society_and_economy_with_ai | Exploring the future of society and economy with AI]].

### Specialized Expertise in High Demand
The intense demand for engineers with specific skills in AI, such as those working with GPUs for complex matrix calculations, is compared to the critical need for specialized experts during the Manhattan Project [[ai_for_science_and_societal_challenges | AI for Science and Societal Challenges]]. Examples include chemists, metallurgists, or those involved in the electromagnetic separation of isotopes, who were in short supply but essential for the project's success [[microsofts_breakthroughs_in_ai_and_quantum_computing | Microsoft's breakthroughs in AI and quantum computing]].

## Transformative Potential and Unintended Consequences

Richard Rhodes acknowledges the compelling nature of "unintended consequences," a theme relevant to both nuclear development and AI, and mentions it could be the subject of a future book [[potential_future_scenarios_of_artificial_intelligence_development | Potential future scenarios of artificial intelligence development]]. He explicitly states, "I don't think there's much question that AI is going to be at least as transformative as nuclear weapons and nuclear energy" [[the_impacts_of_ai_progress_on_economic_and_societal_structures | The impact of AI progress on economic and societal structures]].

## Public and Expert Perception

The societal reaction to AI also draws parallels. Rhodes notes that AI is "scaring the hell out of a lot of people," while others are "putting their heads in the sand" [[ai_safety_and_explorative_research | AI safety and existential risks]]. There are also efforts to regulate AI through laws, akin to attempts made with nuclear weapons [[government_and_policy_coordination_on_ai_risks | Government and Policy Coordination on AI Risks]]. He concludes this observation with the remark, "But people have no idea what's coming, do they?" [[challenges_of_political_leadership_and_governance | Challenges of Political Leadership and Governance]].