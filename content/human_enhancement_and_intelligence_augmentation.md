---
title: Human enhancement and intelligence augmentation
videoId: 41SUp-TRVlg
---

From: [[dwarkesh | The Dwarkesh Podcast]]

Eliezer Yudkowsky, in a discussion on the Lunar Society podcast, outlined several perspectives and potential avenues regarding human enhancement and intelligence augmentation, primarily in the context of existential risks posed by artificial intelligence (AI).

## Rationale and Urgency

Yudkowsky views human intelligence enhancement as a potential, albeit highly speculative, path forward, especially if AI development were to be significantly curtailed or paused. He stated, "Making people smarter has a chance of going right in a way that making an extremely smart AI does not have a realistic chance of going right at this point" <a class="yt-timestamp" data-t="00:06:42">[00:06:42]</a>. This approach is often framed as a "Hail Mary pass" <a class="yt-timestamp" data-t="00:07:45">[00:07:45]</a> in a situation where humanity faces dire existential threats.

He posits that on a "sane planet," the response to current [[llama_3_and_ai_advancements_at_meta | AI advancements]] would be to "shut it all down and work on human intelligence enhancement" <a class="yt-timestamp" data-t="00:07:03">[00:07:03]</a>. While acknowledging the dangers inherent in enhancement technologies, he believes they "do not have the utter lethality of [[ai_safety_and_existential_risks | artificial intelligence]]" <a class="yt-timestamp" data-t="00:08:58">[00:08:58]</a>.

The urgency for such measures is linked to the rapid pace of AI development. If a global moratorium on AI were enacted, human intelligence augmentation would become a critical component of an "exit plan" to develop AI alignment solutions before improving algorithms make even limited compute power dangerous <a class="yt-timestamp" data-t="01:35:46">[01:35:46]</a>, <a class="yt-timestamp" data-t="01:36:06">[01:36:06]</a>. Yudkowsky expressed skepticism about the ability of unaugmented humans to solve AI alignment, stating, "I’ve watched unaugmented humans trying to do alignment. It doesn’t really work... The problem is not that the suggestor is not powerful enough, the problem is that the verifier is broken" <a class="yt-timestamp" data-t="01:39:22">[01:39:22]</a>.

## Proposed Methods

Yudkowsky mentioned several hypothetical methods for enhancing human intelligence:

### Genetic Engineering
While acknowledging that the "timeline for genetically engineered humans to work" might be insufficient <a class="yt-timestamp" data-t="00:06:22">[00:06:22]</a>, he noted a carve-out in his *Time* magazine proposal for AI research focused solely on biology, not trained on general internet text, to facilitate such advancements <a class="yt-timestamp" data-t="00:06:31">[00:06:31]</a>. This could involve using AI to:
*   "Poke around in the space of proteins, collect the genomes, tie to life accomplishments." <a class="yt-timestamp" data-t="00:42:03">[00:42:03]</a>
*   "Extrapolate out the whole proteinomics and the actual interactions and figure out what are likely candidates if you administer this to an adult... [so] the adult gets smarter." <a class="yt-timestamp" data-t="00:42:12">[00:42:12]</a> - <a class="yt-timestamp" data-t="00:42:29">[00:42:29]</a>
The focus on adult administration is due to the lack of time "to raise kids from scratch" <a class="yt-timestamp" data-t="00:42:25">[00:42:25]</a>.

### Neuroscience and Cognitive Training
Several "Hail Mary passes" involving neuroscience were proposed:
*   Using MRIs and neurofeedback to make people "a little saner, to not rationalize so much" <a class="yt-timestamp" data-t="00:07:52">[00:07:52]</a>.
*   Developing technology that could "light up every time somebody is working backwards from what they want to be true... [and] teach people not to do that so much" <a class="yt-timestamp" data-t="00:08:01">[00:08:01]</a>.
*   Training people to be "less idiots" so that "the smartest existing people are then actually able to work on [[ai_alignment_and_safety_concerns | alignment]] due to their increased wisdom" <a class="yt-timestamp" data-t="01:38:24">[01:38:24]</a>.
He referenced the Center For Applied Rationality (CFAR) which "failed" at similar goals, possibly due to a lack of funding for necessary equipment like fMRI machines, suggesting a better-funded attempt might yield results <a class="yt-timestamp" data-t="01:40:06">[01:40:06]</a>.

### Brain Uploading and Simulation
This involves processes to:
*   "Take a brain, slice it, scan it, simulate it, run uploads and upgrade the uploads, or run the uploads faster" <a class="yt-timestamp" data-t="00:08:42">[00:08:42]</a>.
This was also mentioned as a potential "exit plan" strategy <a class="yt-timestamp" data-t="01:38:43">[01:38:43]</a>.

## Associated Risks and Challenges
Yudkowsky emphasized that human enhancement technologies are not without significant risks:
*   Brain uploading methods are described as "quite dangerous things" <a class="yt-timestamp" data-t="00:08:58">[00:08:58]</a>.
*   Using AI to understand biology for enhancement purposes is "not safe" and "sufficiently unsafe that you will probably die" <a class="yt-timestamp" data-t="00:42:37">[00:42:37]</a>.
*   There are "actual but not instantly automatically lethal risks of augmenting human intelligence" that would need to be managed <a class="yt-timestamp" data-t="01:35:51">[01:35:51]</a>.
*   Attempts to make people smarter through interventions could lead to issues like schizophrenia or psychosis, though he speculated these might be "visibly" apparent and could "make them dumber" <a class="yt-timestamp" data-t="01:29:50">[01:29:50]</a>.
Despite these dangers, he believes enhancement "could work" if starting with humans and approached with caution <a class="yt-timestamp" data-t="01:30:09">[01:30:09]</a>.

## Role in AI Safety and Societal Context
Human intelligence enhancement is primarily discussed as a way to gain the necessary cognitive power to solve AI alignment. The interviewer raised the idea of using human-level AI to help with alignment, but Yudkowsky repeatedly steered the conversation towards using such AI capabilities, if available and controllable, for human enhancement first: "Why can we not talk about having it enhance humans instead?" <a class="yt-timestamp" data-t="02:56:02">[02:56:02]</a>, also <a class="yt-timestamp" data-t="00:41:56">[00:41:56]</a>.

He expressed doubt about the societal willingness to pursue such paths, even if technically feasible. A narrow exception for AI applied to augmenting humans might be a "harder sell to the world than just shut it all down" <a class="yt-timestamp" data-t="02:58:35">[02:58:35]</a>. Even if a global shutdown of AI occurred, he suspected humanity might not adopt a viable exit strategy like human enhancement <a class="yt-timestamp" data-t="02:58:50">[02:58:50]</a>. However, if humanity *did* decide to pursue this path seriously and "looked in just the right direction," he estimated a "over 10%" chance of it being a "technically feasible path" <a class="yt-timestamp" data-t="02:59:09">[02:59:09]</a>.

The discussion also touched upon the hypothetical scenario of humans being offered enhancements that might involve replacing their DNA with a more robust substrate. Yudkowsky argued that sufficiently intelligent individuals would likely accept such an offer if it credibly promised healthier, smarter children, thus diverging from "inclusive genetic fitness" <a class="yt-timestamp" data-t="00:26:00">[00:26:00]</a> - <a class="yt-timestamp" data-t="00:27:07">[00:27:07]</a>. This was part of a broader discussion on orthogonality and human motivation, rather than a direct proposal for an enhancement method itself.