---
title: Intersection of AI and Behavioral Science
videoId: VJES_jPWf70
---

From: [[oecdobservatoryofpublicsec3116]] <br/> 

Professor Olivier Sibony, an expert in strategic thinking and the design of decision processes, presented his work at the [[application_of_behavioral_science | intersection of AI and behavioral science]] at a Behavioral Science Meetup organized by the OCD <a class="yt-timestamp" data-t="00:07:00">[00:07:00]</a>. His influential work draws insights from his book, "Noise: A Flaw in Human Judgment," co-authored with Cass Sunstein and the late Daniel Kahneman <a class="yt-timestamp" data-t="01:15:00">[01:15:00]</a>. A central question explored is whether AI can reduce flaws in human judgment or if it merely amplifies biases and noise <a class="yt-timestamp" data-t="01:49:00">[01:49:00]</a>.

## Understanding Errors in Human Judgment

In the context of decision-making, errors can be categorized into two main types: bias and noise <a class="yt-timestamp" data-t="04:06:00">[04:06:06]</a>.

*   **Bias** is a predictable error, where all errors stem from the same underlying reason (e.g., a misaligned rifle consistently shooting off-target) <a class="yt-timestamp" data-t="05:02:00">[05:02:00]</a>. These errors are anticipatable and can have identifiable causes <a class="yt-timestamp" data-t="05:19:00">[05:19:00]</a>.
*   **Noise** is random error, where there's no predictable pattern in the misses (e.g., a shooter randomly missing a target) <a class="yt-timestamp" data-t="05:33:00">[05:33:00]</a>. When examining noise, there's no clear explanation for individual errors <a class="yt-timestamp" data-t="05:51:00">[05:51:00]</a>.

Often, human judgment exhibits a combination of both noise and bias <a class="yt-timestamp" data-t="06:09:00">[06:09:00]</a>. Noise is defined as "unwanted variability" in judgment and is inherently considered negative <a class="yt-timestamp" data-t="43:03:00">[43:03:00]</a>. While variability can be beneficial in certain contexts like competitive markets, creative endeavors, or matters of taste, in professional judgments, where a correct answer is expected, divergence indicates an error <a class="yt-timestamp" data-t="43:56:00">[43:56:00]</a>.

## [[types_of_ai_and_their_similarity_to_human_thinking | Types of AI and their Similarity to Human Thinking]]

AI systems can be broadly categorized in ways that remarkably resemble human thinking systems:

*   **Symbolic AI (Good Old-Fashioned AI - GOFAI)**: This type of AI operates by following explicit rules and using symbolic representations, similar to human "System 2" thinking. It is logical, often slower, and reliable when dealing with less complex rule-based problems <a class="yt-timestamp" data-t="07:20:00">[07:20:00]</a> <a class="yt-timestamp" data-t="09:13:00">[09:13:00]</a>. However, it can make "stupid mistakes" when explicit rules are insufficient for complex real-world scenarios, such as a self-driving car stopping for a child with a "stop" sign on a backpack <a class="yt-timestamp" data-t="07:40:00">[07:40:00]</a>.
*   **Machine Learning (e.g., Large Language Models like ChatGPT)**: This AI type starts from vast amounts of data, discerning implicit rules and patterns, akin to human "System 1" thinking. It is associative, fast, and can generate insights that rule-based systems cannot <a class="yt-timestamp" data-t="08:22:00">[08:22:00]</a> <a class="yt-timestamp" data-t="09:25:00">[09:25:00]</a>. However, LLMs have been shown to replicate human biases (e.g., failing the bat and ball test) <a class="yt-timestamp" data-t="10:29:00">[10:29:00]</a> and can produce "plausible gibberish" or "confabulate" when they don't have true information <a class="yt-timestamp" data-t="11:00:00">[11:00:00]</a> <a class="yt-timestamp" data-t="11:23:00">[11:23:00]</a>. They also lack stability, often providing varied answers to the same question to appear more "natural" or human-like <a class="yt-timestamp" data-t="12:35:00">[12:35:00]</a>. Furthermore, LLMs do not inherently know truth but rather infer it from the frequency of information in their training data, meaning they would have reflected prevailing (incorrect) beliefs at historical times (e.g., Earth being flat) <a class="yt-timestamp" data-t="13:00:00">[13:00:00]</a>.

Given these characteristics, Large Language Models are considered unsuitable for making decisions because they are not stable and do not possess a concept of objective truth <a class="yt-timestamp" data-t="12:17:00">[12:17:17]</a>. The challenges observed in LLMs regarding reliability and truth-telling are analogous to the flaws found in human judgment <a class="yt-timestamp" data-t="14:00:00">[14:00:00]</a>.

## [[impact_of_ai_on_human_decision_making | Impact of AI on Human Decision Making]]

When AI is used as a decision aid, three critical issues arise:

### 1. Trust and Acceptability of AI

Studies comparing human judgment (clinical) with statistical formulas or algorithms (statistical) have consistently shown that algorithms at least match, and often outperform, human experts in decision-making <a class="yt-timestamp" data-t="15:21:00">[15:21:00]</a> <a class="yt-timestamp" data-t="16:29:00">[16:29:00]</a>. This phenomenon has been observed across various fields, including predicting college success, criminal recidivism, medical diagnoses, and especially in personnel selection <a class="yt-timestamp" data-t="15:57:00">[15:57:00]</a> <a class="yt-timestamp" data-t="17:05:00">[17:05:00]</a>. For instance, job interviews are among the least effective methods for personnel selection, with simple formulas often yielding better results <a class="yt-timestamp" data-t="17:20:00">[17:20:00]</a>.

Despite the proven superiority of algorithms, there is a deep-seated resistance to using them when human judgment can be applied <a class="yt-timestamp" data-t="19:18:00">[19:18:00]</a>. This resistance stems from:
*   **Overconfidence in own judgment**: Individuals, particularly experts, tend to believe they are better than average and possess unique insights that algorithms cannot capture <a class="yt-timestamp" data-t="19:59:00">[19:59:00]</a>.
*   **Lack of feedback on accuracy**: People rarely receive clear, timely feedback on the quality of their own decisions, preventing them from recognizing their inaccuracies <a class="yt-timestamp" data-t="24:44:00">[24:44:00]</a>.
*   **Preference for certainty**: Humans desire certainty and often fail to acknowledge the inherent probabilistic nature of their judgments under uncertainty <a class="yt-timestamp" data-t="22:54:00">[22:54:00]</a>.
*   **Loss of control and preference for ambiguity**: Delegating decisions to an algorithm implies a loss of personal control and forces a clear definition of priorities, which many prefer to keep ambiguous <a class="yt-timestamp" data-t="38:48:00">[38:48:00]</a> <a class="yt-timestamp" data-t="23:03:00">[23:03:00]</a>.

To foster trust and acceptability in AI decision aids, strategies include:
*   Allowing users to customize algorithms, even slightly <a class="yt-timestamp" data-t="24:07:00">[24:07:00]</a>.
*   Providing consistent feedback on the accuracy of human judgments versus AI <a class="yt-timestamp" data-t="24:36:00">[24:36:00]</a>.
*   Aligning incentives to prioritize accuracy <a class="yt-timestamp" data-t="24:39:00">[24:39:00]</a>.

The guiding principle for AI adoption should be: an AI doesn't need to be perfect; it just has to be better than the human <a class="yt-timestamp" data-t="25:43:00">[25:43:00]</a>.

### 2. The Dogma of Human Control

The notion that "humans must always remain in control" of AI decisions is a widely accepted dogma across various sectors <a class="yt-timestamp" data-t="25:58:00">[25:58:00]</a>. However, this approach can negate the value of advisory AI <a class="yt-timestamp" data-t="27:30:00">[27:30:00]</a>. If AI is only trusted when it agrees with human judgment and overridden when it disagrees, it merely reinforces confirmation bias and false confidence, providing no real benefit <a class="yt-timestamp" data-t="27:38:00">[27:38:00]</a>.

For AI to be valuable, it must be trusted precisely when it's right and the human is wrong <a class="yt-timestamp" data-t="28:01:00">[28:01:00]</a>. While models should be quality-controlled to ensure they make better decisions on average, once validated, they should generally be trusted <a class="yt-timestamp" data-t="29:06:00">[29:06:00]</a>. The "broken leg problem" is a rare exception where humans should override an AI's advice due to decisive, case-specific information the model lacks <a class="yt-timestamp" data-t="29:24:00">[29:24:00]</a>.

### 3. Algorithmic Bias

The concept of algorithmic bias is complex and often misunderstood. What constitutes "unbiased" is difficult to define, as different definitions of fairness can be mathematically mutually exclusive <a class="yt-timestamp" data-t="32:28:00">[32:28:00]</a> <a class="yt-timestamp" data-t="33:38:00">[33:38:00]</a>. For example, an algorithm predicting recidivism might be deemed biased if its error rates differ between racial groups, or if it assigns different risk scores to individuals of different races with identical data points <a class="yt-timestamp" data-t="32:01:00">[32:01:00]</a>. Both scenarios appear problematic, yet often one must be chosen.

Furthermore, attempts to deliberately "unbias" algorithms by forcing desired outcomes (e.g., increasing diversity in image generation) can lead to nonsensical or inappropriate results if not applied with nuance <a class="yt-timestamp" data-t="34:01:00">[34:01:00]</a>.

An algorithm that reflects past human biases (e.g., Amazon's recruiting tool being sexist because it learned from past male-dominated hiring) acts as a mirror to those biases <a class="yt-timestamp" data-t="35:18:00">[35:18:00]</a>. Scrapping such an algorithm to revert to existing human methods, which are equally or more biased, means avoiding confronting the truth <a class="yt-timestamp" data-t="36:09:00">[36:09:00]</a>. Instead, it would be more effective to explicitly program the algorithm to reflect desired societal outcomes and trade-offs (e.g., "give me the best candidate under the constraint that 50% must be women") <a class="yt-timestamp" data-t="36:41:00">[36:41:00]</a>.

This highlights a core challenge: utilizing AI effectively requires decision-makers to clarify their priorities and define their objectives much more precisely than they typically do with ambiguous human judgment <a class="yt-timestamp" data-t="39:10:00">[39:10:00]</a>.

## Implications for Users and Regulators

The [[application_of_behavioral_science | application of behavioral science]] to AI in decision-making raises several fundamental questions:

For **Users** of AI:
1.  How good are human decisions *without* AI? <a class="yt-timestamp" data-t="39:38:00">[39:38:00]</a>
2.  How much better can decisions be *with* AI? <a class="yt-timestamp" data-t="39:41:00">[39:41:00]</a>
3.  What specific priorities and criteria will be explicitly communicated to the AI, moving beyond convenient ambiguity? <a class="yt-timestamp" data-t="39:47:00">[39:47:00]</a>

For **Regulators**:
1.  Where should the quality of decisions be regulated, considering that human decisions are often not as good as perceived? <a class="yt-timestamp" data-t="40:02:00">[40:02:00]</a>
2.  Who will be responsible for validating the quality and performance of AI models, independent of the vendors? <a class="yt-timestamp" data-t="40:30:00">[40:30:00]</a>
3.  What legally, politically, or socially constitutes an unacceptable bias in algorithms? <a class="yt-timestamp" data-t="41:01:00">[41:01:00]</a>

The discussion on AI in decision-making primarily focuses on professional judgments where there is an obligation to achieve the best possible outcome. Personal decisions, where individual agency and enjoyment of decision-making are valued, fall outside this scope <a class="yt-timestamp" data-t="54:29:00">[54:29:00]</a>. To address responsibility, legal frameworks for AI decision-making may need to evaluate algorithms based on their aggregated probabilistic outcomes rather than individual case errors, similar to how accident rates are assessed for self-driving cars versus human drivers <a class="yt-timestamp" data-t="53:00:00">[53:00:00]</a>.