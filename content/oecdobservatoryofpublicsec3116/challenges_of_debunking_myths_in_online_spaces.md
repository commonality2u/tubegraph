---
title: Challenges of debunking myths in online spaces
videoId: IUItMNuR23s
---

From: [[oecdobservatoryofpublicsec3116]] <br/> 

The spread of [[misinformation_and_disinformation|misinformation]] has become a serious threat to the future of democracy, operating within an information ecosystem characterized by limited regulation and increasing algorithmic determinism <a class="yt-timestamp" data-t="00:01:05">[00:01:05]</a>. Governments often rely on traditional tools like education or regulation, with limited consideration for solutions based on [[behavioral_science_in_combating_misinformation|behavioral science]] <a class="yt-timestamp" data-t="00:02:12">[00:02:12]</a>.

## Complexity of the Problem

Understanding and combating [[misinformation_and_disinformation|misinformation]] is complex due to several factors:

*   **Difficulty in Measurement** Measuring the spread and impact of [[misinformation_and_disinformation|misinformation]] is inherently challenging <a class="yt-timestamp" data-t="00:19:21">[00:19:21]</a>. Most research on spread and sharing originates from platforms like Twitter due to their open APIs, while less is known about platforms where misinformation may be more prevalent, such as Facebook, TikTok, or WhatsApp <a class="yt-timestamp" data-t="00:19:29">[00:19:29]</a>. Researchers often have access to "expression data" (who liked or shared content) but not "impression data" (who actually read or was exposed to content), hindering a true understanding of impact <a class="yt-timestamp" data-t="00:19:53">[00:19:53]</a>.
*   **Lack of Real-World Testing** Interventions developed to date often lack ecological and external validity because opportunities to rigorously test them on social media platforms are minimal <a class="yt-timestamp" data-t="00:20:34">[00:20:34]</a>. This makes it difficult to assess their effectiveness, potential effect sizes, or how they might be optimized for different populations <a class="yt-timestamp" data-t="00:20:45">[00:20:45]</a>.
*   **Reliance on Self-Report Data** The current body of evidence largely relies on self-report data, which presents inherent challenges in accuracy and reliability <a class="yt-timestamp" data-t="00:20:57">[00:20:57]</a>.

### Individual Bandwidth and Engagement

A significant challenge is the limited "bandwidth" of individuals to verify or critically engage with information. In contexts like Lebanon, people often lack the time, energy, or interest to engage with information beyond its surface meaning <a class="yt-timestamp" data-t="00:28:55">[00:28:55]</a>, <a class="yt-timestamp" data-t="00:29:13">[00:29:13]</a>. Instead, they rely on common sense or self-constructed judgments of information sources <a class="yt-timestamp" data-t="00:29:21">[00:29:21]</a>. Fact-checking is not common, often due to a lack of interest <a class="yt-timestamp" data-t="00:29:31">[00:29:31]</a>.

### The Role of Algorithms and Platform Design

The [[role_of_social_media_algorithms_in_misinformation|algorithms]] that govern how information is spread and promoted on social media platforms are seen as problematic, potentially having an unintended effect of spreading [[misinformation_and_disinformation|disinformation]] <a class="yt-timestamp" data-t="00:48:18">[00:48:18]</a>. There is a need to fundamentally redesign how social media platforms promote content and reassess the criteria used for content dissemination <a class="yt-timestamp" data-t="00:48:55">[00:48:55]</a>. To influence change, it is necessary to be able to redesign online environments and gain access to platforms where people are interacting to understand the problem and test interventions <a class="yt-timestamp" data-t="00:50:39">[00:50:39]</a>.

It is noted that creators of [[misinformation_and_disinformation|disinformation]] often possess a deep understanding of human behavior, potentially exceeding that of organizations trying to combat it <a class="yt-timestamp" data-t="00:38:11">[00:38:11]</a>.

### Public Trust and Contextual Factors

In many countries, such as Lebanon, where there is a lack of trust in the government and traditional media outlets are often affiliated with political parties, public trust in information sources is low <a class="yt-timestamp" data-t="00:27:01">[00:27:01]</a>, <a class="yt-timestamp" data-t="00:27:50">[00:27:50]</a>. This impacts the credibility of public health campaigns; for example, using government logos can hurt campaigns more than help them <a class="yt-timestamp" data-t="00:37:17">[00:37:17]</a>. Approaches that foreground the government's role in fact-checking may quickly lose traction <a class="yt-timestamp" data-t="00:37:57">[00:37:57]</a>. Collaborating with trusted [[International collaborations to combat misinformation|international organizations]] like WHO and UNICEF can be an effective workaround <a class="yt-timestamp" data-t="00:37:36">[00:37:36]</a>.

[[The role of digital and virtual platforms in public engagement|The information landscape]] is also influenced by history and traditional media. In Lebanon, [[misinformation_and_disinformation|disinformation]] has a long history, even predating digital platforms, with political parties using rumors during the civil war to maintain fear <a class="yt-timestamp" data-t="00:32:21">[00:32:21]</a>. Traditional media (like TV) remains a primary news source for a majority of citizens <a class="yt-timestamp" data-t="00:33:07">[00:33:07]</a>.

A "one-size-fits-all" approach to interventions will not work; responses need to be tailored to specific contexts, generational divides, and urban-rural specificities <a class="yt-timestamp" data-t="00:34:38">[00:34:38]</a>. Research needs to be grounded in particular contexts, and results should be made accessible to other organizations and communities <a class="yt-timestamp" data-t="00:38:33">[00:38:33]</a>.

## Challenges of Debunking

Specific challenges related to debunking strategies include:

*   **Reinforcing Effect** Myths and conspiracy theories should generally not be actively debunked in public information campaigns and [[behavioral_science_in_combating_misinformation|behavioral interventions]] online, as repeating them can have a reinforcing effect <a class="yt-timestamp" data-t="00:45:10">[00:45:10]</a>. Debunking might be acceptable in on-site workshops where a conversation can occur <a class="yt-timestamp" data-t="00:45:21">[00:45:21]</a>.
*   **"Endowment Effect" for Conspiracy Theories** Supporters of conspiracy theories may exhibit an "endowment effect" or "IKEA effect," where they are more attached to their worldview and collected information due to the time they have invested in building it <a class="yt-timestamp" data-t="00:46:18">[00:46:18]</a>. This makes countering these beliefs a significant challenge <a class="yt-timestamp" data-t="00:46:47">[00:46:47]</a>.
*   **Inoculation Strategies** Inoculation (pre-bunking) approaches show promise, but more evidence is needed to understand their long-term effectiveness and how effects decay over time <a class="yt-timestamp" data-t="00:56:00">[00:56:00]</a>.
*   **Evolving Nature of Misinformation** [[Misinformation_and_disinformation|Misinformation]] is not a static, clearly defined phenomenon; it constantly evolves. Architects of [[misinformation_and_disinformation|disinformation]] continually find new ways of sharing it, meaning that strategies like inoculation, while potentially effective for current forms, may not suffice for future iterations <a class="yt-timestamp" data-t="00:57:16">[00:57:16]</a>.

Overall, there is no single "silver bullet" solution to combat [[misinformation_and_disinformation|misinformation]]. A range of varied solutions is required, tailored to heterogeneous populations and different levels of user engagement <a class="yt-timestamp" data-t="00:55:40">[00:55:40]</a>, <a class="yt-timestamp" data-t="00:56:49">[00:56:49]</a>, <a class="yt-timestamp" data-t="00:57:46">[00:57:46]</a>.