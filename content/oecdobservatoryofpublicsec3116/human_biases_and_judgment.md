---
title: Human biases and judgment
videoId: VJES_jPWf70
---

From: [[oecdobservatoryofpublicsec3116]] <br/> 

Professor Olivier Sibony, an expert in strategic thinking and the design of decision processes, presented his influential work at the intersection of AI and [[Behavioral Science|behavioral science]], drawing insights from his book *Noise: A Flaw in Human Judgment* <a class="yt-timestamp" data-t="00:10:09">[00:10:09]</a>. The central question explored is whether AI can reduce flaws in [[Human Behavior|human]] judgment or if it merely amplifies biases and [[Noise and variability in human decisions|noise]] <a class="yt-timestamp" data-t="00:01:49">[00:01:49]</a>.

## Understanding Errors: Bias and Noise

When making decisions, two types of errors can occur <a class="yt-timestamp" data-t="00:04:06">[00:04:06]</a>:
*   **Bias**: A predictable error, where errors tend to land in the same place, suggesting a consistent problem <a class="yt-timestamp" data-t="00:05:00">[00:05:00]</a>. For example, a rifle that consistently shoots to the left indicates a bias <a class="yt-timestamp" data-t="00:04:43">[00:04:43]</a>. Causes can often be hypothesized, like a faulty rifle or strong wind <a class="yt-timestamp" data-t="00:05:19">[00:05:19]</a>.
*   **[[Noise and variability in human decisions|Noise]]**: Random error, where shots are scattered unpredictably <a class="yt-timestamp" data-t="00:05:41">[00:05:41]</a>. In this case, there's no clear pattern for where the next error will land, and no obvious explanation is sought <a class="yt-timestamp" data-t="00:05:49">[00:05:49]</a>.

Often, decisions exhibit a combination of both [[Noise and variability in human decisions|noise]] and bias <a class="yt-timestamp" data-t="00:06:09">[00:06:09]</a>. Distinguishing between them is crucial for effectively reducing error <a class="yt-timestamp" data-t="00:06:17">[00:06:17]</a>.

## AI as Decision Aids

AI systems, particularly when used as [[Behavioral Insights|decision aids]], can be understood through a parallel with [[Human Behavior|human]] cognition <a class="yt-timestamp" data-t="00:03:56">[00:03:56]</a>:
*   **Symbolic AI**: Follows explicit rules and logical processes, similar to [[Human Behavior|human]] System 2 thinking <a class="yt-timestamp" data-t="00:07:20">[00:07:20]</a>. It's reliable but can make "stupid mistakes" when rules don't apply to complex, nuanced situations (e.g., a self-driving car stopping for a child with a stop sign on a backpack) <a class="yt-timestamp" data-t="00:07:40">[00:07:40]</a>.
*   **Machine Learning (e.g., Large Language Models like ChatGPT)**: Learns from data to discover implicit rules, operating in a way remarkably similar to [[Human Behavior|human]] System 1 <a class="yt-timestamp" data-t="00:08:10">[00:08:10]</a>. While capable of generating amazing insights, LLMs can replicate [[Human Behavior|human]] biases and [[Noise and variability in human decisions|cognitive traps]] because they associate things based on the frequency in their training data <a class="yt-timestamp" data-t="00:10:11">[00:10:11]</a>. They can produce "plausible gibberish" and have no inherent concept of truth, relying on frequency as a proxy <a class="yt-timestamp" data-t="00:11:03">[00:11:03]</a>.

Due to their instability (answers varying for the same question) and lack of truth discernment, [[Noise and variability in human decisions|LLMs are not reliable tools for making decisions]] <a class="yt-timestamp" data-t="00:12:20">[00:12:20]</a>. This unreliability of LLMs serves as a provocative parallel to the unreliability of [[Noise and variability in human decisions|noisy human beings]] <a class="yt-timestamp" data-t="00:14:00">[00:14:00]</a>.

## [[Challenges of trusting AI over human judgment|Challenges of Trusting AI]]

Three main issues arise when considering AI as a [[Behavioral Insights|decision aid]]:

### Trust and Acceptability
Studies consistently show that simple formulas or algorithms often outperform [[Human Behavior|human]] judges in various predictive tasks, such as predicting college success, criminal recidivism, or success in hiring <a class="yt-timestamp" data-t="00:15:30">[00:15:30]</a>. Even basic linear regression algorithms, which are far simpler than modern AI, tend to match or surpass [[Human Behavior|human]] experts <a class="yt-timestamp" data-t="00:16:32">[00:16:32]</a>. In personnel selection, interviewing is among the worst methods, with simple formulas outperforming it <a class="yt-timestamp" data-t="00:17:20">[00:17:20]</a>.

Despite this evidence, [[Human Behavior|humans]] often resist using algorithmic [[Behavioral Insights|decision aids]] <a class="yt-timestamp" data-t="00:18:02">[00:18:02]</a>. Reasons for this deep resistance include <a class="yt-timestamp" data-t="00:19:18">[00:19:18]</a>:
*   **Overconfidence** in their own judgment <a class="yt-timestamp" data-t="00:19:59">[00:19:59]</a>. Experts often believe they are better than average and possess unique insights that algorithms cannot replicate <a class="yt-timestamp" data-t="00:20:01">[00:20:01]</a>.
*   **Preference for certainty**, despite decisions inherently being probabilistic under uncertainty <a class="yt-timestamp" data-t="00:22:54">[00:22:54]</a>.
*   **Lack of accurate feedback** on their own decision-making accuracy <a class="yt-timestamp" data-t="00:24:44">[00:24:44]</a>.
*   **Perceived loss of control** and preference for ambiguity <a class="yt-timestamp" data-t="00:23:44">[00:23:44]</a>.

To overcome this resistance, [[Behavioral Insights|behavioral insights]] suggest <a class="yt-timestamp" data-t="00:24:04">[00:24:04]</a>:
*   Allowing users to **customize algorithms**, even slightly, increases willingness to use them <a class="yt-timestamp" data-t="00:24:07">[00:24:07]</a>.
*   Providing **feedback on accuracy** and aligning incentives to improve decision quality <a class="yt-timestamp" data-t="00:24:36">[00:24:36]</a>.

The key question becomes: how high should the accuracy bar be for AI to be trusted? It doesn't need to be perfect, just better than the [[Human Behavior|human]] alternative <a class="yt-timestamp" data-t="00:25:43">[00:25:43]</a>.

### Human Control
The widespread belief is that [[Human Behavior|humans]] must always remain in control when using AI for decisions <a class="yt-timestamp" data-t="00:25:58">[00:25:58]</a>. However, this perspective has a fundamental flaw <a class="yt-timestamp" data-t="00:26:55">[00:26:55]</a>:
*   If AI agrees with a [[Human Behavior|human]] decision, it's accepted. But if AI disagrees, the [[Human Behavior|human]] often overrides it <a class="yt-timestamp" data-t="00:27:00">[00:27:00]</a>.
*   The value of an AI [[Behavioral Insights|decision aid]] lies precisely in the cases where it is right and the [[Human Behavior|human]] is wrong <a class="yt-timestamp" data-t="00:27:10">[00:27:10]</a>. Overriding AI in such cases negates its benefit, becoming an "additional source of false confidence" <a class="yt-timestamp" data-t="00:27:32">[00:27:32]</a>.
*   Therefore, once an AI model has been quality-controlled and proven to make better decisions on average, it should generally be trusted, except in rare "broken leg problem" situations where the [[Human Behavior|human]] has decisive, new information unavailable to the model <a class="yt-timestamp" data-t="00:29:24">[00:29:24]</a>.

### Algorithmic Biases
Concerns about algorithmic bias (e.g., racist or sexist algorithms) are common <a class="yt-timestamp" data-t="00:30:30">[00:30:30]</a>. However, defining "unbiased" for an algorithm is extremely complex and often depends on the chosen definition of bias <a class="yt-timestamp" data-t="00:33:18">[00:33:18]</a>. Different definitions of bias can be mathematically mutually exclusive <a class="yt-timestamp" data-t="00:32:31">[00:32:31]</a>.

For instance, an algorithm may be deemed biased if its error rate differs between groups (e.g., black vs. white individuals in bail decisions), or if it produces different risk scores for individuals with identical input data but different demographics <a class="yt-timestamp" data-t="00:31:26">[00:31:26]</a>. These two definitions cannot both be true simultaneously, except in highly rare cases <a class="yt-timestamp" data-t="00:32:31">[00:32:31]</a>.

Algorithms trained on historical [[Human Behavior|human]] data can reflect and perpetuate past [[Human Behavior|human]] biases (e.g., Amazon's recruiting tool being sexist because it learned from past male-dominated hiring decisions) <a class="yt-timestamp" data-t="00:35:33">[00:35:33]</a>. Scrapping such algorithms, instead of trying to improve them, can be seen as a refusal to confront these past biases and explicitly decide on desired outcomes <a class="yt-timestamp" data-t="00:36:00">[00:36:00]</a>.

To make an AI truly "unbiased" or aligned with desired outcomes (e.g., a specific gender ratio in hiring), [[Human Behavior|humans]] need to define their priorities clearly and unambiguously, which can be a challenging and uncomfortable decision <a class="yt-timestamp" data-t="00:39:12">[00:39:12]</a>.

## Implications for Users and Regulators

The discussion on AI and [[Human Behavior|human]] judgment raises critical questions:

### For Users
*   How good are your decisions *without* AI? <a class="yt-timestamp" data-t="00:39:38">[00:39:38]</a>
*   How good can your decisions be *with* AI (assuming it's better)? <a class="yt-timestamp" data-t="00:39:41">[00:39:41]</a>
*   What explicit priorities will you set for the AI, moving beyond previous ambiguities? <a class="yt-timestamp" data-t="00:39:47">[00:39:47]</a>

### For Regulators
*   Where should the quality of decisions be regulated, considering that [[Human Behavior|human]] decisions are often not as good as perceived? <a class="yt-timestamp" data-t="00:40:02">[00:40:02]</a>
*   Who will validate the quality of AI models, beyond what vendors claim? <a class="yt-timestamp" data-t="00:40:30">[00:40:30]</a>
*   What constitutes a legally, politically, or socially unacceptable bias in algorithms? <a class="yt-timestamp" data-t="00:41:01">[00:41:01]</a>

## Q&A Highlights

### Noise and Variability in Human Decisions
Professor Sibony emphasizes that in his framework, [[Noise and variability in human decisions|noise]] is defined as "unwanted variability" and is therefore always considered "bad" <a class="yt-timestamp" data-t="00:43:05">[00:43:05]</a>. Variability can be good in specific contexts like markets, creativity (technical, technological, scientific), or matters of taste where there is no objective "correct" answer <a class="yt-timestamp" data-t="00:43:58">[00:43:58]</a>. However, in most professional judgments where a correct answer is sought, disagreement indicates an error from at least one party <a class="yt-timestamp" data-t="00:45:32">[00:45:32]</a>.

### Noise Audits
A methodology for conducting [[Noise and variability in human decisions|noise audits]] is detailed in Appendix A of the book *Noise*, with ongoing technical papers providing further detail <a class="yt-timestamp" data-t="00:41:57">[00:41:57]</a>.

### Democratizing AI and Cost
Simple, effective algorithms for [[Behavioral Insights|decision aids]] can be very inexpensive, fitting "on the back of an envelope" <a class="yt-timestamp" data-t="00:47:51">[00:47:51]</a>. The main barrier to adoption is not cost, as these tools often save money by reducing the need for [[Human Behavior|human]] intervention <a class="yt-timestamp" data-t="00:48:41">[00:48:41]</a>.

### Overcoming Confirmation Bias with AI
While AI (like ChatGPT) can present counter-arguments or different perspectives, potentially helping a researcher overcome confirmation bias, its effectiveness depends on the user's open-mindedness <a class="yt-timestamp" data-t="00:49:12">[00:49:12]</a>. Decision-makers, unlike researchers, may resist advice that contradicts their own judgment <a class="yt-timestamp" data-t="00:50:00">[00:50:00]</a>.

### Responsibility and Accountability
The decision-maker bears the responsibility for their decisions <a class="yt-timestamp" data-t="00:51:18">[00:51:18]</a>. Using an AI that is more often right is in the decision-maker's interest. The legal framework around AI responsibility needs to evolve to account for the probabilistic nature of decisions and evaluate performance in aggregate, rather than solely on a case-by-case basis, as algorithms are more transparently accountable than [[Human Behavior|human]] judgment <a class="yt-timestamp" data-t="00:53:00">[00:53:00]</a>.

### Personal vs. Professional Decisions
The principles discussed apply primarily to professional judgments and decisions where individuals have an obligation to make the best possible choice for stakeholders (e.g., doctors for patients, judges for justice, executives for companies) <a class="yt-timestamp" data-t="00:55:01">[00:55:01]</a>. For personal decisions (e.g., choosing a city, a house, or a partner), the enjoyment of agency and the pleasure of making one's own choices outweigh the pursuit of algorithmic optimization <a class="yt-timestamp" data-t="00:55:09">[00:55:09]</a>.