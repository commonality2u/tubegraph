---
title: Noise and variability in human decisions
videoId: VJES_jPWf70
---

From: [[oecdobservatoryofpublicsec3116]] <br/> 

Human judgment is inherently susceptible to two primary types of errors: bias and noise <a class="yt-timestamp" data-t="05:00:19">[05:00:19]</a>. While bias refers to predictable errors, noise represents unwanted, unpredictable variability in decisions that should ideally be consistent <a class="yt-timestamp" data-t="05:00:02">[05:00:02]</a> <a class="yt-timestamp" data-t="05:01:00">[05:01:00]</a> <a class="yt-timestamp" data-t="05:05:00">[05:05:00]</a>.

## Understanding Bias and Noise

The distinction between bias and noise can be illustrated with a metaphor of a shooting range:
*   **Bias** occurs when a team of shooters consistently misses the target in the same direction <a class="yt-timestamp" data-t="04:43:00">[04:43:00]</a>. This is a predictable error, indicating a systematic problem (e.g., a misaligned rifle) <a class="yt-timestamp" data-t="04:48:00">[04:48:00]</a> <a class="yt-timestamp" data-t="05:21:00">[05:21:00]</a>.
*   **Noise** occurs when shots are randomly scattered around the target <a class="yt-timestamp" data-t="05:33:00">[05:33:00]</a>. This is random error, with no predictable pattern for the next shot, and often no compelling explanation is sought <a class="yt-timestamp" data-t="05:49:00">[05:49:00]</a> <a class="yt-timestamp" data-t="06:05:00">[06:05:00]</a>.
*   In many real-world scenarios, [[human_biases_and_judgment | human biases]] and noise combine, resulting in a cluster of shots off-center and scattered <a class="yt-timestamp" data-t="06:09:00">[06:09:00]</a>.

This distinction is crucial for understanding how to reduce errors in [[decision_making | decision making]] <a class="yt-timestamp" data-t="06:19:00">[06:19:00]</a>.

## Human Judgment vs. Algorithms

Decades of research comparing human experts ("clinical" judgment) with simple statistical formulas or algorithms ("statistical" judgment) consistently show that formulas at least match, and often outperform, human judges <a class="yt-timestamp" data-t="15:18:00">[15:18:00]</a> <a class="yt-timestamp" data-t="15:27:00">[15:27:00]</a>. This pattern holds true across various fields, including:
*   Predicting college success <a class="yt-timestamp" data-t="16:00:00">[16:00:00]</a>.
*   Predicting recidivism in criminals or parole decisions <a class="yt-timestamp" data-t="16:05:00">[16:05:00]</a>.
*   Determining medical diagnoses or prognoses <a class="yt-timestamp" data-t="16:20:00">[16:20:00]</a>.
*   Personnel selection (e.g., hiring decisions), where structured formulas often outperform unstructured interviews <a class="yt-timestamp" data-t="17:05:00">[17:05:05]</a>.

Despite this evidence, there is significant resistance to using [[ai_in_decision_making | AI in decision making]] or even simple formulas <a class="yt-timestamp" data-t="18:02:00">[18:02:00]</a>. People often prefer using their own judgment due to:
*   **Overconfidence**: Experts believe they are better than average and possess unique insights that algorithms cannot capture <a class="yt-timestamp" data-t="19:59:00">[19:59:00]</a>.
*   **Preference for Certainty**: Humans dislike acknowledging that their judgments are probabilistic and often inaccurate <a class="yt-timestamp" data-t="22:54:00">[22:54:00]</a>.
*   **Loss of Control and Preference for Ambiguity**: Relying on algorithms can feel like a loss of agency and forces a clear definition of priorities that people prefer to keep ambiguous <a class="yt-timestamp" data-t="23:44:00">[23:44:00]</a> <a class="yt-timestamp" data-t="38:50:00">[38:50:00]</a>.
*   **Lack of Feedback**: People rarely receive accurate feedback on the quality of their past decisions, hindering improvement and incentivizing the delegation of [[decision_making | decisions]] <a class="yt-timestamp" data-t="24:44:00">[24:44:00]</a>.

## AI and Human Judgment

[[ai_in_decision_making | AI in decision making]] systems can be broadly categorized in ways that remarkably mirror human cognitive systems:
*   **Symbolic AI (GOAI)**: Operates by following explicit rules and logical representations, similar to System 2 thinking (logical, slow, reliable) <a class="yt-timestamp" data-t="07:17:00">[07:17:00]</a> <a class="yt-timestamp" data-t="09:13:00">[09:13:00]</a>. This type of AI can make "stupid mistakes" when rules don't perfectly apply (e.g., self-driving car stopping for a stop sign on a backpack) <a class="yt-timestamp" data-t="07:35:00">[07:35:00]</a>.
*   **Machine Learning (e.g., Large Language Models like ChatGPT)**: Learns implicitly from massive datasets, discovering underlying patterns, similar to System 1 thinking (associative, fast, but prone to [[human_biases_and_judgment | biases]]) <a class="yt-timestamp" data-t="08:10:00">[08:10:00]</a> <a class="yt-timestamp" data-t="08:24:00">[08:24:00]</a> <a class="yt-timestamp" data-t="09:25:00">[09:25:00]</a>. LLMs often replicate [[human_biases_and_judgment | human biases]] present in their training data and can "confabulate" plausible-sounding nonsense <a class="yt-timestamp" data-t="10:29:00">[10:29:00]</a> <a class="yt-timestamp" data-t="11:16:00">[11:16:00]</a>.

### Challenges with AI and Trust
Current [[challenges_of_trusting_ai_over_human_judgment | challenges of trusting AI over human judgment]], especially LLMs for [[decision_making | decision making]], include:
*   **Instability**: LLMs are designed with randomness to "feel more natural," meaning their answers can vary even when asked the same question <a class="yt-timestamp" data-t="12:35:00">[12:35:00]</a>.
*   **Lack of Truth**: LLMs have no inherent concept of truth; their "best proxy for the truth is the frequency of something coming up as the answer" <a class="yt-timestamp" data-t="13:00:00">[13:00:00]</a>. This means they would have confidently "known" the Earth was flat during Galileo's time if trained on historical data <a class="yt-timestamp" data-t="13:14:00">[13:14:00]</a>.

Given these limitations, LLMs are currently poor tools for making decisions <a class="yt-timestamp" data-t="13:37:00">[13:37:00]</a>. This leads to a crucial insight: if one cannot trust an LLM for its instability and lack of truth, then perhaps one should also critically evaluate trusting "noisy human beings" who exhibit similar characteristics <a class="yt-timestamp" data-t="13:56:00">[13:56:00]</a>.

### Fostering Trust in AI and Challenging "Human in Control"
To encourage the adoption of [[ai_in_decision_making | AI in decision making]], several approaches can be considered:
*   **User Customization**: Even slight input from users in designing an algorithm can increase their willingness to use it <a class="yt-timestamp" data-t="24:07:00">[24:07:00]</a>. This is a [[behavioral_insights_in_innovation | behavioral insight]] that demonstrates the importance of a sense of control <a class="yt-timestamp" data-t="24:25:00">[24:25:00]</a>.
*   **Feedback on Accuracy**: Providing clear feedback on the accuracy of human decisions (and how they compare to AI) can incentivize improvement and delegation <a class="yt-timestamp" data-t="24:36:00">[24:36:00]</a>.
*   **Aligning Incentives**: Restructuring incentives to reward accuracy in [[decision_making | decision making]] can drive adoption <a class="yt-timestamp" data-t="24:37:00">[24:37:00]</a>.

The widely accepted dogma that "humans should always remain in control" of [[ai_in_decision_making | AI in decision making]] is problematic <a class="yt-timestamp" data-t="25:58:00">[25:58:00]</a>. If an AI is better on average, its value comes precisely from situations where it disagrees with [[human_biases_and_judgment | human judgment]] <a class="yt-timestamp" data-t="27:10:00">[27:10:00]</a>. Overriding AI simply because it conflicts with human intuition negates its value and can reinforce [[human_biases_and_judgment | confirmation bias]] <a class="yt-timestamp" data-t="27:30:00">[27:30:00]</a>. The rare exception for overriding is the "broken leg problem," where humans possess decisive, specific information the model lacks <a class="yt-timestamp" data-t="29:24:00">[29:24:00]</a>. Therefore, the question should be: when should one trust the model? The answer is: most of the time, once its quality and legality have been established <a class="yt-timestamp" data-t="30:07:00">[30:07:00]</a> <a class="yt-timestamp" data-t="30:20:00">[30:20:00]</a>.

### Algorithmic Bias
The concept of "unbiased algorithms" is complex and difficult to define <a class="yt-timestamp" data-t="33:18:00">[33:18:00]</a>. Different definitions of bias can be mathematically mutually exclusive, meaning an algorithm cannot simultaneously satisfy all notions of fairness <a class="yt-timestamp" data-t="32:31:00">[32:31:00]</a>. Forcing algorithms to reflect desired outcomes rather than past reality can lead to absurd results (e.g., Google's image generation producing "black Nazis" when asked for German soldiers from 1943) <a class="yt-timestamp" data-t="34:01:00">[34:01:00]</a>.

Algorithms often act as mirrors of past [[human_behavior | human behavior]] and [[human_biases_and_judgment | biases]] <a class="yt-timestamp" data-t="36:00:00">[36:00:00]</a>. For example, Amazon's recruiting algorithm became "sexist" because it replicated the company's historical male-dominated hiring patterns <a class="yt-timestamp" data-t="35:18:00">[35:18:00]</a>. Scrapping such an algorithm might hide past biases rather than address them <a class="yt-timestamp" data-t="36:09:00">[36:09:00]</a>. The challenge lies in defining desired priorities (e.g., a specific percentage of women in hires) for the algorithm to enforce, a [[decision_making | decision]] that humans are often uncomfortable making explicitly <a class="yt-timestamp" data-t="36:41:00">[36:41:00]</a> <a class="yt-timestamp" data-t="38:50:00">[38:50:00]</a>.

## Implications for Users and Regulators

The rise of [[ai_in_decision_making | AI in decision making]] prompts critical questions for both users and [[government_role_in_managing_uncertainty_and_complexity | government_role_in_managing_uncertainty_and_complexity | regulators]]:

### For Users
*   **Self-Assessment**: How good are your [[decision_making | decisions]] without AI? <a class="yt-timestamp" data-t="39:38:00">[39:38:00]</a>
*   **Potential of AI**: How much can your decisions improve with AI assistance? <a class="yt-timestamp" data-t="39:41:00">[39:41:00]</a>
*   **Defining Priorities**: What explicit criteria and priorities will you provide to the AI, moving beyond convenient ambiguity? <a class="yt-timestamp" data-t="39:45:00">[39:45:00]</a>

### For Regulators
*   **Regulating Quality**: Where should regulators focus on regulating the quality of decisions, acknowledging that human decisions are often not as good as perceived? <a class="yt-timestamp" data-t="40:02:00">[40:02:00]</a>
*   **AI Model Validation**: Who will validate the quality and effectiveness of AI models? <a class="yt-timestamp" data-t="40:30:00">[40:30:00]</a>
*   **Defining Bias**: What constitutes a legally, politically, or socially unacceptable bias in algorithms? This is not a trivial question. <a class="yt-timestamp" data-t="41:01:00">[41:01:00]</a>

## Is Noise Always Bad?

The speaker defines noise as *unwanted variability*, meaning by definition it is always bad <a class="yt-timestamp" data-t="43:03:00">[43:03:00]</a>. However, *variability* is not always bad. Variability is beneficial in specific circumstances:
*   **Markets**: Disagreement about future value drives markets <a class="yt-timestamp" data-t="43:56:00">[43:56:00]</a>. The market ultimately decides who is right and wrong <a class="yt-timestamp" data-t="44:11:00">[44:11:00]</a>.
*   **Creativity and Innovation**: Different approaches to solving problems or developing solutions (e.g., vaccines, mathematical proofs) are welcome because competition ultimately selects what works <a class="yt-timestamp" data-t="44:22:00">[44:22:00]</a>.
*   **Matters of Taste**: Where there is no objectively correct answer, diversity of opinion is irrelevant (e.g., chocolate vs. coffee preference) <a class="yt-timestamp" data-t="45:00:00">[45:00:00]</a>.

In most other situations, particularly professional judgments where a correct answer exists, if two people disagree, at least one of them must be wrong <a class="yt-timestamp" data-t="45:37:00">[45:37:00]</a> <a class="yt-timestamp" data-t="45:47:00">[45:47:00]</a>.

### Practicalities
*   **Noise Audits**: Methodologies exist, and further technical work is being published on how to conduct them (e.g., Appendix A in "Noise" book) <a class="yt-timestamp" data-t="46:15:00">[46:15:00]</a>. These can be adopted by [[government_role_in_managing_uncertainty_and_complexity | governments]] <a class="yt-timestamp" data-t="47:00:00">[47:00:00]</a>.
*   **Cost of AI**: While advanced [[ai_in_decision_making | AI in decision making]] may be developed by private companies, simpler yet effective algorithms often fit "on the back of an envelope" and are often cheaper than human intervention <a class="yt-timestamp" data-t="47:47:00">[47:47:00]</a> <a class="yt-timestamp" data-t="48:41:00">[48:41:00]</a>.
*   **Confirmation Bias**: While LLMs like ChatGPT can provide counter-arguments, which might help a researcher overcome [[human_biases_and_judgment | confirmation bias]], a decision-maker might ignore dissenting AI advice that challenges their preconceived [[decision_making | decision]] <a class="yt-timestamp" data-t="49:01:00">[49:01:00]</a>.

### Responsibility
The individual decision-maker remains responsible for their [[decision_making | decisions]], even when using AI as an aid <a class="yt-timestamp" data-t="51:14:00">[51:14:00]</a>. It is in their interest to trust an AI that is right more often than they are <a class="yt-timestamp" data-t="51:27:00">[51:27:00]</a>. However, the legal framework surrounding AI responsibility needs to evolve to account for the probabilistic nature of decisions and evaluate outcomes in aggregate, rather than solely on a case-by-case basis (e.g., comparing aggregate accident rates of self-driving cars vs. human drivers) <a class="yt-timestamp" data-t="53:00:00">[53:00:00]</a>. Humans often escape accountability for their mistakes due to the opacity of their judgment processes, while algorithms, by design, are more visible and auditable <a class="yt-timestamp" data-t="53:14:00">[53:14:00]</a>.

This discussion primarily pertains to professional judgments, where decision-makers have obligations to stakeholders (e.g., doctors to patients, judges to justice, executives to companies) <a class="yt-timestamp" data-t="55:01:00">[55:01:00]</a>. Personal decisions, where individual agency and pleasure are paramount, are outside this scope <a class="yt-timestamp" data-t="55:41:00">[55:41:00]</a>.