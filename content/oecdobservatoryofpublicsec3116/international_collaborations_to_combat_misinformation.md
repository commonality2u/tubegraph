---
title: International collaborations to combat misinformation
videoId: soQN97fWlxQ
---

From: [[oecdobservatoryofpublicsec3116]] <br/> 

## Overview of the Collaboration

An international partnership has been established between governments and international organizations, including the Organisation for Economic Co-operation and Development (OECD), the Canadian Impact Innovation Unit, and the French Behavioral Insights Unit, to apply [[behavioral_science_in_combating_misinformation | Behavioral Science]] to address [[misinformation_and_disinformation | misinformation and disinformation]] [00:00:21]. This collaboration aims to foster inclusivity and diversity in framing policy issues and responding to complex challenges that know no boundaries [00:01:06]. The objective is for solutions to benefit the global public welfare [00:01:51].

This initiative is a shining example of interdisciplinary perspectives, rigorous evaluation methods, and evidence-based policy advice translating into a meaningful partnership [00:00:52]. It highlights the importance of knowledge and experience sharing for the responsible and sustained use of [[behavioral_science_in_combating_misinformation | Behavioral Science]] in policy design and implementation [00:02:35]. The project involved experts from Duke City (as stated in the transcript), the Canadian Privy Council Office's Impact Innovation Unit, and the French Ministry of Public Transformation's Behavioral Insights Unit [00:06:52].

### Collaboration Objectives

The collaboration had three main objectives [00:07:12]:
1.  **Understanding Misinformation Spread** To better understand the complexities of why people share [[misinformation_and_disinformation | misinformation]] on social media [00:07:17].
2.  **Evaluating Interventions** To evaluate the impact of [[behavioral_science_in_combating_misinformation | behavioral interventions]] on choices to share fake news [00:07:26].
3.  **Co-developing Best Practices** To co-develop best practices to inform policy responses to [[misinformation_and_disinformation | misinformation]] [00:07:34].

## Experimental Design and Findings

The collaboration replicated and extended a 2021 *Nature* publication by Gordon Public and colleagues in the Canadian context [00:07:51]. The main thesis explored was that people share [[misinformation_and_disinformation | misinformation]] not only on purpose, but also partially due to inattention to accuracy when engaging with news online [00:08:05].

### Interventions Tested

Two scalable interventions were tested to boost attention to accuracy before people engage with news online [00:08:29]:
1.  **Accuracy Evaluation**: Respondents saw a random news headline (e.g., about a neutron star) and were asked to rate its accuracy [00:08:41].
2.  **[[digital_media_literacy_and_misinformation | Digital Media Literacy]] Tips**: Respondents saw a brief list of media literacy tips (e.g., investigate the source, look at other reports, watch for unusual formatting) before seeing both fake and true news [00:09:16].

The experiment was led by the Canadian team as part of a longitudinal survey called COSMO [00:09:50]. The sample included almost 2,000 Canadians, quota-matched to the national distribution on age, gender, and geographic region [00:09:59]. Participants were exposed to 14 headlines (7 true, 7 false) [00:10:25]. Outcome variables measured included the extent to which people believed true versus false news and their likelihood of sharing it on social media [00:10:38].

### Key Results

*   **Belief vs. Sharing Intentions**: People were more likely to rate true news as accurate and had higher intentions to share true headlines compared to false news [00:11:10]. However, the difference in sharing intentions for true versus false headlines was almost four times smaller in terms of effect size than the difference in accuracy judgments [00:11:30]. This indicates a disconnect, meaning some individuals may share fake news even when they do not believe it to be true [00:11:50].
*   **Intervention Effectiveness**: Both interventions were effective [00:12:26]. The [[digital_media_literacy_and_misinformation | digital media literacy]] tips had the greatest impact, reducing intentions to share fake news by 21% [00:12:32]. These interventions, which focus on boosting attention to accuracy, tend to show similar effects regardless of demographic variables or factors like partisanship or political ideology [00:33:21].
*   **Subpopulations and Susceptibility**: An exploratory data analysis identified three clusters of respondents based on trust, information consumption, and cognitive factors like conspiracy mentality and openness to evidence [00:13:39]:
    *   **Non-trusting (approx. 20% of sample)**: Low trust in all information sources, low openness to evidence, high conspiratorial thinking, and high psychological reactance [00:13:59].
    *   **High Trusting**: High trust in all information sources (especially social media), low openness to evidence, medium-high conspiratorial thinking, and high psychological reactance [00:14:21].
    *   **Institution Trusting**: High trust in institutional sources of information but low trust in social media, high openness to evidence, low conspiratorial thinking, and low psychological reactance [00:14:44].

    The non-trusting and social media trusting clusters exhibited significantly greater belief in and sharing of fake news compared to the institution-trusting cluster [00:15:33].

## Policy Implications and Recommendations

### Key Policy Messages

Three main policy messages emerge from this work [00:16:30]:
1.  **Don't Ignore the Human Factor**: A comprehensive policy response to [[misinformation_and_disinformation | misinformation and disinformation]] must include an expanded understanding of human behavior [00:16:43]. [[behavioral_science_in_combating_misinformation | Behavioral Science]] is key to any policy response, as all government actions have a human behavioral component [00:46:43].
2.  **Empowering People**: The collaboration focused on empowering users and changing their behaviors rather than regulating content [00:17:17]. By empowering users, effective and scalable solutions can complement holistic policy strategies and offset the threat of fake news without direct content regulation [00:17:23].
3.  **Find What Works Across Borders**: Governments should conduct rigorous policy experiments in [[international_collaboration_in_crisis_management | collaboration with other countries]] to generate sustainable responses to the spread of [[misinformation_and_disinformation | misinformation]] [00:17:47]. This work is seen as a beginning, with hope for further research to determine which mitigation strategies work for whom and in what context [00:18:01].

### Broader Context and Future Directions

The challenge of [[misinformation_and_disinformation | misinformation]] is global, and effective responses must be designed and tested across borders, ideally through [[international_collaboration_in_crisis_management | collaboration across governments]] [00:06:17]. This partnership, for instance, operated effectively despite time zone differences and high-pressure environments during the COVID-19 pandemic, demonstrating the value of diverse perspectives and a global network of academic experts [00:23:51].

The findings align with Canada's multi-pronged approach to [[addressing_misinformation_and_improving_communication_between_citizens_and_governments | addressing misinformation]], which includes structural initiatives at platform and regulatory levels, alongside empowering users to foster [[digital_media_literacy_and_misinformation | digital competencies]] [00:35:37]. Similarly, in France, there is a focus on [[digital_media_literacy_and_misinformation | media literacy]] and critical thinking interventions, and this research broadens the tools available to policymakers by adding less systemic, more easily operationalized "nudge" type interventions [00:42:16].

[[misinformation_and_disinformation | Misinformation]] is viewed as a symptom of a larger problem: an erosion of trust in Western democracies and government institutions [00:48:13]. Addressing this requires a collective effort among governments, civil society, and other actors to rebuild trust [00:48:47].

The OECD's broader work on "Building Trust and Reinforcing Democracy" emphasizes current governance challenges to democracy, including [[misinformation_and_disinformation | miss and disinformation]], participation and representation, government readiness for global challenges and undue foreign influence, and [[addressing_climate_misinformation | climate change]] delivery [00:51:03]. The current work fits into the strategy of building societal resilience to [[misinformation_and_disinformation | miss and disinformation]], which includes [[digital_media_literacy_and_misinformation | media literacy]] and understanding individual behaviors that contribute to the problem [00:54:05].

Further research is encouraged to understand how interventions might operate differently across various types of misinformation (e.g., scientific, political, cultural) [00:39:16], and to apply these findings more broadly across governments and share them with other actors in the space [00:40:52]. An example of a practical tool developed to combat [[impact_of_misinformation_on_public_health_behaviors | health misinformation]] is the toolkit by OES and the Office of the Surgeon General in the U.S., designed to help communities understand, identify, and stop the spread of [[misinformation_and_disinformation | misinformation]] [00:03:04].