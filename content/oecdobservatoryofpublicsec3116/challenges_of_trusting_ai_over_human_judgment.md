---
title: Challenges of trusting AI over human judgment
videoId: VJES_jPWf70
---

From: [[oecdobservatoryofpublicsec3116]] <br/> 

The integration of artificial intelligence (AI) into [[AI in decision making | decision-making processes]] raises a fundamental question: Can AI reduce flaws in [[Human biases and judgment | human judgment]], or does it merely amplify existing biases and noise? <a class="yt-timestamp" data-t="01:51:22">[01:51:22]</a> This question challenges conventional thinking about the role of technology in human decision-making <a class="yt-timestamp" data-t="02:04:00">[02:04:00]</a>.

## Understanding AI and Human Cognition

AI systems can be broadly categorized into two types, which bear a striking resemblance to human cognitive systems:

1.  **Symbolic AI (Good Old-Fashioned AI - GOAI)**: This type operates by following explicit rules and symbolic representations, much like [[AI in decision making | System 2]] thinking in humans (logical, slow, and rule-based) <a class="yt-timestamp" data-t="07:07:07">[07:07:07]</a>. While reliable for non-complex tasks, GOAI can make "stupid mistakes" when faced with situations not covered by its rules, such as a self-driving car stopping for a backpack with a stop sign <a class="yt-timestamp" data-t="07:35:05">[07:35:05]</a>.
2.  **Machine Learning (e.g., Large Language Models like ChatGPT)**: This newer form learns from data, inferring implicit rules from a blank slate <a class="yt-timestamp" data-t="08:10:07">[08:10:07]</a>. It functions similarly to [[AI in decision making | System 1]] thinking (associative, fast, intuitive) <a class="yt-timestamp" data-t="09:01:07">[09:01:07]</a>. While powerful for tasks like protein folding or generating human-like text <a class="yt-timestamp" data-t="08:37:07">[08:37:07]</a>, LLMs tend to replicate [[Human biases and judgment | human biases]] found in their training data <a class="yt-timestamp" data-t="10:29:07">[10:29:07]</a>. They can also produce "plausible gibberish" or "confabulate" when they lack knowledge <a class="yt-timestamp" data-t="11:03:07">[11:03:07]</a>.

Crucially, Large Language Models (LLMs) are considered bad tools for decision-making because they are inherently unstable (randomness is built-in to make them feel more human) <a class="yt-timestamp" data-t="12:35:05">[12:35:05]</a> and lack an understanding of truth, relying instead on the frequency of information in their data <a class="yt-timestamp" data-t="13:00:00">[13:00:00]</a>. If one cannot trust an LLM for decisions due to these human-like flaws, then by extension, one must question the trustworthiness of [[Human biases and judgment | human judgment]] as well <a class="yt-timestamp" data-t="14:00:00">[14:00:00]</a>.

## Three Key Challenges in Using Algorithmic Decision Aids

The discussion about AI in [[AI in decision making | decision-making]] often revolves around three interconnected problems:

### 1. Trust and Acceptability of Algorithmic Decision Aids

Empirical evidence, including meta-analyses of studies from the late 20th century, consistently shows that simple algorithms (like linear regression formulas) generally perform at least as well as, and often better than, human experts in various judgmental tasks <a class="yt-timestamp" data-t="15:18:07">[15:18:07]</a>. This holds true for predicting success in college, recidivism, parole decisions, medical diagnoses, and especially hiring decisions <a class="yt-timestamp" data-t="15:57:07">[15:57:07]</a>.

Despite this superior or equal performance, there is deep-seated [[Citizen trust and its impact on governance | public resistance]] to using any form of [[algorithmic_decision_aids | algorithmic decision aids]] when human judgment is an option <a class="yt-timestamp" data-t="19:18:07">[19:18:07]</a>. This resistance stems from several factors:
*   **Overconfidence**: Professionals often believe their personal judgment is superior to algorithms, often dismissing research on average performance <a class="yt-timestamp" data-t="20:01:07">[20:01:07]</a>.
*   **Perceived Qualitative Data**: The belief that human insight can account for qualitative data that algorithms cannot process <a class="yt-timestamp" data-t="20:14:07">[20:14:07]</a>.
*   **Lack of Feedback**: Humans rarely receive precise feedback on the accuracy of their own decisions (e.g., in hiring), which prevents learning and reduces incentive to improve or delegate <a class="yt-timestamp" data-t="24:44:07">[24:44:07]</a>.
*   **Preference for Certainty and Loss of Control**: People like certainty and struggle with the inherent probabilistic nature of judgment under uncertainty <a class="yt-timestamp" data-t="22:54:07">[22:54:07]</a>. Relying on an algorithm can feel like a loss of control <a class="yt-timestamp" data-t="23:44:07">[23:44:07]</a>.

To increase trust in AI, researchers suggest avenues such as allowing users to customize algorithms (even slightly) to give them a sense of control <a class="yt-timestamp" data-t="24:07:07">[24:07:07]</a>, providing consistent feedback on human and AI accuracy, and aligning incentives based on accuracy <a class="yt-timestamp" data-t="24:36:07">[24:36:07]</a>. The bar for AI accuracy does not need to be perfection; it only needs to be better than human performance <a class="yt-timestamp" data-t="25:43:07">[25:43:07]</a>.

### 2. The Dogma of Human Control

There is a near-universal consensus that humans should always remain in control when using AI for decisions <a class="yt-timestamp" data-t="25:58:07">[25:58:07]</a>. However, this widely acknowledged truth creates a paradox:
*   If AI agrees with human judgment, it's accepted <a class="yt-timestamp" data-t="27:00:00">[27:00:00]</a>.
*   If AI disagrees, humans often override it <a class="yt-timestamp" data-t="27:05:07">[27:05:07]</a>.

This approach negates the value of the [[algorithmic_decision_aids | advisory AI]], turning it into a tool that merely bolsters [[Human biases and judgment | confirmation bias]] and false confidence <a class="yt-timestamp" data-t="27:38:07">[27:38:07]</a>. The true value of AI lies in situations where the human is wrong, and the AI is right <a class="yt-timestamp" data-t="28:07:07">[28:07:07]</a>. Therefore, once an AI model has been quality-controlled and proven to make better decisions on average, it should generally be trusted, except in very rare "broken leg" cases where the human has decisive, unique information not available to the model <a class="yt-timestamp" data-t="29:24:07">[29:24:07]</a>.

### 3. The Challenge of Algorithmic Bias

The concept of [[Human biases and judgment | algorithmic bias]] is complex and not intuitive <a class="yt-timestamp" data-t="33:18:07">[33:18:07]</a>. Defining what constitutes an "unbiased" algorithm is extremely difficult, as different definitions of bias can be mathematically mutually exclusive <a class="yt-timestamp" data-t="32:28:07">[32:28:07]</a>. For example, an algorithm predicting recidivism might show different error rates for different racial groups (ProPublica's definition of bias) or assign different risk scores to individuals with identical profiles but different races (another definition) <a class="yt-timestamp" data-t="30:40:07">[30:40:07]</a>. Both are perceived as racist, but they cannot both be simultaneously eliminated in most real-world scenarios <a class="yt-timestamp" data-t="32:51:07">[32:51:07]</a>.

Furthermore, attempts to manually "de-bias" algorithms by forcing them to reflect desired outcomes (e.g., more gender diversity) can lead to nonsensical results if not applied with extreme nuance (e.g., Google's image generation showing "black Nazis" or female "Vikings") <a class="yt-timestamp" data-t="34:14:07">[34:14:07]</a>. When an algorithm like Amazon's recruiting tool reflects past human biases (e.g., against women), it acts as a mirror <a class="yt-timestamp" data-t="35:18:07">[35:18:07]</a>. Scrapping such an algorithm, as Amazon did, means reverting to the original, equally biased human process, rather than confronting and explicitly defining desired outcomes <a class="yt-timestamp" data-t="35:57:07">[35:57:07]</a>.

The fundamental challenge in building and using unbiased algorithms is that it forces decision-makers to explicitly define their priorities and values, often moving beyond the convenient [[embracing_uncertainty_in_governance | ambiguity]] of case-by-case human decisions <a class="yt-timestamp" data-t="38:50:07">[38:50:07]</a>. This requires uncomfortable choices, such as prioritizing lower crime rates versus fewer people in jail <a class="yt-timestamp" data-t="37:50:07">[37:50:07]</a>.

## Implications for Users and Regulators

These challenges raise critical questions for both users and regulators of AI:

**For Users**:
*   How good are human decisions without AI? <a class="yt-timestamp" data-t="39:38:07">[39:38:07]</a>
*   How good can decisions be with AI, assuming it's better? <a class="yt-timestamp" data-t="39:41:07">[39:41:07]</a>
*   What explicit priorities will be given to the AI, moving beyond convenient ambiguity? <a class="yt-timestamp" data-t="39:48:07">[39:48:07]</a>

**For Regulators**:
*   Where should the quality of human decisions be regulated, given they are often not as good as perceived? <a class="yt-timestamp" data-t="40:02:07">[40:02:07]</a> This question is often overlooked in favor of immediate AI regulation <a class="yt-timestamp" data-t="40:15:07">[40:15:07]</a>.
*   Who will validate the quality of AI models, beyond the claims of developers? <a class="yt-timestamp" data-t="40:30:07">[40:30:07]</a>
*   What constitutes a legally, politically, or socially unacceptable bias in an algorithm? <a class="yt-timestamp" data-t="41:01:07">[41:01:07]</a>

Ultimately, addressing the [[challenges_and_considerations_in_ai_and_blockchain_integration | challenges and considerations in AI and blockchain integration]] requires confronting the inherent [[Noise and variability in human decisions | noise and variability in human decisions]] and being prepared to define explicit objectives for AI systems, rather than simply expecting them to replicate vague human judgment <a class="yt-timestamp" data-t="39:10:07">[39:10:07]</a>. The legal framework surrounding AI decisions needs to evolve to evaluate probabilistic outcomes in aggregate, rather than focusing solely on individual "mistakes" <a class="yt-timestamp" data-t="53:00:07">[53:00:07]</a>. This shift demands greater [[role_of_technology_and_ai_literacy_in_public_service | technology and AI literacy]] across society.