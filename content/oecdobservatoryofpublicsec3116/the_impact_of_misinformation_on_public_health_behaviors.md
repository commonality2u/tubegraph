---
title: The impact of misinformation on public health behaviors
videoId: IUItMNuR23s
---

From: [[oecdobservatoryofpublicsec3116]] <br/> 

This article summarizes discussions from an OECD meet-up focusing on [[behavioral_science_and_misinformation | behavioral science and misinformation]], particularly how misinformation impacts public health behaviors. The event, co-organized with the UNDP, highlighted the escalating seriousness of misinformation as a threat to democracy and public well-being <a class="yt-timestamp" data-t="00:01:03">[00:01:03]</a>.

## The Misinformation Threat

The modern information ecosystem is characterized by limited regulation and increasing algorithmic determinism, where the media constantly competes for limited public attention <a class="yt-timestamp" data-t="00:01:14">[00:01:14]</a>. While academic literature on misinformation is prolific, governments often rely on traditional tools like education and regulation, with limited adoption of evidence-based solutions from [[behavioral_science_applications_in_combating_misinformation | behavioral science]] <a class="yt-timestamp" data-t="00:02:05">[00:02:05]</a>. This includes proactive strategies to change attitudes and behaviors that contribute to the spread of misinformation <a class="yt-timestamp" data-t="00:02:24">[00:02:24]</a>.

## Canadian Perspective: Lauren Conway (OECD)

Lauren Conway, lead behavioral scientist at the OECD's Impact and Innovation Unit in the Canadian federal government, shared insights from their work <a class="yt-timestamp" data-t="00:03:55">[00:03:55]</a>. Her team pivoted to supporting the government's COVID-19 response, necessitating a rapid research architecture <a class="yt-timestamp" data-t="00:06:51">[00:06:51]</a>.

### COSMO Canada Study Findings

Through their "COSMO Canada" study, a nationwide longitudinal tracking study following 2,000 Canadians since April 2020, they monitor knowledge, risk perceptions, and self-reported behaviors as the pandemic evolves <a class="yt-timestamp" data-t="00:07:33">[00:07:33]</a>. A key finding was the strong association between the ability to discern the accuracy of misinformation and intentions to vaccinate against COVID-19 <a class="yt-timestamp" data-t="00:10:56">[00:10:56]</a>. People better at identifying misinformation were more likely to get vaccinated <a class="yt-timestamp" data-t="00:11:11">[00:11:11]</a>. This pattern consistently emerged as one of the strongest predictors, especially in later waves of the pandemic <a class="yt-timestamp" data-t="00:11:18">[00:11:18]</a>.

For instance, 90% of individuals who intended to get a vaccine as soon as it was available identified all misinformation statements as false, a percentage that significantly decreased across the vaccine acceptance spectrum <a class="yt-timestamp" data-t="00:11:37">[00:11:37]</a>. While an association, this mirrors findings in published literature <a class="yt-timestamp" data-t="00:11:59">[00:11:59]</a>.

### Research Program Objectives

Canada launched a research program grounded in [[behavioral_science_applications_in_combating_misinformation | behavioral science]] with two main objectives:
1.  **Understand the misinformation landscape:** This involves identifying individual and environmental factors underlying susceptibility to misinformation and the propensity to share it online <a class="yt-timestamp" data-t="00:12:41">[00:12:41]</a>. Factors like trust in public institutions and science, reasoning skills, and numeracy showed stronger associations with susceptibility than socio-demographic factors like gender and age <a class="yt-timestamp" data-t="00:15:19">[00:15:19]</a>. The structure of online environments and social network effects can also amplify cognitive biases, such as echo chambers creating an illusion of support and amplifying false consensus and confirmation biases <a class="yt-timestamp" data-t="00:15:46">[00:15:46]</a>.
2.  **Design and test interventions:** Using this knowledge to reduce or slow the spread of misinformation online <a class="yt-timestamp" data-t="00:13:08">[00:13:08]</a>. Factors related to susceptibility may differ from those associated with sharing misinformation <a class="yt-timestamp" data-t="00:16:17">[00:16:17]</a>. One theory suggests people share due to "inattention," focusing on attracting followers rather than accuracy <a class="yt-timestamp" data-t="00:16:42">[00:16:42]</a>. Research suggests attention-based [[interventions_and_strategies_to_reduce_misinformation | interventions]] that nudge users to think about accuracy could be effective <a class="yt-timestamp" data-t="00:17:13">[00:17:13]</a>.

More research is needed, particularly using a behavioral and cognitive lens, to understand which strategies are most effective, for whom, and if they can be realistically implemented and scaled <a class="yt-timestamp" data-t="00:17:44">[00:17:44]</a>.

### Challenges Identified

Key informant interviews revealed several [[challenges_in_addressing_misinformation_in_different_contexts | challenges]]:
*   **Lack of real-world research:** Many knowledge gaps stem from insufficient empirical evidence in natural settings, particularly on social media platforms <a class="yt-timestamp" data-t="00:18:20">[00:18:20]</a>.
*   **Limited access to platform data:** Researchers struggle to access social media platform data, hindering understanding of how misinformation spreads (e.g., more is known about Twitter due to its open API than Facebook, TikTok, or WhatsApp) <a class="yt-timestamp" data-t="00:18:44">[00:18:44]</a>. Without "impression data" (who actually read content), it's hard to understand the true impact of misinformation <a class="yt-timestamp" data-t="00:19:59">[00:19:59]</a>.
*   **Intervention validity:** Tested [[interventions_and_strategies_to_reduce_misinformation | interventions]] often lack ecological and external validity because rigorous testing opportunities on social media platforms are rare <a class="yt-timestamp" data-t="00:20:34">[00:20:34]</a>. The current evidence relies heavily on self-report data, which has limitations <a class="yt-timestamp" data-t="00:20:57">[00:20:57]</a>.

## Lebanese Perspective: Rory Jones (UNDP)

Rory Jones, Head of Experimentation at the UNDP Lebanon Accelerator Lab, shared how [[behavioral_science_applications_in_combating_misinformation | behaviorally informed interventions]] are part of a broader ecosystem <a class="yt-timestamp" data-t="00:24:25">[00:24:25]</a>.

### Context of Compounded Crises

Lebanon faces "compounded crises," including COVID-19, political deadlock, economic collapse (leading to 50% poverty and hyperinflation), and the Beirut explosion <a class="yt-timestamp" data-t="00:25:58">[00:25:58]</a>. This has led to declining public faith in government and rising fatigue <a class="yt-timestamp" data-t="00:27:01">[00:27:01]</a>. The media landscape is further muddied by political affiliations, leading to distrust in traditional media <a class="yt-timestamp" data-t="00:27:47">[00:27:47]</a>.

### Challenges in Misinformation Sharing and Belief

In this environment, most Lebanese have limited "bandwidth" to verify information or critically engage with it <a class="yt-timestamp" data-t="00:28:55">[00:28:55]</a>. They rely on common sense or self-constructed judgments of information sources, and fact-checking is not common due to lack of interest <a class="yt-timestamp" data-t="00:29:21">[00:29:21]</a>.

A COVID-19 chatbot designed to provide reliable health information became an outlet for people to seek information and resources about other crises (e.g., cash assistance, jobs, housing damage) <a class="yt-timestamp" data-t="00:30:27">[00:30:27]</a>. This demonstrated that COVID-19 was not a priority compared to the country's other urgent problems <a class="yt-timestamp" data-t="00:30:49">[00:30:49]</a>.

### Interventions and Strategies

Given the limited bandwidth, [[interventions_and_strategies_to_reduce_misinformation | interventions]] need to be designed accordingly <a class="yt-timestamp" data-t="00:31:04">[00:31:04]</a>.
*   **Lightweight Interactions:** The UNDP Lebanon's collaboration with Ted's Healthy Internet Project encourages lightweight interactions, asking youth to *flag* potential misinformation rather than bearing the burden of fact-checking themselves <a class="yt-timestamp" data-t="00:31:12">[00:31:12]</a>. This crowdsourced moderation allows many people to contribute in a small way to address misinformation structurally <a class="yt-timestamp" data-t="00:31:47">[00:31:47]</a>.
*   **Beyond the Digital Sphere:** Misinformation has a long history outside the digital sphere in Lebanon, such as rumors spread during the civil war to maintain fear <a class="yt-timestamp" data-t="00:32:19">[00:32:19]</a>. Traditional media (like TV, which 84% of Lebanese use for news <a class="yt-timestamp" data-t="00:33:11">[00:33:11]</a>) remains significant, despite being affiliated with political parties and thus distrusted <a class="yt-timestamp" data-t="00:33:23">[00:33:23]</a>. The UNDP is researching common misinformation topics and their relation to behaviors to inform a new [[interventions_and_strategies_to_reduce_misinformation | behaviorally informed campaign]] that will be both digital and analog <a class="yt-timestamp" data-t="00:33:50">[00:33:50]</a>.
*   **Context is Key:** A "one-size-fits-all" approach will not work even in a small country like Lebanon due to significant regional differences in perception and behaviors <a class="yt-timestamp" data-t="00:34:38">[00:34:38]</a>. This highlights the [[challenges_in_addressing_misinformation_in_different_contexts | challenges in addressing misinformation in different contexts]].
*   **Absence of Government Trust:** In Lebanon, with a lack of government and deep public distrust, approaches that foreground the government as a fact-checker lose traction <a class="yt-timestamp" data-t="00:37:07">[00:37:07]</a>. Campaigns need to consider using logos of trusted agencies (e.g., WHO, UNICEF) instead of government ones <a class="yt-timestamp" data-t="00:37:43">[00:37:43]</a>. Surprisingly, a Facebook ad stating "57% of Lebanese believe COVID-19 is a conspiracy intended to control people" was highly successful, attracting clicks and extensive debate, indicating people were more interested in engaging with conspiracies about the government than taking the government seriously <a class="yt-timestamp" data-t="00:36:08">[00:36:08]</a>.

## UNDP's Broader Approach: Nyam Anafin

Nyam Anafin, Senior Advisor in Information Integrity at the UNDP, shared broader strategies for combating "information pollution" <a class="yt-timestamp" data-t="00:39:54">[00:39:54]</a>. The UNDP's framework focuses on:
*   Building public trust in and access to public information sources <a class="yt-timestamp" data-t="00:41:21">[00:41:21]</a>.
*   Building public resilience to information pollution <a class="yt-timestamp" data-t="00:41:29">[00:41:29]</a>.
*   Improving media capacity <a class="yt-timestamp" data-t="00:41:32">[00:41:32]</a>.
*   Supporting evidence-based and rights-based [[policy_implications_for_tackling_misinformation | policy responses]] <a class="yt-timestamp" data-t="00:41:38">[00:41:38]</a>.

Across all these areas, [[behavioral_science_applications_in_combating_misinformation | behavioral science]] plays a crucial role <a class="yt-timestamp" data-t="00:41:43">[00:41:43]</a>. Nyam emphasized that "context is key" and there is no "one-size-fits-all" or "silver bullet" solution given diverse political and social landscapes <a class="yt-timestamp" data-t="00:42:07">[00:42:07]</a>. Combining online research with "human perspectives" (audience/user research, interviews, focus groups) provides a more nuanced and localized understanding of misinformation trends and how people interact with information <a class="yt-timestamp" data-t="00:43:26">[00:43:26]</a>.

## OECD Reflections: Benjamin Khan

Benjamin Khan, Head of Development and the Development Innovation Team at the OECD, shared reflections on what worked and what didn't:
*   **Debunking:** Actively debunking myths and conspiracy theories in public information campaigns should be avoided, as it can have a reinforcing effect by repeating the misinformation <a class="yt-timestamp" data-t="00:45:10">[00:45:10]</a>. However, debunking might be appropriate in on-site workshops where conversations can happen <a class="yt-timestamp" data-t="00:45:19">[00:45:19]</a>.
*   **Segmentation & Endowment Effect:** Segmentation of target groups is vital. For supporters of conspiracy theories, the time invested in collecting information and building their worldview can create an "endowment effect" or "IKEA effect," making them more attached to their beliefs <a class="yt-timestamp" data-t="00:46:18">[00:46:18]</a>. Countering this effect is a significant [[challenges_in_misinformation_sharing_and_belief | challenge]] <a class="yt-timestamp" data-t="00:46:47">[00:46:47]</a>.

## Discussion on Social Media Algorithms and Solutions

A key question from the audience was whether fundamental redesign of [[the_role_of_social_media_platforms_in_spreading_misinformation | social media platforms]] is necessary to fight misinformation <a class="yt-timestamp" data-t="00:48:51">[00:48:51]</a>.
*   **Lauren Conway** agreed that influencing change requires redesigning online environments and gaining access to platform data to understand the problem and test [[interventions_and_strategies_to_reduce_misinformation | interventions]] <a class="yt-timestamp" data-t="00:50:39">[00:50:39]</a>.
*   **Rory Jones** noted the contradiction that social media platforms employ behavioral scientists who may increase virality, not necessarily combat misinformation <a class="yt-timestamp" data-t="00:51:30">[00:51:30]</a>. He questioned the realism of fighting misinformation when big tech appears to act against efforts to filter it, suggesting it's better to push out correct information than to debunk myths, which can amplify them <a class="yt-timestamp" data-t="00:52:51">[00:52:51]</a>.

Regarding "inoculation" or "pre-bunking" as a long-term solution:
*   **Lauren Conway** stated that while inoculation shows promise, there's limited evidence on how long its effects last (decay) <a class="yt-timestamp" data-t="00:56:00">[00:56:00]</a>. There is no single "silver bullet" solution; a range of varied solutions is required, varying based on user engagement levels and context <a class="yt-timestamp" data-t="00:55:40">[00:55:40]</a>.
*   **Rory Jones** added that misinformation is constantly evolving, so inoculation cannot be a static long-term solution <a class="yt-timestamp" data-t="00:57:04">[00:57:04]</a>. A "family of solutions" tailored to different target groups, areas, and personas is necessary, requiring ongoing research and adaptability to new forms of fake news <a class="yt-timestamp" data-t="00:57:49">[00:57:49]</a>.