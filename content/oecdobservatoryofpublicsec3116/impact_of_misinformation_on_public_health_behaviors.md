---
title: Impact of misinformation on public health behaviors
videoId: IUItMNuR23s
---

From: [[oecdobservatoryofpublicsec3116]] <br/> 

The spread of [[misinformation_and_disinformation | misinformation]] is increasingly seen as a serious threat to the future of democracy <a class="yt-timestamp" data-t="00:01:05">[00:01:05]</a>. The current information ecosystem is characterized by limited regulation and increasing algorithmic determinism, where the media business constantly competes for limited public attention <a class="yt-timestamp" data-t="00:01:16">[00:01:16]</a>. While academic literature on [[misinformation_and_disinformation | misinformation]] and [[behavioral_science_in_combating_misinformation | behavioral science]] is prolific, governments still primarily rely on traditional tools like education or regulation, with limited consideration for [[behavioral_science_in_combating_misinformation | behavioral science]]-based solutions <a class="yt-timestamp" data-t="00:02:11">[00:02:11]</a>.

## The Role of Behavioral Science

Organizations like the OECD are working on developing good practice principles for public communications to counter [[misinformation_and_disinformation | misinformation]] and [[misinformation_and_disinformation | disinformation]] <a class="yt-timestamp" data-t="00:01:53">[00:01:53]</a>. The goal is to proactively change attitudes and [[behavioral_change_during_covid19 | behaviors contributing to the spread of misinformation]] <a class="yt-timestamp" data-t="00:02:24">[00:02:24]</a>.

## Case Study: Canada's COVID-19 Response

The Canadian federal government's Impact and Innovation Unit, specifically its [[behavioral_science_in_combating_misinformation | behavioral science]] team, has pivoted extensively to support the government's COVID-19 response work since March 2020 <a class="yt-timestamp" data-t="00:06:51">[00:06:51]</a>. They quickly established a new research architecture for rapid and responsive analysis, collecting data from Canadians since April 2020 using three main channels <a class="yt-timestamp" data-t="00:07:10">[00:07:10]</a>.

### COSMO Canada Study
The COSMO Canada study, a nationwide longitudinal tracking study, adapted the WHO's behavioral insights tool to monitor Canadians' knowledge, risk perceptions, and self-reported [[behavioral_change_during_covid19 | behaviors]] as the pandemic evolves <a class="yt-timestamp" data-t="00:07:36">[00:07:36]</a>. This tool helps identify early warning signs or trends warranting further investigation <a class="yt-timestamp" data-t="00:08:04">[00:08:04]</a>.

A key finding from the COSMO study is that the ability to discern the accuracy of [[misinformation_and_disinformation | misinformation]] has consistently emerged as an important factor related to public health adherence <a class="yt-timestamp" data-t="00:08:52">[00:08:52]</a>.

Respondents were presented with true and false statements related to COVID-19, such as:
*   "The COVID-19 death rate has been deliberately and greatly exaggerated." <a class="yt-timestamp" data-t="00:09:31">[00:09:31]</a>
*   "New COVID vaccine technologies alter DNA." <a class="yt-timestamp" data-t="00:09:38">[00:09:38]</a>

Surprisingly, 20-30% of the sample either classified these false statements as true or definitely true/probably true, or were unsure about their accuracy <a class="yt-timestamp" data-t="00:09:50">[00:09:50]</a>.

### Misinformation and Vaccination Intentions
Analysis showed that the ability to discern the accuracy of [[misinformation_and_disinformation | misinformation]] about COVID-19 is strongly and uniquely associated with Canadians' intentions to get vaccinated <a class="yt-timestamp" data-t="00:10:56">[00:10:56]</a>. The better someone is at correctly identifying [[misinformation_and_disinformation | misinformation]], the more likely they are to get vaccinated <a class="yt-timestamp" data-t="00:11:11">[00:11:11]</a>. This correlation has been one of the strongest predictors, especially in later waves of the pandemic <a class="yt-timestamp" data-t="00:11:18">[00:11:18]</a>.

For instance, 90% of individuals intending to get a vaccine as soon as it's available identified all [[misinformation_and_disinformation | misinformation]] statements as false <a class="yt-timestamp" data-t="00:11:39">[00:11:39]</a>. This percentage significantly decreases among those less accepting of vaccination <a class="yt-timestamp" data-t="00:11:47">[00:11:47]</a>.

### Factors Influencing Susceptibility and Sharing
Research indicates mixed evidence for socio-demographic factors like gender and age in susceptibility to [[misinformation_and_disinformation | misinformation]] <a class="yt-timestamp" data-t="00:15:19">[00:15:19]</a>. However, stronger associations are found with:
*   **Trust in Public Institutions and Science:** The degree of trust in public institutions and science <a class="yt-timestamp" data-t="00:15:34">[00:15:34]</a>.
*   **Cognitive Skills:** Reasoning and numeracy skills <a class="yt-timestamp" data-t="00:15:40">[00:15:40]</a>.

The structure of online environments and social network effects can amplify existing cognitive biases, like echo chambers creating an illusion of support and amplifying false consensus and confirmation biases <a class="yt-timestamp" data-t="00:15:46">[00:15:46]</a>.

Factors related to susceptibility to [[misinformation_and_disinformation | misinformation]] may differ from those associated with the propensity to share it <a class="yt-timestamp" data-t="00:16:17">[00:16:17]</a>. One prominent theory suggests that people share because they are not paying attention to the accuracy of content <a class="yt-timestamp" data-t="00:16:45">[00:16:45]</a>. [[role_of_social_media_algorithms_in_misinformation | Social media algorithms]] often focus attention on other factors, such as the desire to attract and please followers <a class="yt-timestamp" data-t="00:16:53">[00:16:53]</a>. Interventions that nudge users to think about accuracy when engaging on social media platforms show promise <a class="yt-timestamp" data-t="00:17:10">[00:17:10]</a>.

## Case Study: Lebanon's Information Landscape

In Lebanon, a country in a state of compounded crises including political deadlock, economic collapse, and the Beirut explosion, people have little to no bandwidth to verify information or critically engage with it <a class="yt-timestamp" data-t="00:25:58">[00:25:58]</a>. They rely on common sense or self-constructed judgments of information sources rather than fact-checking <a class="yt-timestamp" data-t="00:29:21">[00:29:21]</a>. Lack of interest is the top reason for not fact-checking <a class="yt-timestamp" data-t="00:29:32">[00:29:32]</a>.

### COVID-19 Information Dissemination
A chatbot designed to supply reliable COVID-19 health information in Lebanon ended up becoming an outlet for people to seek information and resources about the country's other various crises <a class="yt-timestamp" data-t="00:30:21">[00:30:21]</a>. Messages received indicated that COVID-19 was not a priority compared to needs like cash assistance, job opportunities, or housing damage <a class="yt-timestamp" data-t="00:30:44">[00:30:44]</a>. This highlights the need to design interventions that account for people's limited bandwidth and priorities <a class="yt-timestamp" data-t="00:31:02">[00:31:02]</a>.

### Beyond Digital Channels
While there's a surge of interest in [[misinformation_and_disinformation | misinformation]] due to social media, these phenomena have long histories and continue to exist outside the digital sphere <a class="yt-timestamp" data-t="00:32:10">[00:32:10]</a>. In Lebanon, traditional media, largely affiliated with political parties, contributes to distrust in formal media <a class="yt-timestamp" data-t="00:27:50">[00:27:50]</a>. People are more interested in engaging with conspiracies about the government than taking the government seriously as a trustworthy source of information <a class="yt-timestamp" data-t="00:37:03">[00:37:03]</a>. Using [[government_and_media_trust_in_misinformation_contexts | government]] logos can even hurt campaigns <a class="yt-timestamp" data-t="00:37:20">[00:37:20]</a>.

UNDP Lebanon's approach acknowledges this by considering:
*   The history of [[misinformation_and_disinformation | disinformation]] (e.g., rumors during the Lebanese civil war) <a class="yt-timestamp" data-t="00:32:20">[00:32:20]</a>.
*   The continued significance of legacy media <a class="yt-timestamp" data-t="00:32:51">[00:32:51]</a>.
*   Generational divides and urban-rural specificities <a class="yt-timestamp" data-t="00:34:27">[00:34:27]</a>.

Their research showed that a one-size-fits-all approach will not work <a class="yt-timestamp" data-t="00:34:38">[00:34:38]</a>.

## Challenges and Considerations

### Data Access and Collaboration
A significant gap in knowledge about [[misinformation_and_disinformation | misinformation]] and [[misinformation_and_disinformation | disinformation]] stems from a lack of research and empirical evidence in real-world contexts, particularly on [[role_of_social_media_algorithms_in_misinformation | social media platforms]] <a class="yt-timestamp" data-t="00:18:16">[00:18:16]</a>. There is a need for researchers to have better access to [[role_of_social_media_algorithms_in_misinformation | social media platform]] data to further knowledge <a class="yt-timestamp" data-t="00:18:42">[00:18:42]</a>. This lack of access and collaboration between researchers and platforms underlies many challenges, including understanding and measuring the spread of [[misinformation_and_disinformation | misinformation]] <a class="yt-timestamp" data-t="00:19:04">[00:19:04]</a>.

For example, most research on spread and sharing comes from Twitter due to its open API, while less is known about platforms like Facebook, TikTok, or WhatsApp where [[misinformation_and_disinformation | misinformation]] may be more rampant <a class="yt-timestamp" data-t="00:19:26">[00:19:26]</a>. It's also difficult to measure the impact of exposure to [[misinformation_and_disinformation | misinformation]] because researchers often have access only to "expression data" (likes, shares) but not "impression data" (who actually read or was exposed to content) <a class="yt-timestamp" data-t="00:19:53">[00:19:53]</a>.

### Intervention Effectiveness
Interventions tested to date often lack ecological and external validity because of insufficient opportunities to rigorously test them on [[role_of_social_media_algorithms_in_misinformation | social media platforms]] <a class="yt-timestamp" data-t="00:20:32">[00:20:32]</a>. The current body of evidence heavily relies on self-report data, which has inherent challenges <a class="yt-timestamp" data-t="00:20:57">[00:20:57]</a>. More research is needed to understand what strategies are most effective, for whom they work, and if they can be realistically implemented and scaled <a class="yt-timestamp" data-t="00:17:44">[00:17:44]</a>.

## Potential Interventions

### Crowdsourced Moderation
Projects like the TED Healthy Internet Project encourage lightweight interactions, such as asking youth to flag potential [[misinformation_and_disinformation | misinformation]] rather than burdening them with fact-checking <a class="yt-timestamp" data-t="00:31:09">[00:31:09]</a>. This shifts responsibility from individuals to a collective effort, with many people contributing in tiny ways to address [[misinformation_and_disinformation | misinformation]] at a structural level <a class="yt-timestamp" data-t="00:31:45">[00:31:45]</a>.

### Debunking vs. Proactive Information
Experts suggest that myths and conspiracy theories should generally not be actively debunked in public information campaigns and [[behavioral_science_in_combating_misinformation | behavioral interventions]] in the online space, as it can have a reinforcing effect <a class="yt-timestamp" data-t="00:45:10">[00:45:10]</a>. It is better to focus on pushing out correct information rather than debunking [[misinformation_and_disinformation | misinformation]] <a class="yt-timestamp" data-t="00:52:57">[00:52:57]</a>. While some literature suggests debunking can have backfire effects, others argue these are overblown and debunking may have a place in certain contexts <a class="yt-timestamp" data-t="00:56:35">[00:56:35]</a>.

### Inoculation (Pre-bunking)
Inoculation type approaches show promise, but there is limited evidence on their long-term effectiveness and how their effects decay over time <a class="yt-timestamp" data-t="00:56:00">[00:56:00]</a>. The constantly evolving nature of [[misinformation_and_disinformation | misinformation]] means that static inoculation strategies may not be sufficient for future forms of false information <a class="yt-timestamp" data-t="00:57:16">[00:57:16]</a>.

### Context-Specific and Multi-faceted Solutions
Given the complexity and heterogeneity of populations, there is no single solution that fits all <a class="yt-timestamp" data-t="00:55:40">[00:55:40]</a>. A range of solutions that vary in terms of user engagement levels and are tailored to specific contexts is required <a class="yt-timestamp" data-t="00:55:47">[00:55:47]</a>. This includes understanding the specific information ecosystem, generational divides, and urban-rural specificities <a class="yt-timestamp" data-t="00:42:41">[00:42:41]</a>. Ultimately, addressing [[misinformation_and_disinformation | misinformation]] requires a "family of solutions" based on different target groups and contexts <a class="yt-timestamp" data-t="00:57:49">[00:57:49]</a>.

[[International collaborations to combat misinformation | International collaborations]] are key to share learning and develop effective strategies <a class="yt-timestamp" data-t="00:13:03">[00:13:03]</a>. Organizations need to understand that the architects of [[misinformation_and_disinformation | disinformation]] often have a deep understanding of human [[behavioral_change_during_covid19 | behavior]] <a class="yt-timestamp" data-t="00:38:11">[00:38:11]</a>. Rigorous research grounded in particular contexts is crucial, and results should be made accessible to other organizations and the communities they serve <a class="yt-timestamp" data-t="00:38:33">[00:38:33]</a>.