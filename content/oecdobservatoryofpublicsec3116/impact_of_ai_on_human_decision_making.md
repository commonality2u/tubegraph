---
title: Impact of AI on Human Decision Making
videoId: VJES_jPWf70
---

From: [[oecdobservatoryofpublicsec3116]] <br/> 

Olivier Sironi, a professor of strategy at HEC Paris and an associate fellow at Sa√Ød Business School at Oxford University, is an expert in strategic thinking and the design of decision processes <a class="yt-timestamp" data-t="00:00:19">[00:00:19]</a>. His work delves into the [[Intersection of AI and Behavioral Science | intersection of AI and behavioral science]], drawing insights from his book *Noise: A Flaw in Human Judgment* <a class="yt-timestamp" data-t="00:01:09">[00:01:09]</a>. A central question in his research is whether AI can reduce flaws in human judgment or if it merely amplifies biases and noise <a class="yt-timestamp" data-t="00:01:51">[00:01:51]</a>. Sironi's discussion primarily focuses on AI's role as a decision aid <a class="yt-timestamp" data-t="00:03:56">[00:03:56]</a>.

## Errors in Human Judgment: Bias vs. Noise

In the context of decision-making, errors can be categorized into two main types:
*   **Bias** is defined as predictable error <a class="yt-timestamp" data-t="00:05:02">[00:05:02]</a>. It is an error that can be anticipated and often has identifiable causes, similar to a consistent aiming problem at a shooting range <a class="yt-timestamp" data-t="00:05:28">[00:05:28]</a>.
*   **Noise** is random error or unwanted variability <a class="yt-timestamp" data-t="00:05:41">[00:05:41]</a>. It is unpredictable and does not necessarily have a clear explanation, like widely scattered shots at a target <a class="yt-timestamp" data-t="00:06:05">[00:06:05]</a>.

Many cases involve a combination of both noise and bias <a class="yt-timestamp" data-t="00:06:09">[00:06:09]</a>. Understanding this distinction is crucial for reducing errors in general, and especially when considering [[artificial_intelligence_in_public_administration | AI in decision making]] <a class="yt-timestamp" data-t="00:06:19">[00:06:19]</a>.

## [[Types of AI and their Similarity to Human Thinking | Types of AI and their Similarity to Human Thinking]]

There are two main types of AI with striking similarities to human cognitive systems:
*   **Symbolic AI (Good Old-Fashioned AI - GOFAI)**: This type of AI follows explicit rules and uses symbolic representations, similar to System 2 thinking <a class="yt-timestamp" data-t="00:07:20">[00:07:20]</a>. It is logical, often slow, and reliable, but can make "stupid mistakes" when faced with complex, non-rule-based scenarios (e.g., a self-driving car stopping for a backpack with a stop sign) <a class="yt-timestamp" data-t="00:07:35">[00:07:35]</a>.
*   **Machine Learning (e.g., Large Language Models - LLMs)**: This type of AI starts from data, not rules, and tries to discover implicit patterns, similar to System 1 thinking <a class="yt-timestamp" data-t="00:08:22">[00:08:22]</a>. LLMs like ChatGPT can generate human-like answers by associating common data patterns <a class="yt-timestamp" data-t="00:08:53">[00:08:53]</a>.

While LLMs can be impressive, they often replicate the biases inherent in System 1 thinking and can produce "plausible gibberish" or confabulations <a class="yt-timestamp" data-t="00:11:05">[00:11:05]</a>. This means LLMs are generally not stable and do not inherently know what is true, making them unreliable tools for decision-making <a class="yt-timestamp" data-t="00:12:24">[00:12:24]</a>.

## [[Challenges and Issues with Trusting AI in Decision Making | Challenges and Issues with Trusting AI in Decision Making]]

### Human Resistance to Algorithmic Decision Aids
Empirical evidence consistently shows that algorithms, even simple ones (like linear regressions from the 1960s-80s), perform at least as well as, and often outperform, human judges and experts across various fields such as college admissions, criminal recidivism prediction, medical diagnosis, and personnel selection <a class="yt-timestamp" data-t="00:16:29">[00:16:29]</a>. For instance, in hiring decisions, a simple formula combining three basic facts about candidates often yields better results than job interviews <a class="yt-timestamp" data-t="00:17:27">[00:17:27]</a>.

Despite this evidence, people often resist using algorithmic decision aids <a class="yt-timestamp" data-t="00:18:02">[00:18:02]</a>. Reasons for this resistance include:
*   **Overconfidence in human judgment**: Individuals, especially experts, believe they are better than average and can perceive nuances that algorithms cannot <a class="yt-timestamp" data-t="00:19:59">[00:19:59]</a>.
*   **Lack of awareness of inaccuracy**: Humans often do not receive effective feedback on their decision accuracy, leading to an overestimation of their own performance <a class="yt-timestamp" data-t="00:20:27">[00:20:27]</a>.
*   **Preference for certainty**: Humans tend to state decisions as certainties (e.g., "this candidate will succeed") rather than probabilistic judgments (e.g., "65% chance of success"), making it harder to quantify and compare against algorithms <a class="yt-timestamp" data-t="00:22:56">[00:22:56]</a>.
*   **Loss of control**: A deep-seated preference for using one's own judgment <a class="yt-timestamp" data-t="00:19:18">[00:19:18]</a>.

To overcome this resistance, strategies include:
*   Allowing users to customize algorithms, even slightly, to give them a sense of control <a class="yt-timestamp" data-t="00:24:07">[00:24:07]</a>.
*   Providing clear feedback on accuracy <a class="yt-timestamp" data-t="00:24:36">[00:24:36]</a>.
*   Aligning incentives with accuracy <a class="yt-timestamp" data-t="00:24:37">[00:24:37]</a>.
Ultimately, the benchmark for AI adoption should be whether it is "better than you" <a class="yt-timestamp" data-t="00:25:46">[00:25:46]</a>.

### The "Human in Control" Dogma
There is a widespread belief that humans must always remain in control when using AI for decisions <a class="yt-timestamp" data-t="00:26:01">[00:26:01]</a>. However, this common assumption presents a paradox:
*   If AI agrees with a human decision, it reinforces existing biases <a class="yt-timestamp" data-t="00:27:00">[00:27:00]</a>.
*   If AI disagrees, overriding it negates the value AI brings, as its value lies precisely in correcting human errors when the human is wrong and the AI is right <a class="yt-timestamp" data-t="00:27:05">[00:27:05]</a>.
This leads to the conclusion that once an AI model has undergone proper quality control and proven to be better on average, it should generally be trusted, except in rare "broken leg problems" where decisive, unshared information exists <a class="yt-timestamp" data-t="00:28:30">[00:28:30]</a>.

### [[Algorithmic Bias and its Implications | Algorithmic Bias]]
Concerns about [[Algorithmic Bias and its Implications | algorithmic bias]] (e.g., racist or sexist algorithms) are prevalent <a class="yt-timestamp" data-t="00:30:30">[00:30:30]</a>. However, defining "unbiased" is extremely complex, as different definitions of fairness can be mathematically mutually exclusive <a class="yt-timestamp" data-t="00:32:31">[00:32:31]</a>. For example, an algorithm predicting recidivism might have different error rates for different racial groups or assign different risk scores based on race, even with the same input data; these two scenarios are often considered biased but cannot simultaneously be "unbiased" <a class="yt-timestamp" data-t="00:32:51">[00:32:51]</a>.

Algorithms often act as a "mirror" reflecting past human biases embedded in the data they are trained on <a class="yt-timestamp" data-t="00:36:07">[00:36:07]</a>. For instance, Amazon's recruiting tool was found to be sexist because it replicated the company's historical male-dominated hiring patterns <a class="yt-timestamp" data-t="00:35:30">[00:35:30]</a>. Scrapping such algorithms simply because they reveal past biases prevents progress <a class="yt-timestamp" data-t="00:36:09">[00:36:09]</a>. Instead, organizations should consciously decide what they want the algorithm to achieve, setting explicit priorities (e.g., target percentages for diversity) <a class="yt-timestamp" data-t="00:36:41">[00:36:41]</a>. This forces clarity on objectives that were previously ambiguous with human judgment <a class="yt-timestamp" data-t="00:38:50">[00:38:50]</a>.

## Implications for Users and Regulators
The integration of AI into decision-making raises critical questions:

### For Users
*   How accurate are human decisions without AI? <a class="yt-timestamp" data-t="00:39:38">[00:39:38]</a>
*   How much can AI improve decisions? <a class="yt-timestamp" data-t="00:39:41">[00:39:41]</a>
*   What explicit priorities should be given to AI, moving beyond convenient ambiguity? <a class="yt-timestamp" data-t="00:39:48">[00:39:48]</a>

### For Regulators
*   Where should the quality of decisions be regulated, considering that human decisions are often less accurate than perceived? <a class="yt-timestamp" data-t="00:40:02">[00:40:02]</a>
*   Who validates the quality of AI models? <a class="yt-timestamp" data-t="00:40:30">[00:40:30]</a>
*   What constitutes legally, politically, or socially unacceptable bias? <a class="yt-timestamp" data-t="00:41:01">[00:41:01]</a>

Ultimately, for professional contexts, individuals are paid to make the best possible decisions on behalf of stakeholders, implying an obligation to leverage tools, like AI, that can improve accuracy <a class="yt-timestamp" data-t="00:55:29">[00:55:29]</a>. However, this does not apply to personal decisions, where the loss of agency and the pleasure of making one's own choices might outweigh the benefits of algorithmic optimization <a class="yt-timestamp" data-t="00:55:01">[00:55:01]</a>.