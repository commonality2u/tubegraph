---
title: AI in decision making
videoId: VJES_jPWf70
---

From: [[oecdobservatoryofpublicsec3116]] <br/> 

Professor Olivier Sibony, an expert in strategic thinking and the design of [[Decision Making | decision processes]], explores the intersection of AI and [[Behavioral Insights | behavioral science]], drawing insights from his book "Noise: A Flaw in Human Judgment" <a class="yt-timestamp" data-t="00:01:09">[00:01:09]</a>. A central question is whether AI can reduce flaws in [[human_biases_and_judgment | human judgment]] or if it merely amplifies existing [[human_biases_and_judgment | biases]] and [[noise_and_variability_in_human_decisions | noise]] <a class="yt-timestamp" data-t="00:01:51">[00:01:51]</a>. Sibony's focus is on [[Decision Making | decision making]] with AI, specifically as [[algorithmic_decision_aids | decision aids]] <a class="yt-timestamp" data-t="00:03:15">[00:03:15]</a>.

## Understanding Errors in Decision Making: Bias vs. Noise

In [[Decision Making | decision making]], errors can be categorized into two main types:
*   **Bias**: A predictable error <a class="yt-timestamp" data-t="00:05:02">[00:05:02]</a>. For example, a shooting team consistently missing the target in the same direction suggests a predictable problem with the rifle or environment <a class="yt-timestamp" data-t="00:04:46">[00:04:46]</a>. This error can be anticipated and its causes hypothesized <a class="yt-timestamp" data-t="00:05:04">[00:05:04]</a>.
*   **Noise**: Random error or unwanted variability <a class="yt-timestamp" data-t="00:05:41">[00:05:41]</a>. If a shooting team's shots are scattered randomly around the target, it indicates random error without a clear pattern or predictable cause <a class="yt-timestamp" data-t="00:05:41">[00:05:41]</a>. Sibony defines noise as inherently "bad" because it is unwanted variability <a class="yt-timestamp" data-t="00:43:05">[00:43:05]</a>.
    *   While variability can be good in specific contexts like competitive markets, scientific creativity, or matters of taste, in most other judgmental situations where a correct answer exists, disagreement indicates error <a class="yt-timestamp" data-t="00:43:56">[00:43:56]</a>. For instance, if two doctors disagree on a diagnosis, at least one is wrong <a class="yt-timestamp" data-t="00:21:20">[00:21:20]</a>.
    *   Techniques like "noise audits" can be used to identify and measure variability in judgments <a class="yt-timestamp" data-t="00:42:38">[00:42:38]</a>.

## Types of AI and Their Human Parallels

Sibony proposes a typology of AI, focusing on its role as a [[algorithmic_decision_aids | decision aid]] <a class="yt-timestamp" data-t="00:03:56">[00:03:56]</a>:
1.  **Symbolic AI (GOI - Good Old-Fashioned AI)**: Follows explicit rules, uses symbolic representations, and adheres to logical rules <a class="yt-timestamp" data-t="00:07:20">[00:07:20]</a>. This resembles **System 2 thinking** (slow, logical, rule-based) <a class="yt-timestamp" data-t="00:09:13">[00:09:13]</a>. While reliable for non-complex tasks, it can make "stupid mistakes" when faced with novel or complex scenarios not covered by its rules <a class="yt-timestamp" data-t="00:07:35">[00:07:35]</a>.
2.  **Machine Learning (e.g., Large Language Models like ChatGPT)**: Starts from data, not rules, and seeks to identify implicit rules governing the data <a class="yt-timestamp" data-t="00:08:24">[00:08:24]</a>. This process generates insights that rule-based systems cannot <a class="yt-timestamp" data-t="00:08:34">[00:08:34]</a>. It eerily resembles **System 1 thinking** (associative, fast, intuitive) <a class="yt-timestamp" data-t="00:09:29">[00:09:29]</a>.

### Limitations of Large Language Models (LLMs) in Decision Making
LLMs replicate many [[human_biases_and_judgment | human biases]], including failures in cognitive traps like the bat and ball test <a class="yt-timestamp" data-t="00:10:29">[00:10:29]</a>. They can produce "plausible gibberish" that is nonsensical but coherent, similar to human confabulation <a class="yt-timestamp" data-t="00:11:05">[00:11:05]</a>.

For [[Decision Making | decision making]], LLMs are considered unreliable because <a class="yt-timestamp" data-t="00:11:41">[00:11:41]</a>:
*   **Lack of Stability**: Their answers can vary even when asked the same question, a feature often built in to make them "feel more natural" <a class="yt-timestamp" data-t="00:12:41">[00:12:41]</a>.
*   **Absence of Truth Sense**: LLMs have no inherent understanding of truth; their "best proxy for the truth is the frequency of something coming up" in their training data <a class="yt-timestamp" data-t="00:13:02">[00:13:02]</a>. For example, a ChatGPT of Galileo's time would have been "absolutely certain that the Earth is flat" due to prevailing data <a class="yt-timestamp" data-t="00:13:18">[00:13:18]</a>.

This implies that if LLMs are not trustworthy for decisions, neither are [[human_biases_and_judgment | noisy human beings]] for the same reasons <a class="yt-timestamp" data-t="00:14:00">[00:14:00]</a>.

## Challenges and Considerations for AI in Decision Making

### 1. Trust and Acceptability
Studies consistently show that simple formulas or algorithms outperform human judges in predictive tasks (e.g., college success, recidivism, hiring decisions) <a class="yt-timestamp" data-t="00:16:29">[00:16:29]</a>. Algorithms generally match or exceed human expert performance <a class="yt-timestamp" data-t="00:16:57">[00:16:57]</a>. Despite this, humans resist using [[algorithmic_decision_aids | decision aids]] and prefer their own judgment <a class="yt-timestamp" data-t="00:18:02">[00:18:02]</a>.

Reasons for resistance to [[algorithmic_decision_aids | algorithms]]:
*   **Overconfidence**: Experts often believe they are "better than average" and possess unique insights into qualitative data that algorithms cannot grasp <a class="yt-timestamp" data-t="00:19:59">[00:19:59]</a>. This includes a lack of awareness of their own inaccuracy <a class="yt-timestamp" data-t="00:20:29">[00:20:29]</a>.
*   **Preference for Certainty**: Humans prefer deterministic judgments ("this candidate will succeed") over probabilistic ones ("65% chance this candidate will succeed"), obscuring their actual error rates <a class="yt-timestamp" data-t="00:22:54">[00:22:54]</a>.
*   **Loss of Control and Preference for Ambiguity**: People resist giving up control and prefer ambiguous criteria in their decision-making processes <a class="yt-timestamp" data-t="00:23:44">[00:23:44]</a>.

Strategies to overcome resistance to [[algorithmic_decision_aids | decision aids]]:
*   **Advisory Role**: AI can be presented as an advisor <a class="yt-timestamp" data-t="00:23:57">[00:23:57]</a>.
*   **Customization**: Allowing users to slightly customize algorithms increases willingness to use them <a class="yt-timestamp" data-t="00:24:07">[00:24:07]</a>.
*   **Feedback and Incentives**: Providing regular, accurate feedback on human judgment quality and aligning incentives with accuracy can encourage improvement and adoption of [[algorithmic_decision_aids | decision aids]] <a class="yt-timestamp" data-t="00:24:36">[00:24:36]</a>.

### 2. Human Control Over AI
A widely accepted "dogma" is that humans "must stay in control" of AI decisions <a class="yt-timestamp" data-t="00:26:30">[00:26:30]</a>. However, Sibony argues that if an AI is, on average, better than human judgment, its value lies precisely in cases where it *disagrees* with human judgment <a class="yt-timestamp" data-t="00:27:10">[00:27:10]</a>. Overriding AI when it contradicts human intuition negates its value and merely reinforces confirmation [[human_biases_and_judgment | bias]] <a class="yt-timestamp" data-t="00:27:38">[00:27:38]</a>.

*   **When to trust AI**: Once an AI model has been rigorously quality-controlled and proven to make better decisions on average, it should generally be trusted, except in rare "broken leg" scenarios where the human possesses decisive information the model lacks (e.g., knowing a person broke their leg after a model predicted they'd go to the movies) <a class="yt-timestamp" data-t="00:29:51">[00:29:51]</a>.

### 3. Algorithmic Bias
The concept of "unbiased" algorithms is complex and lacks a universally accepted definition <a class="yt-timestamp" data-t="00:33:38">[00:33:38]</a>. Different definitions of fairness can be mathematically mutually exclusive <a class="yt-timestamp" data-t="00:32:37">[00:32:37]</a>.

*   **Replication of Past Biases**: Algorithms trained on historical data may replicate existing [[human_biases_and_judgment | human biases]]. For example, Amazon's recruiting tool became biased against women because it learned from past hiring decisions that favored men <a class="yt-timestamp" data-t="00:35:33">[00:35:33]</a>. Scrapping such an algorithm, rather than redesigning it, means choosing to ignore a "mirror of your past behavior" <a class="yt-timestamp" data-t="00:36:09">[00:36:09]</a>.
*   **Defining Desired Outcomes**: To make algorithms "unbiased" or aligned with desired social outcomes (e.g., gender quotas in hiring), humans must explicitly define their priorities and values <a class="yt-timestamp" data-t="00:34:51">[00:34:51]</a>. This forces clarity on difficult decisions that human judges might prefer to keep ambiguous <a class="yt-timestamp" data-t="00:38:50">[00:38:50]</a>.

## Implications

For users of [[algorithmic_decision_aids | AI in decision making]], key questions arise <a class="yt-timestamp" data-t="00:39:35">[00:39:35]</a>:
*   How good are your decisions without AI? <a class="yt-timestamp" data-t="00:39:38">[00:39:38]</a>
*   How good can your decisions be with AI? <a class="yt-timestamp" data-t="00:39:41">[00:39:41]</a>
*   What explicit priorities will you set for the AI, moving beyond convenient ambiguity? <a class="yt-timestamp" data-t="00:39:47">[00:39:47]</a>

For regulators, the questions echo these points <a class="yt-timestamp" data-t="00:39:56">[00:39:56]</a>:
*   Where should the quality of human decisions be regulated, considering they are often not as good as perceived? <a class="yt-timestamp" data-t="00:40:02">[00:40:02]</a>
*   Who will validate the quality of AI models, ensuring they are truly superior? <a class="yt-timestamp" data-t="00:40:30">[00:40:30]</a>
*   What constitutes a legally, politically, or socially unacceptable bias in algorithms? <a class="yt-timestamp" data-t="00:41:01">[00:41:01]</a>

Ultimately, the goal is to evaluate [[Decision Making | decisions]] probabilistically and in aggregate, rather than on a case-by-case basis, to better manage responsibility and outcomes <a class="yt-timestamp" data-t="00:53:02">[00:53:02]</a>. Sibony emphasizes that these insights apply primarily to professional judgments and decisions where there is an obligation to make the best possible choice on behalf of stakeholders, as opposed to personal choices where individual agency and enjoyment are paramount <a class="yt-timestamp" data-t="00:55:01">[00:55:01]</a>.