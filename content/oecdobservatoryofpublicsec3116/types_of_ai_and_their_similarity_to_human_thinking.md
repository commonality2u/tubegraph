---
title: Types of AI and their Similarity to Human Thinking
videoId: VJES_jPWf70
---

From: [[oecdobservatoryofpublicsec3116]] <br/> 
The discussion centers on the [[intersection_of_ai_and_behavioral_science | intersection of AI and behavioral science]], specifically how AI can impact or impair human judgment [00:01:12]. Professor Olivier Sironi, an expert in strategic thinking and the design of decision processes, focuses his analysis on [[impact_of_ai_on_human_decision_making | decision making with AI]], rather than AI's role in innovation or automation [00:03:15].

### Understanding Errors in Decision Making

Errors in judgment can be categorized into two main types:
*   **Bias** Bias represents a predictable, systematic error that can be anticipated, similar to a rifle consistently shooting off-target due to a mechanical fault or environmental factor [00:05:00].
*   **Noise** Noise refers to random, unpredictable error, like a shooter inconsistently missing a target due to poor aim; it's difficult to predict where the next error will occur [00:05:39].

Often, decisions involve a combination of both noise and bias [00:06:09]. Recognizing this distinction is crucial for reducing overall error, particularly when considering the [[impact_of_ai_on_human_decision_making | use of AI in decision making]] [00:06:19].

### Types of AI and their Cognitive Similarities

AI systems, particularly those used as decision aids, exhibit striking similarities to the human mind's two systems of thinking:

#### Symbolic AI (Good Old-Fashioned AI - GOFAI)
*   **Mechanism**: This type of AI operates by following explicit rules and logical processes, using symbolic representations [00:07:20]. Examples include chess programs or navigation apps like Waze [00:07:12].
*   **Limitations**: While effective for non-complex tasks, symbolic AI can make "stupid mistakes" when faced with situations outside its defined rules. An example is a self-driving car stopping for a child wearing a backpack with a stop sign, as it rigidly interprets the sign without contextual understanding [00:07:35].
*   **Human Analogy**: Symbolic AI closely mirrors **System 2 thinking**, which is rule-bound, logical, deliberate, and tends to produce consistent results. While not always perfect, especially with complexity, it is generally reliable [00:09:13].

#### Machine Learning (e.g., Large Language Models - LLMs)
*   **Mechanism**: Unlike symbolic AI, machine learning starts with vast datasets and learns implicit rules or patterns without being explicitly programmed. This "black box" approach allows it to generate insights that explicit rules might not capture [00:08:22]. Large Language Models (LLMs) like ChatGPT are prominent examples [00:08:15]. This approach has led to breakthroughs, such as rapid protein folding [00:08:40].
*   **Limitations and Biases**: LLMs aim to produce human-like answers [00:08:53]. However, they often replicate the same [[algorithmic_bias_and_its_implications | biases]] found in human System 1 thinking. Early versions of ChatGPT, for example, failed classic cognitive bias tests like the bat and ball problem, demonstrating how they learn and replicate associations from the data they are trained on [00:09:40]. They can also produce "highly plausible gibberish" or "confabulate" when answering nonsensical questions, akin to a student trying to bluff knowledge [00:10:48].
*   **Unreliability for Decisions**: LLMs are generally unreliable for [[impact_of_ai_on_human_decision_making | decision making]] [00:11:39] because:
    *   **Lack of Stability**: Their answers are often variable and random, a design choice to make them "feel more natural" and human-like [00:12:24].
    *   **Lack of Truth Comprehension**: LLMs do not inherently know what is true. Their "best proxy for the truth is the frequency of something coming up as the answer to a question or the frequency of something coming up in a particular context" [00:13:05]. This means they would have confidently asserted that the Earth was flat during Galileo's time if their data reflected that prevalent belief [00:13:16].
*   **Human Analogy**: Machine learning mirrors **System 1 thinking**: it's associative, fast, and adept at many tasks but prone to "stupid mistakes" and [[algorithmic_bias_and_its_implications | biases]] [00:09:29].

The presenter argues that if one cannot trust a large language model for decisions, perhaps one "shouldn't trust the noisy human beings either" [00:14:00], given their similar cognitive pitfalls.