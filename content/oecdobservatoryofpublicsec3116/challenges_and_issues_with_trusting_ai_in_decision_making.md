---
title: Challenges and Issues with Trusting AI in Decision Making
videoId: VJES_jPWf70
---

From: [[oecdobservatoryofpublicsec3116]] <br/> 

The discussion around [[impact_of_ai_on_human_decision_making | artificial intelligence]] often focuses on its role as a decision aid. While AI can significantly enhance decision-making processes, its adoption and trustworthiness face considerable challenges, particularly concerning human perception, control, and inherent biases <a class="yt-timestamp" data-t="01:49:00">[01:49:00]</a>.

## Two Types of AI and Human Cognition <a class="yt-timestamp" data-t="07:04:00">[07:04:00]</a>

There are broadly two types of AI with [[types_of_ai_and_their_similarity_to_human_thinking | eerie similarities to human thinking]]:
*   **Symbolic AI (Good Old-Fashioned AI - GOFAI)**: Operates by following explicit logical rules and symbolic representations <a class="yt-timestamp" data-t="07:20:00">[07:20:00]</a>. This system is akin to "System 2" thinking, which is slow, logical, and rule-based, offering reliable answers but struggling with complexity and nuances <a class="yt-timestamp" data-t="09:07:00">[09:07:00]</a>. GOFAI can make "stupid mistakes" when rules don't perfectly apply to complex real-world situations, such as a self-driving car mistaking a backpack stop sign for a traffic sign <a class="yt-timestamp" data-t="07:35:00">[07:35:00]</a>.
*   **Machine Learning (ML)**: Starts from data to discover implicit rules, exemplified by large language models (LLMs) like ChatGPT <a class="yt-timestamp" data-t="08:10:00">[08:10:00]</a>. This is comparable to "System 1" thinking, which is associative, fast, and adept at complex tasks but prone to "stupid mistakes" and biases <a class="yt-timestamp" data-t="09:25:00">[09:25:00]</a>.

## Issues with Trusting Large Language Models (LLMs) <a class="yt-timestamp" data-t="11:39:03">[11:39:03]</a>

LLMs are generally not suitable tools for making decisions due to several critical flaws:
*   **Replication of Biases**: LLMs tend to replicate the same biases found in human System 1 thinking <a class="yt-timestamp" data-t="10:29:40">[10:29:40]</a>. They learn from massive datasets where certain associations frequently occur, leading to biased outputs <a class="yt-timestamp" data-t="10:18:00">[10:18:00]</a>.
*   **Confabulation**: LLMs can produce "plausible gibberish" â€“ coherent but nonsensical answers <a class="yt-timestamp" data-t="11:05:08">[11:05:08]</a>. This mimics human behavior when trying to pretend knowledge <a class="yt-timestamp" data-t="11:28:00">[11:28:00]</a>.
*   **Lack of Stability (Noise)**: LLM answers can vary when asked the same question, a randomness intentionally built in to make them "feel more natural" or human <a class="yt-timestamp" data-t="12:35:00">[12:35:00]</a>. However, this inherent noise makes them unreliable for consistent decision-making <a class="yt-timestamp" data-t="12:24:00">[12:24:00]</a>.
*   **No Concept of Truth**: LLMs' "best proxy for the truth" is the frequency of information in their training data, not an understanding of objective reality <a class="yt-timestamp" data-t="13:00:02">[13:00:02]</a>. For example, an LLM at Galileo's time would have been "absolutely certain" the Earth was flat due to prevailing data <a class="yt-timestamp" data-t="13:16:00">[13:16:00]</a>.

Ultimately, if one cannot trust an LLM for decision-making, it implies a similar caution should be applied to "noisy human beings" for the same reasons <a class="yt-timestamp" data-t="13:56:00">[13:56:00]</a>.

## Trust and Acceptability of Decision Aids <a class="yt-timestamp" data-t="14:45:00">[14:45:00]</a>

*   **Algorithmic Superiority**: Meta-analyses dating back to the 1990s consistently show that simple formulas or algorithms often match or outperform human expert judgment in various fields like predicting college success, recidivism, or medical diagnoses <a class="yt-timestamp" data-t="15:21:00">[15:21:00]</a>. In personnel selection, formulas are known to be superior to job interviews <a class="yt-timestamp" data-t="17:05:08">[17:05:08]</a>.
*   **Deep Resistance**: Despite empirical evidence, there is a "deep resistance" to using any kind of decision aid, with people often preferring their own judgment <a class="yt-timestamp" data-t="19:18:02">[19:18:02]</a>.
*   **Reasons for Distrust**:
    *   **Overconfidence**: Experts often exhibit overconfidence in their own judgment, believing they are better than average <a class="yt-timestamp" data-t="19:59:00">[19:59:00]</a>. They underestimate their inaccuracy and lack awareness of their mistakes due to poor feedback loops <a class="yt-timestamp" data-t="20:27:00">[20:27:00]</a>, <a class="yt-timestamp" data-t="24:44:00">[24:44:00]</a>.
    *   **Desire for Certainty**: People prefer certainty and fail to realize that any judgment under uncertainty is inherently probabilistic <a class="yt-timestamp" data-t="22:52:00">[22:52:00]</a>.
    *   **Qualitative Data Fallacy**: The belief that qualitative data can only be interpreted by human judgment, and algorithms cannot perceive everything a human does <a class="yt-timestamp" data-t="20:14:00">[20:14:00]</a>.
    *   **Loss of Control**: A fundamental resistance stems from the perceived loss of agency and control when delegating decisions to a system <a class="yt-timestamp" data-t="23:42:00">[23:42:00]</a>, <a class="yt-timestamp" data-t="25:42:00">[25:42:00]</a>.

## The Dogma of Human Control <a class="yt-timestamp" data-t="25:58:00">[25:58:00]</a>

The widely accepted notion that "humans should always remain in control" of AI decisions is problematic. If AI is better than human judgment on average, its value lies precisely in cases where human judgment disagrees with the AI's recommendation <a class="yt-timestamp" data-t="27:08:00">[27:08:00]</a>. Overriding AI advice in such situations effectively negates its value, becoming an additional source of confirmation bias and false confidence <a class="yt-timestamp" data-t="27:38:00">[27:38:00]</a>.

*   **When to Trust AI**: Once an AI model has been quality-controlled and proven to make better decisions on average, it should be trusted most of the time <a class="yt-timestamp" data-t="30:07:00">[30:07:00]</a>. The "broken leg problem" is a rare exception where a human possesses decisive, unique information unavailable to the model <a class="yt-timestamp" data-t="29:24:00">[29:24:00]</a>.

## Algorithmic Bias and its Implications <a class="yt-timestamp" data-t="30:28:00">[30:28:00]</a>

The concept of [[algorithmic_bias_and_its_implications | algorithmic bias]] is complex and not intuitively defined <a class="yt-timestamp" data-t="33:18:00">[33:18:00]</a>.
*   **Conflicting Definitions**: Different definitions of "bias" can be mathematically mutually exclusive. For instance, an algorithm predicting recidivism might show different error rates for different racial groups (ProPublica's definition) or assign different risk scores to individuals with identical profiles but different races (another definition) <a class="yt-timestamp" data-t="32:28:00">[32:28:00]</a>. Both seem problematic, yet only one can be true at a time, or neither if both groups have the same base rates <a class="yt-timestamp" data-t="32:31:00">[32:31:00]</a>. This highlights the lack of a universally accepted definition of bias in this context <a class="yt-timestamp" data-t="33:38:00">[33:38:00]</a>.
*   **Mirroring Past Biases**: Algorithms often reflect past human biases present in their training data. For example, Amazon's recruiting tool became "sexist" because it learned from historical hiring patterns that favored men <a class="yt-timestamp" data-t="35:18:00">[35:18:00]</a>. Scrapping such an algorithm means refusing to look in a mirror that reflects one's own past flawed behavior <a class="yt-timestamp" data-t="36:09:00">[36:09:00]</a>.
*   **Forcing Desired Outcomes**: Attempting to "de-bias" an algorithm by forcing it to reflect desired societal proportions (e.g., more women or diverse groups) can lead to nonsensical results if not carefully applied, as seen with Google's image generation producing "black Nazis" or non-historically accurate Vikings <a class="yt-timestamp" data-t="34:01:00">[34:01:00]</a>.
*   **The Implicit Choice**: To overcome [[algorithmic_bias_and_its_implications | algorithmic bias]], one must explicitly define desired priorities, such as accepting a certain percentage of risk in bail decisions or aiming for specific gender proportions in hiring <a class="yt-timestamp" data-t="37:05:00">[37:05:00]</a>. Human decision-makers often prefer to avoid making such explicit, uncomfortable trade-off decisions, maintaining "ambiguous criteria" in their case-by-case judgments <a class="yt-timestamp" data-t="38:50:00">[38:50:00]</a>. This paradoxically means adopting AI requires greater clarity in defining objectives, not less <a class="yt-timestamp" data-t="39:10:00">[39:10:00]</a>.

## Overcoming Challenges <a class="yt-timestamp" data-t="23:48:00">[23:48:00]</a>

*   **Allow Customization**: Giving users even slight input into how an algorithm is designed can increase their willingness to use it <a class="yt-timestamp" data-t="24:07:00">[24:07:00]</a>.
*   **Provide Feedback and Align Incentives**: Implementing mechanisms to provide accurate feedback on the quality of human decisions and aligning incentives on accuracy can encourage improvement and adoption of AI <a class="yt-timestamp" data-t="24:34:00">[24:34:00]</a>.
*   **Redefine Responsibility**: The legal framework needs to acknowledge the probabilistic nature of decisions and evaluate AI performance in aggregate, rather than solely on a case-by-case basis, moving beyond the traditional human accountability model <a class="yt-timestamp" data-t="53:00:00">[53:00:00]</a>.

For individual users, the key questions are: "How good are you without the AI?", "How good can your decisions be with AI?", and crucially, "What will you tell the AI to look for beyond the ambiguity in which you've been conveniently staying?" <a class="yt-timestamp" data-t="39:35:00">[39:35:00]</a>. For regulators, the debate around [[the_role_of_ai_and_technology_in_public_administration | artificial intelligence in public administration]] must include evaluating the quality of human decisions, validating AI model quality independently, and defining what constitutes socially or legally unacceptable bias <a class="yt-timestamp" data-t="39:56:00">[39:56:00]</a>.