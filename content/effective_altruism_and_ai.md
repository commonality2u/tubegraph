---
title: Effective Altruism and AI
videoId: UckqpcOu5SY
---

From: [[dwarkesh | The Dwarkesh Podcast]]

Here is the modified article:

Holden Karnofsky is the co-CEO of Open Philanthropy <a class="yt-timestamp" data-t="00:00:37">[00:00:37]</a>. He previously co-founded GiveWell, an organization that helps donors decide where to give effectively, and he remains on its board <a class="yt-timestamp" data-t="00:00:59">[00:00:59]</a>-<a class="yt-timestamp" data-t="00:01:09">[00:01:09]</a>. His work with Cari Tuna and Dustin Moskovitz (co-founder of Facebook and Asana) led to the creation of Open Philanthropy, which aims to help them give away their fortune to help as many people as possible <a class="yt-timestamp" data-t="00:01:13">[00:01:13]</a>-<a class="yt-timestamp" data-t="00:01:30">[00:01:30]</a>. Karnofsky specializes in finding underappreciated, underrated, and tremendously important ideas where resources can have an outsized return on investment <a class="yt-timestamp" data-t="00:01:40">[00:01:40]</a>-<a class="yt-timestamp" data-t="00:02:00">[00:02:00]</a>. He encountered the idea of "The Most Important Century" through the Effective Altruist community <a class="yt-timestamp" data-t="00:02:04">[00:02:04]</a>-<a class="yt-timestamp" data-t="00:02:18">[00:02:18]</a>.

## The Most Important Century Thesis

The "Most Important Century" thesis, which Karnofsky attributes to others within the Effective Altruist community [[the_most_important_century_thesis]], posits that if humanity develops the right kind of Artificial Intelligence (AI) systems this century, it could be the most important century of all time <a class="yt-timestamp" data-t="00:02:26">[00:02:26]</a>-<a class="yt-timestamp" data-t="00:02:36">[00:02:36]</a>. He argues there's a greater than 50-50 chance this century will see AI systems capable of performing all key tasks humans do to advance science and technology <a class="yt-timestamp" data-t="00:05:17">[00:05:17]</a>-<a class="yt-timestamp" data-t="00:05:23">[00:05:23]</a>.

### Economic Growth Dynamics
Karnofsky explains the thesis partly through the lens of economic growth theory [[economic_growth_and_technological_acceleration]] <a class="yt-timestamp" data-t="00:02:41">[00:02:41]</a>:
*   **Historical Acceleration:** Economic history shows an accelerating rate of growth <a class="yt-timestamp" data-t="00:02:47">[00:02:47]</a>. This can be understood through a feedback loop: more people lead to more ideas, which lead to greater productivity and resources, which in turn support more people <a class="yt-timestamp" data-t="00:03:03">[00:03:03]</a>-<a class="yt-timestamp" data-t="00:03:18">[00:03:18]</a>. Standard economic theory suggests this loop leads to accelerating growth, and projecting this forward implies an infinite growth rate this century <a class="yt-timestamp" data-t="00:03:24">[00:03:24]</a>-<a class="yt-timestamp" data-t="00:03:38">[00:03:38]</a>.
*   **The Broken Feedback Loop:** About a couple of hundred years ago, the "more resources lead to more people" step of the loop broke. People started getting richer instead of more populous when resources increased <a class="yt-timestamp" data-t="00:03:50">[00:03:50]</a>-<a class="yt-timestamp" data-t="00:04:04">[00:04:04]</a>. This is discussed on his blog, Cold Takes <a class="yt-timestamp" data-t="00:04:09">[00:04:09]</a>.
*   **AI Restoring the Loop:** If AI systems could perform the tasks humans do to advance science and technology (the "more ideas" part), the accelerating feedback loop could be restored, leading to explosive growth in science and technology [[ai_for_science_and_societal_challenges]] <a class="yt-timestamp" data-t="00:04:19">[00:04:19]</a>-<a class="yt-timestamp" data-t="00:04:36">[00:04:36]</a>. Automating the creation of new technologies would be transformative <a class="yt-timestamp" data-t="00:04:53">[00:04:53]</a>-<a class="yt-timestamp" data-t="00:05:02">[00:05:02]</a>.

### Potential Outcomes
If such AI develops, the world could change incredibly quickly and dramatically, potentially packing thousands of years of change into a shorter period <a class="yt-timestamp" data-t="00:05:07">[00:05:07]</a>-<a class="yt-timestamp" data-t="00:05:37">[00:05:37]</a>.
*   This could lead to a "deeply unfamiliar future," possibly involving technologies like "digital people" (simulated, realistic individuals in virtual environments) <a class="yt-timestamp" data-t="00:05:43">[00:05:43]</a>-<a class="yt-timestamp" data-t="00:05:53">[00:05:53]</a>.
*   Humanity might hit the limits of science and technology, leading to a stable, post-human civilization expanding beyond Earth with significant control over the environment <a class="yt-timestamp" data-t="00:06:04">[00:06:04]</a>-<a class="yt-timestamp" data-t="00:06:22">[00:06:22]</a>.
*   This century could be our last chance to shape this transformative future <a class="yt-timestamp" data-t="00:06:27">[00:06:27]</a>-<a class="yt-timestamp" data-t="00:06:41">[00:06:41]</a>.

## Karnofsky's Evolving Perspective

Karnofsky's focus shifted from more traditional global development to these far-future concerns over time.

### Initial Focus and Shift
Initially, his work at GiveWell focused on measurable impact in areas like global health <a class="yt-timestamp" data-t="00:07:28">[00:07:28]</a>. In a 2014 conversation, he expressed confidence in his ability to do good in Africa but uncertainty about impacting the far future <a class="yt-timestamp" data-t="00:07:00">[00:07:00]</a>-<a class="yt-timestamp" data-t="00:07:26">[00:07:26]</a>.
His shift was driven by:
1.  **Continued Thinking:** He spent more time considering these "overwhelming" and "crazy" ideas, eventually becoming more familiar with potential AI risks and actions to mitigate them [[ai_alignment_and_potential_risks]] <a class="yt-timestamp" data-t="00:09:27">[00:09:27]</a>-<a class="yt-timestamp" data-t="00:10:11">[00:10:11]</a>.
2.  **Advances in AI:** The "deep learning revolution" since around 2014 demonstrated significant progress from computationally intensive yet simple AI systems across various tasks [[innovations_and_challenges_in_ai_hardware]] <a class="yt-timestamp" data-t="00:10:29">[00:10:29]</a>-<a class="yt-timestamp" data-t="00:10:50">[00:10:50]</a>. This made it less "wild" to imagine current AI development paths leading to extremely powerful AI, making risks more concrete <a class="yt-timestamp" data-t="00:10:59">[00:10:59]</a>-<a class="yt-timestamp" data-t="00:11:24">[00:11:24]</a>.

The motivation remains rooted in maximizing positive impact per dollar: shaping a potentially transformative AI transition to ensure a good future could help a vast number of people <a class="yt-timestamp" data-t="00:07:34">[00:07:34]</a>-<a class="yt-timestamp" data-t="00:08:37">[00:08:37]</a>.

### Addressing Skepticism: The "Weirdness" of Our Time
A significant part of Karnofsky's "Most Important Century" series argues that, even ignoring AI, we live in an extraordinarily strange time [[exploring_the_future_of_society_and_economy_with_ai]] <a class="yt-timestamp" data-t="00:14:15">[00:14:15]</a>-<a class="yt-timestamp" data-t="00:14:25">[00:14:25]</a>. This addresses the initial skepticism that claims of this century's unique importance sound "crazy and suspicious" <a class="yt-timestamp" data-t="00:14:39">[00:14:39]</a>-<a class="yt-timestamp" data-t="00:14:50">[00:14:50]</a>.
*   **Exceptional Growth:** The last few hundred years have seen unprecedented economic growth and a concentration of significant scientific/technological developments [[economic_growth_and_technological_development]] <a class="yt-timestamp" data-t="00:15:31">[00:15:31]</a>-<a class="yt-timestamp" data-t="00:15:59">[00:15:59]</a>. This is a tiny sliver of cosmic time <a class="yt-timestamp" data-t="00:16:04">[00:16:04]</a>-<a class="yt-timestamp" data-t="00:16:28">[00:16:28]</a>.
*   **Unsustainable Growth Rates:** Current economic growth rates seem too high to continue for another 10,000 years without exhausting resources like atoms in the galaxy (assuming no faster-than-light travel) <a class="yt-timestamp" data-t="00:16:33">[00:16:33]</a>-<a class="yt-timestamp" data-t="00:16:43">[00:16:43]</a>, <a class="yt-timestamp" data-t="00:30:13">[00:30:13]</a>-<a class="yt-timestamp" data-t="00:30:45">[00:30:45]</a>. Even if growth slowed significantly (e.g., to 0.5%), it would still imply this period is unique and dynamic <a class="yt-timestamp" data-t="00:30:49">[00:30:49]</a>-<a class="yt-timestamp" data-t="00:32:21">[00:32:21]</a>.
*   **Context for AI Claims:** The "weirdness" argument serves to make the idea of transformative AI this century a "moderate quantitative update" rather than a complete revolution in thinking <a class="yt-timestamp" data-t="00:18:13">[00:18:13]</a>-<a class="yt-timestamp" data-t="00:18:27">[00:18:27]</a>. It's a response to the objection that such a claim is too crazy to be true <a class="yt-timestamp" data-t="00:34:44">[00:34:44]</a>-<a class="yt-timestamp" data-t="00:34:59">[00:34:59]</a>.
*   **Implications Beyond AI:** Regardless of AI, living in such a strange time means global civilization should prioritize identifying and preparing for the "next big thing" that could radically transform the world <a class="yt-timestamp" data-t="00:19:17">[00:19:17]</a>-<a class="yt-timestamp" data-t="00:20:30">[00:20:30]</a>. Currently, the world invests too little in such speculative thinking <a class="yt-timestamp" data-t="00:20:42">[00:20:42]</a>-<a class="yt-timestamp" data-t="00:20:51">[00:20:51]</a>.

## Historical Analogies and Predicting the Future

### The Industrial Revolution
The Industrial Revolution is used as a thought-provoking analogy [[historical_influences_on_leadership_and_innovation]] <a class="yt-timestamp" data-t="00:21:56">[00:21:56]</a>. If one had foreseen it, what could have been done?
*   Karnofsky suggests that influencing the thinking and culture of the country where it happened (e.g., the UK) towards better human rights and individual liberties, as Enlightenment thinkers did, might have been beneficial <a class="yt-timestamp" data-t="00:22:28">[00:22:28]</a>-<a class="yt-timestamp" data-t="00:23:02">[00:23:02]</a>.
*   He draws a parallel between the esoteric questions of Enlightenment thinkers and contemporary research on AI alignment [[ai_alignment_and_safety]] (e.g., getting an AI to do what you want) <a class="yt-timestamp" data-t="00:23:56">[00:23:56]</a>-<a class="yt-timestamp" data-t="00:24:10">[00:24:10]</a>.
*   While not perfectly analogous, it suggests it's not "obvious that there's never anything you could have come up with" <a class="yt-timestamp" data-t="00:25:14">[00:25:14]</a>-<a class="yt-timestamp" data-t="00:25:18">[00:25:18]</a>.

### Track Record of Future Predictions
Karnofsky acknowledges the difficulty of predicting the future [[timeline_predictions_for_agi_development]] <a class="yt-timestamp" data-t="00:12:02">[00:12:02]</a>.
*   He initially (e.g., in 2014) felt that making smart statements about decades out was not something people had a good track record of <a class="yt-timestamp" data-t="00:12:26">[00:12:26]</a>-<a class="yt-timestamp" data-t="00:12:34">[00:12:34]</a>.
*   His current view, after more research (e.g., his blog post "The Track Record of Future"), is that the historical record is "fine" <a class="yt-timestamp" data-t="00:12:45">[00:12:45]</a>-<a class="yt-timestamp" data-t="00:12:50">[00:12:50]</a>. While no one can predict with precision, the track record isn't so bad as to preclude trying, especially with self-awareness about unreliability <a class="yt-timestamp" data-t="00:13:03">[00:13:03]</a>-<a class="yt-timestamp" data-t="00:13:22">[00:13:22]</a>.
*   He believes there's been progress in how to make reasonable predictions, and we might be better at it today than in the past <a class="yt-timestamp" data-t="01:11:20">[01:11:20]</a>-<a class="yt-timestamp" data-t="01:11:41">[01:11:41]</a>.

### Responding to "Everyone Thinks Their Time is Special"
Karnofsky addresses the concern that people often wrongly believe they live in the most important time <a class="yt-timestamp" data-t="00:25:52">[00:25:52]</a>.
*   He's unsure how common this claim actually is historically <a class="yt-timestamp" data-t="00:26:13">[00:26:13]</a>.
*   While acknowledging the potential for self-deception, he argues that ignoring high-stakes possibilities would be a bad rule, especially if one genuinely lives in an important time [[existential_risk_and_societal_collapse]] <a class="yt-timestamp" data-t="00:26:31">[00:26:31]</a>-<a class="yt-timestamp" data-t="00:27:15">[00:27:15]</a>.
*   The proposed approach is to take these beliefs seriously and act on them, but within common sense ethical boundaries, avoiding "ends justify the means" reasoning [[philosophical_perspectives_on_consciousness_and_free_will]] <a class="yt-timestamp" data-t="00:27:50">[00:27:50]</a>-<a class="yt-timestamp" data-t="00:28:24">[00:28:24]</a>. His current confidence level isn't enough to justify such means <a class="yt-timestamp" data-t="00:29:28">[00:29:28]</a>-<a class="yt-timestamp" data-t="00:29:31">[00:29:31]</a>.

## Transformative AI: Scenarios and Challenges

### Defining Success
Success with transformative AI is not about a precise predetermined outcome [[potential_ai_takeover_scenarios_and_implications]] <a class="yt-timestamp" data-t="00:36:01">[00:36:01]</a>-<a class="yt-timestamp" data-t="00:36:14">[00:36:14]</a>.
*   A key aspect of success would be AI systems behaving as intended, acting as tools and amplifiers for humans, without leading to a huge concentration of power <a class="yt-timestamp" data-t="00:36:55">[00:36:55]</a>-<a class="yt-timestamp" data-t="00:37:24">[00:37:24]</a>.
*   This could lead to a continuation of trends like increasing wealth, more tools, and greater self-understanding, ideally making the world better, similar to the progress seen over the last 200 years [[economic_impacts_of_ai_and_automation]] <a class="yt-timestamp" data-t="00:37:37">[00:37:37]</a>-<a class="yt-timestamp" data-t="00:38:06">[00:38:06]</a>.
*   The goal is to avoid identifiable massive disasters, allowing a potentially wiser future to make better decisions <a class="yt-timestamp" data-t="00:38:37">[00:38:37]</a>-<a class="yt-timestamp" data-t="00:38:41">[00:38:41]</a>.

### Role of Humans and AI Rights
*   A possible future involves humans using AI to gain intelligence and wisdom. After some time (e.g., 100 years), society might discuss whether AIs themselves should have rights, potentially leading to AIs voting or sharing the world <a class="yt-timestamp" data-t="00:39:06">[00:39:06]</a>-<a class="yt-timestamp" data-t="00:39:40">[00:39:40]</a>.
*   Challenges like AI voting systems (given AIs could be copied) would need to be addressed, hopefully by a society not derailed by catastrophes [[ai_alignment_challenges_and_ethical_considerations]] <a class="yt-timestamp" data-t="00:40:04">[00:40:04]</a>-<a class="yt-timestamp" data-t="00:40:46">[00:40:46]</a>.

### AI Alignment
A major concern is ensuring AI systems do what humans intend, rather than pursuing unintended goals [[ai_alignment_and_safety_research]].
*   **Orthogonality Thesis:** Coined by Eliezer Yudkowsky, this suggests an AI can be highly intelligent in pursuing any goal, even a "stupid" one <a class="yt-timestamp" data-t="00:45:08">[00:45:08]</a>-<a class="yt-timestamp" data-t="00:45:25">[00:45:25]</a>. Karnofsky is not comforted by the idea that smarter AI will necessarily have better goals <a class="yt-timestamp" data-t="00:45:08">[00:45:08]</a>.
*   Modern AI training via trial and error (encouraging/discouraging behaviors) could lead to an AI intelligently pursuing something humans didn't mean to encourage, without questioning the goal itself <a class="yt-timestamp" data-t="00:45:44">[00:45:44]</a>-<a class="yt-timestamp" data-t="00:46:21">[00:46:21]</a>.
*   **Moral Progress in AI:** Karnofsky uses "moral progress" to refer to changes in morality he deems good (e.g., increased acceptance of homosexuality) <a class="yt-timestamp" data-t="00:46:33">[00:46:33]</a>-<a class="yt-timestamp" data-t="00:47:14">[00:47:14]</a>. He doesn't believe this progress is inevitable or that AI would necessarily undergo a similar evolution or arrive at human-compatible values <a class="yt-timestamp" data-t="00:46:47">[00:46:47]</a>-<a class="yt-timestamp" data-t="00:47:53">[00:47:53]</a>. Human moral progress may stem from shared human experiences and learning, which wouldn't apply to a non-human AI <a class="yt-timestamp" data-t="00:48:00">[00:48:00]</a>-<a class="yt-timestamp" data-t="00:48:36">[00:48:36]</a>.

### Timescales and Preparedness
The perceived timeline for transformative AI development affects strategy:
*   **Short Timelines (e.g., 1-3 months):** Karnofsky finds this "extremely scary" and feels philanthropy's role (helping fields grow over long timescales) would be limited <a class="yt-timestamp" data-t="00:40:51">[00:40:51]</a>-<a class="yt-timestamp" data-t="00:42:34">[00:42:34]</a>. Actions might involve trying to create a test for AI safety/danger and advocating for a slowdown if danger is demonstrated <a class="yt-timestamp" data-t="00:43:53">[00:43:53]</a>-<a class="yt-timestamp" data-t="00:44:15">[00:44:15]</a>.
*   **Longer Timelines (e.g., 10-80 years):** This range is where supporting early careers and building expertise in AI alignment and policy becomes beneficial <a class="yt-timestamp" data-t="00:42:51">[00:42:51]</a>-<a class="yt-timestamp" data-t="00:44:42">[00:44:42]</a>. Open Philanthropy currently funds research on alignment and policy [[philanthropy_and_global_impact_strategies]] <a class="yt-timestamp" data-t="00:41:56">[00:41:56]</a>-<a class="yt-timestamp" data-t="00:42:11">[00:42:11]</a>.
*   **Very Long Timelines (e.g., 500 years):** He would be inclined to ignore specific AI risks and focus on making the world generally better, more robust, and wiser <a class="yt-timestamp" data-t="00:44:15">[00:44:15]</a>-<a class="yt-timestamp" data-t="00:44:25">[00:44:25]</a>.
*   **Recent AI Developments:** Progress in AI, such as GPT-3 (which his wife and brother-in-law worked on <a class="yt-timestamp" data-t="00:49:15">[00:49:15]</a>-<a class="yt-timestamp" data-t="00:49:21">[00:49:21]</a>) and Minerva showing surprising capabilities from simple training procedures, has made Karnofsky "a bit more freaked out" and increased Open Philanthropy's interest in AI risk [[challenges_and_methodologies_in_ai_research_and_development]] <a class="yt-timestamp" data-t="00:48:39">[00:48:39]</a>-<a class="yt-timestamp" data-t="00:50:13">[00:50:13]</a>.

## Addressing Criticisms and Weak Points of the Thesis

### Bottlenecks to Automation ("Full Automation")
The idea that AI might automate most but not all steps in innovation, with human or real-world steps becoming bottlenecks, is considered the "single best criticism" <a class="yt-timestamp" data-t="00:55:29">[00:55:29]</a>-<a class="yt-timestamp" data-t="00:56:03">[00:56:03]</a>.
*   However, Karnofsky argues that AI with human-like reasoning could find ways around these bottlenecks, e.g., by simulating experiments <a class="yt-timestamp" data-t="00:56:10">[00:56:10]</a>-<a class="yt-timestamp" data-t="00:57:35">[00:57:35]</a>.
*   Key R&D areas like energy and AI itself might be less prone to bottlenecks [[innovations_and_challenges_in_ai_hardware]] <a class="yt-timestamp" data-t="00:56:26">[00:56:26]</a>-<a class="yt-timestamp" data-t="00:57:06">[00:57:06]</a>.
*   Some tasks hardest to automate might be physical (robotics, like opening a water bottle <a class="yt-timestamp" data-t="00:58:50">[00:58:50]</a>-<a class="yt-timestamp" data-t="00:59:04">[00:59:04]</a>) or require human trust/interaction (caregiving, teaching, business deals) <a class="yt-timestamp" data-t="00:57:57">[00:57:57]</a>-<a class="yt-timestamp" data-t="00:58:32">[00:58:32]</a>. These may not bottleneck core R&D as much <a class="yt-timestamp" data-t="00:59:27">[00:59:27]</a>. AI might take a scientist's job before a teacher's due to regulation or human preference <a class="yt-timestamp" data-t="00:59:43">[00:59:43]</a>-<a class="yt-timestamp" data-t="00:59:57">[00:59:57]</a>.

### Lock-In
"Lock-in" refers to the possibility of reaching a very stable civilization, potentially with negative consequences <a class="yt-timestamp" data-t="01:00:18">[01:00:18]</a>-<a class="yt-timestamp" data-t="01:00:26">[01:00:26]</a>.
*   Historically, bad governance is often temporary due to factors like leaders aging, new technologies, and shifting power dynamics <a class="yt-timestamp" data-t="01:00:31">[01:00:31]</a>-<a class="yt-timestamp" data-t="01:01:12">[01:01:12]</a>.
*   Advanced technology could eliminate these sources of dynamism (e.g., ending aging, perfecting surveillance), leading to a completely stable world, which is a "very scary thought" <a class="yt-timestamp" data-t="01:01:32">[01:01:32]</a>-<a class="yt-timestamp" data-t="01:02:09">[01:02:09]</a>. The odds are hard to quantify but serious enough to consider (e.g., 1/4 to 1/2 likelihood) <a class="yt-timestamp" data-t="01:02:19">[01:02:19]</a>-<a class="yt-timestamp" data-t="01:02:43">[01:02:43]</a>.
*   Karnofsky generally views lock-in as bad, preferring to preserve optionality and avoid one person's values being set forever <a class="yt-timestamp" data-t="01:03:24">[01:03:24]</a>-<a class="yt-timestamp" data-t="01:03:38">[01:03:38]</a>. Some things, like preventing one person from having all power, might be worth locking in to prevent other forms of lock-in <a class="yt-timestamp" data-t="01:03:55">[01:03:55]</a>-<a class="yt-timestamp" data-t="01:04:20">[01:04:20]</a>.
*   AI alignment research aims to avoid a "bad thing": AI systems pursuing random, unintended goals more powerfully than humans, effectively locking in a future unrelated to human values <a class="yt-timestamp" data-t="01:04:37">[01:04:37]</a>-<a class="yt-timestamp" data-t="01:05:28">[01:05:28]</a>. Even with aligned AI, humans could still make bad decisions leading to lock-in <a class="yt-timestamp" data-t="01:05:32">[01:05:32]</a>-<a class="yt-timestamp" data-t="01:05:39">[01:05:39]</a>.

## Open Philanthropy's Approach

### Balancing Long-Termism with Current Needs
The more likely transformative AI seems, the more it should be a top priority over short-term problems <a class="yt-timestamp" data-t="00:50:51">[00:50:51]</a>-<a class="yt-timestamp" data-t="00:51:05">[00:51:05]</a>. However, Open Philanthropy is not extremist and works on both speculative future risks and direct, current work <a class="yt-timestamp" data-t="00:51:05">[00:51:05]</a>-<a class="yt-timestamp" data-t="00:51:16">[00:51:16]</a>.
*   This includes funding GiveWell's top charities (e.g., bed nets, deworming) and advocacy for foreign aid, better land use, and scientific research for diseases in poor countries <a class="yt-timestamp" data-t="00:51:16">[00:51:16]</a>-<a class="yt-timestamp" data-t="00:51:42">[00:51:42]</a>.
*   Karnofsky expects the positive effects of such direct work on individuals to be permanent in one sense (a life lived better is significant) but likely to "wash out" in terms of predictable, systematic effects on the far future post-transformative AI <a class="yt-timestamp" data-t="00:52:00">[00:52:00]</a>-<a class="yt-timestamp" data-t="00:52:28">[00:52:28]</a>.

### The Competition vs. Caution Frames
Karnofsky distinguishes between:
*   **Competition Frame:** Wanting one's own country, friends, or company to "win" the race to develop AI <a class="yt-timestamp" data-t="00:53:10">[00:53:10]</a>-<a class="yt-timestamp" data-t="00:53:28">[00:53:28]</a>.
*   **Caution Frame:** Emphasizing the need for collaborative care to avoid building AI that spins out of control [[ai_alignment_and_safety]] <a class="yt-timestamp" data-t="00:53:28">[00:53:28]</a>-<a class="yt-timestamp" data-t="00:53:40">[00:53:40]</a>.
He is interested in ways for multiple players to work together to avoid disaster, and is less excited about simply picking a favored group to race ahead <a class="yt-timestamp" data-t="00:53:46">[00:53:46]</a>-<a class="yt-timestamp" data-t="00:54:01">[00:54:01]</a>.

### OpenAI Investment
Open Philanthropy made a $30 million grant to OpenAI around 2016 [[open_source_ai_models_and_their_implications]] <a class="yt-timestamp" data-t="01:26:54">[01:26:54]</a>-<a class="yt-timestamp" data-t="01:28:35">[01:28:35]</a>.
*   Part of the grant secured a board seat for Open Philanthropy for a few years to help with governance <a class="yt-timestamp" data-t="01:28:41">[01:28:41]</a>-<a class="yt-timestamp" data-t="01:28:51">[01:28:51]</a>.
*   Some criticize OpenAI for advancing AI capabilities and hype, potentially reducing preparation time <a class="yt-timestamp" data-t="01:28:51">[01:28:51]</a>-<a class="yt-timestamp" data-t="01:29:06">[01:29:06]</a>. Karnofsky agrees faster advancement is bad all else equal, but argues OpenAI has also done good things, set important precedents, and is more interested in advanced AI risks than a hypothetical alternative company might be <a class="yt-timestamp" data-t="01:29:10">[01:29:10]</a>-<a class="yt-timestamp" data-t="01:29:29">[01:29:29]</a>.
*   He doesn't view the grant as one of their best or worst, acknowledging that philanthropy involves risks and unintended consequences <a class="yt-timestamp" data-t="01:29:50">[01:29:50]</a>-<a class="yt-timestamp" data-t="01:30:10">[01:30:10]</a>. The goal is to operate with integrity, do homework, and anticipate downsides <a class="yt-timestamp" data-t="01:27:16">[01:27:16]</a>-<a class="yt-timestamp" data-t="01:28:06">[01:28:06]</a>.

## Ethical Frameworks

### Future-Proof Ethics
Karnofsky wrote a blog post series on "future-proof ethics," exploring ethical views common among effective altruists <a class="yt-timestamp" data-t="01:30:26">[01:30:26]</a>-<a class="yt-timestamp" data-t="01:30:34">[01:30:34]</a>. The aim is an ethical system that would likely not be seen as monstrous by a wiser future self or society <a class="yt-timestamp" data-t="01:30:44">[01:30:44]</a>-<a class="yt-timestamp" data-t="01:31:19">[01:31:19]</a>. He outlined three principles common in EA, often leading to a flavor of utilitarianism expansive about whose rights count (future generations, animals) [[ai_alignment_and_safety]] <a class="yt-timestamp" data-t="01:31:23">[01:31:23]</a>-<a class="yt-timestamp" data-t="01:31:49">[01:31:49]</a>:
1.  **Systemization:** Morality based on simple, general principles applied consistently is preferred over case-by-case intuitions, which can be biased <a class="yt-timestamp" data-t="01:32:22">[01:32:22]</a>-<a class="yt-timestamp" data-t="01:32:35">[01:32:35]</a>.
2.  **Thin Utilitarianism:** The greatest good for the greatest number <a class="yt-timestamp" data-t="01:32:40">[01:32:40]</a>-<a class="yt-timestamp" data-t="01:32:45">[01:32:45]</a>.
3.  **Sentientism:** Someone matters if they are able to suffer or experience pleasure, regardless of location, time, species, etc. <a class="yt-timestamp" data-t="01:32:45">[01:32:45]</a>-<a class="yt-timestamp" data-t="01:32:56">[01:32:56]</a>.
Karnofsky notes he laid this view out partly to argue against it later and has reservations, particularly finding sentientism potentially the weakest part due to dilemmas it creates <a class="yt-timestamp" data-t="01:31:53">[01:31:53]</a>, <a class="yt-timestamp" data-t="01:32:56">[01:32:56]</a>-<a class="yt-timestamp" data-t="01:33:14">[01:33:14]</a>. He also acknowledges that conflicting moral intuitions might suggest that a single set of fundamental principles is a "doom project" <a class="yt-timestamp" data-t="01:33:46">[01:33:46]</a>-<a class="yt-timestamp" data-t="01:34:03">[01:34:03]</a>.

### Integrity vs. Utilitarianism
In an EA forum post, Karnofsky stated that EAs tend to have unusually high integrity, but this might be *despite* utilitarianism rather than *because* of it <a class="yt-timestamp" data-t="01:37:26">[01:37:26]</a>-<a class="yt-timestamp" data-t="01:37:43">[01:37:43]</a>. He suggests that a drive to align actions with beliefs could lead both to honesty/rule-following and to adopting systematic ethics like utilitarianism. However, whether utilitarianism itself mandates common-sense integrity (like avoiding lying) is unclear <a class="yt-timestamp" data-t="01:37:53">[01:37:53]</a>-<a class="yt-timestamp" data-t="01:38:30">[01:38:30]</a>.

### Moral Parliament
To handle moral uncertainty where different ethical frameworks suggest different actions, Karnofsky finds Nick Bostrom's "moral parliament" idea appealing <a class="yt-timestamp" data-t="01:38:57">[01:38:57]</a>-<a class="yt-timestamp" data-t="01:39:46">[01:39:46]</a>. This involves envisioning different moral perspectives as different people within oneself, trying to reach a deal that all can feel reasonably good about <a class="yt-timestamp" data-t="01:39:50">[01:39:50]</a>-<a class="yt-timestamp" data-t="01:40:02">[01:40:02]</a>. This moderating approach seeks actions good by one view and not too bad by others, relating to his aversion to "ends justify the means" reasoning <a class="yt-timestamp" data-t="01:40:06">[01:40:06]</a>-<a class="yt-timestamp" data-t="01:40:37">[01:40:37]</a>.

## Karnofsky's Personal Work and Philosophy

### Cold Takes Blog
Karnofsky writes the blog "Cold Takes" <a class="yt-timestamp" data-t="01:48:22">[01:48:22]</a>.
*   **Purpose:** To express the unconventional views underlying Open Philanthropy's high-stakes decisions, providing transparency to grant-seekers and the public <a class="yt-timestamp" data-t="01:49:12">[01:49:12]</a>-<a class="yt-timestamp" data-t="01:49:48">[01:49:48]</a>. This can help attract more aligned grantees and, crucially, solicit critiques to identify errors in his thinking <a class="yt-timestamp" data-t="01:49:53">[01:49:53]</a>-<a class="yt-timestamp" data-t="01:50:36">[01:50:36]</a>.
*   **Impact:** He has become more aware of the weakest parts of his arguments through public criticism <a class="yt-timestamp" data-t="01:51:31">[01:51:31]</a>-<a class="yt-timestamp" data-t="01:51:41">[01:51:41]</a>. While critiques haven't made him drop the "Most Important Century" view, they have deepened his understanding of its components <a class="yt-timestamp" data-t="01:51:52">[01:51:52]</a>-<a class="yt-timestamp" data-t="01:52:30">[01:52:30]</a>.
*   **Mascot:** The browser icon is a stuffed animal named Mora, a pink polar bear representing creativity and a love for attention, reflecting the blog's "crazy, out there" nature <a class="yt-timestamp" data-t="01:48:31">[01:48:31]</a>-<a class="yt-timestamp" data-t="01:49:00">[01:49:00]</a>.

### Career Trajectory and Advice
Karnofsky's career advice often emphasizes specializing and building aptitudes [[advice_for_young_professionals_and_career_paths]] <a class="yt-timestamp" data-t="01:46:53">[01:46:53]</a>. He sees his own specialization as doing the "first cut crappy analysis" of important, under-analyzed questions, especially regarding effective giving, and then building teams for deeper analysis <a class="yt-timestamp" data-t="01:47:16">[01:47:16]</a>-<a class="yt-timestamp" data-t="01:48:00">[01:48:00]</a>. He has shifted focus when he identified a very important area receiving too little attention, justifying the sacrifice of specialized knowledge <a class="yt-timestamp" data-t="01:48:05">[01:48:05]</a>-<a class="yt-timestamp" data-t="01:48:19">[01:48:19]</a>.

