---
title: Statistical methods in AI and their limitations
videoId: ltHYWEGsDxY
---

From: [[mk_thisisit]] <br/> 

Artificial intelligence (AI) has seen significant development, particularly through the adoption of statistical methods, but this approach comes with inherent [[limitations_of_artificial_intelligence | limitations]] and challenges, particularly regarding transparency and verifiability.

## The Evolution of AI: From Logic to Statistics

Early developments in [[artificial_intelligence_and_future_prospects | artificial intelligence]] aimed to automate theorem proving using mathematical methods <a class="yt-timestamp" data-t="03:46">[03:46]</a>. Professor Simon, a Nobel Prize winner, was a co-creator of these mathematical methods for intelligence <a class="yt-timestamp" data-t="03:42">[03:42]</a>. However, researchers found that purely mathematical methods were difficult to apply in practice and had practical [[limitations_of_current_ai_systems | limitations]] <a class="yt-timestamp" data-t="04:24">[04:24]</a>.

This led to a significant shift towards statistics, which became a convenient tool to systematically grasp the uncertain world <a class="yt-timestamp" data-t="04:32">[04:32]</a>. While many find statistics unnatural, it is an important tool for this purpose <a class="yt-timestamp" data-t="04:47">[04:47]</a>. This statistical turn allowed for many applications, which are still growing in scope <a class="yt-timestamp" data-t="05:10">[05:10]</a>.

## Current State and [[limitations_of_current_ai_systems | Limitations of Current AI Systems]]

Despite advancements, particularly with models like ChatGPT, current AI systems are fundamentally based on statistics <a class="yt-timestamp" data-t="05:28">[05:28]</a>. While some perceive a form of "own reasoning" or ability to infer things not directly stated, it is described as still being "only statistics" <a class="yt-timestamp" data-t="05:28">[05:28]</a>. Large language models frequently make mistakes, including "hallucinations," and can be fooled with minimal effort, indicating that the work is not yet finished <a class="yt-timestamp" data-t="06:42">[06:42]</a>. These imperfections in AI's behavior are compared to human imperfections, as people also make mistakes and talk nonsense <a class="yt-timestamp" data-t="07:01">[07:01]</a>.

### The "Black Box" Problem and Verifiability

A major [[limitations_of_current_ai_systems | limitation]] of current statistical AI is the "black box" problem <a class="yt-timestamp" data-t="18:13">[18:13]</a>. It is often unclear how these models arrive at their decisions <a class="yt-timestamp" data-t="18:10">[18:10]</a>. This lack of transparency leads to questions of legal liability if an AI system fails <a class="yt-timestamp" data-t="17:08">[17:08]</a>. For example, in medical contexts, a doctor might be asked to trust an AI recommending a drastic procedure without understanding why <a class="yt-timestamp" data-t="18:13">[18:13]</a>.

Even when models provide confidence levels (e.g., "95% convinced"), an intelligent user would question the remaining percentage, for which statistics do not provide good answers <a class="yt-timestamp" data-t="19:06">[19:06]</a>. As models become more complex with more parameters, this "black box" area, including hallucinations, will only expand <a class="yt-timestamp" data-t="19:15">[19:15]</a>. The scientific community is currently taking the risk that users unknowingly accept these [[limitations_of_current_ai_systems | limitations]] when using tools like ChatGPT <a class="yt-timestamp" data-t="19:23">[19:23]</a>.

### Data Dependence and Synthetic Data

The effectiveness of these methods is directly tied to the data on which they are trained <a class="yt-timestamp" data-t="12:43">[12:43]</a>. Currently, these data are limited <a class="yt-timestamp" data-t="12:48">[12:48]</a>. An alternative scenario involves AI generating its own data for training and self-checking <a class="yt-timestamp" data-t="12:53">[12:53]</a>. This concept, known as synthetic data generation, has been practiced for years, especially in mathematical simulations, to avoid expensive and dangerous real-world experiments <a class="yt-timestamp" data-t="13:17">[13:17]</a>. The Monte Carlo method, invented by Polish mathematician Stanisław Ulam, is a basis for this <a class="yt-timestamp" data-t="13:48">[13:48]</a>.

However, a "strange" aspect arises when algorithms create data, which then feeds other algorithms that make decisions impacting human lives <a class="yt-timestamp" data-t="13:35">[13:35]</a>.

## Addressing [[development_and_challenges_of_artificial_intelligence | Challenges]] and Future Directions

To overcome these [[limitations_of_current_ai_systems | limitations]], there's a need to develop a "protective layer" around AI technology, focusing on verifying the truthfulness of decisions, rather than just regulation <a class="yt-timestamp" data-t="19:33">[19:33]</a>. One proposed solution is to return to the roots of AI: mathematical logic <a class="yt-timestamp" data-t="20:27">[20:27]</a>. This approach would use mathematical logic to analyze complex models and mathematically prove their correctness in certain contexts <a class="yt-timestamp" data-t="20:31">[20:31]</a>. This is seen as essential for AI to be truly recognized as a helpful and verifiable technology <a class="yt-timestamp" data-t="21:03">[21:03]</a>.

### Distributed Artificial Intelligence

A new paradigm, **distributed artificial intelligence**, is being developed to address practical inefficiencies of current AI, especially in large organizations with scattered data resources <a class="yt-timestamp" data-t="23:17">[23:17]</a>. The traditional method of collecting massive datasets into one place, harmonizing them, building a model, and then exporting it for use, is proving impractical and too slow, especially in rapidly changing situations <a class="yt-timestamp" data-t="24:15">[24:15]</a>.

Instead, distributed AI, or federated learning, teaches models on data where it resides, and then shares the smaller models or insights, rather than large amounts of raw data <a class="yt-timestamp" data-t="25:31">[25:31]</a>. This approach bypasses issues of data sharing restrictions (e.g., patient privacy in hospitals) and the need for database compatibility <a class="yt-timestamp" data-t="23:59">[23:59]</a>. This allows models to learn locally and remain compatible with each other, making practical sense <a class="yt-timestamp" data-t="26:45">[26:45]</a>. This project is currently in the proof-of-concept phase, with practical solutions expected in the near future <a class="yt-timestamp" data-t="26:55">[26:55]</a>.

### The "Automatic Scientist"

A competition was recently announced by major U.S. funding sources for basic research projects to build an "automatic scientist" <a class="yt-timestamp" data-t="07:36">[07:36]</a>. This envisioned AI would be able to prove mathematical theorems <a class="yt-timestamp" data-t="07:58">[07:58]</a>, discover new chemical compounds, and uncover fundamental laws from large datasets and experiments <a class="yt-timestamp" data-t="08:10">[08:10]</a>. This raises a fundamental question about the future role of human scientists, as an AI capable of discovering scientific laws would question the very existence and purpose of human researchers and funding agencies <a class="yt-timestamp" data-t="08:23">[08:23]</a>.

## [[comparison_of_ai_and_human_intelligence | AI and Human Intelligence]]: A Collaborative Future

While the idea of AI replacing humans is viewed skeptically for the near future, the industrial revolution example (horse replaced by steam engine) highlights that new tools can emerge <a class="yt-timestamp" data-t="09:56">[09:56]</a>. AI is seen as a tool, not a competitor <a class="yt-timestamp" data-t="09:59">[09:59]</a>. Delegating important decisions, such as medical decisions in patient care, entirely to machines is considered dangerous <a class="yt-timestamp" data-t="10:11">[10:11]</a>. Humans should ideally remain in control from an ethical and operational standpoint <a class="yt-timestamp" data-t="10:23">[10:23]</a>.

However, research indicates AI can exhibit empathy; a study from the University of San Diego found that AI-generated medical advice was more empathetic than advice from human doctors <a class="yt-timestamp" data-t="10:38">[10:38]</a>. Humans can learn from these algorithms, and AI can serve as an "intelligent assistant" to monitor decisions and suggest better alternatives <a class="yt-timestamp" data-t="11:07">[11:07]</a>.

The development of AI to analyze various data types—text, visual, numerical, time-series data—is a next step, allowing a single system to absorb multiple data modalities simultaneously <a class="yt-timestamp" data-t="15:01">[15:01]</a>. However, whether AI can develop intuition similar to humans remains debated <a class="yt-timestamp" data-t="15:51">[15:51]</a>. The current understanding is skeptical that the "intuition" perceived in AI tools is genuinely implemented by them <a class="yt-timestamp" data-t="16:11">[16:11]</a>. The question of whether AI will ever be able to generate or feel pain is a separate, complex issue related to self-regulation mechanisms <a class="yt-timestamp" data-t="22:13">[22:13]</a>.