---
title: Role of social media algorithms in political polarization
videoId: od867_xRuQs
---

From: [[mk_thisisit]] <br/> 

Max Fisher, a journalist for The New York Times, was nominated for the Cersa Pool Award in 2019 for an investigative report demonstrating how [[effects_of_social_media_on_democracy | social media]] algorithms influence global chaos <a class="yt-timestamp" data-t="01:06:00">[01:06:00]</a>. His book, "In the Modes of Chaos," explores these conclusions <a class="yt-timestamp" data-t="01:13:00">[01:13:00]</a>. Fisher initially expressed skepticism that [[effects_of_social_media_on_democracy | social media]] could lead to death, but after years of study, tracking, and witnessing its impact on reality, he now believes it can indirectly cause fatalities <a class="yt-timestamp" data-t="02:01:00">[02:01:00]</a>.

## Impact on Society and Human Behavior
While the effect of [[effects_of_social_media_on_democracy | social media]] on an individual is relatively small <a class="yt-timestamp" data-t="02:22:00">[02:22:00]</a>, its operation on the scale of billions of people daily and its tendency to push politics in more dangerously divisive or hateful directions have significant consequences <a class="yt-timestamp" data-t="02:31:00">[02:31:00]</a>.

### Fueling Violence and Hatred
Fisher's investigations show that [[effects_of_social_media_on_democracy | social media]] has fueled racial hatred and political polarization <a class="yt-timestamp" data-t="07:20:00">[07:20:00]</a>. Examples include:
*   The genocide in Myanmar <a class="yt-timestamp" data-t="02:42:00">[02:42:00]</a>, <a class="yt-timestamp" data-t="07:07:00">[07:07:00]</a>
*   Violence in Sri Lanka <a class="yt-timestamp" data-t="02:44:00">[02:44:00]</a>, <a class="yt-timestamp" data-t="10:47:00">[10:47:00]</a>
*   Communal violence in India <a class="yt-timestamp" data-t="02:47:00">[02:47:00]</a>
*   Other places where a significant number of people died due to racist or other violence triggered on these platforms <a class="yt-timestamp" data-t="02:51:00">[02:51:00]</a>.

These events, he argues, would not have occurred without the existence of these platforms <a class="yt-timestamp" data-t="02:59:00">[02:59:00]</a>. Some platforms have even admitted to these extreme cases <a class="yt-timestamp" data-t="03:03:00">[03:03:00]</a>.

### Algorithms and Engagement
The fundamental way [[effects_of_social_media_on_democracy | social media platforms]] work involves algorithms that select what users see, how they see it, and in what order <a class="yt-timestamp" data-t="05:08:00">[05:08:00]</a>. These systems are specifically designed to maximize user time spent on the platforms, which in turn generates more revenue <a class="yt-timestamp" data-t="05:16:00">[05:16:00]</a>. Due to human nature, the most stimulating content that encourages prolonged engagement often includes:
*   Fear of others <a class="yt-timestamp" data-t="05:39:00">[05:39:00]</a>
*   A sense of "us versus them" <a class="yt-timestamp" data-t="05:42:00">[05:42:00]</a>
*   Conspiracy theories <a class="yt-timestamp" data-t="05:44:00">[05:44:00]</a>
*   Hate <a class="yt-timestamp" data-t="05:46:00">[05:46:00]</a>
*   A sense of social outrage <a class="yt-timestamp" data-t="05:46:00">[05:46:00]</a>

This mechanism was evident from the early days of Facebook's News Feed in 2006. While most users liked the convenience, dissent groups formed. When users joined these anti-News Feed groups, the information appeared in their friends' feeds. Since social outrage is a powerful attention-grabber, it created an "illusion of the majority," making it seem like more people were angry than actually were, leading to an avalanche of new joins and increased traffic <a class="yt-timestamp" data-t="11:57:00">[11:57:00]</a>. This early phenomenon demonstrated how even primitive algorithms could amplify outrage and disinformation, a trick that became much more powerful with advanced algorithms in communities already facing divisions <a class="yt-timestamp" data-t="15:08:00">[15:08:00]</a>.

### Responsibility of Social Media Companies
While founders like Mark Zuckerberg likely intended to create something helpful, the people managing these [[responsibility_of_social_media_companies | companies]] had evidence from their own researchers, repeatedly warning that their platforms trigger behaviors leading to violence and death <a class="yt-timestamp" data-t="03:40:00">[03:40:00]</a>. Facebook researchers, for instance, warned about the situation in Myanmar <a class="yt-timestamp" data-t="03:58:00">[03:58:00]</a>. The fact that Mark Zuckerberg was aware of these warnings, even in specific cases predicting thousands of deaths, yet did not take action, makes the [[responsibility_of_social_media_companies | companies responsible]] for the deaths they failed to prevent <a class="yt-timestamp" data-t="06:01:00">[06:01:00]</a>.

## Influence on Political Elections
[[impact_of_bots_and_disinformation_on_the_internet | Social media algorithms]] have had a significant impact on presidential elections in the United States, notably in 2016 and 2020 <a class="yt-timestamp" data-t="17:19:00">[17:19:00]</a>. Although it's difficult to quantify the exact number of votes influenced, the effects are dispersed, making users more polarized or susceptible to conspiracy theories <a class="yt-timestamp" data-t="17:33:00">[17:33:00]</a>.

During the 2016 US election, platforms like Facebook and Twitter artificially amplified far-right news sources such as Breitbart News, which focused on conspiracy theories and supported Donald Trump <a class="yt-timestamp" data-t="18:01:00">[18:01:00]</a>. When these platforms altered their algorithms, Breitbart News' audience "completely disappeared," demonstrating how algorithms artificially amplified extremist content <a class="yt-timestamp" data-t="18:24:00">[18:24:00]</a>.

## Regulation of Social Media
There is a growing consensus that current regulations, often focused on content moderation, are insufficient <a class="yt-timestamp" data-t="21:33:00">[21:33:00]</a>. The problem lies in how platforms are designed to artificially promote content that fosters division and hatred <a class="yt-timestamp" data-t="21:58:00">[21:58:00]</a>. The only effective way to regulate, many believe, is to introduce regulations that address the design of the systems themselves, specifically targeting algorithms and how content is promoted <a class="yt-timestamp" data-t="22:11:00">[22:11:00]</a>. In Europe, efforts like GDPR and ongoing work on [[potential_regulations_for_social_media_platforms | AI regulations]] are underway, while the United States currently lacks [[potential_regulations_for_social_media_platforms | social media regulation]] <a class="yt-timestamp" data-t="21:01:00">[21:01:00]</a>.

## Addiction and Human Connection
[[artificial_intelligence_and_societal_impact | Social media platforms]] are designed to be addictive, utilizing the best minds in [[role_of_artificial_intelligence_in_society | artificial intelligence]] and computer programming to maximize user engagement <a class="yt-timestamp" data-t="09:35:00">[09:35:00]</a>. Despite widespread criticism, users often find themselves still using these platforms due to their effectiveness, making it challenging to navigate the modern world without them <a class="yt-timestamp" data-t="09:00:00">[09:00:00]</a>.

## [[effects_of_social_media_on_democracy | Crisis of Democracy]]
Fisher notes that the [[effects_of_social_media_on_democracy | crisis of democracy]] began in the 1990s, before [[effects_of_social_media_on_democracy | social media]] became prevalent <a class="yt-timestamp" data-t="29:57:00">[29:57:00]</a>. However, [[effects_of_social_media_on_democracy | social media]] undeniably accelerates polarization and exacerbates the crisis <a class="yt-timestamp" data-t="30:08:00">[30:08:00]</a>. It profoundly shapes public perception, influencing what news people care about and how it impacts their sense of identity and politics <a class="yt-timestamp" data-t="30:26:00">[30:26:00]</a>.