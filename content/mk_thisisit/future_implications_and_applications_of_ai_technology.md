---
title: Future implications and applications of AI technology
videoId: w-TtKPRuXL8
---

From: [[mk_thisisit]] <br/> 

Despite significant advancements, current [[Artificial Intelligence and its future implications | AI]] systems are described as "very stupid in many ways" because they lack understanding of the physical world, permanent memory, reasoning, and planning capabilities that humans possess <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a>. These limitations highlight the path for the [[Future of artificial intelligence | future of artificial intelligence]] development <a class="yt-timestamp" data-t="00:05:50">[00:05:50]</a>.

## Current State and Limitations of AI

Currently, [[Artificial Intelligence and its future implications | AI]] systems are proficient at manipulating language, leading to an overestimation of their intelligence <a class="yt-timestamp" data-t="00:00:09">[00:00:09]</a>. However, they do not understand the physical world, lack permanent memory, and cannot truly reason or plan <a class="yt-timestamp" data-t="00:00:13">[00:00:13]</a> <a class="yt-timestamp" data-t="00:05:42">[00:05:42]</a>. These are considered key features of intelligent behavior <a class="yt-timestamp" data-t="00:05:54">[00:05:54]</a>.

A major challenge for [[Artificial Intelligence and its future implications | AI]] is the [[Moravek paradox]] <a class="yt-timestamp" data-t="00:14:50">[00:14:50]</a>. Computers can excel at complex tasks like chess or mathematical puzzles, but struggle with physical tasks such as manipulating objects or jumping, which animals perform with ease <a class="yt-timestamp" data-t="00:15:05">[00:15:05]</a>. This is because the real world is far more complex than discrete symbol spaces, making it difficult for current [[Artificial Intelligence and its future implications | AI]] techniques to operate <a class="yt-timestamp" data-t="00:15:29">[00:15:29]</a>.

## Evolution of Deep Learning

The popularity of deep learning has occurred in two major waves <a class="yt-timestamp" data-t="00:03:16">[00:03:16]</a>:
*   **Late 1980s to Mid-1990s**: The first wave saw good results with multi-layer neural networks for tasks like image and handwritten character recognition <a class="yt-timestamp" data-t="00:03:21">[00:03:21]</a>. Enthusiasm waned by the mid-90s due to the huge amount of high-quality data and expensive computing resources required, which were not readily available before the widespread internet <a class="yt-timestamp" data-t="00:04:08">[00:04:08]</a>.
*   **2000s onwards**: Interest gradually grew, leading to an "explosion" around 2013, as the research world realized deep learning's effectiveness across many fields <a class="yt-timestamp" data-t="00:04:42">[00:04:42]</a>. Since then, it has developed at a dizzying pace <a class="yt-timestamp" data-t="00:05:02">[00:05:02]</a>.

A foundational paper on deep learning from 2015, co-authored with Nobel Prize winner [[Jeff Hinton]], served as a "manifesto" for the scientific community, presenting applications and future development tips, contributing to its popularization <a class="yt-timestamp" data-t="00:01:13">[00:01:13]</a> <a class="yt-timestamp" data-t="00:02:22">[00:02:22]</a>.

### Machine Learning Paradigms
Three main paradigms exist in machine learning:
1.  **Supervised Learning**: The most classic approach, where the system is given correct answers during training, like identifying a "table" in an image. It learns to generalize from many examples <a class="yt-timestamp" data-t="00:08:51">[00:08:51]</a>.
2.  **Reinforcement Learning**: The system receives feedback (good or bad) on its results, similar to how humans learn to ride a bike <a class="yt-timestamp" data-t="00:09:51">[00:09:51]</a>. While effective for games like chess or Go, it is "extremely ineffective" in the real world due to the vast number of trials required, making it impractical for tasks like training autonomous cars <a class="yt-timestamp" data-t="00:10:17">[00:10:17]</a>.
3.  **Self-Supervised Learning (Samad Learning)**: This paradigm, responsible for recent advances in natural language understanding and chatbots, trains the system to capture the structure of input data (e.g., text) by predicting missing elements <a class="yt-timestamp" data-t="00:10:55">[00:10:55]</a>. Large language models (LLMs) are trained on this principle, predicting the next word in a sequence <a class="yt-timestamp" data-t="00:11:38">[00:11:38]</a>.

While self-supervised learning works incredibly well for language, it struggles with the physical world <a class="yt-timestamp" data-t="00:12:27">[00:12:27]</a>. The physical world is continuous and involves too many unpredictable details, unlike discrete language symbols <a class="yt-timestamp" data-t="00:13:35">[00:13:35]</a> <a class="yt-timestamp" data-t="00:13:51">[00:13:51]</a> <a class="yt-timestamp" data-t="00:47:47">[00:47:47]</a>.## [[Artificial Intelligence and its future implications | Future Implications and Applications of AI Technology]]

Currently, [[Artificial Intelligence and its future implications | AI]] systems are described as "very stupid in many ways" <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a>. Despite their ability to manipulate language very well, they lack understanding of the physical world, do not possess permanent memory like humans, and struggle with reasoning and planning <a class="yt-timestamp" data-t="00:00:09">[00:00:09]</a> <a class="yt-timestamp" data-t="00:05:42">[00:05:42]</a> <a class="yt-timestamp" data-t="00:05:50">[00:05:50]</a>. These limitations highlight critical areas for the [[Future of artificial intelligence | future of artificial intelligence]] development.

### Current State and Limitations of AI
While [[Artificial Intelligence and its future implications | AI]] excels at language manipulation, this capability often leads to an overestimation of its general intelligence <a class="yt-timestamp" data-t="00:00:09">[00:00:09]</a> <a class="yt-timestamp" data-t="00:05:37">[00:05:37]</a>. Fundamental gaps remain in [[Artificial Intelligence and its future implications | AI]]'s ability to comprehend the physical world, retain long-term memory, and perform complex reasoning or planning <a class="yt-timestamp" data-t="00:05:45">[00:05:45]</a> <a class="yt-timestamp" data-t="00:05:50">[00:05:50]</a>. These aspects are considered essential for truly intelligent behavior <a class="yt-timestamp" data-t="00:05:54">[00:05:54]</a>.

A key challenge for [[Artificial Intelligence and its future implications | AI]] is [[Moravek paradox]], which observes that computers struggle with physical tasks that are easy for humans or animals, such as manipulating objects, despite excelling at complex logical or mathematical problems <a class="yt-timestamp" data-t="00:14:50">[00:14:50]</a> <a class="yt-timestamp" data-t="00:15:05">[00:15:05]</a>. This difficulty arises because the real world's complexity, with its continuous and unpredictable nature, differs significantly from the discrete, symbolic spaces where [[Artificial Intelligence and its future implications | AI]] typically operates <a class="yt-timestamp" data-t="00:15:29">[00:15:29]</a>.

### Evolution of Deep Learning
The field of deep learning has experienced two major waves of enthusiasm and development:
*   **Late 1980s to Mid-1990s**: The first wave saw promising results using multi-layer neural networks for tasks like image and handwritten character recognition <a class="yt-timestamp" data-t="00:03:21">[00:03:21]</a>. However, this initial excitement subsided due to the immense requirements for large, high-quality datasets and expensive computing power, which were not readily available at the time <a class="yt-timestamp" data-t="00:04:08">[00:04:08]</a>.
*   **2000s Onwards**: Interest in deep learning gradually resurfaced, culminating in a "real explosion" around 2013 <a class="yt-timestamp" data-t="00:04:42">[00:04:42]</a>. The research community recognized the widespread applicability and effectiveness of deep learning, leading to rapid development ever since <a class="yt-timestamp" data-t="00:05:02">[00:05:02]</a>.

A significant publication in 2015, co-authored by [[Jeff Hinton]], served as a "manifesto" for the scientific community, outlining deep learning's applications and future directions, contributing to its popularization <a class="yt-timestamp" data-t="00:01:13">[00:01:13]</a> <a class="yt-timestamp" data-t="00:02:22">[00:02:22]</a>.

#### Machine Learning Paradigms
Three primary paradigms define machine learning:
1.  **Supervised Learning**: This classic method involves training a system by providing correct answers, such as labeling images <a class="yt-timestamp" data-t="00:08:51">[00:08:51]</a>. The system adjusts its parameters to align its output with the expected result, developing the ability to generalize to unseen but similar data <a class="yt-timestamp" data-t="00:09:08">[00:09:08]</a>.
2.  **Reinforcement Learning**: Here, the system learns through feedback on whether its actions were good or bad, akin to how humans learn to ride a bicycle <a class="yt-timestamp" data-t="00:09:51">[00:09:51]</a>. While effective for games (e.g., chess, Go) where systems can play millions of games against themselves, this method is "extremely ineffective" in complex real-world scenarios, making it impractical for tasks like training autonomous cars without thousands of crashes <a class="yt-timestamp" data-t="00:10:17">[00:10:17]</a>.
3.  **Self-Supervised Learning (Samad Learning)**: This paradigm has driven recent breakthroughs in natural language understanding and chatbots <a class="yt-timestamp" data-t="00:10:55">[00:10:55]</a>. Systems are trained to capture the inherent structure of input data (e.g., text) by predicting missing words. Large language models (LLMs) operate on this principle, predicting subsequent words in text sequences <a class="yt-timestamp" data-t="00:11:38">[00:11:38]</a>.

Although self-supervised learning is highly successful for textual data, it faces significant challenges when applied to the physical world <a class="yt-timestamp" data-t="00:12:27">[00:12:27]</a>. The physical world is inherently continuous and unpredictable, unlike the discrete, symbolic nature of language, making accurate prediction of future events difficult or impossible <a class="yt-timestamp" data-t="00:13:35">[00:13:35]</a> <a class="yt-timestamp" data-t="00:47:47">[00:47:47]</a>.

### [[Artificial intelligence progress and future | Future Prospects]] and Challenges
The next significant step for [[Artificial Intelligence and its future implications | AI]] involves designing systems capable of functioning in the physical world, possessing permanent memory, and performing reasoning and planning <a class="yt-timestamp" data-t="00:06:09">[00:06:09]</a> <a class="yt-timestamp" data-t="00:06:16">[00:06:16]</a>. Such systems are expected to exhibit emotions like "excitement or joy," related to predicting successful goal achievement <a class="yt-timestamp" data-t="00:06:25">[00:06:25]</a>. However, they will not be built with inherent negative emotions such as anger or jealousy <a class="yt-timestamp" data-t="00:07:11">[00:07:11]</a>.

A critical challenge is developing [[Artificial Intelligence and its future implications | AI]] that can understand complex sensory data, particularly sight, which is essential for machines to learn as effectively as humans and animals <a class="yt-timestamp" data-t="00:16:40">[00:16:40]</a> <a class="yt-timestamp" data-t="00:16:54">[00:16:54]</a>. The amount of visual information a child processes in its first four years of life is comparable to the entire textual data used to train the largest language models (approximately 10^14 bytes), indicating that text-only training is insufficient for human-level [[Artificial Intelligence and its future implications | AI]] <a class="yt-timestamp" data-t="00:18:11">[00:18:11]</a> <a class="yt-timestamp" data-t="00:18:38">[00:18:38]</a>.

#### Reasoning and Planning
For [[Artificial Intelligence and its future implications | AI]] systems to reason and plan, they need to develop abstract representations from observations <a class="yt-timestamp" data-t="00:24:43">[00:24:43]</a>. Current large language models often employ a "primitive" search strategy in "token space," generating numerous hypothetical sequences and then selecting the best one <a class="yt-timestamp" data-t="00:26:41">[00:26:41]</a>. This method is computationally expensive and differs from human cognitive processes, where reasoning occurs in an internal mental state rather than by generating and analyzing many external actions <a class="yt-timestamp" data-t="00:27:14">[00:27:14]</a> <a class="yt-timestamp" data-t="00:28:11">[00:28:11]</a>.

A key future development involves implementing **hierarchical planning** in [[Artificial Intelligence and its future implications | AI]], allowing systems to define intermediate goals to achieve a larger objective, similar to how humans break down complex tasks <a class="yt-timestamp" data-t="00:29:25">[00:29:25]</a> <a class="yt-timestamp" data-t="00:30:36">[00:30:36]</a>. This is a significant challenge for the coming years <a class="yt-timestamp" data-t="00:30:44">[00:30:44]</a>.

#### [[Ethical Implications of AI | Consciousness]]
The concept of [[Ethical Implications of AI | consciousness]] remains undefined in science, with no measurable indicator to determine its presence <a class="yt-timestamp" data-t="00:07:27">[00:07:27]</a>. The speaker suggests that the difficulty in defining [[Ethical Implications of AI | consciousness]] might stem from asking the wrong questions, likening it to historical confusion over how the brain perceives an inverted retinal image correctly <a class="yt-timestamp" data-t="00:22:29">[00:22:29]</a>. While the world is "obsessed" with [[Ethical Implications of AI | consciousness]], it may be a uniquely human characteristic that makes individuals different, rather than a quantifiable attribute <a class="yt-timestamp" data-t="00:22:18">[00:22:18]</a> <a class="yt-timestamp" data-t="00:23:31">[00:23:31]</a>.

#### Information and [[Ethical Implications of AI | Entropy]]
The quantification of information is a fundamental challenge across computer science, physics, and information theory <a class="yt-timestamp" data-t="00:19:24">[00:19:24]</a>. The amount of information extracted from a message or sensory data is not absolute but depends on the interpreter <a class="yt-timestamp" data-t="00:19:43">[00:19:43]</a>. This relativity implies that many concepts in physics, like [[Ethical Implications of AI | entropy]] (a measure of ignorance about a physical system's state), lack truly objective definitions <a class="yt-timestamp" data-t="00:20:31">[00:20:31]</a> <a class="yt-timestamp" data-t="00:20:38">[00:20:38]</a>.

### Applications of [[Artificial Intelligence and its future implications | AI Technology]]
The [[Future of artificial intelligence | future of AI]] promises widespread applications, particularly in [[Role of new technologies and AI in society | robotics]] and smart devices.

#### [[Role of new technologies and AI in society | Robotics]]
While industrial robots are common for repetitive tasks in controlled environments (e.g., painting cars), autonomous systems like self-driving cars still fall short of human reliability <a class="yt-timestamp" data-t="00:31:12">[00:31:12]</a> <a class="yt-timestamp" data-t="00:31:59">[00:31:59]</a>. [[Elon Musk]]'s repeated predictions of achieving level five autonomy for Tesla in five years have not materialized, indicating the complexity of real-world physical tasks <a class="yt-timestamp" data-t="00:32:19">[00:32:19]</a> <a class="yt-timestamp" data-t="00:32:25">[00:32:25]</a>. The "coming decade" is anticipated to be the "decade of [[Role of new technologies and AI in society | robotics]]" <a class="yt-timestamp" data-t="00:34:42">[00:34:42]</a>. Many humanoid robot companies are betting on rapid [[Artificial intelligence progress and future | AI advances]] in the next 3-5 years to make their physically capable robots intelligent enough to navigate the real world <a class="yt-timestamp" data-t="00:34:10">[00:34:10]</a>.

#### Smart Devices and Medical Applications
The near future envisions billions of people using [[Artificial Intelligence and its future implications | AI]] assistance daily through smart glasses or smartphones <a class="yt-timestamp" data-t="00:42:17">[00:42:17]</a> <a class="yt-timestamp" data-t="00:42:51">[00:42:51]</a>. Such devices, like Meta's smart glasses, already feature [[Artificial Intelligence and its future implications | AI]] systems that can answer questions or identify objects from camera images <a class="yt-timestamp" data-t="00:42:40">[00:42:40]</a>.

Deep learning methods show immense promise in medical applications, particularly in diagnostics like breast cancer mammography <a class="yt-timestamp" data-t="00:57:17">[00:57:17]</a> <a class="yt-timestamp" data-t="00:57:22">[00:57:22]</a>. Startups like Ataris, founded at New York University, aim to leverage [[Artificial Intelligence and its future implications | AI]] for breast cancer prediction, moving beyond just diagnosis to influence treatment <a class="yt-timestamp" data-t="00:57:02">[00:57:02]</a> <a class="yt-timestamp" data-t="00:58:21">[00:58:21]</a>.

### Computing Infrastructure and Investment
The widespread adoption of [[Artificial Intelligence and its future implications | AI]] assistance will necessitate massive computing infrastructure. Running large language models and other [[Artificial Intelligence and its future implications | AI]] systems for billions of users requires significant computing power, with most investments (e.g., Meta's $60-65 billion, Microsoft's $80 billion, and the rumored Stargate project's $500 billion over 5-10 years) directed towards *inference costs* (running models) rather than *training* them, which is comparatively cheaper <a class="yt-timestamp" data-t="00:43:12">[00:43:12]</a> <a class="yt-timestamp" data-t="00:43:31">[00:43:31]</a> <a class="yt-timestamp" data-t="00:44:03">[00:44:03]</a>.

### Open Research and Collaboration
Open research and open-source software are critical drivers of [[Artificial intelligence progress and future | AI progress]] <a class="yt-timestamp" data-t="00:36:34">[00:36:34]</a>. When research and code are published openly, the entire global community benefits, leading to faster development and innovation <a class="yt-timestamp" data-t="00:36:10">[00:36:10]</a> <a class="yt-timestamp" data-t="00:37:01">[00:37:01]</a>. Meta is a strong supporter of this approach, with its Paris lab creating foundational models like LLaMA <a class="yt-timestamp" data-t="00:37:40">[00:37:40]</a> <a class="yt-timestamp" data-t="00:37:42">[00:37:42]</a>. The prevalence of PyTorch, an open-source software created by Meta and now managed by the Linux Foundation, across the [[Artificial Intelligence and its future implications | AI]] industry and academia, demonstrates the power of collaborative development <a class="yt-timestamp" data-t="00:39:37">[00:39:37]</a>.

The speaker emphasizes that good ideas emerge globally, and no single institution or region holds a monopoly <a class="yt-timestamp" data-t="00:38:07">[00:38:07]</a> <a class="yt-timestamp" data-t="00:41:41">[00:41:41]</a>. Open cooperation, rather than perceived competition, accelerates progress in the field <a class="yt-timestamp" data-t="00:42:06">[00:42:06]</a>.

### Europe's Role in AI
Europe holds a significant position in the global [[Artificial Intelligence and its future implications | AI]] landscape, largely due to its talent pool of programmers, mathematicians, physicists, and engineers <a class="yt-timestamp" data-t="00:53:01">[00:53:01]</a>. Many leading [[Artificial Intelligence and its future implications | AI]] scientists globally originate from Europe <a class="yt-timestamp" data-t="00:53:13">[00:53:13]</a>. However, regulatory uncertainty within the European Union has hindered the full deployment of certain [[Artificial Intelligence and its future implications | AI]] applications, such as the vision function in Meta's smart glasses <a class="yt-timestamp" data-t="00:52:02">[00:52:02]</a> <a class="yt-timestamp" data-t="00:52:41">[00:52:41]</a>.

### Personal Reflections on AI Development
One regret expressed is a delayed interest in self-supervised learning until the mid-2000s, about ten years later than it should have been <a class="yt-timestamp" data-t="00:54:20">[00:54:20]</a> <a class="yt-timestamp" data-t="00:54:57">[00:54:57]</a>. This period saw a "drought" in neural network and deep learning research interest <a class="yt-timestamp" data-t="00:55:05">[00:55:05]</a>. Another reflection is on the work on convolutional neural networks (CNNs), invented in 1988, which are fundamental to modern [[Artificial Intelligence and its future implications | AI]] applications ranging from character recognition and driver assistance systems to speech recognition and plant identification apps <a class="yt-timestamp" data-t="00:50:07">[00:50:07]</a> <a class="yt-timestamp" data-t="00:50:49">[00:50:49]</a> <a class="yt-timestamp" data-t="00:51:20">[00:51:20]</a>.