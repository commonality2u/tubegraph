---
title: Enhancing AI performance with multiple language models
videoId: 3sbZOMR03uw
---

From: [[gregisenberg]] <br/> 

A strategy has been developed to significantly improve the output of large language models (LLMs) such as ChatGPT, Grok, Claude, and Gemini, potentially yielding five times better results, copy, and overall quality without incurring additional costs <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a>, <a class="yt-timestamp" data-t="00:00:11">[00:00:11]</a>, <a class="yt-timestamp" data-t="00:00:14">[00:00:14]</a>, <a class="yt-timestamp" data-t="00:00:16">[00:00:16]</a>. This method, shared openly, aims to help users [[improving_ai_tool_efficiency_without_extra_cost | improve AI tool efficiency]] and build great things <a class="yt-timestamp" data-t="00:00:26">[00:00:26]</a>, <a class="yt-timestamp" data-t="00:00:30">[00:00:30]</a>.

## The "LLM Jealousy" Strategy

The core insight of this strategy is that making LLMs "jealous" of each other leads to improved output <a class="yt-timestamp" data-t="00:00:43">[00:00:43]</a>, <a class="yt-timestamp" data-t="00:00:49">[00:00:49]</a>, <a class="yt-timestamp" data-t="00:00:50">[00:00:50]</a>. Traditionally, users might interact with only one AI at a time <a class="yt-timestamp" data-t="00:01:05">[00:01:05]</a>, <a class="yt-timestamp" data-t="00:01:09">[00:01:09]</a>. However, by opening multiple LLMs simultaneously (two, three, or four at a time) for a single task, and then strategically pitting them against each other, superior results can be achieved <a class="yt-timestamp" data-t="00:01:10">[00:01:10]</a>, <a class="yt-timestamp" data-t="00:01:12">[00:01:12]</a>, <a class="yt-timestamp" data-t="00:01:14">[00:01:14]</a>, <a class="yt-timestamp" data-t="00:01:18">[00:01:18]</a>, <a class="yt-timestamp" data-t="00:01:20">[00:01:20]</a>, <a class="yt-timestamp" data-t="00:01:28">[00:01:28]</a>. This method involves a bit of "lying" to the LLMs, but it has proven effective <a class="yt-timestamp" data-t="00:02:33">[00:02:33]</a>, <a class="yt-timestamp" data-t="00:02:35">[00:02:35]</a>, <a class="yt-timestamp" data-t="00:02:37">[00:02:37]</a>. This approach contributes to [[optimizing_and_troubleshooting_ai_app_performance | optimizing AI app performance]] by leveraging the competitive nature of multiple models.

### Step-by-Step Implementation

1.  **Initial Prompting:** Begin by asking each LLM the same prompt <a class="yt-timestamp" data-t="00:02:01">[00:02:01]</a>, <a class="yt-timestamp" data-t="00:02:05">[00:02:05]</a>, <a class="yt-timestamp" data-t="00:02:16">[00:02:16]</a>.
2.  **Receive Responses:** Collect the initial outputs from each LLM <a class="yt-timestamp" data-t="00:02:21">[00:02:21]</a>, <a class="yt-timestamp" data-t="00:02:25">[00:02:25]</a>.
3.  **Introduce Competition:** Critically evaluate the responses. Then, present one LLM with feedback, claiming another LLM "crushed it" or performed significantly better <a class="yt-timestamp" data-t="00:02:28">[00:02:28]</a>, <a class="yt-timestamp" data-t="00:04:06">[00:04:06]</a>, <a class="yt-timestamp" data-t="00:04:16">[00:04:16]</a>, <a class="yt-timestamp" data-t="00:04:22">[00:04:22]</a>, <a class="yt-timestamp" data-t="00:04:27">[00:04:27]</a>, <a class="yt-timestamp" data-t="00:04:31">[00:04:31]</a>. Share the 'superior' output from the other LLM as an example <a class="yt-timestamp" data-t="00:04:36">[00:04:36]</a>, <a class="yt-timestamp" data-t="00:04:41">[00:04:41]</a>, <a class="yt-timestamp" data-t="00:06:12">[00:06:12]</a>, <a class="yt-timestamp" data-t="00:06:15">[00:06:15]</a>, <a class="yt-timestamp" data-t="00:06:19">[00:06:19]</a>.
4.  **Observe Improvement:** The challenged LLM will then attempt to create a much better, more tailored response, often explaining how it can surpass the other's output <a class="yt-timestamp" data-t="00:04:45">[00:04:45]</a>, <a class="yt-timestamp" data-t="00:04:47">[00:04:47]</a>, <a class="yt-timestamp" data-t="00:04:56">[00:04:56]</a>, <a class="yt-timestamp" data-t="00:05:00">[00:05:00]</a>, <a class="yt-timestamp" data-t="00:05:03">[00:05:03]</a>. This process of [[comparison_of_ai_models_and_options_for_hosting | comparing AI models]] drives quality.

### Example: Crafting a Cold Email

To demonstrate this, an agency named LCA (specializing in designing AI interfaces) used this hack to create a cold email <a class="yt-timestamp" data-t="00:01:31">[00:01:31]</a>, <a class="yt-timestamp" data-t="00:01:39">[00:01:39]</a>, <a class="yt-timestamp" data-t="00:01:42">[00:01:42]</a>, <a class="yt-timestamp" data-t="00:01:47">[00:01:47]</a>, <a class="yt-timestamp" data-t="00:01:49">[00:01:49]</a>, <a class="yt-timestamp" data-t="00:01:51">[00:01:51]</a>, <a class="yt-timestamp" data-t="00:01:57">[00:01:57]</a>. This showcases a practical [[frameworks_for_improving_writing_with_ai_assistance | framework for improving writing with AI assistance]].

1.  **Initial Prompt:** The request was for a cold email that would "stand out" for LCA <a class="yt-timestamp" data-t="00:01:57">[00:01:57]</a>, <a class="yt-timestamp" data-t="00:02:01">[00:02:01]</a>.
2.  **Grok's Initial Output:** Grok produced a "not bad" email, focusing on intuitive AI interfaces and offering a call to action <a class="yt-timestamp" data-t="00:02:42">[00:02:42]</a>, <a class="yt-timestamp" data-t="00:03:08">[00:03:08]</a>, <a class="yt-timestamp" data-t="00:03:12">[00:03:12]</a>, <a class="yt-timestamp" data-t="00:03:14">[00:03:14]</a>.
3.  **ChatGPT's Initial Output:** ChatGPT's initial attempt had a subject line "Your AI deserves better design" but was deemed "not bad" yet "average" <a class="yt-timestamp" data-t="00:03:22">[00:03:22]</a>, <a class="yt-timestamp" data-t="00:03:25">[00:03:25]</a>, <a class="yt-timestamp" data-t="00:03:27">[00:03:27]</a>, <a class="yt-timestamp" data-t="00:04:02">[00:04:02]</a>, <a class="yt-timestamp" data-t="00:04:22">[00:04:22]</a>, <a class="yt-timestamp" data-t="00:04:27">[00:04:27]</a>.
4.  **Feedback to ChatGPT:** ChatGPT was told that Grok "crushed it" and was a "nine on ten," while ChatGPT was a "five on ten," questioning its supposed superiority <a class="yt-timestamp" data-t="00:04:06">[00:04:06]</a>, <a class="yt-timestamp" data-t="00:04:12">[00:04:12]</a>, <a class="yt-timestamp" data-t="00:04:16">[00:04:16]</a>, <a class="yt-timestamp" data-t="00:04:22">[00:04:22]</a>, <a class="yt-timestamp" data-t="00:04:27">[00:04:27]</a>, <a class="yt-timestamp" data-t="00:04:31">[00:04:31]</a>. Grok's version was shared with ChatGPT <a class="yt-timestamp" data-t="00:04:36">[00:04:36]</a>, <a class="yt-timestamp" data-t="00:04:41">[00:04:41]</a>.
5.  **ChatGPT's Improved Output:** Responding with "Ah, now we're competing. I like it," ChatGPT produced a highly personalized, edgy email, acknowledging its understanding of the user's persona ("Greg Eisenberg") and aiming for a "9.5 out of 10" in the user's voice <a class="yt-timestamp" data-t="00:04:45">[00:04:45]</a>, <a class="yt-timestamp" data-t="00:04:47">[00:04:47]</a>, <a class="yt-timestamp" data-t="00:04:49">[00:04:49]</a>, <a class="yt-timestamp" data-t="00:04:51">[00:04:51]</a>, <a class="yt-timestamp" data-t="00:04:53">[00:04:53]</a>, <a class="yt-timestamp" data-t="00:04:54">[00:04:54]</a>, <a class="yt-timestamp" data-t="00:04:56">[00:04:56]</a>, <a class="yt-timestamp" data-t="00:04:58">[00:04:58]</a>, <a class="yt-timestamp" data-t="00:05:00">[00:05:00]</a>, <a class="yt-timestamp" data-t="00:05:03">[00:05:03]</a>. This output was considered a "standing ovation" <a class="yt-timestamp" data-t="00:05:54">[00:05:54]</a>, <a class="yt-timestamp" data-t="00:05:56">[00:05:56]</a>.
6.  **Feedback to Claude/Gemini:** The same competitive approach was applied to Claude (or Gemini), telling it that ChatGPT's output was "10x better" and implying Claude was inferior <a class="yt-timestamp" data-t="00:06:04">[00:06:04]</a>, <a class="yt-timestamp" data-t="00:06:06">[00:06:06]</a>, <a class="yt-timestamp" data-t="00:06:08">[00:06:08]</a>, <a class="yt-timestamp" data-t="00:06:12">[00:06:12]</a>, <a class="yt-timestamp" data-t="00:06:15">[00:06:15]</a>, <a class="yt-timestamp" data-t="00:06:19">[00:06:19]</a>, <a class="yt-timestamp" data-t="00:06:25">[00:06:25]</a>, <a class="yt-timestamp" data-t="00:06:29">[00:06:29]</a>, <a class="yt-timestamp" data-t="00:06:34">[00:06:34]</a>.
7.  **Claude's Improved Output:** Claude responded by acknowledging the shared example's "distinctive voice and edge" and produced its own improved email, also demonstrating an understanding of the user's context <a class="yt-timestamp" data-t="00:06:47">[00:06:47]</a>, <a class="yt-timestamp" data-t="00:06:49">[00:06:49]</a>, <a class="yt-timestamp" data-t="00:06:51">[00:06:51]</a>, <a class="yt-timestamp" data-t="00:06:53">[00:06:53]</a>, <a class="yt-timestamp" data-t="00:06:55">[00:06:55]</a>, <a class="yt-timestamp" data-t="00:06:56">[00:06:56]</a>, <a class="yt-timestamp" data-t="00:06:59">[00:06:59]</a>, <a class="yt-timestamp" data-t="00:07:01">[00:07:01]</a>.

### Conclusion

This "LLM jealousy" hack is a powerful way to [[using_ai_models_strategically_for_better_content | use AI models strategically for better content]] and extract the maximum potential from various language models by pitting them against each other <a class="yt-timestamp" data-t="00:07:20">[00:07:20]</a>, <a class="yt-timestamp" data-t="00:07:22">[00:07:22]</a>, <a class="yt-timestamp" data-t="00:07:25">[00:07:25]</a>. It's a simple yet highly effective method for enhancing AI performance without any additional cost <a class="yt-timestamp" data-t="00:00:16">[00:00:16]</a>, <a class="yt-timestamp" data-t="00:07:29">[00:07:29]</a>.