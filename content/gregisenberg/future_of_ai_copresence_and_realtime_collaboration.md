---
title: Future of AI Copresence and Realtime Collaboration
videoId: 6h9y1rLem4c
---

From: [[gregisenberg]] <br/> 

AI copresence refers to the ability of AI to "see" and understand the environment and context of a user, allowing for real-time, interactive assistance and collaboration <a class="yt-timestamp" data-t="00:01:52">[00:01:52]</a>. This concept is explored through Google's Multimodal Live API and AI Studio, showcasing a future where AI acts as an active, intelligent partner in various tasks <a class="yt-timestamp" data-t="00:20:22">[00:20:22]</a>.

## Google's Multimodal Live API

The Multimodal Live API is designed to enable AI to be co-present in user experiences by listening to audio and observing visual information <a class="yt-timestamp" data-t="00:20:11">[00:20:11]</a>. This capability allows models to gain the necessary context to provide real-time assistance <a class="yt-timestamp" data-t="00:20:29">[00:20:29]</a>.

### Demonstration: Real-time Coding Assistance
A key demonstration of AI copresence involves real-time coding assistance <a class="yt-timestamp" data-t="00:20:35">[00:20:35]</a>:
*   **Active Listening and Observation** The AI listens to the user's speech and views their screen, such as a code editor <a class="yt-timestamp" data-t="00:20:40">[00:20:40]</a>.
*   **Contextual Understanding** It identifies open files (e.g., `f.py`) and understands the code's purpose (e.g., using the Gemini API) <a class="yt-timestamp" data-t="00:20:58">[00:20:58]</a>.
*   **Problem-Solving** When the user encounters an error, the AI analyzes the error message (e.g., "no such file or directory," "API key not valid") and provides actionable suggestions <a class="yt-timestamp" data-t="00:21:42">[00:21:42]</a>.
*   **Iterative Guidance** The AI guides the user through debugging steps, asking relevant questions about prior attempts or file paths <a class="yt-timestamp" data-t="00:22:21">[00:22:21]</a>.

### Underlying Capabilities
The Multimodal Live API leverages advanced features:
*   **Multimodality** It processes both audio and visual inputs simultaneously <a class="yt-timestamp" data-t="00:20:11">[00:20:11]</a>.
*   **Tool Integration** It supports native tool integrations, including pseudo function calls and code execution, allowing it to run code in a virtual environment and show outputs <a class="yt-timestamp" data-t="00:25:32">[00:25:32]</a>.
*   **Grounding** The AI can browse the internet to find relevant information without the user leaving the product experience, bridging the outside world into a unified experience <a class="yt-timestamp" data-t="00:25:51">[00:25:51]</a>.

## Implications and Use Cases

The development of AI copresence suggests a significant shift in how people work and learn.

*   **Future of Work** This technology represents the future of work, where an AI partner can "see what you're seeing and provide value and intelligence to whatever it is you're working on at any given moment" <a class="yt-timestamp" data-t="00:24:52">[00:24:52]</a>. Users are likely to accept sharing their screen if it leads to significantly faster work completion (1.5x, 2x, 3x faster) <a class="yt-timestamp" data-t="00:25:17">[00:25:17]</a>.

*   **[[using_ai_tools_in_web_development | Enhanced Development Environments]]** Integrated into an IDE, AI copresence could transform code autocomplete into a full [[using_ai_tools_in_web_development | pair programming]] experience, with the AI acting as a senior developer understanding the entire context <a class="yt-timestamp" data-t="00:23:24">[00:23:24]</a>.

*   **Democratizing Access to Expertise** AI copresence has the potential to flatten the steep learning curve often associated with technology <a class="yt-timestamp" data-t="00:24:03">[00:24:03]</a>. It can provide personalized assistance to individuals learning complex skills like coding or video editing, especially for those without access to tutors or community support <a class="yt-timestamp" data-t="00:27:33">[00:27:33]</a>. This democratizes access to help and expertise that historically wasn't readily available <a class="yt-timestamp" data-t="00:27:56">[00:27:56]</a>.

*   **[[integrating_ai_features_with_web_applications | Combinatorial Innovation]]** By connecting existing products through AI and function calling, a "combinatorial explosion" of new business opportunities arises <a class="yt-timestamp" data-t="00:19:15">[00:19:15]</a>. The effort required to integrate five different products with AI in the middle is "actually pretty small relative to historical SAS product solutions" <a class="yt-timestamp" data-t="00:19:25">[00:19:25]</a>.

## Accessibility and Future Development

The Multimodal Live API is available for free through audio.google.com and offers free API keys for developers to prototype experiences today <a class="yt-timestamp" data-t="00:28:01">[00:28:01]</a>. Google aims to remove the economic burden for developers and startups to build AI products, providing free access to Gemini models and multimodal live experiences <a class="yt-timestamp" data-t="00:29:07">[00:29:07]</a>. While still in early stages, the technology showcases raw capabilities that will continue to be refined <a class="yt-timestamp" data-t="00:22:40">[00:22:40]</a>. For example, fine-tune controls on the API side will allow for more nuanced conversation flow, such as instructing the model to be less eager to interrupt <a class="yt-timestamp" data-t="00:22:53">[00:22:53]</a>.