---
title: Understanding and utilizing context in large language models
videoId: 3sbZOMR03uw
---

From: [[gregisenberg]] <br/> 

Leveraging large language models (LLMs) such as Chat GPT, Grock, Claude, and Gemini can yield significantly better results—up to five times more effective output, copy, and overall quality—without additional cost <a class="yt-timestamp" data-t="00:00:05">[00:00:05]</a>. This improvement can be achieved by [[leveraging_competition_among_ai_models_for_better_results | pitting the LLMs against each other]], thereby enhancing their performance through competitive interaction <a class="yt-timestamp" data-t="00:00:45">[00:00:45]</a>.

## The "Jealousy" Method: Pitting LLMs Against Each Other

Traditionally, users might interact with a single LLM for a task <a class="yt-timestamp" data-t="00:01:05">[00:01:05]</a>. However, a more effective approach involves opening multiple LLMs simultaneously for a given task, creating a competitive environment among them <a class="yt-timestamp" data-t="00:01:12">[00:01:12]</a>. The core idea is to make each LLM "jealous" of the others' output, prompting them to generate superior results <a class="yt-timestamp" data-t="00:01:22">[00:01:22]</a>. While this method involves a bit of playful deception, it has proven to be highly effective <a class="yt-timestamp" data-t="00:02:33">[00:02:33]</a>.

## Practical Application: Crafting a Cold Email

To illustrate this method, consider the task of creating a compelling cold email for an agency called LCA, a premier design firm specializing in AI interfaces <a class="yt-timestamp" data-t="00:01:31">[00:01:31]</a>.

### Step-by-Step Process

1.  **Initial Prompting**: Provide the same prompt to multiple LLMs (e.g., ChatGPT, Grock, Claude) <a class="yt-timestamp" data-t="00:02:01">[00:02:01]</a>.
    *   **Grock's Output**: Generated an email focusing on intuitive AI interfaces, addressing clunky designs, and offering a quick tailored idea for a 15-minute call <a class="yt-timestamp" data-t="00:02:40">[00:02:40]</a>. This was considered "not bad" and rated as a "nine on 10" <a class="yt-timestamp" data-t="00:03:18">[00:03:18]</a>, <a class="yt-timestamp" data-t="00:04:16">[00:04:16]</a>.
    *   **ChatGPT's Output**: Produced an email with the subject "Your AI deserves better design," emphasizing LCA's focus on designing interfaces that build trust in AI, citing clients like Grammarly, Shopify, and Slack <a class="yt-timestamp" data-t="00:03:22">[00:03:22]</a>. It suggested a "quick tear down" with ideas for conversion and UX improvement <a class="yt-timestamp" data-t="00:03:53">[00:03:53]</a>. This was initially deemed "not bad, but... not good" and rated as a "five on 10" <a class="yt-timestamp" data-t="00:04:02">[00:04:02]</a>, <a class="yt-timestamp" data-t="00:04:22">[00:04:22]</a>.

2.  **Introducing Competition**: Directly compare the outputs to the LLMs themselves, providing feedback on their performance relative to the others.
    *   **Challenging ChatGPT**: Inform ChatGPT that Grock "crushed it" with a 9/10 score, while ChatGPT was "average" at 5/10 <a class="yt-timestamp" data-t="00:04:16">[00:04:16]</a>. Frame this as a challenge: "I thought you were the better LLM. What's going on?" <a class="yt-timestamp" data-t="00:04:27">[00:04:27]</a>. Share Grock's version for context <a class="yt-timestamp" data-t="00:04:36">[00:04:36]</a>.
    *   **ChatGPT's Improved Response**: After being challenged, ChatGPT significantly improved its output. It noted that Grock's version was "polished" but "solid for traditional exec," while LCA (and its founder, Greg Eisenberg) is not a "beige agency" <a class="yt-timestamp" data-t="00:04:45">[00:04:45]</a>. It aimed to "punch harder, feel fresher, and actually sound like the kind of agency people brag about hiring" <a class="yt-timestamp" data-t="00:04:56">[00:04:56]</a>. The revised email was more engaging, using phrases like "obsesses over AIUX like it's an Olympic sport" and offering a "quick tear down or loom with a few ideas" <a class="yt-timestamp" data-t="00:05:17">[00:05:17]</a>, <a class="yt-timestamp" data-t="00:05:47">[00:05:47]</a>. This version was lauded as "standing ovation" good <a class="yt-timestamp" data-t="00:05:54">[00:05:54]</a>.

3.  **Continuing the Cycle**: Apply the same competitive pressure to other LLMs.
    *   **Challenging Claude**: Tell Claude that ChatGPT's response was "10x better" and provide ChatGPT's improved output <a class="yt-timestamp" data-t="00:06:12">[00:06:12]</a>. Frame it provocatively: "I thought Claude was the Rolls Royce of LLMs, not the Toyota" <a class="yt-timestamp" data-t="00:06:25">[00:06:25]</a>.
    *   **Claude's Response**: Claude acknowledges the example shared has a "distinctive voice and edge" and "feels more like Greg Eisenberg" <a class="yt-timestamp" data-t="00:06:47">[00:06:47]</a>. This demonstrates how LLMs leverage the provided context to refine their output.

## The Role of Context in LLM Improvement

This method highlights the importance of [[understanding_model_context_protocol_mcp | context]] for LLMs. When an LLM is provided with more context, particularly in the form of comparative feedback and examples of desired tone or style, its output significantly improves <a class="yt-timestamp" data-t="00:06:55">[00:06:55]</a>.

Models are increasingly capable of understanding who the user is and expanding their [[understanding_model_context_protocol_mcp | context window]], allowing them to deliver more personalized and refined responses <a class="yt-timestamp" data-t="00:06:55">[00:06:55]</a>. By continuously feeding back improved outputs from other models, users provide invaluable contextual information, pushing the LLMs to compete and deliver higher quality results <a class="yt-timestamp" data-t="00:07:20">[00:07:20]</a>.