---
title: Prompting techniques for effective use of AI models
videoId: i9kTrcf-gDQ
---

From: [[gregisenberg]] <br/> 

Prompting, particularly with new reasoning models like DeepSeek R1, is crucial for effectively leveraging AI's capabilities, potentially leading to superhuman capabilities in task execution <a class="yt-timestamp" data-t="00:00:18">[00:00:18]</a>. These advanced models can think and reason, offering significant advancements over previous AI iterations <a class="yt-timestamp" data-t="00:00:30">[00:00:30]</a>.

## Understanding DeepSeek R1 and its Implications

DeepSeek R1, an open-source reasoning model developed in China, is considered on par with ChatGPT's 01 reasoning model <a class="yt-timestamp" data-t="00:00:40">[00:00:40]</a>. Its popularity stems from being free to use on its website <a class="yt-timestamp" data-t="00:01:04">[00:01:04]</a>.

### Data Privacy Considerations
When using DeepSeek.com directly, your data is hosted in China, meaning it falls under Chinese laws and regulations <a class="yt-timestamp" data-t="00:02:28">[00:02:28]</a>. It's advisable to be cautious about inputting sensitive data, such as tax returns or medical records, as it would not belong to your region of control <a class="yt-timestamp" data-t="00:02:50">[00:02:50]</a>. This applies to any public-facing DeepSeek app as well <a class="yt-timestamp" data-t="00:20:32">[00:20:32]</a>.

### Alternative Hosting Options for Privacy
To maintain data privacy and control, several alternatives exist for running DeepSeek R1 or similar models:
*   **Web UI with API Providers**: Use a web user interface (UI) to connect to API providers like Fireworks AI or Groq, which can host the models in regions like North America <a class="yt-timestamp" data-t="00:03:08">[00:03:08]</a>. For example, Cursor, a coding app, uses the Fireworks API to host DeepSeek models outside of China <a class="yt-timestamp" data-t="00:05:50">[00:05:50]</a>.
*   **Local Machine Hosting**: Run the model directly on your local machine, ensuring data never leaves your device <a class="yt-timestamp" data-t="00:01:47">[00:01:47]</a>. This option allows for offline use, even on a plane <a class="yt-timestamp" data-t="00:03:17">[00:03:17]</a>.

## Mastering Prompting Techniques

Effective [[prompting_techniques_in_ai_ad_generation | prompting]] is key to getting the desired output from AI models.

### Generating Content from Transcripts
One practical application is using AI to analyze and generate content from transcripts:
1.  **Transcribe Content**: Utilize an application to transcribe videos, producing a text transcript <a class="yt-timestamp" data-t="00:03:37">[00:03:37]</a>.
2.  **Input into AI**: Paste the transcript into the DeepSeek interface (or an alternative host) <a class="yt-timestamp" data-t="00:04:03">[00:04:03]</a>.
3.  **Enable Deep Think**: Ensure "Deep Think" is enabled for the model to engage its reasoning capabilities <a class="yt-timestamp" data-t="00:04:16">[00:04:16]</a>.
4.  **Add Instructions**: Provide specific instructions to the model, such as "analyze this transcript and generate a blog post" <a class="yt-timestamp" data-t="00:04:32">[00:04:32]</a>. Advanced chaining prompts can be used to take full advantage of the model's thinking process over large amounts of text <a class="yt-timestamp" data-t="00:05:00">[00:05:00]</a>.

The results from larger models, like the one hosted by Fireworks AI, can be highly detailed, offering analysis, geopolitical implications, future predictions, and even SEO enhancements <a class="yt-timestamp" data-t="00:12:12">[00:12:12]</a>. This output can be near human-level quality, comparable to a senior writer or research engineer <a class="yt-timestamp" data-t="00:14:20">[00:14:20]</a>.

### Improving and Structuring Prompts
To enhance your prompts:
*   **Use AI Playgrounds**: Platforms like `platform.openai.com` offer a "playground" to describe a desired prompt, which the AI can then reconfigure for better efficiency with language models <a class="yt-timestamp" data-t="00:18:31">[00:18:31]</a>.
*   **Define Expectations**: When [[planning_before_using_ai_tools | planning before using AI tools]] and structuring prompts, consider:
    *   Desired instructions <a class="yt-timestamp" data-t="00:19:32">[00:19:32]</a>
    *   Expected output type <a class="yt-timestamp" data-t="00:19:35">[00:19:35]</a>
    *   Unwanted outputs <a class="yt-timestamp" data-t="00:19:38">[00:19:38]</a>
*   **Iterative Refinement**: Prompts often improve over time through continuous use and refinement based on specific use cases <a class="yt-timestamp" data-t="00:19:47">[00:19:47]</a>.

### Information Verification with Web Search
DeepSeek.com (via its web UI or app) allows you to enable web search for information verification. You can input an article and instruct the model to verify claims by searching the internet <a class="yt-timestamp" data-t="00:20:22">[00:20:22]</a>.

## Model Parameters and Performance
Larger AI models (e.g., 600 billion+ parameters) possess greater intelligence, taking longer to process but yielding superior results <a class="yt-timestamp" data-t="00:07:01">[00:07:01]</a>. Distilled versions of these models are faster and efficient but may not offer the same depth of reasoning <a class="yt-timestamp" data-t="00:07:17">[00:07:17]</a>.

### The "Temperature" Setting
The "temperature" setting influences the creativity and adherence to instructions of the AI model <a class="yt-timestamp" data-t="00:29:42">[00:29:42]</a>:
*   **Lower Temperature (e.g., 0.8 to 0)**: Makes the model "hallucinate less," causing it to follow instructions more precisely and avoid tangents, useful for logical reasoning tasks like coding <a class="yt-timestamp" data-t="00:29:50">[00:29:50]</a>.
*   **Higher Temperature (e.g., up to 1)**: Encourages extreme creativity, helpful for creative writing or non-logical reasoning <a class="yt-timestamp" data-t="00:30:01">[00:30:01]</a>.

Experimenting with different temperatures is recommended to see variations in output <a class="yt-timestamp" data-t="00:30:16">[00:30:16]</a>.

## Advantages of Reasoning Models
Reasoning models, such as DeepSeek's and OpenAI's 01 Pro, stand out because they dedicate extra time to meticulously process and adhere to your instructions <a class="yt-timestamp" data-t="00:15:14">[00:15:14]</a>. They pay attention to every detail, ensuring all specified tasks are completed <a class="yt-timestamp" data-t="00:15:19">[00:15:19]</a>. This attention to detail results in outputs that can significantly outperform those from other models, turning thought-starters into near-finished, human-level content <a class="yt-timestamp" data-t="00:14:07">[00:14:07]</a>.

## Choosing the Right Tool for the Job
It's vital to select the appropriate AI model for your specific use case. You don't always need the most powerful or detailed model; sometimes, a faster, less detailed one is sufficient for tasks like quick information gathering <a class="yt-timestamp" data-t="00:42:24">[00:42:24]</a>. The ability to compare and experiment with different models is crucial for effective workflows <a class="yt-timestamp" data-t="00:31:58">[00:31:58]</a>.

## Setting Up Local AI Models for Privacy and Efficiency

For enhanced privacy and control over your data, running AI models locally is a powerful option. This is one of the key [[tips_for_using_ai_tools_effectively_in_design_workflows | tips for using AI tools effectively in design workflows]].

### Open Web UI
Open Web UI is recommended as a user-friendly interface for running local models <a class="yt-timestamp" data-t="00:22:51">[00:22:51]</a>.
*   **Installation**: Requires Docker (downloadable from `docker.com`) <a class="yt-timestamp" data-t="00:23:00">[00:23:00]</a>.
    1.  Pull the Docker container: `docker pull ghcr.io/open-webui/open-webui:main` <a class="yt-timestamp" data-t="00:23:47">[00:23:47]</a>.
    2.  Run the container: `docker run -d -p 3000:8080 --add-host host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main` (for single-user mode, no sign-in required) <a class="yt-timestamp" data-t="00:24:00">[00:24:00]</a>. For NVIDIA GPUs, add `--gpus all` to leverage GPU power <a class="yt-timestamp" data-t="00:24:14">[00:24:14]</a>.
*   **Access**: Once running, access the UI via `localhost:3000` in your web browser <a class="yt-timestamp" data-t="00:24:47">[00:24:47]</a>.

### Ollama
Ollama (`ollama.com`) is a tool to download and run local AI models easily <a class="yt-timestamp" data-t="00:25:15">[00:25:15]</a>.
*   **Downloading Models**: Find and download models (e.g., DeepSeek R1) through Ollama. These models are typically several gigabytes in size <a class="yt-timestamp" data-t="00:26:56">[00:26:56]</a>.
*   **Integration with Open Web UI**: Ollama API is pre-configured in Open Web UI's admin panel (Connections > `olama API`) <a class="yt-timestamp" data-t="00:26:07">[00:26:07]</a>. You can search and pull models from Ollama directly within Open Web UI <a class="yt-timestamp" data-t="00:26:37">[00:26:37]</a>.

### Connecting API Providers to Open Web UI
Open Web UI can also connect to external API providers like Fireworks AI, Groq, and OpenRouter, allowing you to use their hosted models while routing them through your local UI.
*   **Fireworks AI**:
    1.  Base URL: `https://api.fireworks.ai/inference/v1` <a class="yt-timestamp" data-t="00:34:25">[00:34:25]</a>.
    2.  Generate an API key from the Fireworks AI console <a class="yt-timestamp" data-t="00:34:47">[00:34:47]</a>.
*   **Groq Cloud**:
    1.  Base URL: `https://api.groq.com/openai/v1` <a class="yt-timestamp" data-t="00:35:28">[00:35:28]</a>.
    2.  Generate an API key from `console.groq.com` <a class="yt-timestamp" data-t="00:35:13">[00:35:13]</a>.
*   **OpenRouter**: Another API provider offering access to various models <a class="yt-timestamp" data-t="00:39:01">[00:39:01]</a>.

Once connected, Open Web UI will display available models from these providers, alongside your locally downloaded Ollama models (distinguished by `:latest` suffix and an icon indicating local hosting) <a class="yt-timestamp" data-t="00:35:43">[00:35:43]</a>.

### Running Models on Mobile Devices
The Apollo app (paid app) allows downloading and running AI models directly on mobile devices (e.g., Apple silicon) <a class="yt-timestamp" data-t="00:37:04">[00:37:04]</a>.
*   **Local Models**: The app identifies device memory and suggests compatible models for local download (e.g., distilled versions like distilled Llama 8b8 mlx, distilled Qwen 7B) <a class="yt-timestamp" data-t="00:39:13">[00:39:13]</a>.
*   **API Provider Connection**: Apollo also supports connecting to API providers like OpenRouter for cloud-hosted model usage <a class="yt-timestamp" data-t="00:39:58">[00:39:58]</a>.

Running models locally on mobile devices, even smaller versions, enables reasoning and transcription without an internet connection, opening up possibilities for innovative applications like emergency assistance on wearables <a class="yt-timestamp" data-t="00:43:40">[00:43:40]</a>.

## The Future of AI and Prompting
The rapid advancement of AI models, particularly reasoning models, signals a significant leap in capabilities. OpenAI's upcoming 03 and Mini models are expected to further drive down costs and increase efficiency <a class="yt-timestamp" data-t="00:17:55">[00:17:55]</a>. The ability of models like GPT-4 Omni to understand audio, tone, and cadence hints at future applications in fields like negotiation, providing real-time insights into human micro-expressions and breathing rates <a class="yt-timestamp" data-t="00:46:15">[00:46:15]</a>.

This era emphasizes the importance of experimentation and community sharing to uncover new use cases and app ideas <a class="yt-timestamp" data-t="00:48:47">[00:48:47]</a>. By understanding how to effectively prompt and manage AI models, individuals can gain an "unfair advantage" in their ventures, optimizing efficiency and product quality <a class="yt-timestamp" data-t="00:13:05">[00:13:05]</a>.