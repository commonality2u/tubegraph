---
title: Multimodal live API and its applications
videoId: 6h9y1rLem4c
---

From: [[gregisenberg]] <br/> 

The Multimodal Live API was released by Google to power real-time streaming experiences within AI Studio, aiming to enable "AI co-presence" <a class="yt-timestamp" data-t="00:20:11">[00:20:11]</a>. This technology allows AI to perceive what a user sees and hears, providing contextually relevant assistance <a class="yt-timestamp" data-t="00:20:22">[00:20:22]</a>.

## Core Capabilities and Features

The Multimodal Live API is designed to showcase the raw capabilities of AI models in live interaction <a class="yt-timestamp" data-t="00:22:45">[00:22:45]</a>.

### Real-Time Interaction and Assistance
The API enables the model to listen to speech and simultaneously observe a user's screen <a class="yt-timestamp" data-t="00:20:40">[00:20:40]</a>. This allows for dynamic, context-aware assistance, as demonstrated in a coding scenario:
*   **Visual Recognition**: The AI can identify open applications like a code editor and specific files (e.g., `f.py`) <a class="yt-timestamp" data-t="00:20:52">[00:20:52]</a>, <a class="yt-timestamp" data-t="00:20:58">[00:20:58]</a>.
*   **Error Troubleshooting**: When a user encounters an error (e.g., "no such file or directory" or "API key not valid"), the AI can interpret the error message, deduce the problem, and suggest solutions <a class="yt-timestamp" data-t="00:21:42">[00:21:42]</a>, <a class="yt-timestamp" data-t="00:21:59">[00:21:59]</a>.
*   **Conversational Flow**: While early, the system allows for an interactive dialogue, though ongoing development focuses on refining conversation cadence and model responsiveness <a class="yt-timestamp" data-t="00:22:40">[00:22:40]</a>, <a class="yt-timestamp" data-t="00:22:53">[00:22:53]</a>.

### Tool Integrations
The Multimodal Live API supports various native tool integrations to enhance its capabilities:
*   **[[function_calling_and_api_integration_in_ai_development | Function Calls]]**: Allows for setting up "pseudo [[function_calling_and_api_integration_in_ai_development | function calls]]" <a class="yt-timestamp" data-t="00:25:32">[00:25:32]</a>.
*   **Code Execution**: Can spin up a Python virtual environment to run code and display outputs <a class="yt-timestamp" data-t="00:25:46">[00:25:46]</a>.
*   **Grounding**: Enables the model to browse the internet to find results, bridging the outside world into the unified product experience <a class="yt-timestamp" data-t="00:25:51">[00:25:51]</a>, <a class="yt-timestamp" data-t="00:26:16">[00:26:16]</a>.

## Applications and Future Potential

The Multimodal Live API unlocks a vast new array of product experiences <a class="yt-timestamp" data-t="00:23:49">[00:23:49]</a>.

### The Future of Work
This technology represents the future of work, where an AI partner can see what a user sees, providing valuable intelligence and accelerating task completion <a class="yt-timestamp" data-t="00:24:52">[00:24:52]</a>, <a class="yt-timestamp" data-t="00:25:17">[00:25:17]</a>.

### Enhanced Learning and Support
The API democratizes access to help for learning complex skills <a class="yt-timestamp" data-t="00:27:56">[00:27:56]</a>.
*   **Pair Programming**: Imagine a single button click transforming code autocomplete into a "senior developer" AI pair programming directly within an IDE <a class="yt-timestamp" data-t="00:23:29">[00:23:29]</a>.
*   **Skill Acquisition**: The AI can assist users in learning how to code, edit videos in software like Adobe Premiere, or other technical tasks <a class="yt-timestamp" data-t="00:24:31">[00:24:31]</a>. This is particularly beneficial for those who lack direct human support <a class="yt-timestamp" data-t="00:27:33">[00:27:33]</a>.

> "The ability for the model to... generally for models to be co-present in the experiences that you are... you could see the browser and like literally in real time seeing the same stuff that you're seeing and I think this unlocks like a huge new array of product experiences." <a class="yt-timestamp" data-t="00:23:14">[00:23:14]</a>

## Accessibility and Development
The Multimodal Live API is designed to be highly accessible for developers and startups:
*   **Free Usage**: The experience is available for free at `audio.google.com` (under "stream real time" in the left navigation) <a class="yt-timestamp" data-t="00:28:01">[00:28:01]</a>, <a class="yt-timestamp" data-t="00:28:08">[00:28:08]</a>.
*   **Free API Keys**: Developers can obtain API keys for free, enabling prototyping of the Multimodal Live experience without economic burden <a class="yt-timestamp" data-t="00:28:57">[00:28:57]</a>, <a class="yt-timestamp" data-t="00:29:02">[00:29:02]</a>, <a class="yt-timestamp" data-t="00:29:07">[00:29:07]</a>. Users can also change output formats and voices <a class="yt-timestamp" data-t="00:28:14">[00:28:14]</a>, <a class="yt-timestamp" data-t="00:26:45">[00:26:45]</a>.