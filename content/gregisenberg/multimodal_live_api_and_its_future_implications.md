---
title: Multimodal live API and its future implications
videoId: 6h9y1rLem4c
---

From: [[gregisenberg]] <br/> 

The Multimodal Live API is a key feature demonstrated within Google's [[Manis AI capabilities and use cases | AI Studio]], powering real-time streaming experiences <a class="yt-timestamp" data-t="00:20:11">[00:20:11]</a>. It enables an [[future_potential_and_use_cases_for_ai_technology | AI co-presence]], allowing AI models to "see" what a user sees and "hear" what a user says, thereby providing relevant context and assistance <a class="yt-timestamp" data-t="00:20:16">[00:20:16]</a>.

## Core Capabilities
The API allows for real-time interaction, where the model listens to spoken commands and observes the user's screen <a class="yt-timestamp" data-t="00:20:35">[00:20:35]</a>. This enables it to:
*   **Provide Contextual Help**: As demonstrated with a Python code example, the model can observe a user's code editor, understand errors, and suggest fixes in real-time <a class="yt-timestamp" data-t="00:20:52">[00:20:52]</a>, <a class="yt-timestamp" data-t="00:21:18">[00:21:18]</a>.
*   **Integrate Tools**: It features native tool integrations, allowing for pseudo-function calls and code execution within a virtual environment <a class="yt-timestamp" data-t="00:25:32">[00:25:32]</a>, <a class="yt-timestamp" data-t="00:25:39">[00:25:39]</a>.
*   **Grounding**: The model can browse the internet to find information and provide results without the user leaving their current product experience <a class="yt-timestamp" data-t="00:25:51">[00:25:51]</a>.

## Current State and Limitations
While powerful, the Multimodal Live API is currently in an early stage <a class="yt-timestamp" data-t="00:22:35">[00:22:35]</a>. The cadence of the conversation can sometimes be "rough," and the model might be overly eager to interject <a class="yt-timestamp" data-t="00:22:40">[00:22:40]</a>, <a class="yt-timestamp" data-t="00:22:58">[00:22:58]</a>. Fine-tuned controls on the API side are being developed to improve conversational flow <a class="yt-timestamp" data-t="00:22:53">[00:22:53]</a>.

## Future Implications and Use Cases
The Multimodal Live API is envisioned as the future of work, where AI models are co-present in user experiences <a class="yt-timestamp" data-t="00:24:52">[00:24:52]</a>, <a class="yt-timestamp" data-t="00:23:14">[00:23:14]</a>. This capability is expected to:
*   **Transform Software Development**: Instead of just autocomplete, developers could have a senior AI pair programmer in their IDE, seeing and understanding their entire screen in real-time <a class="yt-timestamp" data-t="00:23:29">[00:23:29]</a>. This could help users get work done 1.5x, 2x, or even 3x faster <a class="yt-timestamp" data-t="00:25:17">[00:25:17]</a>.
*   **Democratize Access to Expertise**: It can significantly flatten the steep learning curve associated with technology and software <a class="yt-timestamp" data-t="00:24:03">[00:24:03]</a>. For example, it could assist individuals learning to code or edit videos, providing the kind of real-time guidance that might otherwise require a human tutor <a class="yt-timestamp" data-t="00:24:25">[00:24:25]</a>, <a class="yt-timestamp" data-t="00:27:33">[00:27:33]</a>.
*   **Unlock New Product Experiences**: The API can lead to a "combinatorial explosion" of new product experiences by connecting disparate existing products with AI in the middle, creating new business opportunities with relatively small development effort compared to historical SaaS solutions <a class="yt-timestamp" data-t="00:19:15">[00:19:15]</a>, <a class="yt-timestamp" data-t="00:23:49">[00:23:49]</a>.

## Accessibility for Developers
The Multimodal Live API experience can be prototyped and used for free, with API keys offering 1.5 billion tokens across various Gemini models <a class="yt-timestamp" data-t="00:28:49">[00:28:49]</a>, <a class="yt-timestamp" data-t="00:29:02">[00:29:02]</a>. This aims to remove the economic burden for developers and startups to [[building_apps_with_ai_for_scalability_and_innovation | build AI products]] <a class="yt-timestamp" data-t="00:29:12">[00:29:12]</a>. Developers can try the experience at audio.Google.com by navigating to "stream real time" in the left-hand navigation <a class="yt-timestamp" data-t="00:28:01">[00:28:01]</a>.