---
title: Increasing ChatGPT effectiveness with a 3step process
videoId: 3sbZOMR03uw
---

From: [[gregisenberg]] <br/> 

A method has been developed to significantly enhance the output quality from large language models (LLMs) such as ChatGPT, Grok, Claude, and Gemini, potentially yielding five times better results, copy, and overall output without incurring additional costs <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a>. This technique involves making the LLMs "jealous" of each other <a class="yt-timestamp" data-t="00:45:00">[00:45:00]</a>.

## The 3-Step Process

### Step 1: Open Multiple LLMs Simultaneously
Traditionally, users might engage with one LLM at a time, such as ChatGPT or [[introduction_to_chatgpt_codex | Codex]] for writing <a class="yt-timestamp" data-t="00:54:00">[00:54:00]</a>. However, for a given task, the first step is to open multiple LLMs concurrentlyâ€”two, three, or even four at the same time <a class="yt-timestamp" data-t="01:10:00">[01:10:00]</a>.

### Step 2: Prompt All LLMs with the Same Request
Once multiple LLMs are open, provide the exact same prompt to each of them <a class="yt-timestamp" data-t="02:05:00">[02:05:00]</a>. For example, a request for a cold email for a design firm, LCA, specializing in designing AI interfaces, was used <a class="yt-timestamp" data-t="01:31:00">[01:31:00]</a>.

### Step 3: Pit LLMs Against Each Other (The "Jealousy" Method)
After receiving responses from all LLMs, the "jealousy" phase begins <a class="yt-timestamp" data-t="02:25:00">[02:25:00]</a>. This involves comparing their outputs and providing feedback to each LLM, often exaggerating or fabricating the performance of competitors to provoke a better response <a class="yt-timestamp" data-t="03:33:00">[03:33:00]</a>.

Consider the example of generating a cold email:
*   Initial responses from Grok and ChatGPT were obtained <a class="yt-timestamp" data-t="02:40:00">[02:40:00]</a>. Grok's output was deemed "not bad," while ChatGPT's was also "not bad" but raised questions about its assumptions <a class="yt-timestamp" data-t="03:18:00">[03:18:00]</a>.
*   To prompt improvement, ChatGPT was told that Grok "crushed it and was a nine on 10," while ChatGPT was merely "average and was five on 10," implying it was the "better LLM" that needed to perform <a class="yt-timestamp" data-t="04:16:00">[04:16:00]</a>.
*   This feedback prompted ChatGPT to produce a significantly improved version, claiming it was a "9.5 out of 10" with "the vibes and the edge Grock doesn't have" <a class="yt-timestamp" data-t="05:00:00">[05:00:00]</a>. This new version was described as "standing ovation" good <a class="yt-timestamp" data-t="05:54:00">[05:54:00]</a>.
*   The same tactic was then applied to Claude, falsely claiming that ChatGPT's response was "10x better" <a class="yt-timestamp" data-t="06:15:00">[06:15:00]</a> and provocatively suggesting Claude was the "Toyota" compared to ChatGPT's "Rolls Royce" <a class="yt-timestamp" data-t="06:29:00">[06:29:00]</a>.
*   Claude subsequently acknowledged the distinct voice and edge of the shared example, producing its own refined version that even incorporated the user's personal context <a class="yt-timestamp" data-t="06:47:00">[06:47:00]</a>.

This "pitting" strategy is presented as a valuable hack to extract the best possible output from LLMs <a class="yt-timestamp" data-t="07:20:00">[07:20:00]</a>.