---
title: Multimodal live API and realtime AI interaction
videoId: 6h9y1rLem4c
---

From: [[gregisenberg]] <br/> 

The Multimodal Live API powers real-time streaming experiences, enabling AI models to be "co-present" with users and interact dynamically based on what they see and hear <a class="yt-timestamp" data-t="00:20:11">[00:20:11]</a>. This technology brings to life the vision of AI being able to observe what a user is doing and provide relevant context and assistance <a class="yt-timestamp" data-t="00:20:22">[00:20:22]</a>.

## How it Works

The real-time streaming experience allows the AI model to simultaneously listen to a user's spoken words and observe their screen content <a class="yt-timestamp" data-t="00:20:35">[00:20:35]</a>. In a demonstration, the model was able to:
*   Listen to a user's speech and respond conversationally <a class="yt-timestamp" data-t="00:20:40">[00:20:40]</a>.
*   Visually identify open applications, such as a Python code editor with a file named `f.py` <a class="yt-timestamp" data-t="00:20:52">[00:20:52]</a>.
*   Understand the context of code within the editor, even identifying that it uses the Gemini API <a class="yt-timestamp" data-t="00:20:58">[00:20:58]</a>.
*   Diagnose coding errors, such as "no such file or directory" (indicating an incorrect file path) and "API key not valid" (identifying missing or incorrect API key placeholder text) <a class="yt-timestamp" data-t="00:21:42">[00:21:42]</a><a class="yt-timestamp" data-t="00:21:59">[00:21:59]</a>.
*   Offer real-time suggestions and guidance based on the observed context and user input <a class="yt-timestamp" data-t="00:21:13">[00:21:13]</a>.

While the conversation cadence might sometimes be rough, it showcases the raw capabilities of the multimodal live API <a class="yt-timestamp" data-t="00:22:40">[00:22:40]</a>. Fine-tuned controls are being developed to improve conversation flow, such as instructing the model to be less eager to interrupt <a class="yt-timestamp" data-t="00:22:53">[00:22:53]</a>.

## Key Capabilities and Integrations

The Multimodal Live API can be enhanced with:
*   **Native Tool Integration:** This includes setting up pseudo [[function_calling_and_tool_integration_in_ai_apps | function calls]] <a class="yt-timestamp" data-t="00:25:32">[00:25:32]</a>.
*   **Code Execution:** The model can spin up a Python virtual environment to run code and display its outputs <a class="yt-timestamp" data-t="00:25:39">[00:25:39]</a>.
*   **Grounding:** This feature allows the model to browse the internet to find results, bridging the outside world into the unified product experience <a class="yt-timestamp" data-t="00:25:51">[00:25:51]</a><a class="yt-timestamp" data-t="00:26:16">[00:26:16]</a>.

## Potential Applications

This real-time interaction capability unlocks a vast array of new product experiences and represents the future of work <a class="yt-timestamp" data-t="00:23:49">[00:23:49]</a><a class="yt-timestamp" data-t="00:24:52">[00:24:52]</a>.
*   **AI Co-presence:** It embodies the concept of [[techniques_to_enhance_ai_interface_design | AI co-presence]], where AI can provide continuous assistance <a class="yt-timestamp" data-t="00:23:20">[00:23:20]</a>.
*   **Enhanced IDEs:** Imagine an Integrated Development Environment (IDE) where a single button click transforms code autocomplete into a senior developer pair-programming alongside you, understanding your entire screen and browser in real-time <a class="yt-timestamp" data-t="00:23:24">[00:23:24]</a>.
*   **Democratizing Access to Learning:** This technology can help bridge the steep learning curve in technology, enabling individuals with limited technical familiarity, such as someone learning to code or edit videos, to receive real-time, personalized assistance as if they had a tutor <a class="yt-timestamp" data-t="00:24:03">[00:24:03]</a><a class="yt-timestamp" data-t="00:27:33">[00:27:33]</a>.
*   **Accelerating Work:** Users are likely to "sacrifice" some screen sharing for the benefit of getting work done 1.5x, 2x, or even 3x faster <a class="yt-timestamp" data-t="00:25:17">[00:25:17]</a>.

Logan Kilpatrick notes that even if the technology is "93% there" or "80% there," playing with the tools can spark connections and ideas for new use cases that might not otherwise have been considered <a class="yt-timestamp" data-t="00:26:52">[00:26:52]</a><a class="yt-timestamp" data-t="00:26:58">[00:26:58]</a>.

## Accessibility and Cost

The Multimodal Live API is available for free, allowing developers and startups to [[building_apps_using_ai_tools | build]] and prototype cool AI products without economic burden <a class="yt-timestamp" data-t="00:29:07">[00:29:07]</a><a class="yt-timestamp" data-t="00:29:12">[00:29:12]</a>.
*   It can be accessed and played with on `audio.Google.com` by selecting "stream real time" in the left-hand navigation <a class="yt-timestamp" data-t="00:28:01">[00:28:01]</a>.
*   Developers can obtain a free API key from AI Studio to power their own experiences, including the reasoning model within tools like Cursor <a class="yt-timestamp" data-t="00:12:50">[00:12:50]</a><a class="yt-timestamp" data-t="00:13:11">[00:13:11]</a>.
*   API keys provide access to 1.5 billion tokens across various Gemini models for free <a class="yt-timestamp" data-t="00:28:49">[00:28:49]</a>.