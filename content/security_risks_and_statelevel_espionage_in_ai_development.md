---
title: Security risks and state-level espionage in AI development
videoId: zdbVtZIn9IM
---

From: [[dwarkesh | The Dwarkesh Podcast]]

The development of Artificial General Intelligence (AGI) and superintelligence presents unprecedented geopolitical and national security challenges. The stakes involve not just technological advancement but the future global order and the survival of political systems like liberal democracy and the Chinese Communist Party (CCP) <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a> <a class="yt-timestamp" data-t="00:00:02">[00:00:02]</a>. This article outlines the security risks, particularly concerning state-level espionage, based on insights from Leopold Aschenbrenner.

## The Geopolitical Stakes of AI

AI, particularly AGI and superintelligence, is viewed as a technology that will be "absolutely decisive for national power" <a class="yt-timestamp" data-t="00:25:11">[00:25:11]</a>. This contrasts with the perception of AI as merely a new product category.

### Superintelligence as Decisive National Power
The realization by national security establishments that superintelligence is a decisive factor for national power is a critical turning point <a class="yt-timestamp" data-t="00:25:11">[00:25:11]</a>. A lead of even a couple of years in superintelligence could offer a decisive military advantage, potentially compressing a century's worth of technological progress into less than a decade <a class="yt-timestamp" data-t="00:27:01">[00:27:01]</a> <a class="yt-timestamp" data-t="00:27:41">[00:27:41]</a>. This could lead to advantages comparable to the Western coalition's technological superiority in the first Gulf War, which resulted in a 100:1 kill ratio due to a 20-30 year technological lead <a class="yt-timestamp" data-t="00:27:11">[00:27:11]</a>. Such an advantage could even preempt nuclear deterrence, for example, by developing capabilities to find and neutralize nuclear stealth submarines or other nuclear assets <a class="yt-timestamp" data-t="00:28:03">[00:28:03]</a> <a class="yt-timestamp" data-t="00:28:13">[00:28:13]</a>.

### The "Intelligence Explosion"
Once AGI is achieved, one of the first applications will be to automate AI research itself <a class="yt-timestamp" data-t="00:25:41">[00:25:41]</a>. With potentially hundreds of millions of human-equivalent automated AI researchers, a decade's worth of ML research progress could be achieved in a year, leading to a "10x speed up" <a class="yt-timestamp" data-t="00:26:10">[00:26:10]</a>. This rapid acceleration could result in AI systems vastly smarter than humans within a year or two <a class="yt-timestamp" data-t="00:26:15">[00:26:15]</a>. This "intelligence explosion" would then be applied to other R&D fields, including robotics [[intelligence_explosion_and_its_implications]] <a class="yt-timestamp" data-t="00:26:29">[00:26:29]</a> <a class="yt-timestamp" data-t="00:26:42">[00:26:42]</a>.

The potential for dictatorships to leverage superintelligence is particularly concerning. A perfectly loyal military and security force, perfect lie detection, and comprehensive surveillance could eliminate dissent and lock in authoritarian rule indefinitely [[ai_alignment_and_safety_concerns]] <a class="yt-timestamp" data-t="00:36:50">[00:36:50]</a>.

## The Threat of State-Level Espionage

As the strategic importance of AGI becomes clear, an "extremely intense international competition" is anticipated <a class="yt-timestamp" data-t="00:29:11">[00:29:11]</a>.

### CCP Efforts to Infiltrate and Outbuild
Aschenbrenner predicts an "all-out effort" by the CCP to infiltrate American AI labs, involving "billions of dollars, thousands of people, and the full force of the Ministry of State Security" <a class="yt-timestamp" data-t="00:00:06">[00:00:06]</a> <a class="yt-timestamp" data-t="00:28:38">[00:28:38]</a>. The CCP will also attempt to outbuild the US in terms of compute infrastructure, leveraging its capacity to rapidly expand power generation [[china_and_the_uss_race_in_ai_and_superintelligence]] <a class="yt-timestamp" data-t="00:00:13">[00:00:13]</a> <a class="yt-timestamp" data-t="00:28:49">[00:28:49]</a>. The intensity of state-level espionage is often underestimated <a class="yt-timestamp" data-t="00:00:13">[00:00:13]</a>.

### Vulnerabilities of AI Labs
Current security at AI labs is considered weak. DeepMind's Frontier Safety Framework, for instance, rates current security at "level zero" out of four, with level four being resistant to state activity <a class="yt-timestamp" data-t="00:58:20">[00:58:20]</a> <a class="yt-timestamp" data-t="00:58:28">[00:58:28]</a>. A recent indictment revealed an individual stole critical AI code from Google by simply copying it into Apple Notes and exporting it as a PDF, bypassing monitoring systems <a class="yt-timestamp" data-t="00:58:34">[00:58:34]</a>. Even Google, with arguably the best security among AI labs, is vulnerable; startups are likely even more so <a class="yt-timestamp" data-t="00:58:51">[00:58:51]</a> <a class="yt-timestamp" data-t="00:58:55">[00:58:55]</a>.

### What Can Be Stolen?
State actors can target several key assets:
1.  **AI Weights:** Stealing the final trained model weights is likened to getting "a direct copy of the atomic bomb" <a class="yt-timestamp" data-t="00:43:14">[00:43:14]</a> <a class="yt-timestamp" data-t="00:59:54">[00:59:54]</a>. If China, with its large compute capacity, could steal a US-developed superintelligence, they could rapidly deploy it <a class="yt-timestamp" data-t="01:00:09">[01:00:09]</a>. Weight security is crucial as AGI approaches [[ai_alignment_and_safety]] <a class="yt-timestamp" data-t="01:00:23">[01:00:23]</a>.
2.  **Secrets and Algorithms:** Algorithmic progress contributes significantly to AI capabilities (estimated at "half an order of magnitude a year") <a class="yt-timestamp" data-t="01:00:45">[01:00:45]</a>. Protecting these "secrets," especially breakthroughs like solutions to the "data wall" (e.g., advanced self-play RL techniques), is vital [[reinforcement_learning_from_human_feedback_rlhf]] <a class="yt-timestamp" data-t="01:01:09">[01:01:09]</a>. If China cannot steal these, they may be "stuck," whereas access allows them to rapidly catch up <a class="yt-timestamp" data-t="01:01:27">[01:01:27]</a>.
3.  **Tacit Knowledge:** While fundamental approaches can be communicated quickly <a class="yt-timestamp" data-t="01:01:45">[01:01:45]</a>, the engineering "schlep" and specific details to make large training runs work constitute tacit knowledge that is also valuable [[role_of_compute_in_ai_development]] <a class="yt-timestamp" data-t="01:02:04">[01:02:04]</a> <a class="yt-timestamp" data-t="01:03:15">[01:03:15]</a>.

The ease with which these assets can be stolen is high, and labs often don't claim their security is robust against state-level threats <a class="yt-timestamp" data-t="00:58:20">[00:58:20]</a>.

## Strategic Importance of Cluster Location

The physical location of AI training clusters is a major security concern <a class="yt-timestamp" data-t="00:00:28">[00:00:28]</a> <a class="yt-timestamp" data-t="00:42:30">[00:42:30]</a>.

### Risks of Locating Clusters in Authoritarian States
Building AGI/superintelligence clusters in authoritarian dictatorships, such as potentially the UAE, creates "irreversible security risk" <a class="yt-timestamp" data-t="00:42:54">[00:42:54]</a> <a class="yt-timestamp" data-t="00:43:07">[00:43:07]</a>. This is likened to conducting the Manhattan Project in the UAE <a class="yt-timestamp" data-t="00:42:54">[00:42:54]</a>. Risks include:
1.  **Exfiltration of Weights/Theft of AGI:** Easier access for hostile states to steal the AI models <a class="yt-timestamp" data-t="00:43:14">[00:43:14]</a>.
2.  **Seizure of Compute:** Authoritarian regimes could seize the physical compute clusters, especially during geopolitical tensions <a class="yt-timestamp" data-t="00:43:29">[00:43:29]</a> <a class="yt-timestamp" data-t="00:43:39">[00:43:39]</a>. Even 25% of global compute capacity in such hands could significantly alter the balance of power <a class="yt-timestamp" data-t="00:43:50">[00:43:50]</a>.
3.  **Implicit Leverage:** Giving authoritarian states a "seat at the AGI table" through hosting compute grants them undue influence over a critical technology <a class="yt-timestamp" data-t="00:44:09">[00:44:09]</a> <a class="yt-timestamp" data-t="00:44:13">[00:44:13]</a>.
4.  **Proliferation:** Stolen AGI or seized compute could be used to develop "crazy new WMDs" <a class="yt-timestamp" data-t="00:45:09">[00:45:09]</a> or be transferred to other adversarial nations like China <a class="yt-timestamp" data-t="00:43:25">[00:43:25]</a>.

There are reports of significant fundraising efforts for AI compute, including chip projects, involving Middle Eastern capital (e.g., Sam Altman's reported fundraising efforts) <a class="yt-timestamp" data-t="00:55:44">[00:55:44]</a>. Aschenbrenner also mentions an unconfirmed rumor that OpenAI leadership once considered funding AGI by "starting a bidding war between the governments of the United States, China, and Russia" <a class="yt-timestamp" data-t="00:56:00">[00:56:00]</a>.

The argument that "if we don't work with them, they'll go to China" is viewed skeptically. While benefit-sharing with a broader coalition is suggested (e.g., access to last-generation models), developing core AGI should be restricted to a "narrow coalition of democracies" <a class="yt-timestamp" data-t="00:54:40">[00:54:40]</a>.

## The Race for AI Supremacy

The development of AGI is framed as a race where lead time is critically important.

### The Importance of Lead Time
A few years, or even months, of lead in AGI/superintelligence development could be decisive <a class="yt-timestamp" data-t="00:27:01">[00:27:01]</a>. During the intelligence explosion, a one-year lead might equate to a five order-of-magnitude difference in capability, moving from human-level to vastly superhuman AI <a class="yt-timestamp" data-t="01:06:56">[01:06:56]</a>. This is because automated AI researchers could dramatically accelerate progress [[recursive_selfimprovement_and_ai_capabilities]] <a class="yt-timestamp" data-t="01:07:05">[01:07:05]</a>. A two-year lead, if secrets are protected, is considered achievable and would provide a significant buffer <a class="yt-timestamp" data-t="01:06:43">[01:06:43]</a>.

### Parallels to Historical Arms Races
The situation is compared to historical technological races, such as the development of the atomic bomb. A key factor in the German nuclear project's failure was their incorrect assessment of graphite as a moderator, a mistake compounded by Enrico Fermi's decision (at Leo Szilard's urging) not to publish results showing graphite would work [[the_making_and_impact_of_the_atomic_bomb]]. This secrecy prevented the Nazis from correcting their course <a class="yt-timestamp" data-t="01:03:36">[01:03:36]</a> - <a class="yt-timestamp" data-t="01:04:59">[01:04:59]</a>. Similarly, leaking how to overcome current AI challenges (like the "data wall") could erase a crucial lead <a class="yt-timestamp" data-t="01:05:07">[01:05:07]</a>.

### The Danger of a "Feverish Struggle"
A scenario where multiple powers are "neck and neck" in AGI development, with only a few months lead, is considered "incredibly dangerous" <a class="yt-timestamp" data-t="00:46:40">[00:46:40]</a> <a class="yt-timestamp" data-t="01:08:01">[01:08:01]</a>. Such a "feverish struggle" would incentivize nations to "throw all caution to the wind," potentially ignoring safety and alignment concerns to avoid falling behind <a class="yt-timestamp" data-t="01:08:12">[01:08:12]</a>. This could lead to a highly unstable world with rapidly evolving WMDs and constantly shifting deterrence dynamics [[challenges_in_ai_alignment_and_potential_risks]]. A significant lead provides "wiggle room" to address safety and alignment properly <a class="yt-timestamp" data-t="00:45:55">[00:45:55]</a> <a class="yt-timestamp" data-t="01:08:42">[01:08:42]</a>.

## Securing AI Development

The current security posture of AI labs is inadequate against determined state-level actors <a class="yt-timestamp" data-t="00:58:28">[00:58:28]</a>. Enhancing security for AI secrets, algorithms, and weights to a level resistant to Chinese espionage will require an intense, multi-year effort, not just simple access controls <a class="yt-timestamp" data-t="01:00:30">[01:00:30]</a>. Locking down the labs effectively is seen as crucial to maintaining a US lead <a class="yt-timestamp" data-t="01:06:45">[01:06:45]</a>.

## Conclusion

The development of AGI and superintelligence is poised to trigger intense geopolitical competition and significant national security risks. State-level espionage, particularly from actors like the CCP, poses a severe threat to labs developing these technologies. The physical location of compute clusters and the security of AI models, algorithms, and underlying research are paramount [[geopolitical_implications_on_technology_and_data_centers]]. A failure to secure these assets could cede a decisive strategic advantage to adversaries, with potentially catastrophic consequences for global stability and the future of governance. The decisions made in the coming years regarding AI security and international collaboration will be critical in navigating this perilous period <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a> <a class="yt-timestamp" data-t="01:09:01">[01:09:01]</a>.