---
title: The role of CUDA and TensorFlow in machine learning software development
videoId: t21REMsFJ_4
---

From: [[hu-po]] <br/> 

Over the past decade, the landscape of machine learning software development has undergone significant transformations. Historically, two major players, NVIDIA's CUDA and Google's TensorFlow, dominated the field, shaping how machine learning models were developed and deployed <a class="yt-timestamp" data-t="02:04:00">[02:04:00]</a>.

## NVIDIA's CUDA Monopoly

For a significant period, most machine learning frameworks heavily relied on leveraging NVIDIA's CUDA, performing optimally on NVIDIA GPUs <a class="yt-timestamp" data-t="02:11:00">[02:11:00]</a>. This was evident in the career of many deep learning practitioners, who consistently used NVIDIA GPUs <a class="yt-timestamp" data-t="02:25:00">[02:25:00]</a>. NVIDIA's dominant position was largely due to its software "moat" â€” the closed-source CUDA libraries that optimized code for its [[Hardware for AI training and deployment | hardware]] <a class="yt-timestamp" data-t="03:36:00">[03:36:00]</a> <a class="yt-timestamp" data-t="05:36:00">[05:36:00]</a>.

However, the efficiency of NVIDIA's GPUs, while vastly increasing computational power (flops), has created a new bottleneck: memory bandwidth <a class="yt-timestamp" data-t="14:46:00">[14:46:00]</a>. Despite tensor cores significantly increasing matrix multiplication speed, the GPU spends a majority of its time (up to 60%) waiting for data to be shuffled between different memory caches <a class="yt-timestamp" data-t="10:54:00">[10:54:00]</a> <a class="yt-timestamp" data-t="11:51:00">[11:51:00]</a> <a class="yt-timestamp" data-t="31:41:00">[31:41:00]</a>. This "memory wall" means that simply increasing GPU flops doesn't proportionally increase performance <a class="yt-timestamp" data-t="29:52:00">[29:52:00]</a> <a class="yt-timestamp" data-t="30:31:00">[30:31:00]</a>.

Optimizing for this memory bottleneck often involves writing custom CUDA kernels, which is significantly more difficult than writing simple Python scripts <a class="yt-timestamp" data-t="41:01:00">[41:01:00]</a>. CUDA is primarily used by specialists in accelerated computing and requires a deep understanding of [[Hardware for AI training and deployment | hardware]] architecture, making it less accessible to typical machine learning researchers and scientists <a class="yt-timestamp" data-t="06:09:00">[06:09:00]</a> <a class="yt-timestamp" data-t="06:46:00">[06:46:00]</a>. This reliance often means machine learning experts depend on CUDA experts to modify and optimize their code <a class="yt-timestamp" data-t="06:51:00">[06:51:00]</a> <a class="yt-timestamp" data-t="07:01:00">[07:01:00]</a>.

## TensorFlow's Trajectory

A few years ago, the machine learning framework ecosystem was fragmented, with TensorFlow as a front-runner, often on par with, or even larger than, [[Advancements in PyTorch 20 and its potential ability to operate on various hardware | PyTorch]] <a class="yt-timestamp" data-t="05:55:00">[05:55:00]</a>. Google appeared poised to control the machine learning industry, having a first-mover advantage, the most commonly used framework (TensorFlow), and successful [[Applications in Machine Learning and AI | AI application-specific accelerator]]s (TPUs) <a class="yt-timestamp" data-t="06:10:00">[06:10:00]</a> <a class="yt-timestamp" data-t="06:18:00">[06:18:00]</a>.

However, Google failed to convert this initial advantage into dominance <a class="yt-timestamp" data-t="07:07:00">[07:07:00]</a>. By 2022, TensorFlow's market share in research papers had significantly decreased from roughly 40% to a very small share, while [[Advancements in PyTorch 20 and its potential ability to operate on various hardware | PyTorch]] rose to nearly 50% <a class="yt-timestamp" data-t="03:17:00">[03:17:00]</a> <a class="yt-timestamp" data-t="03:22:00">[03:22:00]</a> <a class="yt-timestamp" data-t="03:25:00">[03:25:00]</a>. Conferences like ICLR, CVPR, and NeurIPS showed a dramatic shift, with PyTorch unique mentions growing from 10% in 2017 to 70-80% by 2020 <a class="yt-timestamp" data-t="06:44:00">[06:44:00]</a> <a class="yt-timestamp" data-t="06:50:00">[06:50:00]</a> <a class="yt-timestamp" data-t="06:52:00">[06:52:00]</a> <a class="yt-timestamp" data-t="06:54:00">[06:54:00]</a>.

The primary reason for TensorFlow's loss of ground was its increased flexibility and usability compared to PyTorch <a class="yt-timestamp" data-t="08:33:00">[08:33:00]</a> <a class="yt-timestamp" data-t="08:36:00">[08:36:00]</a>. TensorFlow was initially designed with a compiled code mindset, requiring users to create a graph that then needed compilation to run <a class="yt-timestamp" data-t="08:42:00">[08:42:00]</a> <a class="yt-timestamp" data-t="08:44:00">[08:44:00]</a>. This graph-based approach made it challenging to understand and debug code, as issues only became apparent after graph compilation <a class="yt-timestamp" data-t="09:43:00">[09:43:00]</a> <a class="yt-timestamp" data-t="09:46:00">[09:46:00]</a>.

In contrast, [[Advancements in PyTorch 20 and its potential ability to operate on various hardware | PyTorch]] offered a more Python-like, eager execution workflow, where code is read and executed line by line, similar to a scripting language <a class="yt-timestamp" data-t="09:01:00">[09:01:00]</a> <a class="yt-timestamp" data-t="09:04:00">[09:04:00]</a> <a class="yt-timestamp" data-t="09:06:00">[09:06:00]</a>. Although TensorFlow later introduced an eager mode by default, the research community had largely embraced [[Advancements in PyTorch 20 and its potential ability to operate on various hardware | PyTorch]] <a class="yt-timestamp" data-t="10:00:00">[10:00:00]</a> <a class="yt-timestamp" data-t="10:01:00">[10:01:00]</a> <a class="yt-timestamp" data-t="10:03:00">[10:03:00]</a>. This is further exemplified by the fact that nearly every generative [[Applications in Machine Learning and AI | AI]] model that made headlines is based on [[Advancements in PyTorch 20 and its potential ability to operate on various hardware | PyTorch]], while Google's generative [[Applications in Machine Learning and AI | AI]] models (like Imagen and DreamFusion) are based on JAX, a framework that directly competes with TensorFlow <a class="yt-timestamp" data-t="10:08:00">[10:08:00]</a> <a class="yt-timestamp" data-t="10:10:00">[10:10:00]</a> <a class="yt-timestamp" data-t="10:17:00">[10:17:00]</a> <a class="yt-timestamp" data-t="10:19:00">[10:19:00]</a>.

Google remains at the forefront of advanced machine learning models, having invented Transformers and maintaining state-of-the-art results in many areas with models like PaLM, LaMDA, and Chinchilla <a class="yt-timestamp" data-t="08:18:00">[08:18:00]</a> <a class="yt-timestamp" data-t="08:22:00">[08:22:00]</a> <a class="yt-timestamp" data-t="08:23:00">[08:23:00]</a>. However, the company is somewhat isolated within the broader machine learning community due to its preference for its own software stack and [[Hardware for AI training and deployment | hardware]] <a class="yt-timestamp" data-t="07:13:00">[07:13:00]</a> <a class="yt-timestamp" data-t="07:16:00">[07:16:00]</a>.

## The Shifting Landscape: PyTorch 2.0 and OpenAI Triton

The dominance of NVIDIA's CUDA and the decline of TensorFlow's market share are being disrupted by the advent of [[Advancements in PyTorch 20 and its potential ability to operate on various hardware | PyTorch 2.0]] and [[Nvidias GPU dominance and the impact of PyTorch 20 and OpenAI Triton | OpenAI's Triton]] <a class="yt-timestamp" data-t="02:32:00">[02:32:00]</a> <a class="yt-timestamp" data-t="02:34:00">[02:34:00]</a>. These developments are heralding a new age for deep learning [[Hardware for AI training and deployment | hardware]] <a class="yt-timestamp" data-t="01:43:00">[01:43:00]</a> <a class="yt-timestamp" data-t="01:46:00">[01:46:00]</a>. The shift is towards an open-source software stack for machine learning models, moving away from closed-source CUDA <a class="yt-timestamp" data-t="05:33:00">[05:33:00]</a> <a class="yt-timestamp" data-t="05:34:00">[05:34:00]</a>.

[[Advancements in PyTorch 20 and its potential ability to operate on various hardware | PyTorch 2.0]], released for early testing in late 2022 with full availability in March 2023, is a major catalyst <a class="yt-timestamp" data-t="48:11:00">[48:11:00]</a> <a class="yt-timestamp" data-t="48:13:00">[48:13:00]</a> <a class="yt-timestamp" data-t="48:16:00">[48:16:00]</a> <a class="yt-timestamp" data-t="48:20:00">[48:20:00]</a>. Its primary innovation is the addition of a compiled solution that supports graph execution <a class="yt-timestamp" data-t="48:29:00">[48:29:00]</a> <a class="yt-timestamp" data-t="48:31:00">[48:31:00]</a>. This approach makes it significantly easier to properly utilize various [[Hardware for AI training and deployment | hardware]] resources <a class="yt-timestamp" data-t="48:35:00">[48:35:00]</a> <a class="yt-timestamp" data-t="48:37:00">[48:37:00]</a>.

[[Advancements in PyTorch 20 and its potential ability to operate on various hardware | PyTorch 2.0]] brings an 86% performance improvement for training on NVIDIA's A100 GPUs and a 26% improvement on CPUs for inference <a class="yt-timestamp" data-t="48:48:00">[48:48:00]</a> <a class="yt-timestamp" data-t="48:51:00">[48:51:00]</a> <a class="yt-timestamp" data-t="48:53:00">[48:53:00]</a> <a class="yt-timestamp" data-t="48:58:00">[48:58:00]</a>. Crucially, these benefits are expected to extend to other GPUs and accelerators from companies like AMD, Intel, Tesla, Google, and Amazon, among many others <a class="yt-timestamp" data-t="49:04:00">[49:04:00]</a> <a class="yt-timestamp" data-t="49:06:00">[49:06:00]</a> <a class="yt-timestamp" data-t="49:09:00">[49:09:00]</a> <a class="yt-timestamp" data-t="49:12:00">[49:12:00]</a>.

This shift is partly driven by major firms like Meta, who are heavily contributing to [[Advancements in PyTorch 20 and its potential ability to operate on various hardware | PyTorch]] to achieve higher flops utilization with less effort on their multi-billion dollar training clusters <a class="yt-timestamp" data-t="49:39:00">[49:39:00]</a> <a class="yt-timestamp" data-t="49:41:00">[49:41:00]</a> <a class="yt-timestamp" data-t="49:43:00">[49:43:00]</a>. These companies also want to make their software stack more portable to other [[Hardware for AI training and deployment | hardware]] to foster competition <a class="yt-timestamp" data-t="49:54:00">[49:54:00]</a> <a class="yt-timestamp" data-t="49:55:00">[49:55:00]</a>.

### The Role of Compilers and Operator Fusion

The move to a graph-based execution model in [[Advancements in PyTorch 20 and its potential ability to operate on various hardware | PyTorch 2.0]] addresses the memory wall by enabling "operator fusion" <a class="yt-timestamp" data-t="37:08:00">[37:08:00]</a> <a class="yt-timestamp" data-t="37:09:00">[37:09:00]</a>. Instead of executing each operation separately, writing intermediate results to memory, multiple operations are fused into a single pass <a class="yt-timestamp" data-t="37:11:00">[37:11:00]</a> <a class="yt-timestamp" data-t="37:13:00">[37:13:00]</a>. This drastically reduces the back-and-forth memory transfers, which are the main bottleneck <a class="yt-timestamp" data-t="39:59:00">[39:59:00]</a> <a class="yt-timestamp" data-t="40:01:00">[40:01:00]</a>.

[[Advancements in PyTorch 20 and its potential ability to operate on various hardware | PyTorch 2.0]] achieves this through components like:

*   **Torch Dynamo:** This robust graph definition tool ingests any [[Advancements in PyTorch 20 and its potential ability to operate on various hardware | PyTorch]] user script and generates a computational graph <a class="yt-timestamp" data-t="55:38:00">[55:38:00]</a> <a class="yt-timestamp" data-t="55:41:00">[55:41:00]</a> <a class="yt-timestamp" data-t="55:43:00">[55:43:00]</a>. It lowers complex [[Hardware developments in machine learning including innovations by companies like Tesla Google and Apple | PyTorch]] operations to a core set of 250 primitive operations <a class="yt-timestamp" data-t="56:02:00">[56:02:00]</a> <a class="yt-timestamp" data-t="56:04:00">[56:04:00]</a> <a class="yt-timestamp" data-t="56:07:00">[56:07:00]</a>. Dynamo also supports dynamic shapes natively, making it easier to vary sequence lengths for large language models (LLMs) <a class="yt-timestamp" data-t="52:00:00">[52:00:00]</a> <a class="yt-timestamp" data-t="52:02:00">[52:02:00]</a> <a class="yt-timestamp" data-t="52:04:00">[52:04:00]</a>.
*   **Torch Inductor:** This Python-native deep learning compiler takes the graph generated by Dynamo and moves to a scheduling phase. It fuses operators and determines memory planning to minimize memory access <a class="yt-timestamp" data-t="01:02:14:00">[01:02:14:00]</a> <a class="yt-timestamp" data-t="01:02:27:00">[01:02:27:00]</a> <a class="yt-timestamp" data-t="01:02:46:00">[01:02:46:00]</a> <a class="yt-timestamp" data-t="01:02:50:00">[01:02:50:00]</a>. Inductor then generates code that runs on CPUs, GPUs, or other [[Hardware for AI training and deployment | AI accelerators]], dramatically reducing the work for compiler teams building for new [[Hardware for AI training and deployment | hardware]] <a class="yt-timestamp" data-t="01:03:34:00">[01:03:34:00]</a> <a class="yt-timestamp" data-t="01:03:36:00">[01:03:36:00]</a> <a class="yt-timestamp" data-t="01:04:05:00">[01:04:05:00]</a> <a class="yt-timestamp" data-t="01:04:07:00">[01:04:07:00]</a>.
*   [[Nvidias GPU dominance and the impact of PyTorch 20 and OpenAI Triton | OpenAI Triton]]: This highly disruptive tool to NVIDIA's closed-source CUDA model directly generates PTX code for NVIDIA GPUs, bypassing NVIDIA's proprietary CUDA libraries (like CuBLAS) in favor of [[Opensource AI and its implications | open-source]] alternatives (like Cutlass) <a class="yt-timestamp" data-t="01:04:51:00">[01:04:51:00]</a> <a class="yt-timestamp" data-t="01:04:53:00">[01:04:53:00]</a> <a class="yt-timestamp" data-t="01:05:14:00">[01:05:14:00]</a> <a class="yt-timestamp" data-t="01:05:17:00">[01:05:17:00]</a> <a class="yt-timestamp" data-t="01:05:19:00">[01:05:19:00]</a>. [[Nvidias GPU dominance and the impact of PyTorch 20 and OpenAI Triton | Triton]] enables higher-level languages (like Python) to achieve performance comparable to lower-level languages, and its kernels are legible to typical ML researchers, greatly improving usability <a class="yt-timestamp" data-t="01:07:52:00">[01:07:52:00]</a> <a class="yt-timestamp" data-t="01:07:54:00">[01:07:54:00]</a> <a class="yt-timestamp" data-t="01:07:55:00">[01:07:55:00]</a> <a class="yt-timestamp" data-t="01:08:00:00">[01:08:00:00]</a>.

This integrated approach means that software can be written in a user-friendly manner while benefiting from significant performance improvements through automatic compilation and optimization <a class="yt-timestamp" data-t="01:01:11:00">[01:01:11:00]</a> <a class="yt-timestamp" data-t="01:01:12:00">[01:01:12:00]</a> <a class="yt-timestamp" data-t="01:01:15:00">[01:01:15:00]</a> <a class="yt-timestamp" data-t="01:01:16:00">[01:01:16:00]</a>. It allows for more efficient parallelization over a large base of computational resources <a class="yt-timestamp" data-t="01:01:21:00">[01:01:21:00]</a> <a class="yt-timestamp" data-t="01:01:23:00">[01:01:23:00]</a>. The ability for other [[Hardware for AI training and deployment | hardware]] accelerators to integrate directly into [[Nvidias GPU dominance and the impact of PyTorch 20 and OpenAI Triton | Triton]] dramatically reduces the time to build an [[Applications in Machine Learning and AI | AI]] compiler stack for a new piece of [[Hardware for AI training and deployment | hardware]], opening up the market for [[Hardware for AI training and deployment | AI hardware]] and custom ASICs <a class="yt-timestamp" data-t="01:08:40:00">[01:08:40:00]</a> <a class="yt-timestamp" data-t="01:08:43:00">[01:08:43:00]</a> <a class="yt-timestamp" data-t="01:08:46:00">[01:08:46:00]</a> <a class="yt-timestamp" data-t="01:08:48:00">[01:08:48:00]</a>.

## Conclusion

The shift away from closed-source, [[Hardware for AI training and deployment | hardware]]-specific solutions like CUDA, and the declining dominance of frameworks like TensorFlow, marks a significant evolution in machine learning software development <a class="yt-timestamp" data-t="05:33:00">[05:33:00]</a> <a class="yt-timestamp" data-t="05:34:00">[05:34:00]</a> <a class="yt-timestamp" data-t="07:07:00">[07:07:00]</a>. [[Advancements in PyTorch 20 and its potential ability to operate on various hardware | PyTorch 2.0]] and [[Nvidias GPU dominance and the impact of PyTorch 20 and OpenAI Triton | OpenAI's Triton]] are driving a future where the software stack for machine learning is more portable and [[Opensource AI and its implications | open-source]] <a class="yt-timestamp" data-t="01:06:06:00">[01:06:06:00]</a> <a class="yt-timestamp" data-t="01:09:06:00">[01:09:06:00]</a>. This fosters greater competition in the [[Hardware for AI training and deployment | AI hardware]] market, as the ease of use afforded by NVIDIA's proprietary software is diminishing in importance compared to the economics and architecture of competing chip solutions <a class="yt-timestamp" data-t="01:09:08:00">[01:09:08:00]</a> <a class="yt-timestamp" data-t="01:09:10:00">[01:09:10:00]</a> <a class="yt-timestamp" data-t="01:09:51:00">[01:09:51:00]</a>. The market is becoming more open, allowing different winners to emerge as the industry continues to evolve <a class="yt-timestamp" data-t="01:12:07:00">[01:12:07:00]</a> <a class="yt-timestamp" data-t="01:12:10:00">[01:12:10:00]</a> <a class="yt-timestamp" data-t="01:12:13:00">[01:12:13:00]</a>.