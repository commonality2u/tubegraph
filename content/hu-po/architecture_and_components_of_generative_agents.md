---
title: Architecture and components of generative agents
videoId: WjLpCgOX7qY
---

From: [[hu-po]] <br/> 

Generative agents are interactive computational agents designed to simulate believable human behavior <a class="yt-timestamp" data-t="00:01:28">[00:01:28]</a>. This concept was presented in a recent paper by researchers from Stanford and Google <a class="yt-timestamp" data-t="00:01:09">[00:01:09]</a>. These agents are designed to populate a sandbox environment, reminiscent of The Sims or classic *Pokemon* games, with up to 25 agents <a class="yt-timestamp" data-t="00:01:37">[00:01:37]</a>.

The agents are primarily built using Large Language Models (LLMs), specifically GPT 3.5 Turbo <a class="yt-timestamp" data-t="00:43:38">[00:43:38]</a>, and are capable of emergent social behaviors such as sharing news, planning their days, forming relationships, and coordinating group activities <a class="yt-timestamp" data-t="00:02:16">[00:02:16]</a>. This technology is seen as a "proto version" for future gaming experiences that will be more like persistent virtual worlds rather than linear stories <a class="yt-timestamp" data-t="00:03:33">[00:03:33]</a>.

## Core Architecture

The architecture of [[generative_agents_and_simulation_of_human_behavior | generative agents]] is designed to create believable individual and emergent social behaviors <a class="yt-timestamp" data-t="00:04:13">[00:04:13]</a>. It combines LLMs with mechanisms for synthesizing and retrieving relevant information <a class="yt-timestamp" data-t="00:42:24">[00:42:24]</a>. The system utilizes a "sense-plan-act" loop, which the paper renames to "observation, planning, and reflection" <a class="yt-timestamp" data-t="00:04:52">[00:04:52]</a>.

The three main components of the agent architecture are:
*   **Memory Stream** <a class="yt-timestamp" data-t="00:08:01">[00:08:01]</a>
*   **Reflection** <a class="yt-timestamp" data-t="00:08:26">[00:08:26]</a>
*   **Planning** <a class="yt-timestamp" data-t="00:08:47">[00:08:47]</a>

Each agent possesses its own LLM <a class="yt-timestamp" data-t="00:40:09">[00:40:09]</a>. The full architecture performs better across various interview tasks compared to models lacking these components <a class="yt-timestamp" data-t="01:08:26">[01:08:26]</a>.

### Memory Stream

The memory stream serves as a comprehensive, long-term memory that records all of an agent's experiences in natural language <a class="yt-timestamp" data-t="00:08:07">[00:08:07]</a>. Observations, which are the most basic elements, include behaviors and environmental changes, all stored as text (e.g., "Maria is studying for a chemistry test while drinking coffee" <a class="yt-timestamp" data-t="00:46:06">[00:46:06]</a>, "the refrigerator is empty" <a class="yt-timestamp" data-t="00:46:20">[00:46:20]</a>).

To manage the vast amount of accumulated memories, a **retrieval model** is used <a class="yt-timestamp" data-t="00:08:14">[00:08:14]</a>. This model combines three factors to determine which memories are most relevant to the current situation:
*   **Recency**: Memories that occurred more recently are given higher scores, calculated as an exponential decay function over time <a class="yt-timestamp" data-t="00:48:19">[00:48:19]</a>.
*   **Importance**: The agent itself assigns an importance score (1-10) to memories, distinguishing mundane events (e.g., eating breakfast) from significant ones (e.g., a breakup) <a class="yt-timestamp" data-t="00:49:00">[00:49:00]</a>.
*   **Relevance**: This score measures the cosine similarity between the embedding vector of a memory and the embedding vector of the current query or situation <a class="yt-timestamp" data-t="00:50:24">[00:50:24]</a>. This is analogous to how [[generative_latent_space_reasoning | vector databases]] operate <a class="yt-timestamp" data-t="00:50:35">[00:50:35]</a>.

The total score for a memory is the sum of its recency, importance, and relevance scores <a class="yt-timestamp" data-t="00:50:46">[00:50:46]</a>.

### Reflection

Reflections are higher-level, more abstract thoughts generated by the agent based on its memories <a class="yt-timestamp" data-t="00:52:50">[00:52:50]</a>. This process synthesizes lower-level observations into broader inferences over time <a class="yt-timestamp" data-t="00:08:26">[00:08:26]</a>. For example, an agent named Klaus, by consistently reading papers, might reflect and conclude that "Klaus is a little LLM that reads a lot of papers" and is "highly dedicated to his research" <a class="yt-timestamp" data-t="00:51:52">[00:51:52]</a>.

Reflections are generated periodically, roughly two to three times a day <a class="yt-timestamp" data-t="00:53:01">[00:53:01]</a>, and are recursively built upon both observations and other reflections, allowing for a deeper level of self-awareness and understanding <a class="yt-timestamp" data-t="00:54:06">[00:54:06]</a>. These reflections are then fed back into the memory stream to influence future behavior <a class="yt-timestamp" data-t="00:09:29">[00:09:29]</a>.

### Planning

Agents need to plan over longer time horizons to ensure coherent and believable sequences of actions <a class="yt-timestamp" data-t="00:54:26">[00:54:26]</a>. The planning process starts top-down, generating broad strokes for the day (e.g., "wake up," "go to college," "work on music") and then recursively adds more specific details <a class="yt-timestamp" data-t="00:56:39">[00:56:39]</a>. This internal plan helps agents avoid repetitive behaviors, such as eating lunch multiple times in a row <a class="yt-timestamp" data-t="00:55:35">[00:55:35]</a>.

Plans are dynamically updated based on new observations and memories <a class="yt-timestamp" data-t="02:24:47">[02:24:47]</a>. For instance, if an agent's father unexpectedly returns home, the agent might adjust its current plan to interact with him <a class="yt-timestamp" data-t="00:58:03">[00:58:03]</a>.

### Perception and Interaction

The agents operate within a simulated "Smallville" environment, which includes houses, groceries, a college, a bar, and a park <a class="yt-timestamp" data-t="02:40:08">[02:40:08]</a>. This world is represented as a tree data structure, where nodes describe areas and contain objects <a class="yt-timestamp" data-t="00:24:31">[00:24:31]</a>.

Agents are not omniscient; their internal model of the world updates as they move through different areas <a class="yt-timestamp" data-t="01:02:35">[01:02:35]</a>. The sandbox server sends information about agents and objects within an agent's visual range to that agent's memory <a class="yt-timestamp" data-t="01:01:21">[01:01:21]</a>.

Interactions occur in full natural language <a class="yt-timestamp" data-t="00:29:06">[00:29:06]</a>. Agents are aware of others in their local area and use their architecture to decide whether to walk by or engage in conversation <a class="yt-timestamp" data-t="00:29:12">[00:29:12]</a>. When interacting, agents condition their utterances on memories about each other, using their self-narrative and understanding of relationships to guide dialogue <a class="yt-timestamp" data-t="00:59:01">[00:59:01]</a>. For example, John, a caring father, might ask Eddie about his schoolwork <a class="yt-timestamp" data-t="00:59:36">[00:59:36]</a>.

Users can interact with the agents by taking on a persona (e.g., a news reporter) or by directly commanding an agent by acting as its "inner voice" <a class="yt-timestamp" data-t="00:30:53">[00:30:53]</a>. The system can also translate natural language actions into concrete movements and display them in the sandbox world using emojis <a class="yt-timestamp" data-t="00:28:04">[00:28:04]</a>.

## Emergent Behaviors

The system demonstrates significant emergent social dynamics <a class="yt-timestamp" data-t="00:04:13">[00:04:13]</a>:
*   **Party Coordination**: A single prompt to an agent about throwing a party can lead to that agent inviting others, who then spread the news, and some even coordinate dates to attend <a class="yt-timestamp" data-t="01:00:15">[01:00:15]</a>. An agent might even decide to decorate early for a party it's hosting <a class="yt-timestamp" data-t="00:39:31">[00:39:31]</a>.
*   **Information Diffusion**: Information, such as a mayoral candidacy or a party invitation, spreads naturally through conversations among agents <a class="yt-timestamp" data-t="01:12:40">[01:12:40]</a>. Not all invited agents will attend, mirroring real-world social dynamics <a class="yt-timestamp" data-t="01:14:45">[01:14:45]</a>.
*   **Human-like flaws**: Agents can "hallucinate" or embellish memories <a class="yt-timestamp" data-t="01:10:31">[01:10:31]</a>, forget information <a class="yt-timestamp" data-t="01:11:05">[01:11:05]</a>, or exhibit "erratic behaviors" like trying to enter a closed store <a class="yt-timestamp" data-t="01:16:41">[01:16:41]</a>. They can also assimilate interests from their community, reflecting human tendencies <a class="yt-timestamp" data-t="01:17:12">[01:17:12]</a>.

These complex behaviors arise from the [[generative_models_and_visual_space_reasoning | generative model's]] ability to synthesize memories and continually update its internal state, rather than being explicitly pre-programmed <a class="yt-timestamp" data-t="00:36:06">[00:36:06]</a>. This marks a significant step forward in simulating realistic human-like behavior <a class="yt-timestamp" data-t="02:11:09">[02:11:09]</a>.