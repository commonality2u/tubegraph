---
title: Finetuning Pretrained Models for Depth Estimation
videoId: WoiI_Pn9yHw
---

From: [[hu-po]] <br/> 

[[Monocular Depth Estimation using Diffusion Models | Monocular depth estimation]] is a fundamental computer vision task that involves recovering 3D depth from a single 2D image <a class="yt-timestamp" data-t="09:37:37">[09:37:37]</a>. Historically, early methods for depth estimation relied on camera geometry and stereo or multi-view setups using multiple cameras <a class="yt-timestamp" data-t="05:27:00">[05:27:00]</a>. However, modern approaches increasingly focus on [[monocular_depth_estimation_using_diffusion_models | monocular depth estimation]], often using deep learning models <a class="yt-timestamp" data-t="06:01:03">[06:01:03]</a>.

A significant challenge in [[monocular_depth_estimation_using_diffusion_models | monocular depth estimation]] models is their tendency to struggle with unfamiliar content or out-of-distribution data, as their knowledge is limited by their training datasets <a class="yt-timestamp" data-t="11:06:58">[11:06:58]</a>. This often leads to poor zero-shot generalization to new domains <a class="yt-timestamp" data-t="11:34:00">[11:34:00]</a>. For example, a model trained exclusively on autonomous vehicle datasets might perform poorly on images of dogs <a class="yt-timestamp" data-t="12:04:00">[12:04:00]</a>.

## Leveraging Diffusion Models for Depth Estimation

A promising approach to address the generalization challenge is to leverage the extensive priors captured in recent generative diffusion models <a class="yt-timestamp" data-t="12:24:26">[12:24:26]</a>. These "Foundation models" are large general networks trained on internet-scale data, such as LAION-5B <a class="yt-timestamp" data-t="23:57:58">[23:57:58]</a>, and thus possess a comprehensive representation of the visual world, including an inherent understanding of depth <a class="yt-timestamp" data-t="12:12:00">[12:12:00]</a>.

### Marigold: Repurposing Diffusion-Based Image Generators

The paper "Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation," published in December 2023 by ETH Zurich, introduces **Marigold**, a method for [[monocular_depth_estimation_using_diffusion_models | monocular depth estimation]] that [[finetuning_machine_learning_models | finetunes]] pre-trained diffusion models <a class="yt-timestamp" data-t="03:20:22">[03:20:22]</a>.

Marigold casts [[monocular_depth_estimation_using_diffusion_models | monocular depth estimation]] as a conditional denoising diffusion generation task, where the process is conditioned on the input image <a class="yt-timestamp" data-t="25:09:11">[25:09:11]</a>.

#### Finetuning Protocol

The core strategy involves [[finetuning_machine_learning_models | finetuning]] a pre-trained latent diffusion model, specifically Stable Diffusion 2 (LDM) <a class="yt-timestamp" data-t="31:59:02">[31:59:02]</a>. The training is remarkably efficient, taking only a couple of days on a single GPU <a class="yt-timestamp" data-t="16:08:10">[16:08:10]</a>.

The key aspects of Marigold's [[finetuning_machine_learning_models | finetuning]] protocol include:

1.  **Frozen VAE and Latent Space:** The Variational AutoEncoder (VAE), which encodes images into a low-dimensional latent space and decodes them back, is kept frozen <a class="yt-timestamp" data-t="31:53:00">[31:53:00]</a>. This is critical for computational efficiency <a class="yt-timestamp" data-t="30:03:04">[30:03:04]</a>. Surprisingly, Marigold successfully encodes single-channel depth images using a VAE trained exclusively on three-channel real images from LAION-5B, by duplicating the depth channel into three RGB channels <a class="yt-timestamp" data-t="32:28:31">[32:28:31]</a>. This "magic" works with negligible reconstruction error <a class="yt-timestamp" data-t="34:55:56">[34:55:56]</a>.
2.  **Modified Denoising UNet:** The denoising UNet, the part of the diffusion model responsible for predicting and removing noise, is modified and [[finetuning_machine_learning_models | finetuned]] <a class="yt-timestamp" data-t="18:41:43">[18:41:43]</a>. The UNet takes both the encoded image and the encoded depth latent as input, concatenated along the feature dimension <a class="yt-timestamp" data-t="37:28:31">[37:28:31]</a>. To accommodate the expanded input, the input channels of the UNet are doubled. To prevent activation magnitude inflation, the weight tensors are duplicated and their values are divided by two <a class="yt-timestamp" data-t="40:14:14">[40:14:14]</a>.
3.  **Affine Invariant Depth Normalization:** For training, ground truth depth maps are normalized to a value range of -1 to 1 <a class="yt-timestamp" data-t="42:29:00">[42:29:00]</a>. This makes the model produce "affine invariant" depth, meaning it predicts relative distances (e.g., one pixel is closer than another) but not true metric distances (e.g., in meters) <a class="yt-timestamp" data-t="13:27:00">[13:27:00]</a>. While this simplifies the problem, it means the output requires an extra step to recover real-world scale if needed for applications like SLAM <a class="yt-timestamp" data-t="17:50:00">[17:50:00]</a>.
4.  **Synthetic Data Training:** Marigold is trained primarily on synthetic data from datasets like Hypersim and Virtual KITTI <a class="yt-timestamp" data-t="19:00:00">[19:00:00]</a>. This is because real-world depth datasets often suffer from missing values, noise, and artifacts due to the physical limitations of capture sensors (e.g., LiDAR, structured light sensors) <a class="yt-timestamp" data-t="47:18:00">[47:18:00]</a>. Synthetic depth, being generated from explicit scene representations in game engines, is inherently dense and perfectly clean, reducing noise in gradient updates during [[finetuning_machine_learning_models | finetuning]] <a class="yt-timestamp" data-t="51:38:00">[51:38:00]</a>.
5.  **Multi-resolution Noise and Accelerated Inference:** The model utilizes multi-resolution Gaussian noise for better detail capture <a class="yt-timestamp" data-t="55:07:00">[55:07:00]</a>. For inference, Marigold follows the DDIM (Denoising Diffusion Implicit Models) approach, using non-Markovian sampling with respaced steps for accelerated inference <a class="yt-timestamp" data-t="56:42:00">[56:42:00]</a>. This allows the model to skip intermediate steps in the denoising process, significantly speeding up inference <a class="yt-timestamp" data-t="59:51:00">[59:51:00]</a>.
6.  **Test-Time Ensembling:** To further improve prediction quality, Marigold employs a test-time ensembling scheme where multiple inferences are run with different initialization noise, and the median of these predictions is taken as the final depth map <a class="yt-timestamp" data-t="01:01:46">[01:01:46]</a>. This can reduce the absolute relative error by up to 9.5% with 20 ensembles <a class="yt-timestamp" data-t="01:09:18">[01:09:18]</a>.

#### Performance

Marigold achieves state-of-the-art performance across a wide range of datasets, with over 20% performance gains in specific cases <a class="yt-timestamp" data-t="16:10:00">[16:10:00]</a>. Its ability to outperform prior art in most cases, even when trained on relatively few synthetic images, confirms the hypothesis that a comprehensive representation of the visual world (like that learned by large diffusion models) is foundational for [[monocular_depth_estimation_using_diffusion_models | monocular depth estimation]] <a class="yt-timestamp" data-t="01:06:00">[01:06:00]</a>.

## Comparison with Patch Fusion

In contrast, the paper "Patch Fusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation" <a class="yt-timestamp" data-t="07:15:37">[07:15:37]</a>, also published in December 2023, tackles high-resolution [[monocular_depth_estimation_using_diffusion_models | monocular depth estimation]] using a tile-based framework <a class="yt-timestamp" data-t="07:18:00">[07:18:00]</a>. This method involves cutting a 4K image into multiple overlapping patches, running depth prediction independently on each patch, and then fusing these inconsistent predictions <a class="yt-timestamp" data-t="01:13:19">[01:13:19]</a>. While this yields impressive fine details, it essentially treats the large image as many smaller ones, which can be computationally intensive <a class="yt-timestamp" data-t="01:17:19">[01:17:19]</a>. Additionally, Patch Fusion often trains models from scratch using ConvNets <a class="yt-timestamp" data-t="01:17:46">[01:17:46]</a>, which is a less efficient approach compared to [[finetuning_machine_learning_models | finetuning]] large pre-trained models like Marigold.

## Implications for Depth Sensing Hardware

The advances in [[monocular_depth_estimation_using_diffusion_models | monocular depth estimation]], particularly by leveraging pre-trained generative models, suggest a significant shift away from the reliance on dedicated depth sensors like LiDAR and structured light cameras <a class="yt-timestamp" data-t="01:07:06">[01:07:06]</a>. These sensors often produce noisy or incomplete ground truth depth data due to physical constraints and material properties of scenes <a class="yt-timestamp" data-t="01:06:35">[01:06:35]</a>. As models become capable of generating higher-quality depth maps than these physical sensors, the market for specialized depth hardware may shrink, replaced by software-based solutions using standard RGB cameras <a class="yt-timestamp" data-t="01:07:56">[01:07:56]</a>. While current inference speeds for these advanced models might still lag behind real-time sensor output, continued research into faster diffusion models (e.g., latent consistency models) could overcome this limitation <a class="yt-timestamp" data-t="01:42:40">[01:42:40]</a>.