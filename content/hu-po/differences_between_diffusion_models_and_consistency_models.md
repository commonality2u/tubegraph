---
title: Differences between Diffusion Models and Consistency Models
videoId: 8b6NhnNYtpg
---

From: [[hu-po]] <br/> 

**Consistency Models** represent a new family of generative models developed by [[OpenAI and Consistency Models | OpenAI]] that address the computational drawbacks of traditional [[Diffusion models and image generation | diffusion models]] while maintaining high sample quality <a class="yt-timestamp" data-t="01:13:00">[01:13:00]</a>. While both approaches are highly successful in generative tasks like image, audio, and video generation, they differ significantly in their operational paradigms, particularly regarding sampling speed and training methodologies <a class="yt-timestamp" data-t="01:06:05">[01:06:05]</a>.

## Diffusion Models (DMs)

[[Diffusion models and image generation | Diffusion models]] (DMs), also known as score-based generative models, have achieved remarkable success across multiple fields, including [[Diffusion models and image generation | image generation]], audio synthesis, and video generation <a class="yt-timestamp" data-t="01:03:33">[01:03:33]</a> <a class="yt-timestamp" data-t="01:04:05">[01:04:05]</a> <a class="yt-timestamp" data-t="01:04:18">[01:04:18]</a>. Their core mechanism involves a two-part process:
*   **Forward Process (Training)**: Data is progressively perturbed into noise, typically via Gaussian perturbations, using a stochastic differential equation (SDE) <a class="yt-timestamp" data-t="01:00:27">[01:00:27]</a> <a class="yt-timestamp" data-t="01:00:40">[01:00:40]</a> <a class="yt-timestamp" data-t="01:01:04">[01:01:04]</a>. This process converts data distributions into noise distributions over time <a class="yt-timestamp" data-t="01:13:54">[01:13:54]</a>. The direction of time flips for training versus inference <a class="yt-timestamp" data-t="01:02:40">[01:02:40]</a>.
*   **Reverse Process (Inference)**: Starting with noise, DMs iteratively refine it into an actual image by reversing the SDE, often approximated by an ordinary differential equation (ODE) <a class="yt-timestamp" data-t="01:03:19">[01:03:19]</a> <a class="yt-timestamp" data-t="01:04:10">[01:04:10]</a>.

### Limitations of Diffusion Models
The key to the success of [[Diffusion models and image generation | diffusion models]] is their iterative sampling process <a class="yt-timestamp" data-t="01:12:34">[01:12:34]</a>. However, this is also their Achilles' heel, leading to slow sampling speeds <a class="yt-timestamp" data-t="01:05:43">[01:05:43]</a> <a class="yt-timestamp" data-t="01:12:37">[01:12:37]</a>. Generating a single image often requires hundreds to thousands of iterative steps, each involving an inference pass through a neural network <a class="yt-timestamp" data-t="01:05:50">[01:05:50]</a>. This can be 10 to 2000 times more compute-intensive than single-step generative models like GANs or VAEs <a class="yt-timestamp" data-t="01:14:40">[01:14:40]</a>.

## Consistency Models (CMs)

[[Consistency Models in Image Generation | Consistency Models]] aim to overcome the slow sampling speeds of [[Diffusion models and image generation | diffusion models]] <a class="yt-timestamp" data-t="01:04:53">[01:04:53]</a>. They propose a novel approach to image generation based on "probability flow ODEs" that smoothly convert data to noise <a class="yt-timestamp" data-t="01:03:12">[01:03:12]</a> <a class="yt-timestamp" data-t="01:04:14">[01:04:14]</a>.

The core idea of [[Consistency Models in Image Generation | Consistency Models]] is to learn a mapping `F_theta` that can directly convert any point `x_t` on an ODE trajectory (which represents an image with a certain level of noise at time `t`) back to its original, noise-free state `x_0` (or `x_sigma`, a very small amount of noise for numerical stability) <a class="yt-timestamp" data-t="01:03:41">[01:03:41]</a> <a class="yt-timestamp" data-t="01:03:55">[01:03:55]</a> <a class="yt-timestamp" data-t="01:04:09">[01:04:09]</a>. This mapping is "self-consistent," meaning that different points on the same trajectory should map to the same original image <a class="yt-timestamp" data-t="01:14:09">[01:14:09]</a>.

### Key Features of Consistency Models
*   **Fast One-Step Generation**: By design, [[Consistency Models in Image Generation | consistency models]] support fast one-step generation, requiring only a single network evaluation to produce a high-quality sample from random noise <a class="yt-timestamp" data-t="01:06:20">[01:06:20]</a> <a class="yt-timestamp" data-t="01:15:01">[01:15:01]</a>.
*   **Compute-Quality Trade-off**: While designed for one-step generation, they also allow for few-step sampling. This enables users to trade computational cost for sample quality, similar to how classifier-free guidance works in diffusion models <a class="yt-timestamp" data-t="01:06:27">[01:06:27]</a> <a class="yt-timestamp" data-t="01:13:09">[01:13:09]</a>. This means a user can choose higher quality with more steps, or faster generation with fewer steps <a class="yt-timestamp" data-t="01:13:37">[01:13:37]</a>.
*   **Zero-Shot Data Editing**: [[Consistency Models in Image Generation | Consistency models]] support zero-shot data editing tasks such as image inpainting, colorization, and super-resolution without explicit training on these tasks <a class="yt-timestamp" data-t="01:07:42">[01:07:42]</a> <a class="yt-timestamp" data-t="01:14:00">[01:14:00]</a> <a class="yt-timestamp" data-t="01:15:03">[01:15:03]</a>.
*   **Training Methods**:
    *   **Distillation Mode**: CMs can be trained to distill the knowledge of pre-trained [[Diffusion models and image generation | diffusion models]] into a single-step sampler <a class="yt-timestamp" data-t="01:08:04">[01:08:04]</a> <a class="yt-timestamp" data-t="01:17:17">[01:17:17]</a> <a class="yt-timestamp" data-t="01:18:20">[01:18:20]</a>. This involves generating pairs of adjacent points on an ODE trajectory using the pre-trained DM and minimizing the output difference of the CM for these pairs <a class="yt-timestamp" data-t="01:18:38">[01:18:38]</a>.
    *   **Standalone/Isolation Mode**: Alternatively, [[Consistency Models in Image Generation | consistency models]] can be trained from scratch without relying on any pre-trained [[Diffusion models and image generation | diffusion models]], establishing them as an independent family of generative models <a class="yt-timestamp" data-t="01:08:17">[01:08:17]</a> <a class="yt-timestamp" data-t="01:18:24">[01:18:24]</a> <a class="yt-timestamp" data-t="01:19:53">[01:19:53]</a>.

## Summary of Key Differences

| Feature             | [[Diffusion models and image generation | Diffusion Models]] (DMs)                                      | [[Consistency Models in Image Generation | Consistency Models]] (CMs)                                |
| :------------------ | :-------------------------------------------------------------------------- | :------------------------------------------------------------------- |
| **Sampling Speed**  | Slow, iterative generation (10s to 1000s of steps) <a class="yt-timestamp" data-t="01:05:50">[01:05:50]</a> <a class="yt-timestamp" data-t="01:14:40">[01:14:40]</a>         | Fast, primarily one-step generation by design <a class="yt-timestamp" data-t="01:06:20">[01:06:20]</a> |
| **Inference Process** | Progressively denoise from noise over many steps <a class="yt-timestamp" data-t="01:12:34">[01:12:34]</a> | Direct mapping from any noisy point to original data (one forward pass) <a class="yt-timestamp" data-t="01:15:01">[01:15:01]</a> |
| **Training**        | Start by diffusing data to noise <a class="yt-timestamp" data-t="01:02:40">[01:02:40]</a>                      | Can be distilled from pre-trained DMs or trained standalone <a class="yt-timestamp" data-t="01:08:04">[01:08:04]</a> |
| **Flexibility**     | Fixed number of steps for inference, often not directly controllable in real-time for quality <a class="yt-timestamp" data-t="01:04:20">[01:04:20]</a> | Allows trade-off between compute and quality (one-step to few-step) <a class="yt-timestamp" data-t="01:06:27">[01:06:27]</a> |
| **Adversarial Training** | Do not rely on adversarial training, less prone to instability <a class="yt-timestamp" data-t="01:11:30">[01:11:30]</a> | Do not rely on adversarial training, less prone to instability <a class="yt-timestamp" data-t="01:11:30">[01:11:30]</a> |
| **Underlying Math** | Based on reversing SDEs and ODEs <a class="yt-timestamp" data-t="01:03:19">[01:03:19]</a> | Based on "probability flow ODEs" and their self-consistency property <a class="yt-timestamp" data-t="01:03:12">[01:03:12]</a> |

Overall, [[Consistency Models in Image Generation | Consistency Models]] aim to retain the advantages of [[Diffusion models and image generation | diffusion models]], such as high sample quality and the avoidance of adversarial training issues, while significantly improving their sampling efficiency, particularly for real-time applications <a class="yt-timestamp" data-t="01:15:01">[01:15:01]</a> <a class="yt-timestamp" data-t="01:15:03">[01:15:03]</a>.