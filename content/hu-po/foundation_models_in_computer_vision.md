---
title: Foundation models in computer vision
videoId: eMFfMz9uYlc
---

From: [[hu-po]] <br/> 

Foundation models, a term notably coined by Stanford, represent a paradigm shift in machine learning. They are typically very large models trained on immense datasets with relatively generic tasks, such as filling in masks or next-word prediction. The core idea is that these models can then be fine-tuned for various downstream tasks, acting as a "foundation" for future models <a class="yt-timestamp" data-t="03:15:00">[03:15:00]</a> <a class="yt-timestamp" data-t="03:23:00">[03:23:00]</a> <a class="yt-timestamp" data-t="03:44:00">[03:44:00]</a>.

In [[Unsupervised Learning in Computer Vision | computer vision]], prominent examples of foundation models include [[comparison_of_different_vision_language_models | Clip]], ALIGN, and BLIP, which align text and images <a class="yt-timestamp" data-t="08:05:00">[08:05:00]</a>. These models, especially [[comparison_of_different_vision_language_models | Clip]], are considered true foundation models in [[Unsupervised Learning in Computer Vision | computer vision]] due to their widespread and consistent use <a class="yt-timestamp" data-t="08:09:00">[08:09:00]</a>.

## Segment Anything Model (SAM)

The Segment Anything Model (SAM), developed by Meta AI Research (formerly Facebook AI Research or Fair), is a significant advancement in segmentation <a class="yt-timestamp" data-t="00:45:00">[00:45:00]</a>. It is a large segmentation model pre-trained on an enormous amount of data for an extended period, a feat typically achievable only by major AI companies <a class="yt-timestamp" data-t="01:15:00">[01:15:00]</a> <a class="yt-timestamp" data-t="01:24:00">[01:24:00]</a>. SAM is designed to perform zero-shot segmentation, meaning it can segment anything without prior training on specific object categories <a class="yt-timestamp" data-t="01:29:00">[01:29:00]</a> <a class="yt-timestamp" data-t="05:29:00">[05:29:00]</a>. While generally quite good, it can occasionally make mistakes, such as producing a double mask for overlapping objects <a class="yt-timestamp" data-t="01:33:00">[01:33:00]</a> <a class="yt-timestamp" data-t="01:47:00">[01:47:00]</a>.

### Key Components

SAM's architecture consists of three interconnected components <a class="yt-timestamp" data-t="04:27:00">[04:27:00]</a> <a class="yt-timestamp" data-t="13:51:00">[13:51:00]</a>:

1.  **Image Encoder**: A powerful [[transformer_models_in_3d_reconstruction | Vision Transformer (ViT)]] (minimally adapted) that processes the input image once to produce an image embedding <a class="yt-timestamp" data-t="14:17:00">[14:17:00]</a> <a class="yt-timestamp" data-t="38:03:00">[38:03:00]</a> <a class="yt-timestamp" data-t="38:28:00">[38:28:00]</a>. This is the largest and most memory-intensive part of the model <a class="yt-timestamp" data-t="38:32:00">[38:32:00]</a> <a class="yt-timestamp" data-t="38:40:00">[38:40:00]</a>.
2.  **Prompt Encoder**: Embeds flexible prompts, which can be sparse (points, boxes) or dense (masks) <a class="yt-timestamp" data-t="13:59:00">[13:59:00]</a> <a class="yt-timestamp" data-t="13:59:00">[13:59:00]</a> <a class="yt-timestamp" data-t="38:58:00">[38:58:00]</a>. Points and boxes are represented using positional encodings <a class="yt-timestamp" data-t="39:03:00">[39:03:00]</a>. Text prompts are processed through a [[comparison_of_different_vision_language_models | Clip]] text encoder <a class="yt-timestamp" data-t="07:42:00">[07:42:00]</a> <a class="yt-timestamp" data-t="41:10:00">[41:10:10]</a>.
3.  **Mask Decoder**: A lightweight component that combines the image and prompt embeddings to efficiently predict masks <a class="yt-timestamp" data-t="14:26:00">[14:26:00]</a> <a class="yt-timestamp" data-t="14:48:00">[14:48:00]</a> <a class="yt-timestamp" data-t="40:48:00">[40:48:00]</a>. It uses a modified [[transformer_models_in_3d_reconstruction | Transformer]] decoder block with prompt self-attention and cross-attention <a class="yt-timestamp" data-t="40:56:00">[40:56:06]</a>. This decoder can predict multiple masks (typically three) and their confidence scores, using estimated Intersection over Union (IOU) <a class="yt-timestamp" data-t="15:08:00">[15:08:00]</a> <a class="yt-timestamp" data-t="42:05:00">[42:05:00]</a> <a class="yt-timestamp" data-t="42:25:00">[42:25:00]</a>. The prompt encoder and mask decoder can run in approximately 50 milliseconds in a web browser <a class="yt-timestamp" data-t="14:51:00">[14:51:00]</a> <a class="yt-timestamp" data-t="43:00:00">[43:00:00]</a>.

### The SA-1B Dataset

A significant aspect of SAM is its colossal dataset, SA-1B, which comprises over 1 billion masks on 11 million licensed and privacy-respecting images <a class="yt-timestamp" data-t="04:38:00">[04:38:00]</a> <a class="yt-timestamp" data-t="05:10:00">[05:10:00]</a>. This dataset is currently the largest segmentation dataset ever created, being 400 times larger than any existing one <a class="yt-timestamp" data-t="05:06:00">[05:06:06]</a> <a class="yt-timestamp" data-t="19:14:00">[19:14:00]</a>.

The data collection process for SA-1B involved a multi-stage "data engine":

1.  **Assisted Manual Annotation**: Initially, SAM assisted annotators in creating masks, similar to classic interactive segmentation. Annotators used pixel-precise brush and eraser tools to refine model-generated masks <a class="yt-timestamp" data-t="16:37:00">[16:37:00]</a> <a class="yt-timestamp" data-t="16:44:00">[16:44:00]</a> <a class="yt-timestamp" data-t="48:12:00">[48:12:00]</a>. This stage improved SAM, which was then retrained on these newly annotated masks <a class="yt-timestamp" data-t="49:57:00">[49:57:00]</a>. Annotators, primarily from Kenya, were encouraged to label objects they could name and move quickly if annotation took over 30 seconds <a class="yt-timestamp" data-t="02:04:07">[02:04:07]</a> <a class="yt-timestamp" data-t="49:22:00">[49:22:00]</a> <a class="yt-timestamp" data-t="49:46:00">[49:46:00]</a>.
2.  **Semi-automatic Stage**: To increase mask diversity, SAM automatically detected confident masks, and annotators were presented with pre-filled images to annotate additional, less prominent objects <a class="yt-timestamp" data-t="54:09:00">[54:09:00]</a> <a class="yt-timestamp" data-t="54:10:00">[54:10:00]</a> <a class="yt-timestamp" data-t="54:21:00">[54:21:00]</a>. This led to an increase in masks per image, though the annotation time per mask also increased due to the added challenge <a class="yt-timestamp" data-t="54:44:00">[54:44:00]</a> <a class="yt-timestamp" data-t="54:52:00">[54:52:00]</a>.
3.  **Fully Automatic Stage**: In the final stage, annotation became fully automatic. SAM was prompted with a 32x32 regular grid of foreground points, predicting a set of masks for each point <a class="yt-timestamp" data-t="18:16:00">[18:16:00]</a> <a class="yt-timestamp" data-t="56:23:00">[56:23:00]</a>. It used IOU prediction to select confident and "stable" masks (those that don't change much when slightly perturbed) <a class="yt-timestamp" data-t="56:43:00">[56:43:00]</a> <a class="yt-timestamp" data-t="56:52:00">[56:52:00]</a>. Non-maximum suppression (NMS) was applied to filter overlapping masks, and multiple overlapping, zoomed-in image crops were processed to improve the quality of smaller masks <a class="yt-timestamp" data-t="18:39:00">[18:39:00]</a> <a class="yt-timestamp" data-t="57:04:00">[57:04:00]</a> <a class="yt-timestamp" data-t="57:15:00">[57:15:00]</a> <a class="yt-timestamp" data-t="01:5:57:21">[01:57:21]</a>. This process generated 1.1 billion high-quality masks from 11 million images <a class="yt-timestamp" data-t="01:19:00">[01:19:00]</a> <a class="yt-timestamp" data-t="01:57:32">[01:57:32]</a>.

The SA-1B dataset demonstrates greater coverage of image corners and includes a higher percentage of small and medium-sized masks compared to older datasets like [[Challenges and Solutions in Modern Computer Vision Pipelines | COCO]] <a class="yt-timestamp" data-t="01:03:10">[01:03:10]</a> <a class="yt-timestamp" data-t="01:03:38">[01:03:38]</a>. Although photographic biases (objects centered in images) are still present, SA-1B exhibits diverse geographic distribution, with a higher percentage of images from Europe and Asia, particularly Russia and Thailand <a class="yt-timestamp" data-t="01:03:03">[01:03:03]</a> <a class="yt-timestamp" data-t="01:08:46">[01:08:46]</a> <a class="yt-timestamp" data-t="01:08:14">[01:08:14]</a>. Quality evaluations show that 94% of automatically generated masks have an IOU greater than 90% when compared to professionally corrected masks <a class="yt-timestamp" data-t="01:01:16">[01:01:16]</a>.

### Training and Evaluation

SAM was trained using a combination of focal loss and Dice loss for supervised mask prediction <a class="yt-timestamp" data-t="43:51:00">[43:51:00]</a>. The training process involved iteratively sampling prompts (points, boxes, masks, or text) to simulate an interactive setup <a class="yt-timestamp" data-t="44:02:00">[44:02:00]</a>. The model's size scaled from ViT-B (Base) to ViT-H (Huge) as more data was collected <a class="yt-timestamp" data-t="51:41:00">[51:41:00]</a>. Training was distributed across 256 GPUs <a class="yt-timestamp" data-t="02:00:59">[02:00:59]</a>.

SAM's zero-shot transfer capabilities were evaluated on 23 diverse segmentation datasets, including novel image distributions like underwater, egocentric, and X-ray images <a class="yt-timestamp" data-t="01:10:13">[01:10:13]</a> <a class="yt-timestamp" data-t="01:15:47">[01:15:47]</a>. It performs well on tasks such as [[feature_matching_in_computer_vision | Edge detection]], object proposal generation, and instance segmentation <a class="yt-timestamp" data-t="01:11:03">[01:11:03]</a>. Human annotator ratings for mask quality show that SAM's mean ratings fall between seven and nine out of ten <a class="yt-timestamp" data-t="01:18:35">[01:18:35]</a> <a class="yt-timestamp" data-t="01:25:54">[01:25:54]</a>.

Despite its strengths, SAM has limitations. It can sometimes hallucinate small disconnected components or produce boundaries that are not as "crisp" as dedicated interactive segmentation models <a class="yt-timestamp" data-t="01:49:35">[01:49:35]</a> <a class="yt-timestamp" data-t="01:50:50">[01:50:50]</a>. Its text-to-mask capability, while explored, is not entirely robust <a class="yt-timestamp" data-t="01:35:10">[01:35:10]</a> <a class="yt-timestamp" data-t="01:50:31">[01:50:31]</a>.

### Applications and Impact

SAM is released under an Apache 2.0 license, with model checkpoints available <a class="yt-timestamp" data-t="02:09:58">[02:09:58]</a> <a class="yt-timestamp" data-t="02:26:00">[02:26:00]</a>. This open-source release aims to foster research and potentially replace datasets like [[Challenges and Solutions in Modern Computer Vision Pipelines | COCO]] <a class="yt-timestamp" data-t="06:26:00">[06:26:00]</a> <a class="yt-timestamp" data-t="06:34:00">[06:34:00]</a>.

The development of SAM has significant implications for [[Application of Vision Language Models in Robotics | computer vision]] systems. Its ability to perform promptable segmentation makes it a valuable component in larger systems, such as [[Application of Vision Language Models in Robotics | 3D reconstruction]] from single RGB-D images or applications involving gaze points detected by wearable devices <a class="yt-timestamp" data-t="01:49:09">[01:49:09]</a> <a class="yt-timestamp" data-t="01:49:18">[01:49:18]</a>. This aligns with the "Toolformer" concept, where large language models (LLMs) utilize specialized models as tools <a class="yt-timestamp" data-t="03:49:00">[03:49:00]</a>. Meta's extensive image data from Facebook and Instagram likely gives them a significant advantage in creating such models <a class="yt-timestamp" data-t="03:55:00">[03:55:00]</a>.

While SAM pushes the boundaries of segmentation, its classification as a true "foundation model" in the broader sense (adaptable to a wide range of *agnostic* downstream tasks) is debated, as its capabilities are primarily confined to various segmentation tasks <a class="yt-timestamp" data-t="03:10:00">[03:10:00]</a> <a class="yt-timestamp" data-t="03:18:00">[03:18:00]</a> <a class="yt-timestamp" data-t="01:46:43">[01:46:43]</a>. However, its scale and effectiveness in its domain position it as a foundational tool for future [[Application of Vision Language Models in Robotics | computer vision]] pipelines.