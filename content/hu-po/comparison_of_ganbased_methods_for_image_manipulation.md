---
title: comparison of GANbased methods for image manipulation
videoId: ExfMg4v5DMA
---

From: [[hu-po]] <br/> 

Recent advancements in generative models have significantly enhanced the ability to synthesize visual content. Among these, Generative Adversarial Networks (GANs) have shown remarkable success in producing photorealistic images <a class="yt-timestamp" data-t="09:42:00">[09:42:00]</a>. However, the critical functionality required for real-world applications is the precise and flexible controllability of the synthesized visual content <a class="yt-timestamp" data-t="11:40:00">[11:40:00]</a>.

## Limitations of Previous GAN Approaches

Prior methods for achieving controllability in GANs often relied on:
*   **Manually annotated training data or prior 3D models** <a class="yt-timestamp" data-t="01:31:00">[01:31:00]</a>. These approaches struggled to generalize to new object categories and offered a limited range of spatial attribute control <a class="yt-timestamp" data-t="13:29:00">[13:29:00]</a>.
*   **Latent space manipulation** <a class="yt-timestamp" data-t="02:49:10">[02:49:10]</a>: A common technique involved performing "math" in the GAN's latent space, such as subtracting the vector for "neutral woman" from "smiling woman" and adding "neutral man" to get "smiling man" <a class="yt-timestamp" data-t="05:23:00">[05:23:00]</a>. While innovative, these results were often primitive and only worked well for specific datasets like CelebA <a class="yt-timestamp" data-t="05:33:00">[05:33:00]</a>, failing to generalize to other categories <a class="yt-timestamp" data-t="01:31:00">[01:31:00]</a>. Such methods lacked fine-grained control over spatial attributes <a class="yt-timestamp" data-t="26:50:00">[26:50:00]</a>.
*   **Conditional GANs**: These models receive an additional input, or "condition," such as [[applications_of_GANs_in_image_editing | segmentation maps]] or 3D variables <a class="yt-timestamp" data-t="24:11:00">[24:11:00]</a>. For instance, `EditGAN` aimed to edit images by modeling a joint distribution of images and segmentation maps <a class="yt-timestamp" data-t="24:23:00">[24:23:00]</a>.
*   **3D-aware GANs**: These methods modify GAN architectures to generate 3D representations, but control is typically limited to global pose or lighting <a class="yt-timestamp" data-t="29:29:00">[29:29:00]</a>.
*   **`GANWarping`**: This approach uses point-based editing but enables "out of distribution" image editing without guaranteeing realistic results or control over 3D object poses <a class="yt-timestamp" data-t="28:40:00">[28:40:00]</a>.
*   **`User Controllable LT` (2022)**: This paper also explored dragging-based manipulation <a class="yt-timestamp" data-t="14:59:00">[14:59:00]</a>. While similar in concept, it primarily supported editing with a single point and lacked precision and the ability to handle multiple point constraints <a class="yt-timestamp" data-t="29:10:00">[29:10:00]</a>. Qualitatively, `User Controllable LT` often led to undesired semantic changes, such as altering an ocean scene to a forest when moving the sun, or changing a dress to pants when manipulating an arm's pose <a class="yt-timestamp" data-t="01:07:07">[01:07:07]</a>, <a class="yt-timestamp" data-t="30:26:00">[30:26:00]</a>.

## DragGAN: A New Approach to GAN-Based Image Manipulation

The paper "Drag Your GAN: [[interactive_pointbased_manipulation_using_GANs | Interactive Point-Based Manipulation on the Generative Image Manifold]]" introduces DragGAN, a novel approach to controlling GANs <a class="yt-timestamp" data-t="00:46:00">[00:46:00]</a>. Developed by researchers from the Max Planck Institute, MIT CSAIL, and Google AR/VR <a class="yt-timestamp" data-t="01:12:00">[01:12:00]</a>, DragGAN addresses the limitations of previous methods by offering precise, flexible, and generalizable control.

### How DragGAN Works

DragGAN allows users to "drag" any points in a generated image to precisely reach target points <a class="yt-timestamp" data-t="06:00:00">[06:00:00]</a>. The user defines handle points (start) and target points (end) <a class="yt-timestamp" data-t="03:00:00">[03:00:00]</a>. The system then iteratively moves the handle points towards their targets while maintaining the semantic integrity of the image <a class="yt-timestamp" data-t="02:20:00">[02:20:00]</a>.

DragGAN consists of two main components <a class="yt-timestamp" data-t="06:10:00">[06:10:00]</a>:
1.  **[[feature_space_motion_supervision_in_GANs | Feature-based motion supervisor]]**: This component drives the handle point to move towards the target position. It leverages the discriminative features of the GAN's intermediate layers (specifically, after the sixth block of StyleGAN2) <a class="yt-timestamp" data-t="01:11:00">[01:11:00]</a>, <a class="yt-timestamp" data-t="53:39:00">[53:39:00]</a>. A simple L1 loss function is used to optimize the latent code, guiding the points in the desired direction <a class="yt-timestamp" data-t="56:38:00">[56:38:00]</a>.
2.  **Point tracking approach**: This component leverages the same discriminative generator features to continuously localize the position of the handle points as the image changes <a class="yt-timestamp" data-t="01:18:00">[01:18:00]</a>, <a class="yt-timestamp" data-t="01:11:00">[01:11:00]</a>. Tracking is performed via a nearest-neighbor search in the feature space <a class="yt-timestamp" data-t="01:12:00">[01:12:00]</a>.

The process is iterative: motion supervision moves the points slightly, then point tracking updates their positions, and this cycle repeats until the target is reached <a class="yt-timestamp" data-t="01:04:00">[01:04:00]</a>. This typically takes 30 to 200 iterations <a class="yt-timestamp" data-t="01:16:00">[01:16:00]</a>, but can be completed in a few seconds on an RTX 3090 GPU <a class="yt-timestamp" data-t="01:52:00">[01:52:00]</a>.

### Key Advantages of DragGAN

*   **Semantic Preservation**: Unlike prior methods that might produce unrealistic warps or alter semantic context, DragGAN ensures that manipulations stay on the "learned generative image manifold" <a class="yt-timestamp" data-t="06:50:00">[06:50:00]</a>. This means it can "hallucinate" occluded content (e.g., generating teeth inside a lion's opened mouth) <a class="yt-timestamp" data-t="02:00:00">[02:00:00]</a> and deform shapes while consistently following object rigidity (e.g., bending a horse leg naturally) <a class="yt-timestamp" data-t="02:00:00">[02:00:00]</a>.
*   **Fine-grained Control**: Users gain precise control over spatial attributes like pose, shape, expression, and layout across diverse object categories <a class="yt-timestamp" data-t="02:27:00">[02:27:00]</a>.
*   **Generality**: DragGAN works effectively across various object categories, including faces, cats, cars, houses, landscapes, and even microscopic images <a class="yt-timestamp" data-t="03:52:00">[03:52:00]</a>.
*   **Efficiency and Simplicity**: A significant advantage is that DragGAN does not rely on any additional neural networks for motion supervision or point tracking <a class="yt-timestamp" data-t="01:52:00">[01:52:00]</a>, <a class="yt-timestamp" data-t="05:44:00">[05:44:00]</a>. This contributes to its computational efficiency and allows for interactive performance <a class="yt-timestamp" data-t="35:29:00">[35:29:00]</a>. The simplicity of the approach allows it to outperform state-of-the-art point tracking methods like RAFT and PIPS <a class="yt-timestamp" data-t="35:34:00">[35:34:00]</a>.
*   **Masking**: The system allows users to define a binary mask to specify movable regions, although applying the mask in feature space rather than image space can still lead to slight changes outside the masked region <a class="yt-timestamp" data-t="04:59:00">[04:59:00]</a>, <a class="yt-timestamp" data-t="01:05:06">[01:05:06]</a>.
*   **Out-of-Distribution Manipulation**: By optimizing the latent code in the W+ space (which is less constrained than the base W space), DragGAN can achieve [[potential_applications_and_future_work_with_DragGAN | out-of-distribution manipulations]], such as making a cat blink one eye, even if such images were not in the training data distribution <a class="yt-timestamp" data-t="01:55:00">[01:55:00]</a>.

## Conclusion

DragGAN represents a significant step forward in GAN-based image manipulation, offering an intuitive and powerful tool for precise and realistic image editing <a class="yt-timestamp" data-t="01:52:00">[01:52:00]</a>. Its ability to leverage the discriminative qualities of intermediate GAN feature maps for both motion supervision and point tracking, without additional networks, is a core innovation <a class="yt-timestamp" data-t="01:52:00">[01:52:00]</a>, <a class="yt-timestamp" data-t="01:11:00">[01:11:00]</a>. This approach is superior to previous GAN-based methods in terms of fidelity, control, and generalization <a class="yt-timestamp" data-t="01:52:00">[01:52:00]</a>, <a class="yt-timestamp" data-t="01:14:00">[01:14:00]</a>.

### [[potential_applications_and_future_work_with_DragGAN | Future Work]]

The paper suggests extending [[potential_applications_and_future_work_with_DragGAN | point-based image editing]] to [[Advancements in 3D generative models using neural networks | 3D generative models]] <a class="yt-timestamp" data-t="01:53:00">[01:53:00]</a>, indicating a natural progression for this line of research. While not explored in the paper, adapting this technique for [[repurposing_diffusionbased_image_generators_for_depth_estimation | diffusion models]] or [[Consistency Models in Image Generation | video generation]] could also be promising avenues, though potential speed bottlenecks (e.g., due to iterative denoising steps in diffusion models) would need to be addressed <a class="yt-timestamp" data-t="01:34:51">[01:34:51]</a>, <a class="yt-timestamp" data-t="01:49:01">[01:49:01]</a>.