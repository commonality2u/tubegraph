---
title: Gamba Jamba Cobra Overview of Mamba Models
videoId: 9s-9aSobky8
---

From: [[hu-po]] <br/> 

This article explores various applications and architectural considerations of [[mamba_models_and_their_applications | Mamba models]], drawing insights from recent developments in 3D reconstruction, vision-language models, and large language models. The discussion is framed around three specific projects: Gamba, Cobra, and Jamba <a class="yt-timestamp" data-t="00:01:47">[00:01:47]</a>.

## Mamba Models: Core Concepts and Efficiency

[[mamba_models_and_their_applications | Mamba models]] are formally known as **State Space Models (SSMs)**, or more specifically, **Selective State Space Models** <a class="yt-timestamp" data-t="00:04:52">[00:04:52]</a>. The term "Mamba" is used due to the architecture's emphasis on speed <a class="yt-timestamp" data-t="00:06:16">[00:06:16]</a>.

The fundamental equation for an SSM describes a linear time-invariant system that captures the dynamics of a system's state variable <a class="yt-timestamp" data-t="00:10:15">[00:10:15]</a>. What makes them "selective" is that key matrices (B, Delta, and C) are functions of the input, making them input-dependent <a class="yt-timestamp" data-t="00:13:57">[00:13:57]</a>. This allows them to "select" relevant parts of the internal state, similar in function to how a Transformer's query and keys select relevant values <a class="yt-timestamp" data-t="00:14:52">[00:14:52]</a>.

A primary advantage of [[efficiency_and_performance_of_mamba_models_in_AI | Mamba models]] over [[hybrid_architectures_with_transformers_and_mambas | Transformers]] lies in their computational efficiency <a class="yt-timestamp" data-t="00:15:54">[00:15:54]</a>. While Transformers exhibit quadratic complexity with respect to sequence length, [[mamba_models_and_their_applications | Mambas]] maintain a constant-sized hidden state (memory), resulting in linear growth of computational cost with sequence length <a class="yt-timestamp" data-t="00:16:50">[00:16:50]</a>. This makes them significantly faster <a class="yt-timestamp" data-t="00:15:54">[00:15:54]</a>.

These [[mamba_models_and_their_applications | Mamba models]] share conceptual similarities with older recurrent neural networks (RNNs) and LSTMs, particularly in their use of a fixed-length hidden vector to carry information forward <a class="yt-timestamp" data-t="00:31:58">[00:31:58]</a>. However, [[mamba_models_and_their_applications | Mambas]] are more efficient in GPU memory during training, partly because they can be treated like a convolutional network for gradient calculations, making training much faster <a class="yt-timestamp" data-t="00:33:10">[00:33:10]</a>.

## Gamba: Mambas for 3D Reconstruction

Gamba is a model that leverages [[mamba_models_and_their_applications | Mamba models]] for single-view 3D reconstruction <a class="yt-timestamp" data-t="00:04:04">[00:04:04]</a>. Its goal is to create a three-dimensional representation of an object from a single input image, specifically using Gaussian Splats as the 3D representation <a class="yt-timestamp" data-t="00:04:19">[00:04:19]</a>.

**Architecture**:
The Gamba model is an end-to-end amortized 3D reconstruction model <a class="yt-timestamp" data-t="00:04:29">[00:04:29]</a>. It uses a Mamba-based sequential network to process image tokens generated by a Dino V2 (Vision Transformer) image tokenizer <a class="yt-timestamp" data-t="00:20:36">[00:20:36]</a>. The output is a fixed-length sequence of 16,384 3D Gaussians <a class="yt-timestamp" data-t="00:20:05">[00:20:05]</a>. Camera condition parameters (extrinsic and intrinsic) and learnable 3DGS tokens are also fed into the model <a class="yt-timestamp" data-t="00:25:05">[00:25:05]</a>.

**Performance and Limitations**:
While not state-of-the-art in reconstruction quality, Gamba demonstrates remarkable speed, taking only about 1 second on a single Nvidia A100 GPU to generate a 3D representation <a class="yt-timestamp" data-t="00:30:08">[00:30:08]</a>. The paper honestly admits its results are "not that amazing" and that it "does not always yield plausible results," attributing this partly to being pre-trained on a significantly smaller dataset (Omni Object 3D) compared to larger datasets like Obverse <a class="yt-timestamp" data-t="00:05:41">[00:05:41]</a>, <a class="yt-timestamp" data-t="00:30:40">[00:30:40]</a>.

**Critiques**:
*   **Hybrid Architecture**: A notable critique is the reliance on a Vision Transformer (Dino V2) for the image tokenizer, despite the paper's focus on [[mamba_models_and_their_applications | Mambas]] <a class="yt-timestamp" data-t="00:20:50">[00:20:50]</a>.
*   **Unidirectional Scan**: The paper claims an inherent unidirectional scan order for [[mamba_models_and_their_applications | Mamba]], which is not entirely true, as multi-directional scanning has been demonstrated in other Mamba papers <a class="yt-timestamp" data-t="00:26:13">[00:26:13]</a>.
*   **Dependency on External Data**: The model relies on camera pose tokens, which are often unavailable in real-world scenarios <a class="yt-timestamp" data-t="00:28:08">[00:28:08]</a>. It also uses an object mask, suggesting inductive biases were introduced to aid in Gaussian initialization <a class="yt-timestamp" data-t="00:29:41">[00:29:41]</a>.

## Cobra: Mambas for Vision Language Models

Cobra extends the application of [[mamba_models_and_their_applications | Mamba models]] to [[mamba_models_in_vision_language_and_multimodal_models | multimodal large language models (MLLMs)]], which consume both image and text tokens to answer questions <a class="yt-timestamp" data-t="00:22:18">[00:22:18]</a>.

**Architecture**:
Cobra utilizes an ensemble of existing Vision Transformers (Dino image encoder and Siglip encoder) to process images <a class="yt-timestamp" data-t="00:21:48">[00:21:48]</a>. These visual tokens are then fed through an MLP projector into a language model composed of 64 [[mamba_models_and_their_applications | Mamba blocks]] <a class="yt-timestamp" data-t="00:36:35">[00:36:35]</a>. The language model predicts tokens auto-regressively <a class="yt-timestamp" data-t="00:37:53">[00:37:53]</a>.

**Training**:
The training paradigm for Cobra simplifies previous multi-stage processes (like those used in Lava models). It discards the separate pre-alignment stage and directly fine-tunes the entire MLLM backbone and the projector simultaneously <a class="yt-timestamp" data-t="00:39:15">[00:39:15]</a>. The dataset used is largely derived from distilling knowledge from larger models like GPT-4 Vision <a class="yt-timestamp" data-t="00:40:15">[00:40:15]</a>. The training takes about 26.5 hours on 8 Nvidia A100 80GB GPUs <a class="yt-timestamp" data-t="00:41:13">[00:41:13]</a>. Starting with a fine-tuned "chat" language model (rather than a base pre-trained model) yielded better performance <a class="yt-timestamp" data-t="00:45:06">[00:45:06]</a>.

**Performance and Applications**:
Cobra achieves competitive performance against computationally efficient models like Lava, Fi, TinyLava, and MobileVLM V2 <a class="yt-timestamp" data-t="00:34:56">[00:34:56]</a>. However, the authors are criticized for selectively comparing to weaker models rather than state-of-the-art [[comparison_of_sora_and_gemini_models | Vision Language Models]] like GPT-4V <a class="yt-timestamp" data-t="00:35:27">[00:35:27]</a>.

The most significant advantage is its inference speed, processing 166 tokens per second, substantially faster than Transformer-based VLM counterparts (39-40 tokens/second) <a class="yt-timestamp" data-t="00:42:36">[00:42:36]</a>. This speed makes [[mamba_models_and_their_applications | Mamba-based]] VLM suitable for time-sensitive applications such as visual-based robotic feedback control and autonomous vehicles, even if their accuracy is slightly lower than slower Transformer models <a class="yt-timestamp" data-t="00:47:45">[00:47:45]</a>.

**Critiques**:
Similar to Gamba, Cobra still uses Vision Transformers for image encoding <a class="yt-timestamp" data-t="00:37:01">[00:37:01]</a>.

## Jamba: A Hybrid Language Model

Jamba is an open-source language model developed by AI21 Labs, an Israeli company <a class="yt-timestamp" data-t="00:50:01">[00:50:01]</a>. It represents a [[hybrid_architectures_with_transformers_and_mambas | hybrid architecture]], combining [[mamba_models_and_their_applications | Mamba blocks]] with Transformer blocks, and notably incorporates Mixture of Experts (MoE) <a class="yt-timestamp" data-t="00:50:08">[00:50:08]</a>, <a class="yt-timestamp" data-t="00:54:51">[00:54:51]</a>.

**Architecture**:
The Jamba model consists of 32 alternating layers of Mamba and Mamba+MoE blocks <a class="yt-timestamp" data-t="00:54:45">[00:54:45]</a>. A Mamba+MoE block integrates a Mamba component with a Mixture of Experts module that routes tokens to different MLPs based on their characteristics <a class="yt-timestamp" data-t="00:55:28">[00:55:28]</a>. While the model has 52 billion parameters, it only draws on 12 billion parameters during inference due to the nature of MoE, which only activates a subset of experts <a class="yt-timestamp" data-t="00:53:19">[00:53:19]</a>.

**Performance and Advantages**:
Jamba is competitive with other open-source language models like Mixtral, Gemma, and Llama <a class="yt-timestamp" data-t="00:51:59">[00:51:59]</a>. Its linear complexity allows for a large context window, fitting up to 140k tokens <a class="yt-timestamp" data-t="00:52:45">[00:52:45]</a>. The model is optimized for 80GB Nvidia A100 GPUs, reflecting the common hardware used in AI research <a class="yt-timestamp" data-t="00:56:48">[00:56:48]</a>.

**Critiques**:
The use of MoE is viewed by some as a "cheat code" to advertise a smaller active parameter count while the total parameter count remains large, requiring all weights to be loaded into GPU memory <a class="yt-timestamp" data-t="00:56:13">[00:56:13]</a>.

## The A100 GPU: The Workhorse of AI Development

All three papers (Gamba, Cobra, Jamba) utilize Nvidia A100 80GB GPUs for training and inference, solidifying the A100 as a standard workhorse in the current AI development landscape <a class="yt-timestamp" data-t="00:56:57">[00:56:57]</a>. The prevalence of A100s is evident in the competitive cloud market, where companies subsidize access to these GPUs (burning venture capital money) to attract users, hoping for vendor lock-in, though this strategy's long-term viability is questioned given increasing ease of switching cloud providers <a class="yt-timestamp" data-t="00:59:52">[00:59:52]</a>, <a class="yt-timestamp" data-t="01:03:02">[01:03:02]</a>.

## The Mamba Quantization Challenge

A significant potential weakness for [[mamba_models_and_their_applications | Mamba models]] is their apparent difficulty with [[challenges_and_advantages_in_quantizing_mamba_models | quantization]] <a class="yt-timestamp" data-t="01:08:20">[01:08:20]</a>.

*   **Cobra's Precision Requirement**: The Cobra paper states that due to the "numerical precision sensitiveness of the Mamba recurrent dynamics," [[mamba_models_and_their_applications | Mamba]] models need to maintain relatively high precision (no lower than bf16) for their main parameters during inference <a class="yt-timestamp" data-t="01:05:03">[01:05:03]</a>.
*   **Jamba's Quantization Exclusion**: The Jamba model's Hugging Face page explicitly recommends excluding [[mamba_models_and_their_applications | Mamba blocks]] from 8-bit [[challenges_and_advantages_in_quantizing_mamba_models | quantization]] to avoid degrading model quality <a class="yt-timestamp" data-t="01:07:46">[01:07:46]</a>.

These observations suggest an "Achilles' heel" for [[mamba_models_and_their_applications | Mambas]]. While [[mamba_models_and_their_applications | Mambas]] are efficient by design, [[hybrid_architectures_with_transformers_and_mambas | Transformers]] are increasingly becoming efficient through aggressive [[challenges_and_advantages_in_quantizing_mamba_models | quantization]] (e.g., down to 1.58 bits) without significant performance loss <a class="yt-timestamp" data-t="01:06:02">[01:06:02]</a>. If [[mamba_models_and_their_applications | Mambas]] cannot be quantized effectively, their initial efficiency advantage might be negated, potentially limiting their widespread adoption in resource-constrained environments <a class="yt-timestamp" data-t="01:09:22">[01:09:22]</a>. However, it is possible that future research may find ways to overcome this [[challenges_and_advantages_in_quantizing_mamba_models | quantization]] challenge <a class="yt-timestamp" data-t="01:10:07">[01:10:07]</a>.