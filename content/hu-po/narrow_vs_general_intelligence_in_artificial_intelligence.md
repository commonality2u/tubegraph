---
title: Narrow vs General Intelligence in Artificial Intelligence
videoId: KYlbny1rN1g
---

From: [[hu-po]] <br/> 

Artificial intelligence (AI) can be categorized into different levels of capability, notably Artificial General Intelligence (AGI) and Artificial Super Intelligence (ASI). These terms describe the scope and superiority of an AI's cognitive abilities <a class="yt-timestamp" data-t="02:20:00">[02:20:00]</a>.

## Defining ASI and AGI

### Artificial Super Intelligence (ASI)
[[artificial_super_intelligence_asi_vs_artificial_general_intelligence_agi | Artificial Super Intelligence]] (ASI) refers to AI that is "super intelligent" within a specific, limited domain <a class="yt-timestamp" data-t="02:20:00">[02:20:00]</a>. This means it can perform tasks in that narrow field better than any human <a class="yt-timestamp" data-t="02:40:00">[02:40:00]</a>.

### Artificial General Intelligence (AGI)
[[artificial_super_intelligence_asi_vs_artificial_general_intelligence_agi | Artificial General Intelligence]] (AGI) refers to AI that possesses a "general" form of intelligence, capable of adapting to a variety of different tasks <a class="yt-timestamp" data-t="02:20:00">[02:20:00]</a>.

### Key Difference: "Super" vs. "General"
The fundamental difference lies in the words "super" and "general" <a class="yt-timestamp" data-t="02:27:00">[02:27:00]</a>. An ASI excels in a specific area, while an AGI can perform broadly across multiple domains.

## Narrow Artificial Super Intelligence (ASI)

Humans have had [[generalist_vs_narrow_ai_models_in_robotics | narrow artificial super intelligence]] for hundreds of years <a class="yt-timestamp" data-t="02:50:00">[02:50:00]</a>.

### Historical Examples
Calculators, for instance, are a form of ASI. A calculator is "super intelligent" in arithmetic, able to multiply 10-digit numbers perfectly every time, far surpassing human capability <a class="yt-timestamp" data-t="02:37:00">[02:37:00]</a>. Mechanical calculators from as early as 1623 and 1943 demonstrate this concept <a class="yt-timestamp" data-t="02:54:00">[02:54:00]</a>.

### Modern Examples
Today, we live in a world filled with narrow ASIs <a class="yt-timestamp" data-t="06:17:00">[06:17:00]</a>:
*   **Balancing robots:** These robots are superhuman at balancing, able to do so indefinitely, unlike humans <a class="yt-timestamp" data-t="05:52:00">[05:52:00]</a>.
*   **Machines with superhuman strength:** Examples include industrial robots <a class="yt-timestamp" data-t="06:10:00">[06:10:00]</a>.
*   **Machines with superhuman precision and repeatability:** These perform tasks with accuracy and consistency beyond human ability <a class="yt-timestamp" data-t="06:13:00">[06:13:00]</a>.
*   **Go AI:** AI like AlphaGo and its successors (AlphaGo Fan, AlphaGo Lee, AlphaGo Master, AlphaGo Zero) quickly surpassed top human performance in the game of Go, becoming superhuman Go AIs <a class="yt-timestamp" data-t="08:20:00">[08:20:00]</a>.

### Characteristics of Narrow ASI
Narrow ASIs operate within a very limited and well-defined field <a class="yt-timestamp" data-t="02:42:00">[02:42:00]</a>. Their intelligence is specialized and does not transfer to other domains.

## Artificial General Intelligence (AGI)

### Biological Analogy: Specialists vs. Generalists
The concept of "general" in AI can be understood through a biological analogy:
*   **Specialists:** Animals like pandas and koalas are specialists. They thrive within a narrow ecological niche, often relying on one specific food source. Outside of that niche, they struggle <a class="yt-timestamp" data-t="03:40:00">[03:40:00]</a>.
*   **Generalists:** Animals like crows or dogs are generalists. They can adapt to almost any niche and eat a wide variety of foods <a class="yt-timestamp" data-t="04:02:00">[04:02:00]</a>. They possess a more general form of intelligence <a class="yt-timestamp" data-t="04:17:00">[04:17:00]</a>.

### Current AGI: Language Models and Robotics
According to the speaker, we already have AGI <a class="yt-timestamp" data-t="04:26:00">[04:26:00]</a>:
*   **Language Models:** Large Language Models (LLMs) like ChatGPT, especially after significant advancements such as the "03 moment," demonstrate AGI. They can perform a wide range of language tasks <a class="yt-timestamp" data-t="04:28:00">[04:28:00]</a>.
*   **Robotics:** Robots like the Tesla Optimus are considered almost AGI in the real world, as they are being developed to perform nearly any task humans do physically <a class="yt-timestamp" data-t="05:20:00">[05:20:00]</a>. Humanoid demos, such as those by Unitree, show rapid progress toward this goal <a class="yt-timestamp" data-t="05:42:00">[05:42:00]</a>.

### Limitations of Current AGI
While considered AGI, current models still have [[limitations_of_current_ai_agents | limitations]]:
*   **Language Models:** GPT is a generalist but is limited to language or text-based tasks in the digital world <a class="yt-timestamp" data-t="05:13:00">[05:13:00]</a>.
*   **Robots:** The Tesla Optimus is not yet fully capable of "pretty much do anything that humans do" <a class="yt-timestamp" data-t="05:30:00">[05:30:00]</a>.
*   **Domain Specificity:** Even a generalist animal like a dog, while adaptable, would die if placed in the ocean. Similarly, AGI can be general within its domain but still limited in scope <a class="yt-timestamp" data-t="05:04:00">[05:04:00]</a>.

## The Path to Ultimate Artificial Super Intelligence (ASI)

### Defining Ultimate ASI
The "ultimate ASI" is defined as a system better than any human or group of humans at *anything*, in both the physical and digital worlds <a class="yt-timestamp" data-t="06:23:00">[06:23:00]</a>. An example would be a cluster running hundreds of advanced language models controlling hundreds of humanoid robots <a class="yt-timestamp" data-t="06:40:00">[06:40:00]</a>.

### Exponential Growth of AI Intelligence
AI intelligence is increasing exponentially <a class="yt-timestamp" data-t="07:09:00">[07:09:00]</a>. The time window between an AI being as good as a "dumb human" and better than a "smart human" is extremely small <a class="yt-timestamp" data-t="07:17:00">[07:17:00]</a>. This rapid progression has already been observed in specific domains.

### Learning from Superhuman Performance: The AlphaGo Example
The progression to superhuman AI performance has been seen in the game of Go <a class="yt-timestamp" data-t="07:48:00">[07:48:00]</a>.
*   **Human Data vs. Synthetic Data:** Early Go AIs trained on human games (like AlphaGo Master) could only reach human-level performance <a class="yt-timestamp" data-t="27:21:00">[27:21:00]</a>. To achieve superhuman intelligence, AIs need to train on data beyond human capabilities <a class="yt-timestamp" data-t="27:30:00">[27:30:00]</a>.
*   **Self-Play and Reinforcement Learning:** AlphaGo Zero mastered Go without human knowledge <a class="yt-timestamp" data-t="27:54:00">[27:54:00]</a>. It used self-play and reinforcement learning, generating its own data by playing against itself <a class="yt-timestamp" data-t="09:16:00">[09:16:00]</a>. This process allowed it to explore the vast "tree" of all possible Go games and moves, leading to discoveries no human had made, like "Move 37" <a class="yt-timestamp" data-t="33:55:00">[33:55:00]</a>.

## Action Space and Intelligence

### Defining Action Space
An [[ai_algorithms_and_computational_constraints | action space]] is the set of all valid actions in a given environment <a class="yt-timestamp" data-t="14:44:00">[14:44:44]</a>.

### Branching Factor in Different Domains
The size of this action space, or "branching factor" (number of possible next moves/tokens), varies greatly:
*   **Chess:** Approximately 35 possible moves per turn <a class="yt-timestamp" data-t="15:45:00">[15:45:00]</a>.
*   **Go:** Approximately 250 possible moves per turn <a class="yt-timestamp" data-t="15:39:00">[15:39:00]</a>.
*   **Large Language Models (LLMs):** A vocabulary size of 32,000 means there are 32,000 possible tokens to pick as the next token, indicating a very large branching factor <a class="yt-timestamp" data-t="18:19:00">[18:19:00]</a>. While the language space is vast, it is considered finite if vocabulary and sequence length are limited <a class="yt-timestamp" data-t="18:52:00">[18:52:00]</a>.

### The Role of Learnable "Reward Signals"
For AIs to achieve superhuman performance through self-play, there needs to be a clear "reward signal" or outcome that can be verified as correct or incorrect.
*   **Games (Go, Chess):** The winner or loser of the game provides a clear "Z" signal, allowing the AI to propagate this information back through the entire game's "chain of thought" to label good and bad moves <a class="yt-timestamp" data-t="22:57:00">[22:57:00]</a>.
*   **Language Models:** For most language tasks, there is no definitive "win/lose" signal, making it difficult to use reinforcement learning directly <a class="yt-timestamp" data-t="23:48:00">[23:48:00]</a>. Current LLMs often rely on next-token prediction, copying human-generated text <a class="yt-timestamp" data-t="24:01:00">[24:01:00]</a>.
*   **Mathematics and Coding:** These domains provide a clear, verifiable "Z" signal (correct or incorrect answer), allowing for the application of reinforcement learning and self-play techniques similar to Go <a class="yt-timestamp" data-t="38:57:00">[38:57:00]</a>. This enables small language models to achieve superhuman performance in math reasoning by generating and refining millions of synthetic solutions <a class="yt-timestamp" data-t="38:09:00">[38:09:09]</a>.

## Future Implications and Recursive Self-Improvement

### Generating Superhuman Data
The key to achieving ASI lies in generating "superhuman data" <a class="yt-timestamp" data-t="34:30:00">[34:30:00]</a>. This data is created through iterative self-play, where AI models systematically explore vast "idea spaces," filtering for optimal reasoning traces or solutions <a class="yt-timestamp" data-t="41:01:00">[41:01:01]</a>. This process is expensive in terms of computational inference <a class="yt-timestamp" data-t="1:07:07">[1:07:07]</a>, but the resulting "filtered" and "distilled" data allows subsequent models to become even better <a class="yt-timestamp" data-t="1:06:41">[1:06:41]</a>.

### Transfer of Abilities
While current superhuman performance is constrained to domains like math and coding, it's theorized that superhuman reasoning ability could transfer to other logical fields like biology, chemistry, or even philosophy <a class="yt-timestamp" data-t="1:36:51">[1:36:51]</a>. This would allow ASIs to discover new knowledge in these areas that humans have not yet found <a class="yt-timestamp" data-t="1:37:15">[1:37:15]</a>.

### Recursive Self-Improvement
The ability of an AI to become superhuman at math and coding allows it to create the next generation of AI <a class="yt-timestamp" data-t="1:22:05">[1:22:05]</a>. This creates a "flywheel of improvement" where better models generate better data, which trains even better models, leading to recursive self-improvement and potentially true [[artificial_super_intelligence_asi_vs_artificial_general_intelligence_agi | ASI]] <a class="yt-timestamp" data-t="1:21:37">[1:21:37]</a>.

Current AI models, mostly trained on [[human_data_vs_synthetic_data_in_training_ai | human data]], exhibit human-like characteristics, including "dark sides" like lies and deception <a class="yt-timestamp" data-t="1:05:56">[1:05:56]</a>. This is because humans have already explored those "paths" in the language space <a class="yt-timestamp" data-t="1:12:50">[1:12:50]</a>. Future ASIs, primarily trained on filtered, superhuman synthetic data, may feel very different, representing "platonic nuggets" of optimal knowledge <a class="yt-timestamp" data-t="1:06:37">[1:06:37]</a>.