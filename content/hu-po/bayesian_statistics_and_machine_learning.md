---
title: Bayesian Statistics and Machine Learning
videoId: VLrqFH1Xtrs
---

From: [[hu-po]] <br/> 

[[Bayesian Flow Networks]] (BFNs) represent a new class of generative models that leverage [[KL Divergence and Bayesian Inference | Bayesian inference]] within a neural network framework to learn and generate data distributions <a class="yt-timestamp" data-t="00:02:21">[00:02:21]</a>. The development of [[Bayesian Flow Networks | Bayesian Flow Networks]] is notably influenced by computer scientist Alex Graves, known for his work on [[neural_turing_machines | Neural Turing Machines]], Differentiable Neural Computers, and the LSTM <a class="yt-timestamp" data-t="00:01:50">[00:01:50]</a>. Graves has a history of developing "new and intuitive ways to do machine learning in a different way" <a class="yt-timestamp" data-t="00:02:03">[00:02:03]</a>.

## Core Concepts of [[Bayesian Flow Networks]]

[[Bayesian Flow Networks]] operate by iteratively updating the parameters of independent probability distributions (such as the mean and variance of a [[concepts_of_probability_distributions_in_ml | normal distribution]]) based on noisy data samples <a class="yt-timestamp" data-t="00:02:31">[00:02:31]</a>. These updated parameters are then fed into a neural network that outputs a second, interdependent distribution <a class="yt-timestamp" data-t="00:03:00">[00:03:00]</a>.

### Comparison to [[diffusion_models | Diffusion Models]]
The generative procedure of [[Bayesian Flow Networks]] is conceptually similar to the reverse process of [[diffusion_models | diffusion models]] <a class="yt-timestamp" data-t="00:03:42">[00:03:42]</a>. In [[diffusion_models | diffusion models]], noise is progressively added to and removed from data to learn its structure <a class="yt-timestamp" data-t="00:03:52">[00:03:52]</a>. A key difference is that BFNs do not require a forward process, making them conceptually simpler <a class="yt-timestamp" data-t="00:04:16">[00:04:16]</a>. Unlike [[diffusion_models | diffusion models]] which operate on noisy versions of the data itself, [[Bayesian Flow Networks]] operate on the *parameters* of a data distribution <a class="yt-timestamp" data-t="00:03:02">[00:03:02]</a>. This ensures the generative process remains continuous and differentiable, even when dealing with discrete data <a class="yt-timestamp" data-t="00:37:41">[00:37:41]</a>.

### The Alice and Bob Analogy
The paper frames the process as an exchange of messages between "Alice" and "Bob" <a class="yt-timestamp" data-t="00:15:05">[00:15:05]</a>.
*   **Bob:** The model, which starts with a simple prior belief about a distribution (e.g., a [[concepts_of_probability_distributions_in_ml | Gaussian]] or uniform categorical distribution) <a class="yt-timestamp" data-t="00:03:18">[00:03:18]</a>. Bob aims to improve his guess about the distribution <a class="yt-timestamp" data-t="00:15:52">[00:15:52]</a>.
*   **Alice:** Represents nature or the true data distribution, which sends noisy data samples (messages) to Bob <a class="yt-timestamp" data-t="00:22:01">[00:22:01]</a>.
*   **Messages:** Each message reveals something about the data, allowing Bob to improve his "prior" (his belief about the distribution) <a class="yt-timestamp" data-t="00:15:39">[00:15:39]</a>.

The process iteratively updates Bob's distributions, leading to a generative procedure <a class="yt-timestamp" data-t="00:03:38">[00:03:38]</a>. The goal is to minimize the total number of bits needed for Alice to transmit all messages to Bob, which directly relates to data compression <a class="yt-timestamp" data-t="00:08:05">[00:08:05]</a>. This concept aligns with the idea that intelligence and generalization are fundamentally tied to compression <a class="yt-timestamp" data-t="00:08:26">[00:08:26]</a>.

## How [[Bayesian Flow Networks]] Work

1.  **Input Distribution (Bob's Guess):** Bob starts with an "input distribution," initially a simple prior (e.g., a standard [[concepts_of_probability_distributions_in_ml | normal distribution]] for continuous data or a uniform categorical distribution for discrete data) <a class="yt-timestamp" data-t="00:37:51">[00:37:51]</a>. The parameters of this distribution (e.g., mean and variance for normal, probabilities for categorical) are fed into a neural network <a class="yt-timestamp" data-t="00:38:49">[00:38:49]</a>.
2.  **Neural Network Output (Output Distribution):** The neural network outputs the parameters of a "second interdependent distribution," called the "output distribution" <a class="yt-timestamp" data-t="00:39:12">[00:39:12]</a>. This output distribution benefits from the neural network's ability to process all parameters jointly, allowing it to exploit contextual information <a class="yt-timestamp" data-t="00:48:34">[00:48:34]</a>.
3.  **Sender Distribution (Alice's Noisy Data):** Alice creates a "sender distribution" by adding noise to the actual data according to a predefined schedule (an "accuracy schedule" denoted by Alpha, `α`) <a class="yt-timestamp" data-t="00:39:52">[00:39:52]</a>. As Alpha increases, the samples become progressively more informative about the true data <a class="yt-timestamp" data-t="01:08:21">[01:08:21]</a>.
4.  **Receiver Distribution (Bob's Noisy Output):** Bob creates a "receiver distribution" by convolving his output distribution with the same noisy distribution used by Alice <a class="yt-timestamp" data-t="00:40:04">[00:40:04]</a>.
5.  **Loss Function ([[KL Divergence and Bayesian Inference | KL Divergence]]):** The "transmission cost" at each step is measured by the [[KL Divergence and Bayesian Inference | Kullback-Leibler (KL) Divergence]] between the receiver distribution and the sender distribution <a class="yt-timestamp" data-t="00:41:01">[00:41:01]</a>. This measures the "distance" between Alice's true (noisy) distribution and Bob's predicted (noisy) distribution <a class="yt-timestamp" data-t="00:22:37">[00:22:37]</a>. Minimizing this [[KL Divergence and Bayesian Inference | KL Divergence]] is equivalent to maximizing the evidence lower bound (ELBO) in [[KL Divergence and Bayesian Inference | variational inference]] <a class="yt-timestamp" data-t="00:23:44">[00:23:44]</a>.
6.  **Bayesian Update:** Bob uses a sample drawn from the sender distribution to update his input distribution, following the rules of [[KL Divergence and Bayesian Inference | Bayesian inference]] <a class="yt-timestamp" data-t="00:41:06">[00:41:06]</a>. This update is mathematically optimal for collecting and summarizing information about individual variables <a class="yt-timestamp" data-t="00:48:11">[00:48:11]</a>.
7.  **Iteration:** This process repeats for a number of steps, allowing Bob's prediction of the data to become increasingly accurate <a class="yt-timestamp" data-t="00:41:33">[00:41:33]</a>. The iterative updates cause the parameters of Bob's input distribution to converge towards the true data distribution <a class="yt-timestamp" data-t="02:09:58">[02:09:58]</a>.

### Probability Simplex
For discrete data, the network inputs (probabilities of a categorical distribution) inherently lie on a [[concepts_of_probability_distributions_in_ml | probability simplex]] <a class="yt-timestamp" data-t="00:55:45">[00:55:45]</a>. A [[concepts_of_probability_distributions_in_ml | probability simplex]] is a mathematical space where each point represents a [[concepts_of_probability_distributions_in_ml | probability distribution]] between a finite number of mutually exclusive events <a class="yt-timestamp" data-t="00:05:11">[00:05:11]</a>. For example, a coin flip is a one-simplex (a line), and three possible outcomes form a two-simplex (a triangle) <a class="yt-timestamp" data-t="00:05:53">[00:05:53]</a>. This allows for native differentiability, paving the way for [[Optimization Methods in Machine Learning | gradient-based optimization]] <a class="yt-timestamp" data-t="00:06:50">[00:06:50]</a>.

## Mathematical Underpinnings

The paper derives both discrete and continuous time loss functions for [[Bayesian Flow Networks]] <a class="yt-timestamp" data-t="00:04:25">[00:04:25]</a>.
*   **Discrete Time Loss:** The loss is the sum of [[KL Divergence and Bayesian Inference | KL Divergences]] between the sender and receiver distributions over `N` steps <a class="yt-timestamp" data-t="01:39:47">[01:39:47]</a>.
*   **Continuous Time Loss:** By allowing the number of transmission steps `N` to approach infinity, the Bayesian update process generalizes to continuous time <a class="yt-timestamp" data-t="01:29:51">[01:29:51]</a>. In this limit, Bayesian updates become a "Bayesian flow," which gives the network its name <a class="yt-timestamp" data-t="01:50:01">[01:50:01]</a>. The continuous time loss function is presented as mathematically simpler and easier to compute <a class="yt-timestamp" data-t="01:46:47">[01:46:47]</a>.

The "accuracy schedule" (Alpha, `α`, or Beta, `β`) dictates the rate at which information flows into the input distribution <a class="yt-timestamp" data-t="01:30:27">[01:30:27]</a>. A higher Alpha signifies greater confidence in a sample, leading to more dramatic shifts in the distribution parameters <a class="yt-timestamp" data-t="02:04:10">[02:04:10]</a>.

## Performance and Applications

[[Bayesian Flow Networks]] achieve competitive log-likelihoods for image modeling on dynamically binarized MNIST and CIFAR-10 datasets <a class="yt-timestamp" data-t="00:09:11">[00:09:11]</a>. They also outperform discrete [[diffusion_models | diffusion models]] on the Text8 character-level language modeling task <a class="yt-timestamp" data-t="01:03:40">[01:03:40]</a>. The Text8 benchmark involves predicting the next character in a large chunk of English Wikipedia <a class="yt-timestamp" data-t="01:11:56">[01:11:56]</a>.

### Example Visualizations
*   **Continuous Data Update:** When observing new samples (e.g., from a [[concepts_of_probability_distributions_in_ml | univariate Gaussian]]), the model's posterior distribution (representing its guess about the true mean and variance) shifts and narrows, reflecting increased confidence and convergence towards the observed data point <a class="yt-timestamp" data-t="01:57:22">[01:57:22]</a>.
*   **Discrete Data Simplex:** For categorical data (e.g., three possible tokens A, B, C), the model's probability distribution is visualized as a point on a triangle ([[concepts_of_probability_distributions_in_ml | probability simplex]]) <a class="yt-timestamp" data-t="02:35:40">[02:35:40]</a>. The model starts with equal probabilities (center of the triangle) and iteratively moves towards the corner representing the actual data, demonstrating the learning process <a class="yt-timestamp" data-t="02:37:14">[02:37:14]</a>.

## Limitations and Discussion
While [[Bayesian Flow Networks]] offer a theoretically elegant approach, a key concern is their [[parallelism_and_scalability_in_machine_learning | scalability]] to high-dimensional data, such as large images or extensive language vocabularies <a class="yt-timestamp" data-t="01:14:49">[01:14:49]</a>. The current benchmarks (MNIST, CIFAR-10, Text8 with limited characters) are relatively simple <a class="yt-timestamp" data-t="01:17:19">[01:17:19]</a>. The efficacy of these models on significantly larger datasets and higher-dimensional output spaces remains to be fully explored <a class="yt-timestamp" data-t="02:55:08">[02:55:08]</a>. The speaker highlights that while the mathematical theory might be "beautiful," practical implementation and [[performance_and_efficiency_in_machine_learning_models | efficiency]] on GPUs often determine the adoption of new architectures over existing optimized ones like Transformers <a class="yt-timestamp" data-t="02:12:47">[02:12:47]</a>.