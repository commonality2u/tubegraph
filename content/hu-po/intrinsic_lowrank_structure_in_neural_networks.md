---
title: Intrinsic lowrank structure in neural networks
videoId: vjEPXSCbmDE
---

From: [[hu-po]] <br/> 

[[introduction_to_laura_techniques_for_neural_networks|Low-Rank Adaptation (LoRA)]] is a technique that has gained popularity for fine-tuning existing [[deep_learning_and_neural_networks|Deep Learning]] models, including large language models (LLMs) and image generation models like Stable Diffusion <a class="yt-timestamp" data-t="01:03:00">[01:03:00]</a>, <a class="yt-timestamp" data-t="01:31:00">[01:31:00]</a>. While LoRA is not a full fine-tuning method that retrains all model parameters, it instead trains additional parameters <a class="yt-timestamp" data-t="01:13:00">[01:13:00]</a>. The approach of low-rank adaptation has "stood the test of time" since its paper was published in October 2021 and continues to be used in open-source fine-tuning repositories <a class="yt-timestamp" data-t="01:50:00">[01:50:00]</a>.

## Core Idea of LoRA
An important paradigm in natural language processing involves large-scale pre-training on general domain data and then adapting these models to particular domains or tasks <a class="yt-timestamp" data-t="02:22:00">[02:22:00]</a>. As models become larger, such as GPT-3 with 175 billion parameters, traditional full fine-tuning (which retrains all parameters) becomes prohibitively expensive <a class="yt-timestamp" data-t="03:18:00">[03:18:00]</a>, <a class="yt-timestamp" data-t="03:31:00">[03:31:00]</a>.

LoRA addresses this by freezing the pre-trained model weights and injecting trainable rank decomposition matrices into each layer of the Transformer architecture <a class="yt-timestamp" data-t="03:40:00">[03:40:00]</a>, <a class="yt-timestamp" data-t="03:46:00">[03:46:00]</a>. This means gradients are not pushed into the original model <a class="yt-timestamp" data-t="03:54:00">[03:54:00]</a>. Instead, new, smaller model weights, called "trainable rank decomposition matrices" (or "LoRAs"), are added and trained <a class="yt-timestamp" data-t="04:07:00">[04:07:07]</a>. These matrices are a "fancy way of saying basically smaller weight matrices that have a lower rank" <a class="yt-timestamp" data-t="04:19:00">[04:19:00]</a>.

The high-level intuition is that LoRA creates a "tiny little [[deep_learning_and_neural_networks|neural network]] that just kind of attaches to the big pre-trained [[deep_learning_and_neural_networks|neural network]]" and only pushes gradients into this tiny network <a class="yt-timestamp" data-t="05:01:00">[05:01:00]</a>.

### Hypotheses and Observations
The efficacy of LoRA is rooted in the hypothesis that the "change in weights during model adaptation also has low intrinsic rank" <a class="yt-timestamp" data-t="01:33:00">[01:33:00]</a>, <a class="yt-timestamp" data-t="03:50:00">[03:50:00]</a>. This aligns with prior research suggesting that over-parameterized models in fact reside on low intrinsic dimensions <a class="yt-timestamp" data-t="01:25:00">[01:25:00]</a>.

## Benefits of LoRA
Compared to full fine-tuning with Adam, LoRA offers significant advantages:
*   **Reduced Trainable Parameters** It can reduce the number of trainable parameters by 10,000 times for GPT-3 (from 175 billion to as small as 0.01% of the original parameters) <a class="yt-timestamp" data-t="05:32:00">[05:32:00]</a>, <a class="yt-timestamp" data-t="02:15:00">[02:15:00]</a>.
*   **Lower GPU Memory Requirement** It can reduce GPU memory requirements by three times <a class="yt-timestamp" data-t="05:36:00">[05:36:00]</a>. This is because the large foundational model operates mostly in inference mode, meaning there's no need to calculate gradients or maintain optimizer states for most parameters <a class="yt-timestamp" data-t="01:41:00">[01:41:00]</a>, <a class="yt-timestamp" data-t="01:53:00">[01:53:00]</a>. For example, it reduced VRAM consumption from 1.2 terabytes to 35 megabytes for checkpoint size <a class="yt-timestamp" data-t="05:00:00">[05:00:00]</a>.
*   **No Additional Inference Latency** Unlike adapter-based methods, LoRA introduces no additional inference latency because its trainable matrices can be merged with the frozen pre-trained weights when deployed <a class="yt-timestamp" data-t="05:49:00">[05:49:00]</a>, <a class="yt-timestamp" data-t="01:19:00">[01:19:00]</a>. This means the number of weights remains the same after the addition <a class="yt-timestamp" data-t="07:55:00">[07:55:00]</a>.
*   **Efficient Task Switching** A pre-trained model can be shared, and LoRA allows for efficient task switching by simply recovering the original weights (by subtracting the LoRA delta) and then adding a different LoRA delta <a class="yt-timestamp" data-t="04:46:00">[04:46:00]</a>, <a class="yt-timestamp" data-t="04:50:00">[04:50:00]</a>. This significantly reduces storage requirements and task switching overhead <a class="yt-timestamp" data-t="01:22:00">[01:22:00]</a>.

LoRA performs on par or even better than full fine-tuning in model quality despite having far fewer trainable parameters and higher training throughput <a class="yt-timestamp" data-t="05:40:00">[05:40:00]</a>. It also provides a 25% speed-up during training compared to full fine-tuning <a class="yt-timestamp" data-t="05:02:00">[05:02:00]</a>.

## Mechanism of LoRA
LoRA operates by re-parameterizing the accumulated gradient update. For a pre-trained weight matrix $W_0$ of dimensionality $d \times k$, its update $\Delta W$ is constrained by representing it with a low-rank decomposition: $W_0 + \Delta W = W_0 + BA$ <a class="yt-timestamp" data-t="03:36:00">[03:36:00]</a>. Here, $B$ is a $d \times r$ matrix and $A$ is an $r \times k$ matrix, where $r$ (the rank) is much smaller than $d$ and $k$ <a class="yt-timestamp" data-t="03:51:00">[03:51:00]</a>.

During training, $W_0$ is frozen and does not receive gradient updates, while $A$ and $B$ contain the trainable parameters <a class="yt-timestamp" data-t="04:05:00">[04:05:00]</a>. The matrix $A$ is initialized with a random Gaussian distribution, and $B$ is initialized to zeros <a class="yt-timestamp" data-t="04:33:00">[04:33:00]</a>. The output of $BA \cdot x$ is scaled by $\alpha / r$, where $\alpha$ is a constant <a class="yt-timestamp" data-t="04:57:00">[04:57:00]</a>.

LoRA can be applied to any subset of weight matrices in a [[deep_learning_and_neural_networks|neural network]] <a class="yt-timestamp" data-t="04:53:00">[04:53:00]</a>. In Transformers, it is typically applied to the Query (WQ) and Value (WV) weight matrices within the [[attention_mechanism_and_its_role_in_neural_networks|self-attention module]] <a class="yt-timestamp" data-t="04:46:00">[04:46:00]</a>. While this choice is somewhat arbitrary, it has proven effective <a class="yt-timestamp" data-t="04:51:00">[04:51:00]</a>.

## Intrinsic Rank and Model Characteristics
The concept of low-rank structure is common in [[deep_learning_and_neural_networks|machine learning]], as many problems have certain intrinsic low-rank structures <a class="yt-timestamp" data-t="01:10:00">[01:10:00]</a>. For over-parameterized [[deep_learning_and_neural_networks|neural network]]s, it is known that the learned network will exhibit low-rank properties after training <a class="yt-timestamp" data-t="01:21:00">[01:21:00]</a>.

A surprising finding is that a very small rank, such as $R=1$, can suffice for adapting both WQ and WV on certain datasets like WikiSQL and MultiNLI <a class="yt-timestamp" data-t="01:30:15">[01:30:15]</a>. This suggests that the update matrix itself could have a very small intrinsic rank <a class="yt-timestamp" data-t="01:30:30">[01:30:30]</a>.

The success of LoRA can be linked to the model's capacity relative to the dataset. For larger models like GPT-2 and GPT-3, which have vastly more weights, there is more "model capacity" <a class="yt-timestamp" data-t="01:07:26">[01:07:26]</a>. This implies that many of those weights may not be strictly useful, making it easier to find low-rank versions of the weight matrices during adaptation <a class="yt-timestamp" data-t="01:07:51">[01:07:51]</a>. In contrast, smaller models like RoBERTa, where "more signal is being squeezed into the smaller amount of weights," finding a low-rank decomposition is harder <a class="yt-timestamp" data-t="01:07:07">[01:07:07]</a>.

### Determining Optimal Rank (R)
The choice of rank ($R$) is determined by setting a "parameter budget" <a class="yt-timestamp" data-t="01:25:34">[01:25:34]</a>. For example, a budget of 18 million trainable parameters corresponds to an $R=8$ for a specific configuration <a class="yt-timestamp" data-t="01:25:34">[01:25:34]</a>.

Subspace similarity, measured using the Grassmann distance, can be used to empirically investigate the intrinsic rank of learned adaptation matrices <a class="yt-timestamp" data-t="01:31:22">[01:31:22]</a>. This analysis can reveal how much overlap exists between singular vectors across different ranks or random seeds. For instance, the top singular vector directions (e.g., first 8) from a higher rank (e.g., $R=64$) often overlap significantly with those from a lower rank (e.g., $R=8$), while other directions may contain mostly random noise <a class="yt-timestamp" data-t="01:38:10">[01:38:10]</a>. This suggests that the adaptation matrix can indeed have a very low rank, providing a formal way to determine what R value to pick for a given weight matrix <a class="yt-timestamp" data-t="01:43:02">[01:43:02]</a>.

## Limitations and Future Work
While LoRA is broadly applicable to any [[deep_learning_and_neural_networks|neural network]] with dense layers, it currently has limitations, such as the difficulty of batching inputs for different tasks (which require different A and B matrices) in a single forward pass <a class="yt-timestamp" data-t="01:14:00">[01:14:00]</a>.

Future research directions include combining LoRA with other efficient adaptation methods for orthogonal improvements <a class="yt-timestamp" data-t="01:50:00">[01:50:00]</a>. Furthermore, the "mechanism behind fine-tuning or LoRA is far from clear" <a class="yt-timestamp" data-t="01:50:00">[01:50:00]</a>, and LoRA's simplicity may make it more tractable to understand how features learned during pre-training adapt <a class="yt-timestamp" data-t="01:50:00">[01:50:00]</a>. The rank deficiency of the delta weights also suggests that the original model weights themselves could be rank efficient, inspiring further research into optimizing model architecture <a class="yt-timestamp" data-t="01:50:00">[01:50:00]</a>.