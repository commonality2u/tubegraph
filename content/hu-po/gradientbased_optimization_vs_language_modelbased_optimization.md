---
title: Gradientbased optimization vs language modelbased optimization
videoId: lR9isPmwZ3s
---

From: [[hu-po]] <br/> 

Optimization is a widely used term to describe a process of improving something <a class="yt-timestamp" data-t="02:43:46">[02:43:46]</a>. In a machine learning context, optimization often involves [[finetuning_language_models_for_specific_tasks | gradient descent]], where derivatives or slopes are calculated to adjust parameters and improve a loss function <a class="yt-timestamp" data-t="03:37:39">[03:37:39]</a> <a class="yt-timestamp" data-t="03:53:54">[03:53:54]</a>. However, the absence of a gradient poses challenges for many real-world applications <a class="yt-timestamp" data-t="03:27:27">[03:27:27]</a>.

This article explores two main approaches to optimization: traditional gradient-based algorithms and a novel method leveraging [[large_language_models_as_optimizers | Large Language Models (LLMs)]] for optimization, known as Optimization by Prompting (OPrO) <a class="yt-timestamp" data-t="03:34:25">[03:34:25]</a>.

## Gradient-based Optimization

Traditional optimization algorithms, particularly in [[finetuning_language_models_for_specific_tasks | machine learning]], largely depend on the ability to calculate the slope or gradient of a function <a class="yt-timestamp" data-t="04:39:26">[04:39:26]</a> <a class="yt-timestamp" data-t="04:47:04">[04:47:04]</a>. This process is known as gradient descent, where one iteratively moves in the direction of the steepest descent to minimize a loss function <a class="yt-timestamp" data-t="03:43:45">[03:43:45]</a>.

### Challenges of Gradient-based Methods
*   **Differentiability Requirement**: Every part of the pipeline needs to be differentiable, meaning a slope can be found <a class="yt-timestamp" data-t="04:59:02">[04:59:02]</a>.
*   **Continuous Spaces**: Gradients can only be taken in continuous spaces, not discrete ones <a class="yt-timestamp" data-t="06:21:40">[06:21:40]</a>. This is a challenge when applying optimization to problems described in [[large_language_models_and_their_applications | natural language]], which is inherently discrete <a class="yt-timestamp" data-t="06:04:07">[06:04:07]</a> <a class="yt-timestamp" data-t="06:07:14">[06:07:14]</a>.
*   **Complex Loss Landscapes**: Neural networks, for example, have billions of parameters, leading to highly complicated "loss landscapes" where it's difficult to guarantee finding the global minimum rather than just a local minimum <a class="yt-timestamp" data-t="04:08:08">[04:08:08]</a> <a class="yt-timestamp" data-t="04:59:57">[04:59:57]</a>. This necessitates continuous calculation of derivatives and careful navigation <a class="yt-timestamp" data-t="05:07:26">[05:07:26]</a>.

## Language Model-based Optimization (OPrO)

OPrO is a simple and effective approach that leverages [[large_language_models_as_optimizers | LLMs as optimizers]] where the optimization task is described in natural language <a class="yt-timestamp" data-t="05:52:52">[05:52:52]</a> <a class="yt-timestamp" data-t="05:56:57">[05:56:57]</a>. In each optimization step, the LLM generates new solutions based on a "metaprompt" that contains previously generated solutions and their values <a class="yt-timestamp" data-t="06:10:10">[06:10:10]</a> <a class="yt-timestamp" data-t="06:13:14">[06:13:14]</a>. This iterative process allows LLMs to optimize without explicitly calculating slopes or gradients <a class="yt-timestamp" data-t="05:23:22">[05:23:22]</a> <a class="yt-timestamp" data-t="05:26:27">[05:26:27]</a>.

[[large_language_models_and_their_applications | LLMs]] are considered [[large_language_models_and_their_applications | general pattern machines]] that excel at finding and extrapolating from patterns <a class="yt-timestamp" data-t="06:22:41">[06:22:41]</a> <a class="yt-timestamp" data-t="06:26:23">[06:26:23]</a>. Optimization can be viewed as a type of pattern matching, explaining why LLMs are capable of this task <a class="yt-timestamp" data-t="06:28:16">[06:28:16]</a>.

### Key Aspects and Challenges of OPrO:
*   **Natural Language Interface**: OPrO allows optimization tasks to be described in [[large_language_models_and_their_applications | natural language]], eliminating the need for formal specifications or deriving update steps <a class="yt-timestamp" data-t="07:28:10">[07:28:10]</a> <a class="yt-timestamp" data-t="07:39:18">[07:39:18]</a>.
*   **Iterative Process**: The LLM iteratively generates new solutions based on problem descriptions and previously found solutions <a class="yt-timestamp" data-t="07:30:16">[07:30:16]</a>. This is similar to the multi-step nature of training neural networks <a class="yt-timestamp" data-t="06:31:31">[06:31:31]</a>.
*   **Black Box Nature**: The precise reasons why certain prompts or iterations lead to improved performance are unknown, highlighting the "black box" nature of [[large_language_models_and_their_applications | LLMs]] <a class="yt-timestamp" data-t="07:22:14">[07:22:14]</a> <a class="yt-timestamp" data-t="07:26:24">[07:26:24]</a>.
*   **Exploration vs. Exploitation**: A fundamental challenge is balancing exploring new regions of the search space with exploiting promising areas where good solutions have been found <a class="yt-timestamp" data-t="08:12:09">[08:12:09]</a> <a class="yt-timestamp" data-t="08:29:29">[08:29:29]</a>. This can be tuned by adjusting the LLM's "sampling temperature" <a class="yt-timestamp" data-t="09:37:05">[09:37:05]</a> <a class="yt-timestamp" data-t="09:50:50">[09:50:50]</a>.
*   **Sensitivity to Initial Conditions**: OPrO's performance can be very sensitive to initial conditions or the initial prompt, potentially getting stuck in local optima if starting from a "terrible place" <a class="yt-timestamp" data-t="09:04:06">[09:04:06]</a> <a class="yt-timestamp" data-t="09:47:04">[09:47:04]</a>.
*   **Context Window Limitations**: The length limit of an LLM's context window can make it difficult to fit large-scale optimization problem descriptions into the prompt <a class="yt-timestamp" data-t="10:59:37">[10:59:37]</a>.

## Comparison and Applications

OPrO was demonstrated on two simple examples:
1.  **Linear Regression**: A continuous optimization problem where an LLM (Text Bison) performed comparably to GPT-4 in finding optimal coefficients (slope and y-intercept) <a class="yt-timestamp" data-t="08:39:26">[08:39:26]</a> <a class="yt-timestamp" data-t="08:50:33">[08:50:33]</a> <a class="yt-timestamp" data-t="09:59:59">[09:59:59]</a>.
2.  **Traveling Salesman Problem (TSP)**: A discrete optimization problem where LLMs showed proper capture of optimization directions on small-scale problems <a class="yt-timestamp" data-t="08:51:09">[08:51:09]</a> <a class="yt-timestamp" data-t="10:04:06">[10:04:06]</a>. However, Text Bison struggled with larger TSP sizes, suggesting that its internal "slope-based" intuition, effective for continuous problems, is less suited for discrete ones <a class="yt-timestamp" data-t="10:28:04">[10:28:04]</a> <a class="yt-timestamp" data-t="10:37:05">[10:37:05]</a>. GPT-4 significantly outperformed GPT-3.5 Turbo and Text Bison in finding the global optimum for TSP <a class="yt-timestamp" data-t="10:46:00">[10:46:00]</a>.

The most compelling application of OPrO is **Prompt Optimization** <a class="yt-timestamp" data-t="09:40:02">[09:40:02]</a>.
*   **Automating Prompt Engineering**: LLMs can optimize prompts to maximize task accuracy, effectively automating the human process of "prompt engineering" <a class="yt-timestamp" data-t="09:42:04">[09:42:04]</a> <a class="yt-timestamp" data-t="09:50:50">[09:50:50]</a>.
*   **Outperforming Humans**: LLM-optimized prompts outperformed human-designed prompts by up to 8% on benchmarks like GSM 8K (grade school math word problems) and Big Bench Hard (a diverse set of challenging tasks) <a class="yt-timestamp" data-t="10:32:00">[10:32:00]</a> <a class="yt-timestamp" data-t="11:21:41">[11:21:41]</a> <a class="yt-timestamp" data-t="11:32:00">[11:32:00]</a>. For example, adding "Take a deep breath and work on this problem step by step" increased accuracy on GSM 8K by 10% <a class="yt-timestamp" data-t="10:55:03">[10:55:03]</a>.
*   **Evaluation Metrics**: [[evaluation_metrics_for_language_models | Evaluation metrics for language models]] such as GSM 8K (8.5k high-quality math word problems) <a class="yt-timestamp" data-t="11:55:58">[11:55:58]</a> and Big Bench Hard (BBH, a benchmark of about 200 tasks created by Google) <a class="yt-timestamp" data-t="15:11:42">[15:11:42]</a> are used to assess the effectiveness of optimized prompts.
*   **Psychology of LLMs**: The significant impact of subtle prompt changes (e.g., "Let's think step by step" vs. "Take a deep breath and work on this problem step by step") suggests a "psychology for [[large_language_models_and_their_applications | LLMs]]" where specific phrasing can dramatically alter performance <a class="yt-timestamp" data-t="14:50:34">[14:50:34]</a>.

### Ablation Studies for OPrO
Ablation studies revealed several insights into how OPrO performs:
*   **Order of Instructions**: Sorting previous instructions by score (lowest to highest) in the metaprompt performed better than random or highest to lowest ordering, indicating a "recency bias" where LLMs are more influenced by tokens closer to the end of the prompt <a class="yt-timestamp" data-t="23:15:37">[23:15:37]</a> <a class="yt-timestamp" data-t="24:18:03">[24:18:03]</a>.
*   **Instruction Scores**: Providing accuracy scores to the LLM (e.g., bucketing them into 20 or 100 buckets) generally helps performance compared to no scores, though the optimal bucketing varied by task <a class="yt-timestamp" data-t="25:52:04">[25:52:04]</a> <a class="yt-timestamp" data-t="26:36:00">[26:36:00]</a>.
*   **Number of Exemplars**: Including a few examples (e.g., three) from the task was critical for the optimizer LLM to understand and phrase new instructions, while more exemplars didn't necessarily improve performance and could even distract the LLM <a class="yt-timestamp" data-t="28:48:47">[28:48:47]</a>.
*   **Generated Instructions per Step**: Generating multiple solutions (e.g., 8) at each optimization step improved stability, akin to how mini-batches reduce variance in stochastic gradient descent <a class="yt-timestamp" data-t="31:37:00">[31:37:00]</a>.

## Implications and Future Outlook

While OPrO currently has limitations, such as context window size and difficulty with highly "bumpy" loss landscapes, it demonstrates the capacity of [[large_language_models_and_their_applications | LLMs]] to progressively generate improved solutions based on past optimization trajectories <a class="yt-timestamp" data-t="45:17:51">[45:17:51]</a> <a class="yt-timestamp" data-t="45:51:50">[45:51:50]</a>.

The paper serves as a proof of concept, suggesting a future where [[large_language_models_as_optimizers | LLMs]] could potentially replace traditional gradient-based methods entirely <a class="yt-timestamp" data-t="39:02:00">[39:02:00]</a>. One speculative next step is to use LLMs to directly optimize the weights and biases of neural networks on simple tasks like MNIST, which would be a "huge result" <a class="yt-timestamp" data-t="46:59:00">[46:59:00]</a> <a class="yt-timestamp" data-t="47:11:07">[47:11:07]</a>. This capability could lead to a world where computers are interacted with purely through [[large_language_models_and_their_applications | natural language]], and any optimization problem describable in text could be solved by an LLM <a class="yt-timestamp" data-t="39:40:00">[39:40:00]</a> <a class="yt-timestamp" data-t="39:50:50">[39:50:50]</a>.

The ability for LLMs to generate meta-instructions for other LLMs, leading to a recursive optimization process, points towards a form of "Meta Meta Meta Meta Meta learning" and potentially signifies reaching the Singularity <a class="yt-timestamp" data-t="43:33:00">[43:33:00]</a> <a class="yt-timestamp" data-t="44:00:00">[44:00:00]</a>.