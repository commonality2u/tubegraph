---
title: Large Language Models in Gaming
videoId: hhawa3tFN2s
---

From: [[hu-po]] <br/> 

[[large_language_models_and_their_applications | Large Language Models]] (LLMs) are being increasingly applied to gaming environments, leveraging their vast "world model" intuited from text data. One notable example is "Voyager," an open-ended embodied agent designed for Minecraft that uses LLMs for lifelong learning <a class="yt-timestamp" data-t="01:14:16">[01:14:16]</a>.

## Voyager: An LLM-Powered Agent
Voyager is an LLM-powered embodied lifelong learning agent <a class="yt-timestamp" data-t="01:16:16">[01:16:16]</a> that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention <a class="yt-timestamp" data-t="01:32:01">[01:32:01]</a>. It leverages the internet-scale knowledge contained within models like GPT-4 and GPT-3.5 <a class="yt-timestamp" data-t="00:55:54">[00:55:54]</a>.

### Key Components
Voyager consists of three main components <a class="yt-timestamp" data-t="00:34:34">[00:34:34]</a>:
1.  **Automatic Curriculum:** This component maximizes exploration by proposing increasingly difficult tasks <a class="yt-timestamp" data-t="00:37:36">[00:37:36]</a>. It is generated by GPT-4 based on the goal of discovering as many diverse things as possible, similar to curiosity-driven exploration <a class="yt-timestamp" data-t="00:38:00">[00:38:00]</a>. The curriculum unfolds in a "bottom-up" fashion, adapting to the agent's exploration progress and current state <a class="yt-timestamp" data-t="00:46:34">[00:46:34]</a>.
2.  **Skill Library:** A never-growing library of executable code for storing and retrieving complex behaviors <a class="yt-timestamp" data-t="00:11:25">[00:11:25]</a>. Each program is stored as an embedding in a vector database <a class="yt-timestamp" data-t="00:36:26">[00:36:26]</a>, allowing the agent to retrieve relevant skills for similar situations <a class="yt-timestamp" data-t="00:37:12">[00:37:12]</a>. Complex skills can be synthesized by composing simpler programs <a class="yt-timestamp" data-t="00:38:14">[00:38:14]</a>.
3.  **Iterative Prompting Mechanism:** This system incorporates environment feedback, execution errors, and self-verification for program improvement <a class="yt-timestamp" data-t="00:12:02">[00:12:02]</a>. It addresses the challenge of LLMs struggling to consistently produce correct action code in one shot <a class="yt-timestamp" data-t="00:39:09">[00:39:09]</a>.

### Interaction with the Environment
Voyager interacts with Minecraft through a high-level JavaScript API called "Mindflayer" <a class="yt-timestamp" data-t="01:15:48">[01:15:48]</a>. This means the agent does not interact with the game via pixels or low-level motor commands (like moving a mouse or clicking) <a class="yt-timestamp" data-t="00:21:54">[00:21:54]</a>. Instead, it generates and executes code that calls functions in the game's API, such as `craft stone sword` <a class="yt-timestamp" data-t="00:21:25">[00:21:25]</a>.

The agent receives detailed environmental feedback as structured data, including inventory, equipment, nearby blocks and entities (within a 32-block distance), biome, time, health, and hunger bars <a class="yt-timestamp" data-t="01:10:29">[01:10:29]</a>, <a class="yt-timestamp" data-t="01:42:24">[01:42:24]</a>. This perfect knowledge of the game state is a significant advantage over human players <a class="yt-timestamp" data-t="01:43:02">[01:43:02]</a>.

### Performance
Voyager shows strong in-context lifelong learning capabilities <a class="yt-timestamp" data-t="01:14:16">[01:14:16]</a>. It obtains 3.3 times more unique items, travels 2.3 times longer distances, and unlocks key "Tech Tree Milestones" up to 15 times faster than previous state-of-the-art methods <a class="yt-timestamp" data-t="00:07:47">[00:07:47]</a>. It can utilize its learned skill library in a new Minecraft world to solve novel tasks from scratch, demonstrating generalization capabilities <a class="yt-timestamp" data-t="00:42:54">[00:42:54]</a>.

### Limitations and Challenges
1.  **Dependency on LLM World Knowledge:** The success of Voyager heavily relies on the fact that [[large_language_models_and_their_applications | large language models]] like GPT-4 have been trained on vast amounts of internet data, including information about Minecraft <a class="yt-timestamp" data-t="00:30:49">[00:30:49]</a>. This means the LLM already "knows" how to play and beat the game, making the "exploration" aspect less about true discovery and more about leveraging existing knowledge <a class="yt-timestamp" data-t="00:31:31">[00:31:31]</a>. This approach might not generalize to new or obscure games not heavily discussed online <a class="yt-timestamp" data-t="00:32:21">[00:32:21]</a>.
2.  **API vs. Pixel Input:** The use of a high-level API simplifies the problem significantly compared to methods that interpret screen pixels and output low-level controls <a class="yt-timestamp" data-t="01:17:33">[01:17:33]</a>. This means Voyager doesn't solve 3D perception or sensory-motor control problems, which are crucial for [[large_language_models_in_robotics | Large language models in robotics]] and real-world applications <a class="yt-timestamp" data-t="01:18:10">[01:18:10]</a>. The real world does not provide clean text-based error messages like a game API <a class="yt-timestamp" data-t="01:39:17">[01:39:17]</a>.
3.  **Cost and Efficiency:** Using GPT-4 is expensive, being 15 times more costly than GPT-3.5 <a class="yt-timestamp" data-t="01:29:07">[01:29:07]</a>. The reliance on external LLM APIs means that improvements in performance might largely depend on waiting for better, more powerful LLMs to be released <a class="yt-timestamp" data-t="01:31:45">[01:31:45]</a>.
4.  **Hallucinations:** Despite the iterative prompting mechanism, the agent can still get stuck or hallucinate non-existent items or invalid fuel sources, reflecting inaccuracies in the LLM's knowledge base <a class="yt-timestamp" data-t="01:30:05">[01:30:05]</a>.

## Implications for Reinforcement Learning and AI
The success of Voyager highlights a shift in the approach to reinforcement learning (RL):
*   **LLMs as World Models:** The ability of LLMs to build effective world models from text data challenges traditional RL methods that explicitly learn world models <a class="yt-timestamp" data-t="01:34:58">[01:34:58]</a>.
*   **Prompting over Fine-tuning:** Voyager bypasses the need for model parameter fine-tuning or explicit gradient-based training <a class="yt-timestamp" data-t="01:33:50">[01:33:50]</a>, showcasing the power of "in-context learning" and sophisticated prompting strategies.
*   **Simplified RL Components:** Concepts like "curiosity" are no longer requiring complex mathematical definitions but can be guided by simple text prompts like "discover as many diverse things as possible" <a class="yt-timestamp" data-t="01:36:37">[01:36:37]</a>. Similarly, reward functions are simplified to asking the LLM if a task is completed <a class="yt-timestamp" data-t="01:50:42">[01:50:42]</a>.
*   **Future of RL:** This approach suggests a future where RL systems might revolve around LLMs, vector databases for skill retrieval, and sophisticated Chain of Thought processes, potentially rendering traditional deep reinforcement learning algorithms less relevant <a class="yt-timestamp" data-t="01:09:47">[01:09:47]</a>. This aligns with the "bitter lesson" in AI research, where simpler, scalable methods often outperform complex, specialized ones given sufficient compute and data <a class="yt-timestamp" data-t="01:33:47">[01:33:47]</a>.

While impressive in simulation, the direct application of Voyager's exact methodology to real-world robotics is challenging due to the lack of perfect state information and textual error feedback <a class="yt-timestamp" data-t="01:39:52">[01:39:52]</a>. However, it motivates the development of powerful "generalist agents" that do not require extensive model tuning <a class="yt-timestamp" data-t="01:40:48">[01:40:48]</a>. The trend of LLMs being at the core of [[impact_of_large_language_models_on_robotic_capabilities | robotic capabilities]] and embodied agents for [[large_language_models_in_robotics | Large language models in robotics]] is evident <a class="yt-timestamp" data-t="01:51:42">[01:51:42]</a>.

> [!NOTE] Embodied Agent
> An "embodied agent" refers to an entity that performs actions and receives rewards in a reinforcement learning setup, possessing a position within space and time, analogous to a human body as an interface to spacetime <a class="yt-timestamp" data-t="01:49:00">[01:49:00]</a>.

> [!NOTE] Lifelong Learning
> "Lifelong learning" refers to an agent's ability to progressively acquire, update, accumulate, and transfer knowledge over extended time spans and multiple episodes or games <a class="yt-timestamp" data-t="00:56:59">[00:56:59]</a>.

> [!NOTE] Catastrophic Forgetting
> This phenomenon occurs when a neural network, after being trained on one task, forgets how to perform that task when subsequently trained on a different task <a class="yt-timestamp" data-t="00:13:31">[00:13:31]</a>. Voyager aims to alleviate this through its skill library <a class="yt-timestamp" data-t="00:13:27">[00:13:27]</a>.

> [!NOTE] Chain of Thought / Iterative Prompting
> An iterative prompting mechanism involves taking an LLM's output, feeding it back into the LLM, and asking it to refine its output based on feedback (e.g., environment observations, execution errors) <a class="yt-timestamp" data-t="00:39:51">[00:39:51]</a>. This process allows the model to "self-verify" its progress <a class="yt-timestamp" data-t="00:40:02">[00:40:02]</a>.