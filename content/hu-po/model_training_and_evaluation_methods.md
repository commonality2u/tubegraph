---
title: Model training and evaluation methods
videoId: yxAcTRp9EyQ
---

From: [[hu-po]] <br/> 

The development of large artificial neural networks has seen significant progress, particularly with the advent of the Transformer architecture. However, the science behind their success remains complex amid a vast array of results <a class="yt-timestamp" data-t="00:07:02">[00:07:02]</a>. While scaling laws indicate performance improvement with increased compute or network size, a more refined approach involves focusing on data quality <a class="yt-timestamp" data-t="00:08:08">[00:08:08]</a>.

This article explores a methodology proposed by Microsoft Research in their paper "Textbooks Are All You Need," which introduces Phi-1, a language model for code that achieves competitive performance with significantly smaller size and less training compute by emphasizing high-quality training data <a class="yt-timestamp" data-t="00:04:06">[00:04:06]</a>.

## Key Model: Phi-1

Phi-1 is a Transformer-based language model specifically designed for code <a class="yt-timestamp" data-t="00:02:02">[00:02:02]</a>. A notable characteristic is its small size: 1.3 billion parameters, which is significantly smaller than models like Falcon (40 billion) or Llama (65 billion) <a class="yt-timestamp" data-t="00:03:39">[00:03:39]</a>. Despite its small scale, Phi-1 attains a Pass@1 accuracy of 50% on HumanEval, a benchmark for code generation <a class="yt-timestamp" data-t="00:05:02">[00:05:02]</a>.

The model's training cost was also relatively low, taking four days on eight A100 GPUs, which represents a single server rack and tens of thousands of dollars, as opposed to hundreds of thousands or millions <a class="yt-timestamp" data-t="00:03:44">[00:03:44]</a>.

## Data Quality: The "Textbook" Approach

The central hypothesis of this work is that higher quality data leads to better results, aligning with the long-standing understanding that data cleaning is crucial in data set creation <a class="yt-timestamp" data-t="00:08:30">[00:08:30]</a>. The paper argues that improving data quality can dramatically alter scaling laws, potentially allowing smaller models with leaner training to match the performance of large-scale models <a class="yt-timestamp" data-t="00:09:08">[00:09:08]</a>. This approach likens the training process to teaching a human child, where the choice of curriculum and the sequence of information matter <a class="yt-timestamp" data-t="00:09:37">[00:09:37]</a>.

The training data for Phi-1 consisted of:
*   **Filtered web data:** 6 billion tokens of "textbook quality" data from the web <a class="yt-timestamp" data-t="00:04:06">[00:04:06]</a>.
*   **Synthetically generated data:** 1 billion tokens of synthetically generated textbooks and exercises using GPT-3 <a class="yt-timestamp" data-t="00:04:16">[00:04:16]</a>.
    *   This [[training_and_data_preparation_methodologies|synthetic data]] allows for greater control over the training distribution, thereby influencing the desired behavior of the LLM <a class="yt-timestamp" data-t="00:17:37">[00:17:37]</a>.

## Training Stages and Data Sets

The model underwent two distinct training phases:

1.  **Pre-training:**
    *   **Model:** Phi-1 base (1.3 billion parameters) <a class="yt-timestamp" data-t="01:03:04">[01:03:04]</a>.
    *   **Data:** A combination of filtered code language from the stack and Stack Overflow, and the synthetically generated textbook data. This combined data set is referred to as "[[training_and_data_preparation_methodologies|Code Textbook]]" <a class="yt-timestamp" data-t="00:31:58">[00:31:58]</a>. It contains less than 7 billion tokens in total <a class="yt-timestamp" data-t="00:31:54">[00:31:54]</a>.
    *   **Process:** Trained for 8 epochs, equivalent to over 50 billion training tokens <a class="yt-timestamp" data-t="01:00:19">[01:00:19]</a>.

2.  **Fine-tuning:**
    *   **Model:** Phi-1 (fine-tuned from Phi-1 base) and Phi-1 small (a 350-million parameter version) <a class="yt-timestamp" data-t="01:03:01">[01:03:01]</a>.
    *   **Data:** A small [[training_and_data_preparation_methodologies|Synthetic Exercises]] data set, consisting of less than 100 million tokens of Python exercises where each exercise is a docstring of a function to be completed <a class="yt-timestamp" data-t="00:43:47">[00:43:47]</a>. This data set was also generated by GPT-3.5 <a class="yt-timestamp" data-t="00:44:05">[00:44:05]</a>.
    *   **Purpose:** To align the model to perform function completion tasks based on natural language instructions <a class="yt-timestamp" data-t="00:43:58">[00:43:58]</a>.

Both training phases used a decoder-only Transformer model architecture, incorporating techniques like Flash Attention and rotary position embeddings (ROPE) <a class="yt-timestamp" data-t="00:47:55">[00:47:55]</a>. Training was conducted using floating point 16 (FP16) precision <a class="yt-timestamp" data-t="00:54:05">[00:54:05]</a>.

## Challenges with Standard Data Sets

Traditional code data sets, while large and diverse, often pose challenges for training language models:
*   **Lack of instructiveness:** Many code snippets are not ideal for learning basic coding concepts <a class="yt-timestamp" data-t="00:23:14">[00:23:14]</a>.
*   **Lack of self-containment:** Samples often depend on external modules or files, making them hard to understand without additional context <a class="yt-timestamp" data-t="00:24:09">[00:24:09]</a>.
*   **Skewed distribution:** Examples are often biased towards certain topics or use cases, such as coding challenges (e.g., LeetCode problems), rather than reflecting real-world production code <a class="yt-timestamp" data-t="00:28:39">[00:28:39]</a>. This results in an [[Data Curation and Model Evaluation in AI|unbalanced distribution]] of coding concepts <a class="yt-timestamp" data-t="00:28:41">[00:28:41]</a>.

Manual [[Data Curation and Model Evaluation in AI|data cleaning]] and inspection are crucial to address these issues, as they help identify problematic examples that might hinder model performance <a class="yt-timestamp" data-t="00:23:57">[00:23:57]</a>.

## Data Curation & Synthetic Data Generation

To overcome the limitations of standard data, the researchers implemented a rigorous data curation and generation process:
*   **Filtered Code Language Data Set:** A Transformer-based classifier was trained to filter 35 million Python files and samples from the stack and Stack Overflow <a class="yt-timestamp" data-t="00:33:01">[00:33:01]</a>. GPT-4 was used to annotate a small subset of this data for its "educational value" <a class="yt-timestamp" data-t="00:33:41">[00:33:41]</a>, enabling the classifier to identify high-quality, self-contained, and instructive code examples <a class="yt-timestamp" data-t="00:35:48">[00:35:48]</a>.
*   **Synthetic Textbook Data Set:** Generated using GPT-3.5, this data set focuses on natural language heavy text interleaved with relevant code snippets, promoting reasoning and basic algorithmic skills <a class="yt-timestamp" data-t="00:41:36">[00:41:36]</a>. Diversity was achieved by constraining topics and target audiences within the prompts <a class="yt-timestamp" data-t="00:42:12">[00:42:12]</a>.
*   **Synthetic Exercises Data Set:** Also generated by GPT-3.5, this data set focuses on function completion tasks, with diversity achieved by constraining function names <a class="yt-timestamp" data-t="00:44:10">[00:44:10]</a>. The testability of code (ability to run and check for errors) makes synthetic generation for code particularly effective <a class="yt-timestamp" data-t="00:39:46">[00:39:46]</a>.

The use of clear function names and type hints in Python was identified as important for LLMs to understand the purpose and expected input/output of code snippets from the beginning of token processing <a class="yt-timestamp" data-t="00:45:15">[00:45:15]</a>.

## Model Evaluation

[[Data Curation and Model Evaluation in AI|Model evaluation]] was performed using established benchmarks:
*   **HumanEval:** A widely adopted benchmark for comparing LLM performance on code <a class="yt-timestamp" data-t="01:13:58">[01:13:58]</a>.
*   **MBP (Mostly Basic Python Programs):** Another relevant benchmark <a class="yt-timestamp" data-t="01:05:01">[01:05:01]</a>.
*   **Metric:** Pass@1 accuracy, which means the model must get the answer right on the first attempt <a class="yt-timestamp" data-t="00:05:04">[00:05:04]</a>.

Despite its small scale, Phi-1 achieved 50.6% accuracy on HumanEval and 55% on MBP <a class="yt-timestamp" data-t="01:05:01">[01:05:01]</a>. The paper highlights that this performance surpasses many larger open-source models, some of which are 10 times bigger and trained on 100 times more data <a class="yt-timestamp" data-t="01:22:18">[01:22:18]</a>.

### Emergent Capabilities and Generalization
Fine-tuning on the code exercises data set unexpectedly improved the model's ability to use external libraries (like Pygame and Tkinter), even though these libraries were not present in the fine-tuning data <a class="yt-timestamp" data-t="01:04:08">[01:04:08]</a>. This suggests that the fine-tuning process helped the model reorganize and consolidate knowledge acquired during pre-training, leading to [[challenges_and_innovations_in_model_adaptation|emergent abilities]] and generalization beyond the specific training tasks <a class="yt-timestamp" data-t="01:01:50">[01:01:50]</a>. Phi-1 also demonstrated better chat capabilities than its base model, despite chat being exclusive to pre-training and not fine-tuning <a class="yt-timestamp" data-t="01:13:00">[01:13:00]</a>.

### Contamination and Pruning
A significant concern in [[Data Curation and Model Evaluation in AI|model evaluation]] is data contamination, where models might perform well on benchmarks because they have "memorized" parts of the test data <a class="yt-timestamp" data-t="01:14:13">[01:14:13]</a>. The researchers addressed this by:
*   **Unconventional Evaluations:** Creating new evaluation problems unlikely to appear in any training data set <a class="yt-timestamp" data-t="01:17:47">[01:17:47]</a>.
*   **Data Pruning:** Aggressively pruning over 40% of the code exercises data set using a combination of embedding and syntax-based distances (Abstract Syntax Trees - AST) to identify similar code snippets. Even after this significant pruning, Phi-1 still outperformed other models <a class="yt-timestamp" data-t="01:24:24">[01:24:24]</a>.

The paper also discusses the controversial practice of using GPT-4 to grade solutions, noting that while it leverages GPT-4's knowledge for fine-grained evaluation, there are concerns about potential biases, such as judging solutions based on subtle word ordering or choice that a human might not prioritize <a class="yt-timestamp" data-t="01:22:23">[01:22:23]</a>.

## Conclusion

The "Textbooks Are All You Need" paper provides strong evidence that high-quality, carefully curated, and synthetically generated data sets can dramatically improve the learning efficiency of language models, particularly for code generation tasks <a class="yt-timestamp" data-t="01:32:00">[01:32:00]</a>. This approach allows for the creation of smaller, more efficient models that still achieve state-of-the-art performance, highlighting [[model_architecture_and_data_quality_in_ai|the critical role of data quality]] over sheer data quantity or model size <a class="yt-timestamp" data-t="01:32:25">[01:32:25]</a>.

While Phi-1 is specialized in Python coding and has some limitations (e.g., less robustness to stylistic variations in prompts, and challenges with external APIs), the researchers believe these are not fundamental and can be addressed with further work. The paper suggests that future research should focus on developing methodologies for creating high-quality, diverse, and non-repetitive data sets, possibly even using more advanced LLMs like GPT-4 for synthetic data generation <a class="yt-timestamp" data-t="01:33:50">[01:33:50]</a>. The idea of "domain randomization" for text, analogous to techniques used in computer vision synthetic data, is also proposed to enhance diversity <a class="yt-timestamp" data-t="01:34:09">[01:34:09]</a>.