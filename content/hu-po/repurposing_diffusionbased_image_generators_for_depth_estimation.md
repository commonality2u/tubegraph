---
title: Repurposing diffusionbased image generators for depth estimation
videoId: WoiI_Pn9yHw
---

From: [[hu-po]] <br/> 

[[monocular_depth_estimation_and_its_challenges | Monocular depth estimation]] is a fundamental computer vision task that aims to recover 3D depth from a single 2D image <a class="yt-timestamp" data-t="09:37:37">[09:37:37]</a>. This task is geometrically ill-posed and requires significant scene understanding <a class="yt-timestamp" data-t="09:43:41">[09:43:41]</a>. Historically, approaches leveraged camera geometry, often using stereo or multi-view setups with multiple cameras to infer depth <a class="yt-timestamp" data-t="05:27:00">[05:27:00]</a>. However, modern research increasingly focuses on [[monocular_depth_estimation_and_its_challenges | monocular depth estimation]] from a single image <a class="yt-timestamp" data-t="06:01:00">[06:01:00]</a>.

Traditional [[monocular_depth_estimation_and_its_challenges | monocular depth estimators]] (e.g., using CNNs or Transformer architectures) often struggle when presented with unfamiliar or out-of-distribution content, as their knowledge is limited by their training data <a class="yt-timestamp" data-t="11:06:06">[11:06:06]</a>. This challenge has led to exploration of leveraging extensive priors captured in recent generative [[diffusion_models_and_image_generation | diffusion models]] to enable more generalizable depth estimation <a class="yt-timestamp" data-t="12:20:00">[12:20:00]</a>.

## Marigold: Leveraging Diffusion Models

The paper "Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation" (published December 4, 2023, by ETH Zurich) introduces Marigold <a class="yt-timestamp" data-t="03:20:00">[03:20:00]</a>, a method that repurposes [[text_to_image_diffusion_models | diffusion models]] like Stable Diffusion for [[monocular_depth_estimation_and_its_challenges | monocular depth estimation]] <a class="yt-timestamp" data-t="03:51:00">[03:51:00]</a>. The core idea is to capitalize on the robust image priors learned by these models, which have been trained on internet-scale datasets (like LAION-5B, containing billions of images) <a class="yt-timestamp" data-t="24:21:00">[24:21:00]</a>, giving them a deep understanding of the visual world, including implicit depth <a class="yt-timestamp" data-t="04:00:00">[04:00:00]</a>.

### Core Approach
Marigold poses [[monocular_depth_estimation_and_its_challenges | monocular depth estimation]] as a conditional denoising diffusion generation task <a class="yt-timestamp" data-t="25:09:00">[25:09:00]</a>. This involves:

1.  **Latent Space Operation:** Marigold operates in a low-dimensional latent space for computational efficiency and suitability for high-resolution images <a class="yt-timestamp" data-t="29:58:00">[29:58:00]</a>.
2.  **Frozen VAE:** It utilizes a pre-trained, *frozen* Variational Autoencoder (VAE) (from Stable Diffusion 2) to encode both the input image and the corresponding depth map into this latent space <a class="yt-timestamp" data-t="31:51:00">[31:51:00]</a>.
    *   **"Deep Learning Magic":** Surprisingly, this VAE, trained solely on RGB images from LAION-5B (which contains no explicit depth images) <a class="yt-timestamp" data-t="33:10:00">[33:10:00]</a>, can encode depth images (which are single-channel) by simply duplicating the single depth channel across three dimensions to simulate an RGB image <a class="yt-timestamp" data-t="34:19:00">[34:19:00]</a>. The encoded latent code allows for negligible error reconstruction <a class="yt-timestamp" data-t="35:01:00">[35:01:00]</a>.
3.  **Adapted Denoising U-Net:** The denoising U-Net (the core of the [[diffusion_models_and_image_generation | diffusion model]]) is fine-tuned to predict noise in the depth latent given the image latent and time step <a class="yt-timestamp" data-t="27:51:00">[27:51:00]</a>.
    *   **Concatenation Trick:** The image and depth latent codes are directly concatenated along the feature dimension as input to the U-Net <a class="yt-timestamp" data-t="37:28:00">[37:28:00]</a>. To accommodate the expanded input, the U-Net's input channels are doubled, and the initial weights are duplicated and divided by two to prevent activation magnitude inflation <a class="yt-timestamp" data-t="38:47:00">[38:47:00]</a>. This unconventional modification remarkably works <a class="yt-timestamp" data-t="40:40:00">[40:40:00]</a>.
4.  **Training on [[synthetic_data_for_training_depth_estimation_models | Synthetic Data]]**: Marigold is fine-tuned for a couple of days on a single consumer GPU (like an Nvidia 90) <a class="yt-timestamp" data-t="16:06:00">[16:06:00]</a> using only [[synthetic_data_for_training_depth_estimation_models | synthetic training data]] (e.g., Hypersim and Virtual KITTI) <a class="yt-timestamp" data-t="19:00:00">[19:00:00]</a>.
    *   **Advantages of Synthetic Data:** Real depth datasets suffer from missing values and noise due to physical sensor constraints (e.g., reflective surfaces affecting LiDAR, shadowing in structured light sensors) <a class="yt-timestamp" data-t="47:18:00">[47:18:00]</a>. [[synthetic_data_for_training_depth_estimation_models | Synthetic depth]], generated from game engines, is inherently dense and perfect <a class="yt-timestamp" data-t="51:38:00">[51:38:00]</a>, providing clean examples that reduce noise during fine-tuning <a class="yt-timestamp" data-t="52:03:00">[52:03:00]</a>.
5.  **Affine Invariant Depth:** The model predicts "affine invariant depth" <a class="yt-timestamp" data-t="12:46:00">[12:46:00]</a>. This means the output depth values represent relative distances (e.g., pixel A is closer than pixel B) but not absolute physical distances (like meters) <a class="yt-timestamp" data-t="14:27:00">[14:27:00]</a>. This is partly due to the normalization approach where depth values are mapped to a range like -1 to 1, independent of original data statistics <a class="yt-timestamp" data-t="44:02:00">[44:02:00]</a>.
    *   > [!NOTE] Practical Implications
        > For applications requiring true metric depth (e.g., [[techniques_for_text_to_3d_conversion_involving_diffusion_models | 3D reconstruction]] or SLAM), an additional step would be needed to scale the affine invariant depth to real-world units, often by making an assumption about a known distance in the scene <a class="yt-timestamp" data-t="13:37:00">[13:37:00]</a>.
6.  **Accelerated Inference:** Marigold follows the DDIM (Denoising Diffusion Implicit Models) approach of non-Markovian sampling with respaced steps for accelerated inference <a class="yt-timestamp" data-t="56:41:00">[56:41:00]</a>. This allows skipping intermediate steps in the denoising process, significantly reducing the number of inferences required to generate a depth map <a class="yt-timestamp" data-t="59:51:00">[59:51:00]</a>.
7.  **Test-Time Ensembling:** To further improve results, Marigold employs a test-time ensembling scheme <a class="yt-timestamp" data-t="01:01:48">[01:01:48]</a>. Multiple inference passes (e.g., 10 or 20) are performed with different initial noise samples, and their median is taken for the final depth map <a class="yt-timestamp" data-t="01:02:06">[01:02:06]</a>. This reduces the absolute relative error by up to 9.5% for 20 ensembles <a class="yt-timestamp" data-t="01:09:20">[01:09:20]</a>. However, this significantly increases inference time, as each pass involves multiple steps <a class="yt-timestamp" data-t="01:10:07">[01:10:07]</a>.

Marigold delivers state-of-the-art performance across a wide range of datasets, outperforming prior art in most cases and securing the highest overall ranking <a class="yt-timestamp" data-t="01:05:58">[01:05:58]</a>. This confirms the hypothesis that a comprehensive representation of the visual world, as captured by large-scale [[diffusion_models_and_image_generation | diffusion models]], is key to robust [[monocular_depth_estimation_and_its_challenges | monocular depth estimation]] <a class="yt-timestamp" data-t="01:06:03">[01:06:03]</a>.

## Patch Fusion: A Tile-Based Alternative

Another paper, "Patch Fusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation" (also published December 4, 2023, by King Abdullah University of Science and Technology) <a class="yt-timestamp" data-t="07:16:00">[07:16:00]</a>, addresses the challenge of high-resolution depth estimation.

### Core Approach
Patch Fusion's main trick is to take a high-resolution image, cut it into overlapping patches, and then run [[monocular_depth_estimation_and_its_challenges | monocular depth estimation]] independently on each patch <a class="yt-timestamp" data-t="01:14:10">[01:14:10]</a>. Because these predictions are independent, the depth values across patch boundaries are inconsistent <a class="yt-timestamp" data-t="01:14:58">[01:14:58]</a>. The paper then introduces a "consistency-aware training" process to fuse these inconsistent patch predictions, ensuring better alignment at boundaries <a class="yt-timestamp" data-t="01:15:17">[01:15:17]</a>. This method yields highly detailed depth maps <a class="yt-timestamp" data-t="01:16:11">[01:16:11]</a>.

### Critique
Unlike Marigold, Patch Fusion trains its models (a course network, a fine network, and a guided fusion network) from scratch <a class="yt-timestamp" data-t="01:17:46">[01:17:46]</a>. It also relies on older architectures like ConvNets <a class="yt-timestamp" data-t="01:17:51">[01:17:51]</a>, which is considered less aligned with modern deep learning practices that leverage large pre-trained models.

## Implications and Future Outlook
The progress in [[monocular_depth_estimation_and_its_challenges | monocular depth estimation]] driven by [[diffusion_models_and_image_generation | diffusion models]] has significant implications:

*   **Replacing Hardware Sensors:** The ability to generate high-quality depth maps from a single RGB image challenges the need for expensive, specialized depth sensors like LiDAR and structured light sensors, which often produce noisy or incomplete "ground truth" data due to physical limitations <a class="yt-timestamp" data-t="01:06:50">[01:06:50]</a>. This could disrupt entire industries reliant on these sensors <a class="yt-timestamp" data-t="01:08:20">[01:08:20]</a>.
*   **Accessibility:** Obtaining accurate depth becomes more accessible, requiring only a standard camera (e.g., a cell phone camera) and software inference <a class="yt-timestamp" data-t="01:07:56">[01:07:56]</a>.
*   **Computational Cost:** While current methods like Marigold are state-of-the-art, the inference speed, especially with ensembling, can be a limitation for real-time applications (e.g., autonomous vehicles) <a class="yt-timestamp" data-t="01:10:09">[01:10:09]</a>. However, advancements in [[video_diffusion_models_in_generative_3d | video diffusion models]] and techniques like Latent Consistency Models (LCMs) promise drastically faster inference <a class="yt-timestamp" data-t="01:42:42">[01:42:42]</a>, potentially overcoming this hurdle.

> [!NOTE] Future Research Direction
> Combining the strengths of both papers—Marigold's robust pre-trained [[diffusion_models_and_image_generation | diffusion model]]-based approach with Patch Fusion's tile-based strategy for high-resolution details—could lead to even better [[monocular_depth_estimation_and_its_challenges | monocular depth estimation]] results <a class="yt-timestamp" data-t="01:18:50">[01:18:50]</a>. This could involve taking an image, splitting it into patches, feeding each patch through Marigold's pipeline, and then fusing the results <a class="yt-timestamp" data-t="01:19:00">[01:19:00]</a>. This "simple trick" could potentially achieve new [[future_potential_of_3d_diffusion_models | state-of-the-art]] performance <a class="yt-timestamp" data-t="01:19:03">[01:19:03]</a>.