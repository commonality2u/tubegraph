---
title: developments in multiturn conversational AI
videoId: YLdK-683lCY
---

From: [[hu-po]] <br/> 

Multi-turn conversation is the standard for how users interact with AI systems <a class="yt-timestamp" data-t="00:40:01">[00:40:01]</a>. This involves an ongoing dialogue where the AI system needs to maintain context and adapt its responses based on prior interactions <a class="yt-timestamp" data-t="00:40:03">[00:40:03]</a>.

## Benchmarking Conversational RAG: The Coral Benchmark

Most existing [[Challenges and potentials of AI in language and reasoning tasks | Retrieval Augmented Generation]] (RAG) benchmarks are designed for single-turn interactions, highlighting a need for multi-turn benchmarks <a class="yt-timestamp" data-t="00:40:26">[00:40:26]</a>. To address this, the Coral benchmark was introduced as a large-scale RAG system specifically for multi-turn information-seeking conversations <a class="yt-timestamp" data-t="00:39:53">[00:39:53]</a>. This benchmark was automatically derived from Wikipedia <a class="yt-timestamp" data-t="00:39:58">[00:39:58]</a>.

### Challenges in Multi-Turn Settings

In multi-turn settings, AI systems must:
*   Handle redundant or irrelevant information from previous interactions <a class="yt-timestamp" data-t="00:40:34">[00:40:34]</a>.
*   Cope with abrupt topic shifts <a class="yt-timestamp" data-t="00:40:39">[00:40:39]</a>. An example showed GPT-4o effectively realizing that an earlier part of a multi-turn conversation was irrelevant when a new, unrelated question was posed <a class="yt-timestamp" data-t="00:40:53">[00:40:53]</a>.

### Key Features of Multi-Turn RAG Systems

According to the Coral paper, critical features for multi-turn conversational RAG systems include:
*   **Open domain coverage:** The ability to handle questions across a wide range of topics <a class="yt-timestamp" data-t="00:41:28">[00:41:28]</a>.
*   **Knowledge intensiveness:** The capacity to retrieve and generate deep, contextual knowledge <a class="yt-timestamp" data-t="00:41:32">[00:41:32]</a>.
*   **Free-form response generation:** Producing detailed, contextually rich answers <a class="yt-timestamp" data-t="00:41:37">[00:41:37]</a>.
*   **Handling of topic shifts:** Managing sudden changes in dialogue context without carrying over irrelevant information <a class="yt-timestamp" data-t="00:41:40">[00:41:40]</a>.
*   **Citation labeling:** Citing sources for generated information <a class="yt-timestamp" data-t="00:41:46">[00:41:46]</a>. However, it's noted that the quality of citations can be questionable, as there isn't always a clear "ground truth" for information <a class="yt-timestamp" data-t="00:41:55">[00:41:55]</a>.

### Coral Dataset Construction Process

The Coral benchmark's dataset is constructed by converting Wikipedia pages into a structured format that an LLM can understand <a class="yt-timestamp" data-t="00:42:53">[00:42:53]</a>. This allows for the creation of a "knowledge graph" based on the page's hierarchical structure (headers, subheaders) <a class="yt-timestamp" data-t="00:43:17">[00:43:17]</a>.

Fake multi-turn conversations are then generated by simulating human-like question-asking patterns through graph traversal techniques <a class="yt-timestamp" data-t="00:43:25">[00:43:25]</a>. Different traversal methods are used to sample uniformly across the space of possible "walks" through these knowledge graphs, assuming this represents human behavior in multi-turn settings <a class="yt-timestamp" data-t="00:44:44">[00:44:44]</a>:
*   **Linear Descent Sampling:** Similar to a depth-first search, following a path deeply into the knowledge graph <a class="yt-timestamp" data-t="00:44:56">[00:44:56]</a>.
*   **Sibling Inclusive Descent Sampling:** Resembles a breadth-first search, exploring topics across the same level before going deeper <a class="yt-timestamp" data-t="00:45:05">[00:45:05]</a>.
*   **Dual Tree Random Walk:** A more complex method that involves moving up and down the knowledge graph randomly <a class="yt-timestamp" data-t="00:45:15">[00:45:15]</a>.

The resulting conversations typically consist of 11 to 20 turns <a class="yt-timestamp" data-t="00:45:51">[00:45:51]</a>. The dataset contains approximately 8,000 such conversations <a class="yt-timestamp" data-t="00:45:55">[00:45:55]</a>.

## Implications for [[Future Directions for Multimodal and AGI Development | Future Directions for Multimodal and AGI Development]]

The discussion around multi-turn conversations highlights a fundamental trade-off in AI development:
*   **Long Context Lengths:** One [[Future Directions for Multimodal and AGI Development | future direction]] suggests that as context lengths become significantly larger, it might be possible to simply put all relevant information directly into the model's context <a class="yt-timestamp" data-t="01:10:06">[01:10:06]</a>. This would lead to rarely trained but highly intelligent models <a class="yt-timestamp" data-t="01:10:21">[01:10:21]</a>. However, this approach could increase inference costs <a class="yt-timestamp" data-t="01:11:20">[01:11:20]</a>.
*   **Continuous Learning/Fine-tuning:** Another [[Future Directions for Multimodal and AGI Development | future direction]] involves constantly pushing gradients and fine-tuning models with new data (e.g., daily or weekly news) <a class="yt-timestamp" data-t="01:10:34">[01:10:34]</a>. This would mean less information in the context during inference, making it faster, but incurring higher continuous training costs <a class="yt-timestamp" data-t="01:11:25">[01:11:25]</a>. This is sometimes referred to as [[Challenges and Advancements in AI Research | continual learning]] <a class="yt-timestamp" data-t="01:12:55">[01:12:55]</a>.

The optimal approach depends on various factors, including typical user query types, advances in algorithmic efficiency for long contexts, and the capabilities and availability of hardware for continuous training <a class="yt-timestamp" data-t="01:11:38">[01:11:38]</a>. The impact of prompt caching is also a consideration, as it could favor the strategy of largely frozen base models augmented by in-context information <a class="yt-timestamp" data-t="01:12:42">[01:12:42]</a>.