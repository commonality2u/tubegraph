---
title: Hybrid Architectures with Transformers and Mambas
videoId: 9s-9aSobky8
---

From: [[hu-po]] <br/> 
Jamba, Gamba, and Cobra showcase the increasing adoption of [[mamba_models_and_their_applications | Mamba models]] across diverse modalities and problem spaces, often in [[hybrid_architectures_in_machine_learning_models | hybrid architectures]] that combine their strengths with traditional [[comparison_of_various_transformer_architectures | Transformer]] components <a class="yt-timestamp" data-t="00:01:47">[00:01:47]</a> <a class="yt-timestamp" data-t="00:01:50">[00:01:50]</a>.

### Gamba: 3D Reconstruction with Hybrid Architecture

Gamba is an end-to-end amortized 3D reconstruction model that generates a three-dimensional representation of an object from a single image <a class="yt-timestamp" data-t="00:04:11">[00:04:11]</a> <a class="yt-timestamp" data-t="00:04:29">[00:04:29]</a>. It utilizes a [[selective_state_space_models_and_mambas | Mamba-based sequential Network]] <a class="yt-timestamp" data-t="00:04:33">[00:04:33]</a>.

The architecture takes a single input image, feeds it into an image tokenizer, and then into the Gamba model <a class="yt-timestamp" data-t="00:18:22">[00:18:22]</a> <a class="yt-timestamp" data-t="00:18:25">[00:18:25]</a>. The image tokenizer employed is DINO, which is itself a [[comparison_of_various_transformer_architectures | Vision Transformer]] <a class="yt-timestamp" data-t="00:20:36">[00:20:36]</a> <a class="yt-timestamp" data-t="00:20:42">[00:20:42]</a>. This means Gamba is fundamentally a [[hybrid_architectures_in_machine_learning_models | combination]] of a [[comparison_of_various_transformer_architectures | Vision Transformer]] encoder and a [[selective_state_space_models_and_mambas | Mamba]]-based encoder-decoder <a class="yt-timestamp" data-t="00:20:46">[00:20:46]</a> <a class="yt-timestamp" data-t="00:20:50">[00:20:50]</a>. The pre-trained DINO encoder remains "frozen" during training, with gradients propagated through the [[selective_state_space_models_and_mambas | Mamba]] components and connector <a class="yt-timestamp" data-t="00:23:56">[00:23:56]</a> <a class="yt-timestamp" data-t="00:24:05">[00:24:05]</a>.

A notable critique of Gamba's hybrid approach is the continued reliance on a [[comparison_of_various_transformer_architectures | Vision Transformer]] for image encoding, despite the paper's focus on [[mamba_models_and_their_applications | Mamba]] technology <a class="yt-timestamp" data-t="00:21:00">[00:21:00]</a> <a class="yt-timestamp" data-t="00:22:40">[00:22:40]</a> <a class="yt-timestamp" data-t="00:22:44">[00:22:44]</a>. The model's results are not state-of-the-art, partly due to being pre-trained on a smaller dataset (OmniObject3D) compared to larger ones like Objaverse <a class="yt-timestamp" data-t="00:05:41">[00:05:41]</a> <a class="yt-timestamp" data-t="00:05:48">[00:05:48]</a> <a class="yt-timestamp" data-t="00:15:48">[00:15:48]</a>. However, it demonstrates significant speed advantages, taking only about 6 seconds on a single NVIDIA A100 GPU for 3D reconstruction <a class="yt-timestamp" data-t="00:06:09">[00:06:09]</a> <a class="yt-timestamp" data-t="00:06:33">[00:06:33]</a> <a class="yt-timestamp" data-t="00:06:37">[00:06:37]</a>.

### Cobra: Multimodal Language Models with Mamba Backbone

Cobra is another example of a [[hybrid_architectures_in_machine_learning_models | hybrid architecture]], extending [[selective_state_space_models_and_mambas | Mamba]] to [[multimodal_diffusion_transformer_architecture | Multimodal Large Language Models]] (MLLMs), also known as Vision Language Models <a class="yt-timestamp" data-t="00:21:23">[00:21:23]</a> <a class="yt-timestamp" data-t="00:25:27">[00:25:27]</a>. Cobra also employs a [[comparison_of_various_transformer_architectures | Vision Transformer]] for image encoding, specifically an ensemble of DINO and SigLIP encoders <a class="yt-timestamp" data-t="00:21:46">[00:21:46]</a> <a class="yt-timestamp" data-t="00:21:48">[00:21:48]</a>. The output from these encoders is combined and fed into a "projector" (an MLP) that converts them into image tokens consumable by the language model <a class="yt-timestamp" data-t="00:24:10">[00:24:10]</a> <a class="yt-timestamp" data-t="00:24:15">[00:24:15]</a>. The language model itself is composed of 64 [[selective_state_space_models_and_mambas | Mamba blocks]] <a class="yt-timestamp" data-t="00:36:35">[00:36:35]</a>.

Similar to Gamba, Cobra maintains a [[comparison_of_various_transformer_architectures | Vision Transformer]] as the base of its vision encoding pipeline, leading to a critique about not making the entire architecture [[selective_state_space_models_and_mambas | Mamba-based]] <a class="yt-timestamp" data-t="00:22:55">[00:22:55]</a> <a class="yt-timestamp" data-t="00:36:59">[00:36:59]</a>.

Cobra boasts a linear computational complexity due to its [[selective_state_space_models_and_mambas | Mamba]] foundation, making it highly efficient compared to [[comparison_of_various_transformer_architectures | Transformers]] with quadratic complexity <a class="yt-timestamp" data-t="00:36:13">[00:36:13]</a> <a class="yt-timestamp" data-t="00:36:18">[00:36:18]</a>. It achieves competitive performance against other computationally efficient methods while using approximately 43% fewer parameters than Lava <a class="yt-timestamp" data-t="00:35:19">[00:35:19]</a> <a class="yt-timestamp" data-t="00:35:50">[00:35:50]</a>. Its inference speed is notably higher, reaching 166 tokens per second compared to 39-40 tokens per second for [[comparison_of_various_transformer_architectures | Transformer]]-based models <a class="yt-timestamp" data-t="00:41:59">[00:41:59]</a> <a class="yt-timestamp" data-t="00:42:30">[00:42:30]</a> <a class="yt-timestamp" data-t="00:42:36">[00:42:36]</a>. This speed makes [[mamba_models_and_their_applications | Mamba-based]] models promising for time-sensitive applications like robotics and autonomous vehicles <a class="yt-timestamp" data-t="00:46:46">[00:46:46]</a> <a class="yt-timestamp" data-t="00:47:37">[00:47:37]</a>.

### Jamba: Language Model with Alternating Layers

Jamba is an open-source language model developed by AI21, characterized by its [[hybrid_architectures_in_machine_learning_models | hybrid]] structure combining [[selective_state_space_models_and_mambas | Mamba]] blocks and [[comparison_of_various_transformer_architectures | Transformer]] blocks <a class="yt-timestamp" data-t="00:50:01">[00:50:01]</a> <a class="yt-timestamp" data-t="00:50:04">[00:50:04]</a> <a class="yt-timestamp" data-t="00:50:08">[00:50:08]</a>. The architecture consists of 32 alternating layers, with some layers being standard [[selective_state_space_models_and_mambas | Mamba]] layers and others being "Mamba + MoE" layers, which integrate a [[mixture_of_depths_in_transformers | Mixture of Experts]] (MoE) component <a class="yt-timestamp" data-t="00:54:45">[00:54:45]</a> <a class="yt-timestamp" data-t="00:54:48">[00:54:48]</a> <a class="yt-timestamp" data-t="00:55:24">[00:55:24]</a>.

Jamba's MoE layers allow it to use a subset of its 52 billion parameters (e.g., 12 billion) during inference, contributing to efficiency <a class="yt-timestamp" data-t="00:53:17">[00:53:17]</a> <a class="yt-timestamp" data-t="00:53:19">[00:53:19]</a>. It supports large context windows (up to 140k tokens) due to the linear scaling of [[selective_state_space_models_and_mambas | Mamba]] blocks with sequence length, addressing a limitation of [[comparison_of_various_transformer_architectures | Transformer]] models <a class="yt-timestamp" data-t="00:52:43">[00:52:43]</a> <a class="yt-timestamp" data-t="00:52:48">[00:52:48]</a>.

This [[hybrid_architectures_in_machine_learning_models | hybrid approach]] of alternating [[selective_state_space_models_and_mambas | Mamba]] and [[comparison_of_various_transformer_architectures | Transformer]] (or RNN/attention) layers has precedents, such as Google's Griffin model <a class="yt-timestamp" data-t="01:11:03">[01:11:03]</a> <a class="yt-timestamp" data-t="01:11:11">[01:11:11]</a> <a class="yt-timestamp" data-t="01:12:10">[01:12:10]</a>.

### Challenges in Hybrid Architectures: Quantization

A potential weakness identified in [[mamba_models_and_their_applications | Mamba models]], and thus in [[hybrid_architectures_in_machine_learning_models | hybrid architectures]] employing them, is their sensitivity to numerical precision <a class="yt-timestamp" data-t="01:05:00">[01:05:00]</a> <a class="yt-timestamp" data-t="01:05:03">[01:05:03]</a>. Both Cobra and Jamba indicate that [[selective_state_space_models_and_mambas | Mamba]] blocks require a relatively high precision (no lower than bf16 for Cobra) <a class="yt-timestamp" data-t="01:05:05">[01:05:05]</a> <a class="yt-timestamp" data-t="01:05:08">[01:05:08]</a>, and Jamba's documentation explicitly recommends excluding [[selective_state_space_models_and_mambas | Mamba]] blocks from quantization to avoid degrading model quality <a class="yt-timestamp" data-t="01:07:38">[01:07:38]</a> <a class="yt-timestamp" data-t="01:07:43">[01:07:43]</a> <a class="yt-timestamp" data-t="01:07:46">[01:07:46]</a>.

This limitation suggests that while [[selective_state_space_models_and_mambas | Mambas]] offer inherent speed advantages, they may struggle with extreme quantization, a technique that significantly boosts the efficiency of [[comparison_of_various_transformer_architectures | Transformer]] models <a class="yt-timestamp" data-t="01:08:12">[01:08:12]</a> <a class="yt-timestamp" data-t="01:08:30">[01:08:30]</a> <a class="yt-timestamp" data-t="01:08:33">[01:08:33]</a>. This could present an "Achilles heel" for [[mamba_models_and_their_applications | Mambas]] if advancements in [[comparison_of_various_transformer_architectures | Transformer]] quantization outpace their ability to operate at lower bit depths <a class="yt-timestamp" data-t="01:08:44">[01:08:44]</a> <a class="yt-timestamp" data-t="01:09:55">[01:09:55]</a>.

Despite this, the continued exploration of [[hybrid_architectures_in_machine_learning_models | hybrid architectures]] suggests that combining [[selective_state_space_models_and_mambas | Mambas]] with [[comparison_of_various_transformer_architectures | Transformers]] (or other components) allows researchers to leverage the strengths of each, achieving competitive performance and efficiency in various AI applications <a class="yt-timestamp" data-t="01:12:09">[01:12:09]</a> <a class="yt-timestamp" data-t="01:12:11">[01:12:11]</a>.