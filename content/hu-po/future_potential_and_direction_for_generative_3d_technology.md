---
title: Future potential and direction for generative 3D technology
videoId: IsRHGf2rGCs
---

From: [[hu-po]] <br/> 

The field of [[advancements_in_3d_generative_models_using_neural_networks | generative 3D]] is rapidly evolving, with significant progress being made towards creating [[generative_ai_for_highquality_meshes | 3D assets]] from text and images <a class="yt-timestamp" data-t="02:50:47">[02:50:47]</a>. The ultimate goal is a scenario where users can interact with [[future_directions_and_potential_breakthroughs_in_ai_models | AI models]] in virtual reality (VR) headsets, generating and manipulating [[3d_implicit_functions_and_generative_models | 3D objects]] through speech or even thought <a class="yt-timestamp" data-t="03:29:54">[03:29:54]</a>.

## Key Directions and Advancements

### Data Set Evolution
A primary challenge in developing foundational [[challenges_and_limitations_in_3d_generation | 3D generative models]] is the limited availability of high-quality [[challenges_and_limitations_in_3d_generation | 3D data]] <a class="yt-timestamp" data-t="00:07:51">[00:07:51]</a>. Popular data sets like Objectron often have an object-centric bias, meaning they primarily contain single objects, which limits the models' ability to generate complex, multi-object scenes <a class="yt-timestamp" data-t="02:12:05">[02:12:05]</a>.

The future of [[future_potential_of_3d_diffusion_models | 3D generation]] will involve creating synthetic [[future_developments_and_challenges_in_ai_generated_simulations | 3D data sets]] from vast sources of existing 2D data <a class="yt-timestamp" data-t="00:08:31">[00:08:31]</a>:
*   **Video Diffusion Models as Data Sources**: [[video_diffusion_models_in_generative_3d | Video diffusion models]] like Sora and Emu are seen as crucial for generating large-scale synthetic multi-view data sets <a class="yt-timestamp" data-t="00:09:00">[00:09:00]</a>. By fine-tuning these models to produce 360-degree views of objects, high-quality multi-view video data can be created <a class="yt-timestamp" data-t="00:10:05">[00:10:05]</a>. This synthetic data can then be used to train [[3d_implicit_functions_and_generative_models | generative 3D models]] <a class="yt-timestamp" data-t="00:22:15">[00:22:15]</a>.
*   **Overcoming Biases**: Utilizing videos from platforms like YouTube to create [[generative_ai_for_highquality_meshes | 3D data sets]] will help overcome the "object-centric bias" present in current data sets like Objectron <a class="yt-timestamp" data-t="02:24:06">[02:24:06]</a>. This will enable the generation of more complex and compositional [[generative_ai_for_highquality_meshes | 3D objects]] <a class="yt-timestamp" data-t="02:23:38">[02:23:38]</a>.

### Evolution of 3D Representations
The choice of [[3d_implicit_functions_and_generative_models | 3D representation]] is critical for model efficiency and quality:
*   **Gaussian Splats vs. NeRFs**: While NeRFs (Neural Radiance Fields) remain popular, Gaussian Splats are emerging as a promising alternative <a class="yt-timestamp" data-t="03:17:10">[03:17:10]</a>. Gaussian Splats are considered to have a much higher representational capacity than NeRFs, leading to higher quality results <a class="yt-timestamp" data-t="03:10:03">[03:10:03]</a>. This explicit representation allows for more detailed and cleaner [[evaluation_of_3d_generative_techniques | 3D reconstructions]] <a class="yt-timestamp" data-t="03:10:03">[03:10:03]</a>.
*   **4D Representations**: Future models will likely move towards 4D Gaussian Splats, which include the time dimension, allowing for dynamic [[future_developments_and_challenges_in_ai_generated_simulations | 3D content generation]] <a class="yt-timestamp" data-t="03:31:09">[03:31:09]</a>. This means [[generative_ai_for_highquality_meshes | 3D objects]] can have defined movements and trajectories over time <a class="yt-timestamp" data-t="03:27:02">[03:27:02]</a>.

### Human Feedback and Preference Tuning
Incorporating human preferences will be crucial for models to generate content that aligns with user intentions:
*   **Reward Models**: Techniques inspired by instruction tuning in large language models (LLMs) will be used to refine [[advancements_in_3d_generative_models_using_neural_networks | generative 3D models]]. This involves collecting human feedback on generated [[generative_ai_for_highquality_meshes | 3D outputs]] (e.g., scoring quality, alignment, consistency) and training a "reward model" to guide the optimization process <a class="yt-timestamp" data-t="02:37:07">[02:37:07]</a>. This helps bridge the gap between a model's inherent distribution and desired human preferences <a class="yt-timestamp" data-t="02:41:41">[02:41:41]</a>.

### User Interaction and Accessibility
The future will see more intuitive and interactive [[future_directions_and_potential_breakthroughs_in_ai_models | AI models]] for 3D creation:
*   **VR and Voice Commands**: Imagine a future where users wear VR headsets and generate [[generative_ai_for_highquality_meshes | 3D assets]] by talking to the system <a class="yt-timestamp" data-t="03:29:57">[03:29:57]</a>. This could extend to gestures or even direct brain signals, allowing users to think an object into existence <a class="yt-timestamp" data-t="03:39:57">[03:39:57]</a>.
*   **Real-time Generation**: For a seamless user experience, [[future_potential_of_3d_diffusion_models | generative 3D models]] will need to operate in real-time, capable of running on mobile devices and VR headsets <a class="yt-timestamp" data-t="03:26:42">[03:26:42]</a>. This necessitates efficient, "feedforward" methods that do not require multi-step, per-instance optimization <a class="yt-timestamp" data-t="03:28:03">[03:28:03]</a>.

### Hardware and Inference Costs
The progress of [[advancements_in_3d_generative_models_using_neural_networks | generative 3D models]] is deeply tied to hardware capabilities:
*   **Increased Resolution and Feature Size**: Expect future models to utilize higher resolution for [[3d_implicit_functions_and_generative_models | triplane representations]] and larger feature dimensions, leading to more nuanced and intricate results <a class="yt-timestamp" data-t="01:34:57">[01:34:57]</a>.
*   **Computational Budget**: The size and complexity of models are often dictated by hardware constraints and inference costs <a class="yt-timestamp" data-t="01:41:04">[01:41:04]</a>. Companies with access to advanced chip manufacturing (like TSMC's latest processes for Nvidia and Apple) will likely lead the way in developing larger, more capable [[future_directions_and_potential_breakthroughs_in_ai_models | AI models]] <a class="yt-timestamp" data-t="01:50:51">[01:50:51]</a>.

## Impact on Traditional 3D Art and Animation
While generative [[future_potential_of_3d_diffusion_models | 3D technology]] will inevitably render "traditional animation" techniques (like rigging and skeletal animation) obsolete <a class="yt-timestamp" data-t="00:53:38">[00:53:38]</a>, the demand for compelling [[future_developments_and_challenges_in_ai_generated_simulations | 3D content]] will persist <a class="yt-timestamp" data-t="00:54:06">[00:54:06]</a>. Artists proficient in current tools like Blender will find it easier to adapt to new [[future_directions_and_potential_breakthroughs_in_ai_models | AI-powered tools]] due to their foundational understanding of high-level concepts and abstractions <a class="yt-timestamp" data-t="00:54:41">[00:54:41]</a>. The tools will become easier to use, democratizing 3D artistry <a class="yt-timestamp" data-t="01:00:08">[01:00:08]</a>.

## Conclusion
The [[future_potential_of_3d_diffusion_models | future of generative 3D]] is characterized by a continued drive towards higher quality, more consistent, and compositionally aware [[generative_ai_for_highquality_meshes | 3D asset generation]] <a class="yt-timestamp" data-t="02:43:31">[02:43:31]</a>. This will be achieved through the leveraging of large-scale video data sets, the adoption of more expressive [[3d_implicit_functions_and_generative_models | 3D representations]] like Gaussian Splats, the integration of human feedback, and the development of real-time, interactive user interfaces <a class="yt-timestamp" data-t="03:19:01">[03:19:01]</a>. While challenges remain, the field is expected to see significant breakthroughs in the coming years <a class="yt-timestamp" data-t="03:38:23">[03:38:23]</a>.