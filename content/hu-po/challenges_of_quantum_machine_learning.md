---
title: Challenges of quantum machine learning
videoId: ABEkChn3inY
---

From: [[hu-po]] <br/> 

The field of [[quantum_machine_learning|quantum machine learning]] (QML) is still in its infancy <a class="yt-timestamp" data-t="01:14:15">[01:14:15]</a> and presents several significant challenges.

## Complexity of the Field
The complexity of [[quantum_machine_learning|Quantum machine learning]] itself is a hurdle, as even experts may need to consult AI and external resources to understand specific terminology and concepts <a class="yt-timestamp" data-t="01:01:02">[01:01:02]</a>.

## Limitations of Current Quantum Natural Language Processing (QNLP)
Early efforts in [[Quantum NLP|Quantum NLP]] have shown promise but face practical limitations:
*   **Heavy Pre-processing**: Existing methods often require heavy syntactic pre-processing, which can involve tasks like tokenization or dataset cleaning <a class="yt-timestamp" data-t="03:04:04">[03:04:04]</a>, <a class="yt-timestamp" data-t="03:06:04">[03:06:04]</a>, <a class="yt-timestamp" data-t="03:23:00">[03:23:00]</a>.
*   **Syntax-Dependent Architectures**: These models are built with syntax-dependent network architectures <a class="yt-timestamp" data-t="03:08:00">[03:08:00]</a>, <a class="yt-timestamp" data-t="03:10:00">[03:10:00]</a>.
*   **Impracticality for Large Datasets**: Due to these limitations, existing QNLP approaches are deemed impractical for larger and real-world datasets <a class="yt-timestamp" data-t="03:21:00">[03:21:00]</a>. This means they cannot effectively process the vast amounts of data needed for robust [[artificial_intelligence|AI]] applications <a class="yt-timestamp" data-t="04:21:00">[04:21:00]</a>, <a class="yt-timestamp" data-t="04:24:00">[04:24:00]</a>.

## Noisy Intermediate-Scale Quantum (NISQ) Devices
Current [[quantum_computers|quantum computers]], known as [[noisy intermediate scale quantum devices|Noisy Intermediate-Scale Quantum (NISQ) devices]], present specific challenges:
*   **Noise**: NISQ devices are affected by both coherent and incoherent noise <a class="yt-timestamp" data-t="08:11:00">[08:11:00]</a>, <a class="yt-timestamp" data-t="08:12:00">[08:12:00]</a>. This noise originates from the external environment, requiring quantum components to be kept in highly controlled, vibration-proof, and super-cooled environments <a class="yt-timestamp" data-t="11:08:00">[11:08:00]</a>, <a class="yt-timestamp" data-t="11:10:00">[11:10:00]</a>, <a class="yt-timestamp" data-t="11:12:00">[11:12:00]</a>, <a class="yt-timestamp" data-t="11:14:00">[11:14:00]</a>, <a class="yt-timestamp" data-t="11:16:00">[11:16:00]</a>, <a class="yt-timestamp" data-t="11:18:00">[11:18:00]</a>, <a class="yt-timestamp" data-t="11:19:00">[11:19:00]</a>.
*   **Error Propagation**: Even slight disturbances to a qubit can cause noise to propagate through the entire quantum system, potentially leading to "nonsense" answers <a class="yt-timestamp" data-t="06:39:00">[06:39:00]</a>, <a class="yt-timestamp" data-t="06:42:00">[06:42:00]</a>, <a class="yt-timestamp" data-t="06:44:00">[06:44:00]</a>. Algorithms must be designed to be robust to this low-level quantum noise <a class="yt-timestamp" data-t="06:48:00">[06:48:00]</a>, <a class="yt-timestamp" data-t="06:51:00">[06:51:00]</a>, <a class="yt-timestamp" data-t="06:53:00">[06:53:00]</a>, <a class="yt-timestamp" data-t="06:55:00">[06:55:00]</a>, <a class="yt-timestamp" data-t="08:26:00">[08:26:00]</a>, <a class="yt-timestamp" data-t="08:27:00">[08:27:00]</a>.
*   **Limited Qubit Count**: [[noisy intermediate scale quantum devices|NISQ devices]] typically have a limited number of qubits, ranging from tens to a few hundreds <a class="yt-timestamp" data-t="07:39:00">[07:39:00]</a>, <a class="yt-timestamp" data-t="07:41:00">[07:41:00]</a>, <a class="yt-timestamp" data-t="11:01:00">[11:01:00]</a>, <a class="yt-timestamp" data-t="11:03:00">[11:03:00]</a>, <a class="yt-timestamp" data-t="09:07:00">[09:07:00]</a>, <a class="yt-timestamp" data-t="09:11:00">[09:11:00]</a>. For example, Google's Sycamore chip has 53 qubits <a class="yt-timestamp" data-t="01:56:18">[01:56:18]</a>, <a class="yt-timestamp" data-t="01:56:30">[01:56:30]</a>. This small scale limits the complexity of models and problems that can be tackled.
*   **Lack of Full Error Correction**: [[noisy intermediate scale quantum devices|NISQ devices]] are from an era prior to the development of full [[quantum_computing_advancements#Error_Correction|error correction]] for [[quantum_computers|quantum computers]] <a class="yt-timestamp" data-t="10:38:00">[10:38:00]</a>, <a class="yt-timestamp" data-t="10:40:00">[10:40:00]</a>, <a class="yt-timestamp" data-t="11:30:00">[11:30:00]</a>, <a class="yt-timestamp" data-t="11:34:00">[11:34:00]</a>.
*   **Shallow Algorithms**: Due to noise, [[quantum_algorithms|quantum algorithms]] designed for [[noisy intermediate scale quantum devices|NISQ devices]] need to be "shallow," meaning they cannot have too many consecutive operations or deep layers <a class="yt-timestamp" data-t="12:22:00">[12:22:00]</a>, <a class="yt-timestamp" data-t="12:24:00">[12:24:00]</a>, <a class="yt-timestamp" data-t="12:26:00">[12:26:00]</a>. This contrasts with classical deep learning, which relies on many layers for performance <a class="yt-timestamp" data-t="12:38:00">[12:38:00]</a>.

## Quantum Measurement and Information Loss
When a measurement is performed on a [[quantum_state|quantum state]], it collapses into one of its basis states, causing the loss of information stored in its superposition <a class="yt-timestamp" data-t="02:53:00">[02:53:00]</a>, <a class="yt-timestamp" data-t="02:56:00">[02:56:00]</a>, <a class="yt-timestamp" data-t="02:58:00">[02:58:00]</a>, <a class="yt-timestamp" data-t="03:01:00">[03:01:00]</a>, <a class="yt-timestamp" data-t="03:03:00">[03:03:00]</a>, <a class="yt-timestamp" data-t="03:11:00">[03:11:00]</a>. This is a fundamental aspect of [[quantum_computing|quantum computing]] that needs to be managed when designing hybrid [[quantum_algorithms|quantum algorithms]] that interface with classical inputs and outputs <a class="yt-timestamp" data-t="02:57:00">[02:57:00]</a>, <a class="yt-timestamp" data-t="03:00:00">[03:00:00]</a>.

## Limited Scale of Quantum Machine Learning Models
Current [[quantum_neural_networks|quantum neural networks]] (QNNs) developed for tasks like text classification are very small:
*   **Few Parameters**: They have a very small number of parameters (e.g., 49 parameters in one QNN example) <a class="yt-timestamp" data-t="02:13:51">[02:13:51]</a>, far less than the billions in modern classical models <a class="yt-timestamp" data-t="01:40:02">[01:40:02]</a>.
*   **Shallow Depth**: They typically consist of very few layers (e.g., six layers deep) <a class="yt-timestamp" data-t="01:53:12">[01:53:12]</a>, <a class="yt-timestamp" data-t="01:53:15">[01:53:15]</a>.
*   **Tiny Datasets**: They are trained and evaluated on extremely small datasets, sometimes with only dozens or hundreds of words and sequences (e.g., 17 words, 130 data points, or 1000 sequences) <a class="yt-timestamp" data-t="02:10:07">[02:10:07]</a>, <a class="yt-timestamp" data-t="02:11:00">[02:11:00]</a>, <a class="yt-timestamp" data-t="02:15:00">[02:15:00]</a>, <a class="yt-timestamp" data-t="02:15:02">[02:15:02]</a>, <a class="yt-timestamp" data-t="02:15:05">[02:15:05]</a>. This makes it difficult to draw broad conclusions about their "powerful ability" <a class="yt-timestamp" data-t="02:16:02">[02:16:02]</a>, <a class="yt-timestamp" data-t="02:16:04">[02:16:04]</a>.

## Initialization Sensitivity
[[quantum_circuits_and_anzats_in_neural_networks|Quantum neural networks]] can be more sensitive to the initialization of their parameters compared to classical neural networks. For example, if [[quantum_circuits_and_anzats_in_neural_networks|ansatz parameters]] (which define rotations) are initialized with different random seeds, they can lead to a wider spread in test accuracy <a class="yt-timestamp" data-t="02:13:41">[02:13:41]</a>, <a class="yt-timestamp" data-t="02:14:10">[02:14:10]</a>, <a class="yt-timestamp" data-t="02:14:27">[02:14:27]</a>, <a class="yt-timestamp" data-t="02:14:29">[02:14:29]</a>, <a class="yt-timestamp" data-t="02:14:31">[02:14:31]</a>.

## Simulation vs. Real Hardware
The overwhelming majority of [[quantum_algorithms|quantum algorithms]] and papers are currently developed and tested via [[simulated_versus_real_quantum_computing|classical simulation]] rather than on actual [[quantum_computers|quantum computing]] hardware, largely due to the size and expense of real devices <a class="yt-timestamp" data-t="01:58:38">[01:58:38]</a>, <a class="yt-timestamp" data-t="01:58:41">[01:58:41]</a>, <a class="yt-timestamp" data-t="01:58:42">[01:58:42]</a>, <a class="yt-timestamp" data-t="01:58:49">[01:58:49]</a>, <a class="yt-timestamp" data-t="01:58:52">[01:58:52]</a>, <a class="yt-timestamp" data-t="01:59:00">[01:59:00]</a>, <a class="yt-timestamp" data-t="01:59:03">[01:59:03]</a>, <a class="yt-timestamp" data-t="01:59:05">[01:59:05]</a>, <a class="yt-timestamp" data-t="01:59:06">[01:59:06]</a>. While tools like PennyLane allow for the simulation of quantum circuits <a class="yt-timestamp" data-t="01:54:11">[01:54:11]</a>, running on hardware backends like Amazon Braket or Google Cirq is still an advanced step <a class="yt-timestamp" data-t="01:55:02">[01:55:02]</a>, <a class="yt-timestamp" data-t="01:55:10">[01:55:10]</a>.

## Absence of Advanced Classical Techniques
Current [[quantum_neural_networks|quantum neural networks]] for NLP do not yet incorporate advanced techniques common in classical Transformers, such as [[positional_encoding|positional encoding]] or [[multi-head_attention|multi-head attention]] <a class="yt-timestamp" data-t="02:21:05">[02:21:05]</a>, <a class="yt-timestamp" data-t="02:21:08">[02:21:08]</a>. The applicability of [[multi-head_attention|multi-head attention]] in quantum contexts is also unclear, as its primary benefit in classical computing comes from parallelization on GPUs, and the concept of a "Quantum GPU" is still theoretical <a class="yt-timestamp" data-t="02:21:15">[02:21:15]</a>, <a class="yt-timestamp" data-t="02:21:17">[02:21:17]</a>, <a class="yt-timestamp" data-t="02:21:19">[02:21:19]</a>, <a class="yt-timestamp" data-t="02:21:22">[02:21:22]</a>, <a class="yt-timestamp" data-t="02:21:24">[02:21:24]</a>, <a class="yt-timestamp" data-t="02:21:33">[02:21:33]</a>, <a class="yt-timestamp" data-t="02:21:35">[02:21:35]</a>, <a class="yt-timestamp" data-t="02:21:37">[02:21:37]</a>.