---
title: Transformerbased model architectures
videoId: yxAcTRp9EyQ
---

From: [[hu-po]] <br/> 

The [[challenges_and_insights_in_transformer_architecture_and_training | Transformer architecture]] is considered the "architecture du jour" due to its popularity in the field [00:02:21]. Its discovery has led to extraordinary progress in training large artificial neural networks [00:07:05].

## Phi-1 Model Architecture
The Phi-1 model, developed by Microsoft Research, is a [[transformer_models_in_audio_generation | Transformer-based]] language model specifically designed for code [00:02:02, 00:02:05, 00:02:17]. It is significantly smaller than competing models, with 1.3 billion parameters [00:02:08, 00:02:28]. For comparison, other models like Falcon have 40 billion parameters and Llama has 65 billion parameters [00:03:16, 00:03:28].

Phi-1's architecture utilizes a decoder-only [[challenges_and_insights_in_transformer_architecture_and_training | Transformer model]] [00:46:27]. This choice for the Phi-1 model is based on the GPT Neo X architecture [00:49:03].

### Components and Features
The architecture of Phi-1 includes several specific components and techniques:
*   **Decoder-only Transformer** [00:47:55]: Unlike the original Transformer that uses both an encoder and a decoder for tasks like translation, Phi-1 employs only the decoder [00:47:21, 00:47:57]. The decoder part of the Transformer uses a masked self-attention mechanism, which prevents the model from looking at future tokens during generation [00:48:11, 00:48:22].
*   **Flash Attention** [00:48:31]: This technique is used to reduce the memory and computational footprint of the [[challenges_and_insights_in_transformer_architecture_and_training | Transformer]] [00:48:38].
*   **Multi-Head Attention (MHA) and Multi-Layer Perceptron (MLP) Layers** [00:48:47]: These are fundamental building blocks within the [[challenges_and_insights_in_transformer_architecture_in_image_processing | Transformer]] architecture [00:48:53].
*   **Rotary Position Embedding (RoPE)** [00:50:40]: Also known as "rope," these embeddings handle the positional information of tokens within the sequence [00:50:42, 00:50:47].
*   **Fill-in-the-Middle** [01:05:07]: An additional trick or feature [01:05:07].

### Model Sizes and Parameters
*   **Phi-1 Base (1.3 Billion Parameters)** [00:27:07, 00:27:09]:
    *   24 layers [00:49:06, 00:49:19].
    *   Hidden dimension of 2048 [00:49:08, 00:49:34].
    *   MLP inner dimension of 8192 [00:50:19].
    *   32 attention heads, each with a dimension of 64 [00:50:22].
*   **Phi-1 Small (350 Million Parameters)** [00:06:51]:
    *   20 layers [00:50:28].
    *   Smaller hidden dimension [00:50:30].
    *   Smaller MLP inner dimension [00:50:30].
    *   16 attention heads [00:50:33].

### Training Infrastructure
The Phi-1 model was trained for four days on eight A100 GPUs [00:03:44]. This setup, representing a single server rack, is considered relatively inexpensive, costing tens of thousands of dollars rather than hundreds of thousands or millions [00:03:49, 00:03:51, 00:03:54, 00:55:10, 00:56:06]. The use of eight GPUs is common because that is the number of GPUs that typically fit on a single server rack [00:56:00, 00:56:24].

## Evolution and Trends
The field of artificial neural networks has seen significant progress, particularly with the advent of the [[challenges_and_insights_in_transformer_architecture_and_training | Transformer architecture]] [00:07:05]. However, some argue that the exponential growth in compute power, particularly with GPUs, has been a more significant factor than the architecture itself [00:07:13, 00:07:21].

The concept of "scaling laws" suggests that performance predictably improves as one scales up either the amount of compute or the size of the network [00:07:42, 00:08:08]. This idea, where bigger models and bigger datasets lead to increased performance, has been known for a long time [00:07:57, 00:08:01, 00:08:03].

There's an emerging trend of using [[ai_model_architecture_and_parallelism_strategies | synthesized data]] for training new generations of [[challenges_and_insights_in_transformer_architecture_and_training | LLMs]] [01:05:07, 01:05:08, 01:05:10, 01:05:12, 01:05:15, 01:05:18, 01:05:22, 01:05:23, 01:05:25, 01:05:28, 01:05:32, 01:05:35, 01:05:37, 01:05:40, 01:05:42, 01:05:44, 01:05:47, 01:05:51, 01:05:53, 01:05:55, 01:05:57, 01:06:01, 01:06:03, 01:06:06, 01:06:08, 01:06:10, 01:06:12, 01:06:15, 01:06:17, 01:06:18, 01:06:20, 01:06:21, 01:06:24, 01:06:26, 01:06:28, 01:06:30, 01:06:33, 01:06:35, 01:06:37, 01:06:38, 01:06:40, 01:06:42, 01:06:44, 01:06:47, 01:06:48, 01:06:50, 01:06:53, 01:06:55, 01:06:56, 01:06:58, 01:06:59, 01:07:01, 01:07:04, 01:07:06, 01:07:08, 01:07:11, 01:07:13, 01:07:16, 01:07:18, 01:07:21, 01:07:23, 01:07:25, 01:07:27, 01:07:30, 01:07:32, 01:07:34, 01:07:37, 01:07:40, 01:07:42, 01:07:43, 01:07:45, 01:07:48, 01:07:50, 01:07:52, 01:07:53, 01:07:55, 01:07:57, 01:07:59, 01:08:01, 01:08:03, 01:08:06, 01:08:08, 01:08:10, 01:08:12, 01:08:15, 01:08:17, 01:08:20, 01:08:22, 01:08:24, 01:08:26, 01:08:29, 01:08:31, 01:08:33, 01:08:35, 01:08:38, 01:08:40, 01:08:42, 01:08:44, 01:08:47, 01:08:49, 01:08:50, 01:08:52, 01:08:54, 01:08:56, 01:08:58, 01:09:00, 01:09:02, 01:09:04, 01:09:07, 01:09:09, 01:09:11, 01:09:13, 01:09:16, 01:09:18, 01:20:57, 01:20:58, 01:21:00, 01:21:01, 01:21:04, 01:21:07, 01:21:09, 01:21:11, 01:21:13, 01:21:14, 01:21:16, 01:21:18, 01:21:19, 01:21:21, 01:21:22, 01:21:23, 01:21:25, 01:21:26, 01:21:27, 01:21:29, 01:21:31, 01:21:32, 01:21:34, 01:21:35, 01:21:36, 01:21:38, 01:21:39, 01:21:42, 01:21:43, 01:21:45, 01:21:46, 01:21:48, 01:21:49, 01:21:51, 01:21:52, 01:21:53, 01:21:55, 01:21:57, 01:21:58, 01:22:01, 01:22:03, 01:22:04, 01:22:06, 01:22:08, 01:22:11, 01:22:13, 01:22:16, 01:22:18, 01:22:20, 01:22:23, 01:22:25, 01:22:27, 01:22:28, 01:22:30, 01:22:32, 01:22:34, 01:22:36, 01:22:38, 01:22:40, 01:22:43, 01:22:45, 01:22:48, 01:22:51, 01:22:53, 01:22:55, 01:22:57, 01:22:59, 01:23:01, 01:23:03, 01:23:04, 01:23:06, 01:23:09, 01:23:12, 01:23:20, 01:23:21, 01:23:23, 01:23:24, 01:23:26, 01:23:29, 01:23:32, 01:23:34, 01:23:36, 01:23:38, 01:23:40, 01:23:43, 01:23:45, 01:23:46, 01:23:48, 01:23:50, 01:23:51, 01:23:53, 01:23:55, 01:23:56, 01:23:59, 01:24:03, 01:24:05, 01:24:08, 01:24:11, 01:24:13, 01:24:14, 01:24:16, 01:24:19, 01:24:21, 01:24:23, 01:24:24, 01:24:26, 01:24:29, 01:24:33, 01:24:35, 01:24:39, 01:24:44, 01:24:45, 01:24:47, 01:24:48, 01:24:50, 01:24:51, 01:24:52, 01:24:54, 01:24:56, 01:24:58, 01:25:00, 01:25:02, 01:25:04, 01:25:08, 01:25:10, 01:25:13, 01:25:15, 01:25:17, 01:25:19, 01:25:20, 01:25:21, 01:25:23, 01:25:26, 01:25:28, 01:25:31, 01:25:32, 01:25:34, 01:25:36, 01:25:37, 01:25:39, 01:25:41, 01:25:43, 01:25:45, 01:25:47, 01:25:48, 01:25:49, 01:25:51, 01:25:53, 01:25:54, 01:25:56, 01:25:58, 01:25:59, 01:26:01, 01:26:02, 01:26:04, 01:26:07, 01:26:08, 01:26:09, 01:26:12, 01:26:13, 01:26:16, 01:26:17, 01:26:19, 01:26:23, 01:26:26, 01:26:28, 01:26:29, 01:26:30, 01:26:33, 01:26:36, 01:26:38, 01:26:41, 01:26:43, 01:26:46, 01:26:47, 01:26:49, 01:26:52, 01:26:55, 01:27:01, 01:27:02, 01:27:04, 01:27:05, 01:27:06, 01:27:07, 01:27:09, 01:27:11, 01:27:12, 01:27:15, 01:27:16, 01:27:29, 01:27:30, 01:27:31, 01:27:34, 01:27:36, 01:27:37, 01:27:39, 01:27:40, 01:27:43, 01:27:45, 01:27:46, 01:27:50, 01:27:53, 01:27:54, 01:27:56, 01:27:57, 01:27:59, 01:28:00, 01:28:03, 01:28:05, 01:28:07, 01:28:08, 01:28:10, 01:28:12, 01:28:14, 01:28:16, 01:28:18, 01:28:21, 01:28:23, 01:28:25, 01:28:29, 01:28:31, 01:28:33, 01:28:37, 01:28:39, 01:28:40, 01:28:43, 01:28:45, 01:28:46, 01:28:50, 01:28:53, 01:28:54, 01:28:56, 01:28:58, 01:29:00, 01:29:02, 01:29:03, 01:29:05, 01:29:08, 01:29:11, 01:29:14, 01:29:16, 01:29:17, 01:29:21, 01:29:22, 01:29:24, 01:29:27, 01:29:28, 01:29:31, 01:29:33, 01:29:35, 01:29:37, 01:29:39, 01:29:40, 01:29:42, 01:29:45, 01:29:47, 01:29:48, 01:29:52, 01:29:55, 01:29:57, 01:30:01, 01:30:03, 01:30:12, 01:30:15, 01:30:19, 01:30:22, 01:30:25, 01:30:26, 01:30:28, 01:30:30, 01:30:32, 01:30:38, 01:30:39, 01:30:43, 01:30:46, 01:30:49, 01:30:50, 01:30:52, 01:30:53, 01:31:01, 01:31:03, 01:31:04, 01:31:07, 01:31:08, 01:31:10, 01:31:15, 01:31:16, 01:31:18, 01:31:22, 01:31:30, 01:31:34, 01:31:37, 01:31:39, 01:31:40, 01:31:43, 01:31:46, 01:31:48, 01:31:50, 01:31:52, 01:31:54, 01:31:56, 01:31:57, 01:31:59, 01:32:00, 01:32:01, 01:32:02, 01:32:03, 01:32:05, 01:32:07, 01:32:09, 01:32:11, 01:32:13, 01:32:16, 01:32:19, 01:32:22, 01:32:25, 01:32:27, 01:32:28, 01:32:30, 01:32:31, 01:32:33, 01:32:35, 01:32:37, 01:32:41, 01:32:43, 01:32:44, 01:32:46, 01:32:48, 01:32:51, 01:32:53, 01:32:55, 01:32:57, 01:33:01, 01:33:02, 01:33:03, 01:33:05, 01:33:06, 01:33:08, 01:33:09, 01:33:11, 01:33:16, 01:33:18, 01:33:20, 01:33:25, 01:33:27, 01:33:30, 01:33:32, 01:33:33, 01:33:34, 01:33:37, 01:33:39, 01:33:41, 01:33:42, 01:33:44, 01:33:45, 01:33:47, 01:33:49, 01:33:50, 01:33:54, 01:33:56, 01:33:59, 01:34:01, 01:34:05, 01:34:06, 01:34:09, 01:34:11, 01:34:14, 01:34:16, 01:34:20, 01:34:21, 01:34:25, 01:34:27, 01:34:31, 01:34:33, 01:34:34, 01:34:37, 01:34:39, 01:34:41, 01:34:43, 01:34:44, 01:34:47, 01:34:48, 01:34:50, 01:34:52, 01:34:53, 01:34:55, 01:34:58, 01:35:01, 01:35:04, 01:35:08, 01:35:09, 01:35:12, 01:35:14, 01:35:16, 01:35:17, 01:35:21, 01:35:22, 01:35:24, 01:35:28, 01:35:30, 01:35:34, 01:35:36, 01:35:37, 01:35:40, 01:35:43, 01:35:44, 01:35:46, 01:35:48, 01:35:51, 01:35:53, 01:35:54, 01:35:56, 01:35:57, 01:35:59, 01:36:01, 01:36:02, 01:36:04, 01:36:06, 01:36:07, 01:36:08, 01:36:10, 01:36:14, 01:36:17, 01:36:19, 01:36:21, 01:36:24, 01:36:28, 01:36:31, 01:36:34, 01:36:36, 01:36:39, 01:36:41, 01:36:45, 01:36:47, 01:36:49, 01:36:51, 01:36:53, 01:36:55, 01:36:57, 01:36:59, 01:37:01, 01:37:04, 01:37:08, 01:37:13, 01:37:15, 01:37:17, 01:37:20, 01:37:23, 01:37:25, 01:37:27, 01:37:29, 01:37:31, 01:37:34, 01:37:36, 01:37:37, 01:37:40, 01:37:42, 01:37:44, 01:37:47, 01:37:49, 01:37:51, 01:37:54, 01:37:57, 01:38:00, 01:38:02, 01:38:03, 01:38:07, 01:38:10, 01:38:12, 01:38:15, 01:38:17, 01:38:21, 01:38:24, 01:38:26, 01:38:29, 01:38:37, 01:38:40, 01:38:43, 01:38:45, 01:38:46, 01:38:47, 01:38:50, 01:38:52, 01:38:54, 01:38:56, 01:38:59, 01:39:01, 01:39:03, 01:39:05, 01:39:06, 01:39:08, 01:39:11, 01:39:13, 01:39:15, 01:39:22, 01:39:26, 01:39:28, 01:39:33, 01:39:35, 01:39:39, 01:39:41, 01:39:43, 01:39:46, 01:39:47, 01:39:49, 01:39:51, 01:39:53, 01:39:56, 01:40:00, 01:40:02, 01:40:04, 01:40:11, 01:40:13, 01:40:17, 01:40:19, 01:40:30, 01:40:33, 01:40:35, 01:40:37, 01:40:39, 01:40:42, 01:40:45, 01:40:46, 01:40:48, 01:40:50, 01:40:52, 01:40:54, 01:40:55, 01:40:58, 01:41:00, 01:41:02, 01:41:04, 01:41:06, 01:41:08, 01:41:11, 01:41:21, 01:41:23, 01:41:26, 01:41:28, 01:41:30, 01:41:32, 01:41:34, 01:41:35, 01:41:43, 01:41:44, 01:41:46, 01:41:47, 01:41:49, 01:41:53, 01:41:55, 01:42:00, 01:42:03, 01:42:04, 01:42:06, 01:42:09, 01:42:13, 01:42:15, 01:42:17, 01:42:18, 01:42:21, 01:42:23, 01:42:28, 01:42:31, 01:42:32, 01:42:35, 01:42:37, 01:42:40, 01:42:43, 01:42:45, 01:42:47, 01:42:51, 01:42:53, 01:42:55, 01:42:58, 01:43:02, 01:43:03, 01:43:08, 01:43:18, 01:43:26, 01:43:31, 01:43:33, 01:43:35, 01:43:36, 01:43:38, 01:43:41, 01:43:42, 01:43:44, 01:43:48, 01:43:50, 01:43:52, 01:43:53, 01:44:00, 01:44:02, 01:44:05, 01:44:10, 01:44:15, 01:44:19, 01:44:21, 01:44:24, 01:44:26, 01:44:28, 01:44:30, 01:44:31, 01:44:34, 01:44:37, 01:44:39, 01:44:48, 01:44:50, 01:44:52, 01:44:57, 01:44:59, 01:45:01, 01:45:03, 01:45:05, 01:45:11, 01:45:13, 01:45:18, 01:45:21, 01:45:25, 01:45:27, 01:45:40, 01:45:44, 01:45:47, 01:46:06, 01:46:08, 01:46:10, 01:46:12, 01:46:14, 01:46:17, 01:46:19, 01:46:20, 01:46:23, 01:46:24, 01:46:27, 01:46:28, 01:46:30, 01:46:33, 01:46:34, 01:46:37, 01:46:41, 01:46:44, 01:46:46, 01:46:48, 01:47:00, 01:47:03, 01:47:04, 01:47:07, 01:47:08, 01:47:10, 01:47:12, 01:47:13, 01:47:15, 01:47:16, 01:47:18, 01:47:19, 01:47:22, 01:47:24, 01:47:26, 01:47:27, 01:47:30, 01:47:32, 01:47:35, 01:47:36, 01:47:39, 01:47:40, 01:47:44, 01:47:47, 01:47:49, 01:47:51, 01:47:53, 01:47:55, 01:47:58, 01:48:00, 01:48:01, 01:48:04, 01:48:06, 01:48:08, 01:48:10, 01:48:12, 01:48:13, 01:48:15, 01:48:16, 01:48:18, 01:48:19, 01:48:22, 01:48:24, 01:48:26, 01:48:27, 01:48:30, 01:48:32, 01:48:36, 01:48:38, 01:48:39, 01:48:41, 01:48:44, 01:48:45, 01:48:47, 01:48:49, 01:48:51, 01:48:52, 01:48:54, 01:48:56, 01:48:57, 01:49:00, 01:49:01, 01:49:05, 01:49:07, 01:49:09, 01:49:11, 01:49:12, 01:49:15, 01:49:17, 01:49:19, 01:49:20, 01:49:23):
*   **Data Quality Focus**: Higher quality data is known to lead to better results [00:08:30, 00:31:05]. This can also yield smaller, yet more effective, datasets [00:08:49].
*   **Curriculum Learning**: Much like teaching a human child, the choice of curriculum matters for neural networks, where the type of information and the timing of training, gradually increasing in complexity, is important [00:09:37, 00:09:48, 01:09:09]. This approach is seen in reinforcement learning but is not widely applied to pre-training large natural language models [00:09:57, 01:00:14].
*   **Synthetic Data Generation**: Using models like GPT-3.5 and GPT-4 to generate "textbook quality" data and coding exercises is a key strategy [00:14:45, 00:31:36, 00:44:05, 01:33:08]. This allows for control over the training distribution and, consequently, the behavior of the LLM [01:37:37, 01:37:40, 01:37:47, 01:38:00, 01:38:02, 01:38:03, 01:38:07, 01:38:10, 01:38:12, 01:38:15, 01:38:17].
*   **Data Pruning/Decontamination**: Techniques to filter or "prune" datasets, such as using [[transformer_architecture_in_image_processing | Transformer-based classifiers]] or analyzing embedding and syntax-based distances, are employed to remove low-quality or repetitive examples and ensure unbiased evaluation [01:23:44, 01:23:55, 01:24:05, 01:27:45, 01:27:51, 01:27:59, 01:28:00, 01:28:37, 01:28:40, 01:28:45, 01:28:57, 01:28:58, 01:29:00, 01:29:02, 01:29:03, 01:29:05, 01:29:31, 01:29:33, 01:29:35].
*   **Mixed Precision Training**: Using floating-point 16 (FP16) for training is a common practice to reduce memory footprint and speed up training [00:54:06, 00:54:11, 00:54:20, 00:54:23, 00:54:26, 00:54:29, 00:54:32, 00:54:34, 00:54:36, 00:54:38, 00:54:41, 00:54:43, 00:54:46, 00:54:49, 00:54:51, 00:54:53, 00:54:55].
*   **Emergent Capabilities**: Fine-tuning can lead to unexpected coding capabilities and improvements in tasks not explicitly featured in the fine-tuning dataset [01:00:07, 01:00:09, 01:01:40, 01:01:42, 01:01:47, 01:01:50, 01:01:52, 01:01:53, 01:01:55, 01:01:58, 01:02:23, 01:02:25, 01:02:27, 01:02:29, 01:02:30, 01:02:33, 01:02:36, 01:02:39, 01:02:40, 01:02:43, 01:02:45, 01:02:48, 01:02:50, 01:02:52, 01:02:53, 01:02:57, 01:02:59, 01:03:02, 01:03:04, 01:03:06, 01:03:12, 01:03:16, 01:03:18, 01:03:20, 01:03:23, 01:03:26, 01:03:28, 01:03:31, 01:03:34, 01:03:36, 01:03:38, 01:03:40, 01:03:43, 01:03:44, 01:03:48, 01:03:52, 01:03:54, 01:03:56, 01:03:58, 01:04:01, 01:04:03, 01:04:08, 01:04:10, 01:04:12, 01:04:13, 01:41:00, 01:41:02, 01:41:04].

> [!NOTE] Comparison with Other Models
> Despite its smaller size (1.3 billion parameters) and less training data (7 billion tokens in total) [00:04:59, 00:05:01, 00:05:02, 00:14:04, 01:32:22, 01:32:25], Phi-1 achieves strong performance, outperforming models like StarCoder that are 10 times larger and trained on 100 times more data in coding benchmarks [01:20:50, 01:21:01, 01:21:04, 01:32:19, 01:32:22]. This suggests that high-quality data dramatically improves the learning efficiency of language models for code [01:32:27].

### Limitations
The Phi-1 model has some limitations:
*   **Specialization**: It is specialized in Python coding [01:32:41].
*   **Domain-Specific Knowledge**: It lacks domain-specific knowledge for particular APIs [01:32:48].
*   **Robustness to Stylistic Variations**: Due to the structured nature of its dataset, Phi-1 is less robust to stylistic variations or errors in prompts [01:32:55, 01:43:50]. This sensitivity to prompt variations might be a characteristic of smaller models [01:40:45].