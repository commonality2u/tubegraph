---
title: Quantization techniques in machine learning
videoId: pov3pLFMOPY
---

From: [[hu-po]] <br/> 

[[Quantization in machine learning models | Quantization]] is a fundamental technique in machine learning, particularly vital for efficient model deployment and [[finetuning_machine_learning_models | fine-tuning]] large models <a class="yt-timestamp" data-t="01:28:31">[01:28:31]</a>. It involves converting a data type with more bits to one with fewer bits, effectively reducing the information stored <a class="yt-timestamp" data-t="04:26:06">[04:26:06]</a>. This process is inherently lossy, meaning some precision is lost <a class="yt-timestamp" data-t="04:25:34">[04:25:34]</a>. However, it drastically increases model efficiency in terms of memory and compute <a class="yt-timestamp" data-t="06:05:05">[06:05:05]</a>.

## Why Quantization?
The need for [[Quantization in machine learning models | quantization]] stems from the substantial memory requirements of large models. For instance, fine-tuning a LLaMA 65B model in 16-bit precision requires over 780 gigabytes of GPU memory <a class="yt-timestamp" data-t="00:17:13">[00:17:13]</a>. This necessitates multiple server racks with dozens of GPUs, making it unapproachable for many researchers <a class="yt-timestamp" data-t="01:17:44">[01:17:44]</a>.

## QLoRA: Efficient Fine-tuning of Quantized LLMs
QLoRA (Quantized Low-Rank Adapters) is an efficient [[finetuning_machine_learning_models | fine-tuning]] approach that significantly reduces memory usage while preserving full 16-bit [[finetuning_machine_learning_models | fine-tuning]] task performance <a class="yt-timestamp" data-t="00:26:26">[00:26:26]</a>. Authored primarily by Tim Detmers, a prominent figure in the [[Quantization in machine learning models | quantization]] research space <a class="yt-timestamp" data-t="00:30:17">[00:30:17]</a>, QLoRA enables [[finetuning_machine_learning_models | fine-tuning]] of large language models (LLMs) like LLaMA 65B on a single 48-gigabyte GPU <a class="yt-timestamp" data-t="00:37:36">[00:37:36]</a>. This makes large LLM [[finetuning_machine_learning_models | fine-tuning]] accessible on consumer-grade GPUs <a class="yt-timestamp" data-t="00:37:57">[00:37:57]</a>.

The core idea of QLoRA is to backpropagate gradients through a frozen, 4-bit quantized LLM into a small set of trainable [[finetuning_machine_learning_models | Low-Rank Adapter (LoRA)]] weights <a class="yt-timestamp" data-t="00:43:03">[00:43:03]</a>.

### Key Innovations in QLoRA
QLoRA introduces several innovations to achieve its memory efficiency without sacrificing performance:

1.  **4-bit NormalFloat (NF4)**: A new data type specifically designed for normally distributed weights, which are common in neural networks <a class="yt-timestamp" data-t="01:04:14">[01:04:14]</a>. It's information-theoretically optimal for zero-mean normal distributions <a class="yt-timestamp" data-t="01:13:38">[01:13:38]</a>.
2.  **Double [[Quantization in machine learning models | Quantization]]**: This technique quantizes the [[Quantization in machine learning models | quantization]] constants themselves, further reducing memory footprint <a class="yt-timestamp" data-t="01:24:40">[01:24:40]</a>.
3.  **Paged Optimizers**: Utilizes NVIDIA's unified memory to manage memory spikes during gradient checkpointing, preventing out-of-memory (OOM) errors <a class="yt-timestamp" data-t="01:02:56">[01:02:56]</a>.

### How QLoRA Works
In QLoRA, the large, pre-trained model (e.g., LLaMA 65B) is **frozen** and quantized to a 4-bit precision <a class="yt-timestamp" data-t="00:27:07">[00:27:07]</a>. This means its weights are not changed during [[finetuning_machine_learning_models | fine-tuning]] <a class="yt-timestamp" data-t="00:28:46">[00:28:46]</a>. Instead, small, learnable [[finetuning_machine_learning_models | LoRA]] adapters are attached to various layers of the model <a class="yt-timestamp" data-t="00:54:52">[00:54:52]</a>. Gradients are only pushed into these much smaller [[finetuning_machine_learning_models | LoRA]] parameters, not the entire base model <a class="yt-timestamp" data-t="00:55:09">[00:55:09]</a>. This significantly reduces the computational overhead of [[finetuning_machine_learning_models | fine-tuning]] <a class="yt-timestamp" data-t="00:55:09">[00:55:09]</a>.

While the base model is stored in 4-bit, the computations (like matrix multiplications) during forward and backward passes are performed in 16-bit precision after de-quantizing the 4-bit weights <a class="yt-timestamp" data-t="01:03:51">[01:03:51]</a>. The [[finetuning_machine_learning_models | LoRA]] parameters themselves are kept at 16-bit BrainFloat (BF16) during training <a class="yt-timestamp" data-t="01:37:44">[01:37:44]</a>.

### Performance and Accessibility
QLoRA reduces the average memory requirements for [[finetuning_machine_learning_models | fine-tuning]] a 65 billion parameter model from over 780 GB to less than 48 GB <a class="yt-timestamp" data-t="00:20:18">[00:20:18]</a>. This memory reduction comes without degrading runtime or predictive performance compared to a 16-bit fully [[finetuning_machine_learning_models | fine-tuned]] baseline <a class="yt-timestamp" data-t="00:20:25">[00:20:25]</a>. For example, QLoRA can train a Guanaco 65B model on a single GPU in 24 hours <a class="yt-timestamp" data-t="00:17:41">[00:17:41]</a>. This marks a significant shift in the accessibility of LLM [[finetuning_machine_learning_models | fine-tuning]], making it possible for the largest publicly available models to be [[finetuning_machine_learning_models | fine-tuned]] on a single GPU <a class="yt-timestamp" data-t="00:38:00">[00:38:00]</a>.

The Guanaco family of models, trained with QLoRA, has demonstrated competitive performance, reaching 99.3% of ChatGPT's performance on the Vicuna Benchmark <a class="yt-timestamp" data-t="00:08:35">[00:08:35]</a>.

## Technical Deep Dive into QLoRA Innovations

### 4-bit NormalFloat (NF4)
[[Quantization in machine learning models | Quantization]] typically involves rescaling input data to fit the target data type's range <a class="yt-timestamp" data-t="00:43:26">[00:43:26]</a>. When quantizing, for example, 32-bit floats to 8-bit integers, the range of possible values shrinks significantly (e.g., -127 to 127 for int8) <a class="yt-timestamp" data-t="00:45:34">[00:45:34]</a>.

The NF4 data type is built on quantile [[Quantization in machine learning models | quantization]], which aims to ensure each [[Quantization in machine learning models | quantization]] bin has an equal number of values from the input tensor <a class="yt-timestamp" data-t="01:04:42">[01:04:42]</a>. For normally distributed data, this means thinner bins around the mean (where data is dense) and wider bins towards the tails (where data is sparse) <a class="yt-timestamp" data-t="01:09:12">[01:09:12]</a>.

A key challenge with standard quantile [[Quantization in machine learning models | quantization]] is that it can be computationally expensive to estimate quantiles <a class="yt-timestamp" data-t="01:09:55">[01:09:55]</a>. Also, it can lead to large [[Quantization in machine learning models | quantization]] errors for outliers, which are often crucial values in neural networks <a class="yt-timestamp" data-t="01:10:24">[01:10:24]</a>. NF4 addresses this by assuming a zero-centered normal distribution for neural network weights <a class="yt-timestamp" data-t="01:12:21">[01:12:21]</a>. This allows for the use of a fixed, pre-computed set of quantiles, making the process computationally feasible <a class="yt-timestamp" data-t="01:11:34">[01:11:34]</a>.

An interesting observation regarding [[impact_of_model_size_on_quantization_effectiveness | model size]] and [[Quantization in machine learning models | quantization]] is that as models scale up, the percentage of layers exhibiting "outliers" (values far from the mean) increases and then stabilizes <a class="yt-timestamp" data-t="02:20:00">[02:20:00]</a>. For models larger than 6.7 billion parameters, around 75% of layers have outliers, which is a consistent pattern across different model sizes <a class="yt-timestamp" data-t="02:23:03">[02:23:03]</a>.

### Double Quantization
While block-wise [[Quantization in machine learning models | quantization]] (dividing tensors into blocks and quantizing them independently) helps with outliers, it introduces the overhead of storing multiple [[Quantization in machine learning models | quantization]] constants <a class="yt-timestamp" data-t="00:50:50">[00:50:50]</a>. These constants are initially stored as 32-bit floats, which consume considerable memory <a class="yt-timestamp" data-t="01:24:52">[01:24:52]</a>.

Double [[Quantization in machine learning models | quantization]] tackles this by quantizing these [[Quantization in machine learning models | quantization]] constants themselves <a class="yt-timestamp" data-t="01:24:40">[01:24:40]</a>. It takes the 32-bit float constants, subtracts their mean to center them around zero (similar to how weights are normalized), and then quantizes them to 8-bit floats <a class="yt-timestamp" data-t="01:29:14">[01:29:14]</a>. This secondary [[Quantization in machine learning models | quantization]] step further reduces memory footprint, saving approximately 0.37 bits per parameter <a class="yt-timestamp" data-t="01:27:39">[01:27:39]</a>.

### Paged Optimizers
During the [[finetuning_machine_learning_models | fine-tuning]] process, particularly with long sequence lengths in mini-batches, memory spikes can occur due to the storage of optimizer states and activation gradients <a class="yt-timestamp" data-t="00:59:12">[00:59:12]</a>. These spikes often lead to dreaded "out of memory" errors, crashing the training program <a class="yt-timestamp" data-t="01:02:56">[01:02:56]</a>.

Paged optimizers leverage NVIDIA's unified memory, which allows for automatic page-to-page transfers between CPU RAM and GPU VRAM <a class="yt-timestamp" data-t="01:31:34">[01:31:34]</a>. When the GPU runs out of memory, it can automatically "evict" (transfer) parts of the optimizer states to the CPU RAM and retrieve them when needed <a class="yt-timestamp" data-t="01:33:41">[01:33:41]</a>. This ensures that the training process doesn't halt due to temporary memory overflows <a class="yt-timestamp" data-t="01:33:41">[01:33:41]</a>. While this can introduce slowdowns if paging occurs frequently, QLoRA claims that with a batch size of 16, paged optimizers offer similar training speeds to regular optimizers <a class="yt-timestamp" data-t="01:50:06">[01:50:06]</a>.

## Impact and Benchmarking

### Evaluation Challenges
Benchmarking the performance of LLMs is complex and still largely an art <a class="yt-timestamp" data-t="00:08:46">[00:08:46]</a>. QLoRA utilizes various benchmarks including Vicuna, GLUE, and MMLU <a class="yt-timestamp" data-t="01:47:44">[01:47:44]</a>.

A notable aspect of QLoRA's evaluation is its use of automated evaluation systems, primarily GPT-4, to judge model responses <a class="yt-timestamp" data-t="01:49:50">[01:49:50]</a>. While this offers a cheap alternative to human evaluation <a class="yt-timestamp" data-t="01:05:04">[01:05:04]</a>, the paper acknowledges that model-based evaluations have noticeable biases, such as GPT-4 assigning higher scores to systems appearing first in its prompt <a class="yt-timestamp" data-t="02:41:50">[02:41:50]</a>. Despite some disagreements, human and GPT-4 evaluations largely agree on model rankings <a class="yt-timestamp" data-t="02:29:00">[02:29:00]</a>.

The paper also highlights that standard benchmarks like MMLU (Massive Multitask Language Understanding), while common, do not always imply strong chatbot performance and vice-versa <a class="yt-timestamp" data-t="03:51:51">[03:51:51]</a>. This suggests that benchmarks can sometimes steer research in directions that don't align with desired real-world capabilities <a class="yt-timestamp" data-t="02:54:45">[02:54:45]</a>.

### Data Quality vs. Size
A key finding from QLoRA's extensive ablation studies (over 1000 models trained) <a class="yt-timestamp" data-t="01:14:14">[01:14:14]</a> is that **data quality is far more important than data set size** for instruction [[finetuning_machine_learning_models | fine-tuning]] <a class="yt-timestamp" data-t="00:33:43">[00:33:43]</a>. For example, a high-quality dataset like OASST1 (9,000 examples) can outperform much larger datasets like Flan V2 (450,000 examples) in instruction-following generalization <a class="yt-timestamp" data-t="00:34:33">[00:34:33]</a>. This corroborates findings from other papers, such as the Lima paper, which also observed that small, high-quality datasets yield state-of-the-art results <a class="yt-timestamp" data-t="00:33:43">[00:33:43]</a>.

## Future Outlook and Broader Implications
QLoRA's success demonstrates that [[Quantization in machine learning models | quantization]] combined with [[finetuning_machine_learning_models | low-rank adapters]] can replicate 16-bit performance for LLM [[finetuning_machine_learning_models | fine-tuning]] <a class="yt-timestamp" data-t="02:08:43">[02:08:43]</a>. This is a significant win for the open-source AI community, making state-of-the-art natural language processing (NLP) research and development more accessible outside of large industry labs <a class="yt-timestamp" data-t="02:57:41">[02:57:41]</a>.

The approach potentially opens doors for even more aggressive [[Quantization in machine learning models | quantization]], such as 3-bit base models, as [[finetuning_machine_learning_models | fine-tuning]] after [[Quantization in machine learning models | quantization]] appears to recover most lost information <a class="yt-timestamp" data-t="02:57:05">[02:57:05]</a>. The principles behind QLoRA, such as using [[finetuning_machine_learning_models | low-rank adapters]] and intelligent [[Quantization in machine learning models | quantization]] schemes, are likely transferable to other domains beyond LLMs, such as vision models (e.g., in architectures like ControlNet) <a class="yt-timestamp" data-t="02:15:00">[02:15:00]</a>.