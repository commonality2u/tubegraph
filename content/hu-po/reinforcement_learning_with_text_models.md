---
title: Reinforcement Learning with Text Models
videoId: hhawa3tFN2s
---

From: [[hu-po]] <br/> 

Project Voyager introduces an [[Integration of large language models in interactive agents | embodied agent]] that utilizes [[Large Language Models in Gaming | Large Language Models]] (LLMs) to achieve open-ended, lifelong learning within the Minecraft environment <a class="yt-timestamp" data-t="01:14:00">[01:14:00]</a>. This work showcases a novel approach to [[AI and Reinforcement Learning | reinforcement learning]] by leveraging the extensive "world model" knowledge embedded within pre-trained LLMs like GPT-4 <a class="yt-timestamp" data-t="02:50:00">[02:50:00]</a>, <a class="yt-timestamp" data-t="06:55:00">[06:55:00]</a>.

## Limitations of Traditional Reinforcement Learning
Traditionally, [[AI and Reinforcement Learning | reinforcement learning]] faces significant challenges in environments with "choke points" or sparse rewards, such as the classic Atari game Montezuma's Revenge <a class="yt-timestamp" data-t="05:24:00">[05:24:00]</a>. In such games, agents often struggle with exploration and require stumbling upon very specific actions to progress, making it difficult to learn complex, multi-step tasks <a class="yt-timestamp" data-t="06:19:00">[06:19:00]</a>, <a class="yt-timestamp" data-t="06:47:00">[06:47:00]</a>. Even advanced techniques like deep [[AI and Reinforcement Learning | reinforcement learning]], which power systems like AlphaGo, excel in environments where the entire game state is visible and choke points are absent <a class="yt-timestamp" data-t="07:11:00">[07:11:00]</a>, <a class="yt-timestamp" data-t="07:21:00">[07:21:00]</a>.

## Voyager's Approach: LLMs as the Core
Voyager reimagines the [[AI and Reinforcement Learning | reinforcement learning]] paradigm by integrating LLMs into key components of its learning process <a class="yt-timestamp" data-t="01:09:00">[01:09:00]</a>. Instead of fine-tuning model parameters, Voyager interacts with a black-box LLM (GPT-4) via prompting and in-context learning <a class="yt-timestamp" data-t="03:31:00">[03:31:00]</a>, <a class="yt-timestamp" data-t="03:46:00">[03:46:00]</a>. This approach bypasses the need for explicit gradient-based training, making it highly adaptable <a class="yt-timestamp" data-t="03:52:00">[03:52:00]</a>.

The impressive performance of Voyager stems from the fact that LLMs like GPT-4 have built a model of the world (including Minecraft) from vast internet text corpuses <a class="yt-timestamp" data-t="01:39:00">[01:39:00]</a>, <a class="yt-timestamp" data-t="03:13:00">[03:13:00]</a>. This allows the LLM to possess prior knowledge about game mechanics, crafting sequences, and common strategies, essentially leveraging existing human knowledge without direct human intervention during gameplay <a class="yt-timestamp" data-t="03:07:00">[03:07:00]</a>, <a class="yt-timestamp" data-t="04:54:00">[04:54:00]</a>.

### Key Components of Voyager
Voyager consists of three main components:

1.  **Automatic Curriculum** <a class="yt-timestamp" data-t="01:36:00">[01:36:00]</a>
    *   This component maximizes exploration by proposing progressively harder tasks <a class="yt-timestamp" data-t="01:38:00">[01:38:00]</a>, <a class="yt-timestamp" data-t="03:57:00">[03:57:00]</a>.
    *   It leverages GPT-4's extensive knowledge to create a curriculum that is both challenging and manageable <a class="yt-timestamp" data-t="04:54:00">[04:54:00]</a>, <a class="yt-timestamp" data-t="04:52:00">[04:52:00]</a>.
    *   The curriculum is driven by a form of "curiosity," explicitly prompted to the LLM to "discover as many diverse things as possible" <a class="yt-timestamp" data-t="03:57:00">[03:57:00]</a>, <a class="yt-timestamp" data-t="04:06:00">[04:06:00]</a>, a concept traditionally mathematically defined in [[AI and Reinforcement Learning | reinforcement learning]] <a class="yt-timestamp" data-t="03:57:00">[03:57:00]</a>.
    *   The LLM also considers the agent's current state (inventory, health, nearby entities) and past successes/failures when generating tasks <a class="yt-timestamp" data-t="04:50:00">[04:50:00]</a>.

2.  **Skill Library** <a class="yt-timestamp" data-t="01:25:00">[01:25:00]</a>
    *   Voyager incrementally builds a never-growing library of executable code, where each "skill" is a Python function <a class="yt-timestamp" data-t="01:26:00">[01:26:00]</a>, <a class="yt-timestamp" data-t="03:26:00">[03:26:00]</a>.
    *   These skills represent complex behaviors (e.g., `combat_zombie`, `craft_stone_sword`) <a class="yt-timestamp" data-t="03:26:00">[03:26:00]</a>, <a class="yt-timestamp" data-t="03:56:00">[03:56:00]</a>.
    *   The skills are stored and retrieved using a vector database, indexed by the embedding of their descriptions <a class="yt-timestamp" data-t="03:33:00">[03:33:00]</a>, <a class="yt-timestamp" data-t="05:15:00">[05:15:00]</a>. This allows for similarity search and the composition of simpler programs into more complex ones <a class="yt-timestamp" data-t="03:33:00">[03:33:00]</a>, <a class="yt-timestamp" data-t="03:36:00">[03:36:00]</a>, <a class="yt-timestamp" data-t="05:50:00">[05:50:00]</a>.
    *   This library enables "lifelong learning" by allowing the agent to progressively acquire, update, accumulate, and transfer knowledge over extended periods, alleviating catastrophic forgetting <a class="yt-timestamp" data-t="01:29:00">[01:29:00]</a>, <a class="yt-timestamp" data-t="02:54:00">[02:54:00]</a>.

3.  **Iterative Prompting Mechanism** <a class="yt-timestamp" data-t="01:02:00">[01:02:00]</a>
    *   LLMs often struggle to produce correct action code consistently in one shot <a class="yt-timestamp" data-t="03:56:00">[03:56:00]</a>, <a class="yt-timestamp" data-t="04:09:00">[04:09:00]</a>. To address this, Voyager uses an iterative loop for self-improvement <a class="yt-timestamp" data-t="01:02:00">[01:02:00]</a>, <a class="yt-timestamp" data-t="03:31:00">[03:31:00]</a>.
    *   It executes the generated code and receives three types of feedback:
        *   **Environment Feedback**: Intermediate progress from the Minecraft simulation, accessed via JavaScript APIs (e.g., `bot.chat`, `bot.inventory.count`) <a class="yt-timestamp" data-t="01:05:00">[01:05:00]</a>, <a class="yt-timestamp" data-t="05:57:00">[05:57:00]</a>, <a class="yt-timestamp" data-t="06:11:00">[06:11:00]</a>. The agent has "perfect knowledge" of the game state, including inventory, equipment, nearby blocks, entities, biome, time, health, and hunger <a class="yt-timestamp" data-t="04:22:00">[04:22:00]</a>, <a class="yt-timestamp" data-t="04:47:00">[04:47:00]</a>.
        *   **Execution Errors**: Error traces from the code interpreter (the game engine) that reveal invalid operations or syntax errors <a class="yt-timestamp" data-t="01:02:00">[01:02:00]</a>, <a class="yt-timestamp" data-t="05:57:00">[05:57:00]</a>.
        *   **Self-Verification**: A GPT-4 agent acts as a critic to check for task success and reflect on mistakes <a class="yt-timestamp" data-t="01:02:00">[01:02:00]</a>, <a class="yt-timestamp" data-t="01:06:00">[01:06:00]</a>, <a class="yt-timestamp" data-t="06:51:00">[06:51:00]</a>. This feedback loop is a form of "Chain of Thought" reasoning <a class="yt-timestamp" data-t="01:02:00">[01:02:00]</a>, <a class="yt-timestamp" data-t="03:31:00">[03:31:00]</a>, <a class="yt-timestamp" data-t="06:06:00">[06:06:00]</a>, where the LLM refines its program until the task is successfully completed <a class="yt-timestamp" data-t="03:56:00">[03:56:00]</a>, <a class="yt-timestamp" data-t="04:02:00">[04:02:00]</a>.

## Performance and Implications
Voyager achieved impressive results, obtaining 3.3 times more unique items, traveling 2.3 times longer distances, and unlocking key Tech Tree Milestones up to 15 times faster than previous [[stateoftheart_in_reinforcement_learning | state-of-the-art]] methods <a class="yt-timestamp" data-t="07:48:00">[07:48:00]</a>, <a class="yt-timestamp" data-t="08:02:00">[08:02:00]</a>, <a class="yt-timestamp" data-t="08:10:00">[08:10:00]</a>. It demonstrates efficient "zero-shot generalization" by applying its learned skill library to new, unseen Minecraft worlds <a class="yt-timestamp" data-t="01:48:00">[01:48:00]</a>, <a class="yt-timestamp" data-t="04:48:00">[04:48:00]</a>. The ablation study confirms that all three components—automatic curriculum, skill library, and iterative prompting—are crucial for optimal performance <a class="yt-timestamp" data-t="01:26:00">[01:26:00]</a>, <a class="yt-timestamp" data-t="01:40:00">[01:40:00]</a>. The use of GPT-4 for code generation is particularly impactful, showing a "quantum leap" in quality compared to GPT-3.5 <a class="yt-timestamp" data-t="01:40:00">[01:40:00]</a>, <a class="yt-timestamp" data-t="01:46:00">[01:46:00]</a>.

Despite its success, the approach has limitations. It relies on high-level JavaScript APIs (Mindflayer) for interacting with Minecraft, rather than raw pixel input or low-level motor commands <a class="yt-timestamp" data-t="02:11:00">[02:11:00]</a>, <a class="yt-timestamp" data-t="02:22:00">[02:22:00]</a>, <a class="yt-timestamp" data-t="02:54:00">[02:54:00]</a>. This simplifies the problem significantly, as real-world robotics lacks such clean APIs for perception and control <a class="yt-timestamp" data-t="02:54:00">[02:54:00]</a>, <a class="yt-timestamp" data-t="03:07:00">[03:07:00]</a>, <a class="yt-timestamp" data-t="03:07:00">[03:07:00]</a>. Additionally, the high cost of GPT-4 API calls and occasional LLM hallucinations (e.g., proposing non-existent items or invalid fuel sources) remain challenges <a class="yt-timestamp" data-t="01:40:00">[01:40:00]</a>, <a class="yt-timestamp" data-t="01:40:00">[01:40:00]</a>, <a class="yt-timestamp" data-t="01:40:00">[01:40:00]</a>.

The project highlights a potential shift in [[AI and Reinforcement Learning | reinforcement learning]] research, where the focus moves from traditional deep [[AI and Reinforcement Learning | reinforcement learning]] algorithms (like PPO or A3C) towards LLM-centric systems leveraging prompt engineering, vector databases, and Chain of Thought processes <a class="yt-timestamp" data-t="01:08:00">[01:08:00]</a>, <a class="yt-timestamp" data-t="01:08:00">[01:08:00]</a>, <a class="yt-timestamp" data-t="01:08:00">[01:08:00]</a>. This emerging paradigm suggests that future improvements in performance may increasingly depend on advancements in underlying LLM capabilities <a class="yt-timestamp" data-t="01:08:00">[01:08:00]</a>, <a class="yt-timestamp" data-t="01:08:00">[01:08:00]</a>.

## Related Concepts
*   [[stateoftheart_in_reinforcement_learning | Stateoftheart in Reinforcement Learning]]
*   [[Ensemble Learning for Language Models | Ensemble Learning for Language Models]]
*   [[reinforcement_learning_from_human_feedback | reinforcement learning from human feedback]]
*   [[AI and Reinforcement Learning | AI and Reinforcement Learning]]
*   [[Integration of large language models in interactive agents | Integration of large language models in interactive agents]]
*   [[Large Language Models in Gaming | Large Language Models in Gaming]]
*   [[Challenges and Costs in Reinforcement Learning Implementation | Challenges and Costs in Reinforcement Learning Implementation]]
*   [[Taskspecific agent loops and reinforcement learning | Taskspecific agent loops and reinforcement learning]]
*   [[Applications in machine learning and reinforcement learning | Applications in machine learning and reinforcement learning]]