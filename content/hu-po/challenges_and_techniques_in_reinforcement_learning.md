---
title: Challenges and techniques in reinforcement learning
videoId: nTdJEQLday0
---

From: [[hu-po]] <br/> 

## Introduction to Reinforcement Learning (RL)

[[Reinforcement learning in AI | Reinforcement learning]] is a type of machine learning that predates deep learning <a class="yt-timestamp" data-t="00:02:37">[00:02:37]</a>. Its foundation is built upon the concept of a Markov Decision Process (MDP) <a class="yt-timestamp" data-t="00:02:49">[00:02:49]</a>. An MDP describes the world as a set of states where [[Reinforcement learning concepts applied to AI agents | agents]] take actions, producing new states <a class="yt-timestamp" data-t="00:02:57">[00:02:57]</a>. In this framework, an environment rewards the agent for successfully completing tasks <a class="yt-timestamp" data-t="00:03:14">[00:03:14]</a>.

## Reinforcement Learning with Human Feedback (RLHF)

[[Reinforcement Learning with Human Feedback RLHF | RLHF]], or Reinforcement Learning from Human Feedback, is a technique that gained popularity because OpenAI states they use it to fine-tune their ChatGPT models <a class="yt-timestamp" data-t="00:00:10">[00:00:10]</a>. It is considered a go-to approach for building powerful AI systems like ChatGPT <a class="yt-timestamp" data-t="00:04:17">[00:04:17]</a>. While the vast majority of an LLM's intelligence comes from its initial pre-training <a class="yt-timestamp" data-t="00:04:42">[00:04:42]</a>, [[Reinforcement Learning with Human Feedback RLHF | RLHF]] is primarily focused on aspects like AI safety and ensuring the LLM's output aligns specifically with desired task outcomes <a class="yt-timestamp" data-t="00:05:00">[00:05:00]</a>.

### Three Steps of [[Reinforcement Learning with Human Feedback RLHF | RLHF]]

Training a large language model (LLM) with [[Reinforcement Learning with Human Feedback RLHF | RLHF]] typically involves three steps <a class="yt-timestamp" data-t="00:06:23">[00:06:23]</a>:

1.  **Fine-tune a Pre-trained LLM** <a class="yt-timestamp" data-t="00:06:27">[00:06:27]</a>: This involves training the LLM on a specific domain or corpus of instructions and human demonstrations <a class="yt-timestamp" data-t="00:06:41">[00:06:41]</a>. This human feedback constitutes a much smaller dataset used for fine-tuning <a class="yt-timestamp" data-t="00:06:44">[00:06:44]</a>. This initial step is a form of supervised learning, requiring a very clean prompt dataset, likely created in-house rather than from user data <a class="yt-timestamp" data-t="00:10:47">[00:10:47]</a>.
    *   *Example:* A prompt like "Explain [[Reinforcement learning in AI | reinforcement learning]] to a six-year-old" is given, and a human labeler demonstrates the desired output behavior <a class="yt-timestamp" data-t="00:10:00">[00:10:00]</a>. This data fine-tunes the base model using supervised learning <a class="yt-timestamp" data-t="00:10:12">[00:10:12]</a>.
2.  **Collect Human-Annotated Data and Train a Reward Model** <a class="yt-timestamp" data-t="00:06:58">[00:06:58]</a>: This step involves training a "reward model," which is essentially a value function <a class="yt-timestamp" data-t="00:07:09">[00:07:09]</a>. This model, often another LLM, approximates what the reward (goodness) of an action (LLM answer) is for a given state (human prompt) <a class="yt-timestamp" data-t="00:07:49">[00:07:49]</a>. Human labelers rank various LLM outputs for a given prompt, providing the learning signal for this reward model <a class="yt-timestamp" data-t="00:11:28">[00:11:28]</a>. The reward is typically a normalized number, like between 0 and 1 <a class="yt-timestamp" data-t="00:25:50">[00:25:50]</a>.
3.  **Fine-tune the LLM with the Reward Model using [[Reinforcement learning in AI | RL]]** <a class="yt-timestamp" data-t="00:08:30">[00:08:30]</a>: Once the reward model is established, it can be used to continuously push gradients into the LLM <a class="yt-timestamp" data-t="00:08:39">[00:08:39]</a>. This is distinct from in-context learning or prompt engineering, where model parameters are not modified <a class="yt-timestamp" data-t="00:09:04">[00:09:04]</a>. A common [[Reinforcement learning and stateoftheart models | reinforcement learning algorithm]] used for this is Proximal Policy Optimization (PPO) <a class="yt-timestamp" data-t="00:12:07">[00:12:07]</a>. PPO is considered a basic and relatively easy-to-use algorithm, often tried first <a class="yt-timestamp" data-t="00:12:48">[00:12:48]</a>.

In summary, [[Reinforcement Learning and Q Learning in AI | RLHF]] involves three different function approximators or AIs: the initial supervised policy (fine-tuned LLM), the reward model (value function), and the PPO model (the actual [[Reinforcement learning and stateoftheart models | reinforcement learning model]]) <a class="yt-timestamp" data-t="00:15:01">[00:15:01]</a>.

## Challenges in Large-Scale RL Training

[[Challenges in endtoend robotic learning | Training at scale]] for large language models presents significant hurdles:

*   **Memory Requirements** <a class="yt-timestamp" data-t="00:28:40">[00:28:40]</a>: Fitting the model and its optimizer states onto available GPU devices is a primary challenge <a class="yt-timestamp" data-t="00:28:42">[00:28:42]</a>.
    *   **Precision (d-type)** <a class="yt-timestamp" data-t="00:28:48">[00:28:48]</a>: The amount of GPU memory a single parameter takes depends on its precision (e.g., `float32`, `float16`, `int8`, or even 4-bit) <a class="yt-timestamp" data-t="00:28:48">[00:28:48]</a>. A 20-billion parameter model in `float32` would require 80 GB of memory, which is beyond consumer GPUs <a class="yt-timestamp" data-t="00:44:58">[00:44:58]</a>.
*   **Computational Cost & Speed** <a class="yt-timestamp" data-t="00:58:17">[00:58:17]</a>: The overall training speed can be a significant downside <a class="yt-timestamp" data-t="00:58:19">[00:58:19]</a>.
*   **Distributed Training Complexity** <a class="yt-timestamp" data-t="00:33:47">[00:33:47]</a>: Distributing models across multiple GPUs and machines is complex, requiring a deep understanding of hardware and potential bottlenecks <a class="yt-timestamp" data-t="00:33:50">[00:33:50]</a>.
*   **Maintaining Original Model Behavior** <a class="yt-timestamp" data-t="00:29:08">[00:29:08]</a>: When fine-tuning, it's crucial to avoid the active model deviating too much from its original pre-trained behavior and distribution <a class="yt-timestamp" data-t="00:29:08">[00:29:08]</a>. This often requires having two copies of the model during training <a class="yt-timestamp" data-t="00:27:24">[00:27:24]</a>.

## Techniques for Efficient RL Training

Several techniques are employed to address the challenges of training large models, especially in [[Reinforcement Learning and Selfplay in AI Development | RL]] contexts:

*   **Quantization / Lower Precision Training** <a class="yt-timestamp" data-t="00:36:45">[00:36:45]</a>:
    *   **Reduced Precision:** Storing parameters at lower bit precisions (e.g., `float16`, `int8`, 4-bit) significantly reduces memory footprint <a class="yt-timestamp" data-t="00:16:55">[00:16:55]</a>. For example, a 20-billion parameter model can fit into 20 GB using `int8` precision <a class="yt-timestamp" data-t="00:45:30">[00:45:30]</a>.
    *   **8-bit Matrix Multiplication** <a class="yt-timestamp" data-t="00:36:16">[00:36:16]</a>: This method, introduced in the "LLM.int8()" paper, helps mitigate performance degradation that typically occurs when quantizing large models <a class="yt-timestamp" data-t="00:38:24">[00:38:24]</a>. It intelligently performs certain parts (outlier hidden states) in `float16` and others (`non-outlier` parts) in `int8` <a class="yt-timestamp" data-t="00:38:08">[00:38:08]</a>.
*   **Parameter-Efficient Fine-Tuning (PEFT) / Low-Rank Adaptation (LoRA)** <a class="yt-timestamp" data-t="00:39:39">[00:39:39]</a>:
    *   **Freezing Weights:** Instead of changing the values of all 20 billion weights in an LLM, a large portion can be "frozen," preventing gradients from changing their values <a class="yt-timestamp" data-t="00:40:24">[00:40:24]</a>. Only a small top portion or specific adapter layers are fine-tuned <a class="yt-timestamp" data-t="00:40:30">[00:40:30]</a>.
    *   **Low-Rank Adapter Layers:** LoRA specifically proposes freezing pre-trained weights and creating low-rank versions of the query and value layer attention matrices within the Transformer architecture <a class="yt-timestamp" data-t="00:40:59">[00:40:59]</a>. These low-rank matrices have far fewer parameters, enabling fine-tuning with less GPU memory <a class="yt-timestamp" data-t="00:41:31">[00:41:31]</a>.
    *   **Disabling Adapters:** The PEFT library allows using the same model to get both reference and active logits for PPO without creating two separate copies, by enabling or disabling adapters <a class="yt-timestamp" data-t="00:50:45">[00:50:45]</a>.
*   **Distributed Training Strategies** <a class="yt-timestamp" data-t="00:30:03">[00:30:03]</a>:
    *   **Data Parallelism:** The same model is hosted on several machines in parallel, with each instance fed a different batch of data <a class="yt-timestamp" data-t="00:31:10">[00:31:10]</a>. This is the most straightforward strategy <a class="yt-timestamp" data-t="00:32:25">[00:32:25]</a>.
    *   **Model Parallelism (Pipeline and Tensor Parallelism):** The model itself is distributed across many machines <a class="yt-timestamp" data-t="00:32:38">[00:32:38]</a>. Pipeline parallelism splits the model layer-wise, while tensor parallelism splits tensors (operations) across GPUs <a class="yt-timestamp" data-t="00:32:45">[00:32:45]</a>. Sharding is the process of breaking down a large blob of data (model weights) into smaller parts to be sent to different machines <a class="yt-timestamp" data-t="00:33:23">[00:33:23]</a>.
*   **Optimization Techniques** <a class="yt-timestamp" data-t="00:34:43">[00:34:43]</a>:
    *   **Adaptive Activation Checkpointing and Fused Kernels:** Kernels are GPU-executable code for mathematical concepts in a model <a class="yt-timestamp" data-t="00:34:47">[00:34:47]</a>. Fusing multiple operations into a single kernel can significantly increase speed <a class="yt-timestamp" data-t="00:35:36">[00:35:36]</a>.
*   **Using a Reward Model/Value Function** <a class="yt-timestamp" data-t="00:07:09">[00:07:09]</a>: In [[Reinforcement learning concepts applied to AI agents | reinforcement learning environments]], a key component, often a deep neural network, approximates the "value function" to predict the reward for any given state or action <a class="yt-timestamp" data-t="00:07:23">[00:07:23]</a>. This function guides the [[agent loops and reinforcement learning in AI | agent]] to choose the best actions <a class="yt-timestamp" data-t="00:13:55">[00:13:55]</a>.
*   **Libraries and Tools:** Libraries like Hugging Face's TRL (Transformer Reinforcement Learning) and PEFT, as well as `Accelerate` and `bitsandbytes`, aim to make [[Reinforcement learning and stateoftheart models | RL fine-tuning]] more accessible <a class="yt-timestamp" data-t="00:17:40">[00:17:40]</a> <a class="yt-timestamp" data-t="00:43:24">[00:43:24]</a> <a class="yt-timestamp" data-t="00:57:01">[00:57:01]</a>.

## Outlook

While tools are emerging to make [[Reinforcement learning in AI | RL]] fine-tuning more accessible, applying advanced parallelization techniques for multi-GPU setups remains complex, requiring knowledge of hardware and hyperparameter tuning <a class="yt-timestamp" data-t="00:57:41">[00:57:41]</a>. Techniques to increase training speed, such as adjusting learning rates, batch sizes, or using different optimizers, often introduce additional complexity and trade-offs <a class="yt-timestamp" data-t="00:58:29">[00:58:29]</a>. It is anticipated that major players like OpenAI (via Microsoft), Google, and Meta will eventually release their own comprehensive tools and libraries for [[Reinforcement learning and stateoftheart models | RL fine-tuning]] for specific applications, potentially sidelining smaller, bespoke libraries <a class="yt-timestamp" data-t="00:59:08">[00:59:08]</a>.