---
title: Finetuning and Training Curriculums in AI Models
videoId: Y0gYsq7tOnM
---

From: [[hu-po]] <br/> 

## Introduction to Finetuning and Training Curriculums
In the field of AI, particularly with large language models (LLMs), efficacy can be greatly improved on applications requiring a combination of natural and domain-specific language understanding by [[training_and_finetuning_processes_for_ai_models | training]] on domain-specific datasets <a class="yt-timestamp" data-t="00:17:33">[00:17:33]</a>. This process often involves gradually specializing and increasing the capabilities of a base model through a cascade of [[training_and_finetuning_processes_for_ai_models | training]] and [[finetuning_machine_learning_models | fine-tuning]] steps <a class="yt-timestamp" data-t="00:17:56">[00:17:56]</a>. This multi-stage approach is increasingly being referred to as a "curriculum" <a class="yt-timestamp" data-t="00:18:09">[00:18:09]</a>.

## The "Curriculum" Approach / Specialization Pipeline
The "curriculum" or "specialization pipeline" refers to a structured approach to [[training_and_finetuning_processes_for_ai_models | AI model training]] where models are progressively exposed to more complex or specialized data and tasks <a class="yt-timestamp" data-t="00:18:17">[00:18:17]</a>.

### Stages of Training
A typical curriculum for a specialized LLM like Code Llama (Meta AI's latest large language model specifically for code <a class="yt-timestamp" data-t="00:03:45">[00:03:45]</a>) might involve:
1.  **Foundation Model Pretraining:** Starting with a generic [[pretraining_and_finetuning_in_ai_models | foundation model]] (e.g., Llama 2) trained on a vast corpus of generic web and internet data to predict the next token <a class="yt-timestamp" data-t="00:18:46">[00:18:46]</a>. This general [[training_and_finetuning_processes_for_ai_models | training]] can surprisingly improve a model's logic, making it better even at tasks outside the original domain <a class="yt-timestamp" data-t="00:21:32">[00:21:32]</a>.
2.  **Domain-Specific Training:** Further [[training_and_finetuning_processes_for_ai_models | training]] the model on domain-specific data, such as code <a class="yt-timestamp" data-t="00:19:11">[00:19:11]</a>. Code Llama models are initialized with Llama 2 model weights and trained on 500 billion tokens from a code-heavy dataset <a class="yt-timestamp" data-t="00:39:41">[00:39:41]</a>.
3.  **Specialized Task Training:**
    *   **Infilling Training:** [[training_and_finetuning_processes_for_ai_models | Training]] for infilling capabilities, which is the ability to fill in missing parts of code given surrounding context <a class="yt-timestamp" data-t="00:19:14">[00:19:14]</a>, <a class="yt-timestamp" data-t="00:27:21">[00:27:21]</a>.
    *   **Long Context Training:** Further [[training_and_finetuning_processes_for_ai_models | training]] on long-context code to enable the model to handle larger input contexts, such as entire codebases <a class="yt-timestamp" data-t="00:19:16">[00:19:16]</a>, <a class="yt-timestamp" data-t="00:28:28">[00:28:28]</a>.
    *   **Instruction Finetuning:** [[finetuning_language_models_for_specific_tasks | Instruction fine-tuning]] to enable conversational back-and-forth and zero-shot instruction following for programming tasks <a class="yt-timestamp" data-t="00:19:22">[00:19:22]</a>, <a class="yt-timestamp" data-t="00:08:12">[00:08:12]</a>.

### Benefits and Rationale
*   **Cost-Effectiveness:** This phased approach can be more economical. For instance, in long context [[finetuning_machine_learning_models | fine-tuning]], limiting the time spent on processing long sequences to a dedicated [[finetuning_machine_learning_models | fine-tuning]] stage gains long-range capabilities without significantly increasing overall [[challenges_and_methodologies_in_ai_training | training]] costs <a class="yt-timestamp" data-t="01:07:29">[01:07:29]</a>, <a class="yt-timestamp" data-t="01:07:50">[01:07:50]</a>.
*   **Performance Specialization:** By gradually narrowing the domain, models can achieve state-of-the-art performance in specific areas <a class="yt-timestamp" data-t="00:06:38">[00:06:38]</a>. For example, Code Llama Python 7B outperforms Llama 2 70B on human eval and MBP code benchmarks, demonstrating that a smaller, specialized AI can currently beat a larger general AI in its specific domain <a class="yt-timestamp" data-t="01:05:02">[01:05:02]</a>.
*   **Retaining General Knowledge:** Including a small proportion of natural language data during [[training_and_finetuning_processes_for_ai_models | training]] helps models retain natural language understanding skills <a class="yt-timestamp" data-t="00:47:09">[00:47:09]</a>.

### Learning Rate Strategy
A key aspect of curriculum [[training_and_finetuning_processes_for_ai_models | training]] is the adjustment of the learning rate.
*   AdamW is a common optimizer used <a class="yt-timestamp" data-t="01:24:37">[01:24:37]</a>.
*   A cosine schedule with warm-up steps is often employed for the learning rate, allowing for occasional larger steps to avoid getting stuck in local minima <a class="yt-timestamp" data-t="01:24:46">[01:24:46]</a>, <a class="yt-timestamp" data-t="01:25:56">[01:25:56]</a>.
*   Generally, lower learning rates are recommended for [[finetuning_machine_learning_models | fine-tuning]] than for initial [[pretraining_and_finetuning_in_ai_models | pretraining]] to avoid overwriting prior knowledge <a class="yt-timestamp" data-t="01:26:57">[01:26:57]</a>. This concept of changing "neuroplasticity" depending on the [[training_and_finetuning_processes_for_ai_models | training]] stage is crucial <a class="yt-timestamp" data-t="01:28:21">[01:28:21]</a>.
*   For Code Llama, the learning rate for Python [[finetuning_machine_learning_models | fine-tuning]] was set lower (1e-4) than the initial [[training_and_finetuning_processes_for_ai_models | training]] (3e-4), and even lower for long-context [[finetuning_machine_learning_models | fine-tuning]] (2e-5) <a class="yt-timestamp" data-t="01:28:56">[01:28:56]</a>.

## Specific Techniques and Considerations

### Infilling Training
Infilling is the ability to fill in missing parts of code, crucial for applications like real-time code completion in editors or docstring generation <a class="yt-timestamp" data-t="02:00:26">[02:00:26]</a>.
*   **Causal Masking:** This involves moving parts of a [[training_and_finetuning_processes_for_ai_models | training]] sequence to the end and predicting the reordered sequence auto-regressively <a class="yt-timestamp" data-t="00:53:09">[00:53:09]</a>.
*   **Prefix-Suffix-Middle (PSM) vs. Suffix-Prefix-Middle (SPM):** When generating the middle part of a code sequence, providing the prefix (beginning) then the suffix (end) (PSM) often yields better results than suffix then prefix (SPM) <a class="yt-timestamp" data-t="01:37:09">[01:37:09]</a>.
*   **Cost:** Ablation studies show that infilling comes at a low cost to code generation performance <a class="yt-timestamp" data-t="00:28:23">[00:28:23]</a>, <a class="yt-timestamp" data-t="01:02:48">[01:02:48]</a>.

### Long Context Finetuning
Extending the maximum context length is critical for handling large codebases, moving towards repository-level understanding rather than just function or file-level <a class="yt-timestamp" data-t="00:28:50">[00:28:50]</a>.
*   **Rotary Positional Embeddings (RoPE):** Code Llama extended its context from 4,000 to 100,000 tokens by modifying the parameters of the RoPE positional embedding <a class="yt-timestamp" data-t="00:29:55">[00:29:55]</a>. RoPE rotates token vectors, preserving their relative angles and thus their relationships regardless of their absolute position in a sequence <a class="yt-timestamp" data-t="00:30:49">[00:30:49]</a>.
*   **Extrapolation:** Code Llama models exhibit stable behavior and extrapolation capabilities on very long sequences, even beyond those seen during initial [[training_and_finetuning_processes_for_ai_models | training]] <a class="yt-timestamp" data-t="01:14:10">[01:14:10]</a>.
*   **Domain Gap:** Long context [[finetuning_machine_learning_models | fine-tuning]] can potentially hurt short context results, suggesting a domain gap between these types of tasks <a class="yt-timestamp" data-t="01:30:37">[01:30:37]</a>.

### Instruction Finetuning
This makes LLMs more conversational and capable of following instructions, significantly improving usability for end-users <a class="yt-timestamp" data-t="01:18:20">[01:18:20]</a>.
*   **Data Sources:** Uses instruction [[finetuning_machine_learning_models | tuning]] datasets from general LLMs (like Llama 2's RLHF V5 data) and proprietary code-related tasks <a class="yt-timestamp" data-t="01:20:07">[01:20:07]</a>, <a class="yt-timestamp" data-t="01:21:07">[01:21:07]</a>.
*   **Execution Feedback:** For coding tasks, instead of human feedback (which is expensive for professional coding tasks), execution feedback is used. Models generate multiple solutions, and unit tests are run on them to select the correct ones for the instruction dataset <a class="yt-timestamp" data-t="01:20:05">[01:20:05]</a>, <a class="yt-timestamp" data-t="01:20:13">[01:20:13]</a>. This can involve a model generating its own [[the_role_of_data_quality_and_training_scale_in_ai_models | training data]] <a class="yt-timestamp" data-t="01:20:50">[01:20:50]</a>.
*   **Safety and Helpfulness:** Instruction [[finetuning_machine_learning_models | tuning]] often incorporates objectives for helpfulness (answering questions) and safety (avoiding harmful content), which can sometimes be competing objectives <a class="yt-timestamp" data-t="01:19:13">[01:19:13]</a>.
*   **Impact of Temperature:** Higher temperature (more randomness) generally lowers "pass@1" scores (correctness on the first try) but can significantly increase "pass@100" scores (correctness within 100 tries), suggesting that a smaller model with high temperature and filtering can be more effective than a large model with low temperature for some applications <a class="yt-timestamp" data-t="01:46:50">[01:46:50]</a>, <a class="yt-timestamp" data-t="01:47:51">[01:47:51]</a>.

### Data Quality and Generation
[[the_role_of_data_quality_and_training_scale_in_ai_models | Data quality]] is critical for [[training_and_finetuning_processes_for_ai_models | AI model training]] <a class="yt-timestamp" data-t="01:50:20">[01:50:20]</a>.
*   **Deduplication:** Cleaning datasets to remove duplicates is a Herculean task <a class="yt-timestamp" data-t="00:46:06">[00:46:06]</a>.
*   **Synthetic Data:** There is a growing trend towards using synthetically generated data (e.g., questions and answers generated by an LLM) for [[training_and_finetuning_processes_for_ai_models | training]] <a class="yt-timestamp" data-t="01:35:06">[01:35:06]</a>. This can make datasets more accessible and cheaper, potentially democratizing [[challenges_and_methodologies_in_ai_training | AI training]] <a class="yt-timestamp" data-t="01:35:35">[01:35:35]</a>.
*   **Data Secrecy:** Companies often keep their [[the_role_of_data_quality_and_training_scale_in_ai_models | training data]] sets secret due to licensing concerns and the desire to maintain a competitive moat <a class="yt-timestamp" data-t="00:39:50">[00:39:50]</a>. Evidence suggests models are trained on publicly available data like Stack Overflow, even if not explicitly stated <a class="yt-timestamp" data-t="00:41:22">[00:41:22]</a>.
*   **Benchmark Relevance:** While achieving high scores on simple coding benchmarks (like HumanEval and MBP) is a goal, developers realize that [[the_role_of_data_quality_and_training_scale_in_ai_models | training]] too heavily on these "quiz-like" problems can make the model narrow and less effective for real-world production codebases <a class="yt-timestamp" data-t="01:51:42">[01:51:42]</a>, <a class="yt-timestamp" data-t="01:57:20">[01:57:20]</a>.

## Implications and Future Outlook
The increasing complexity of [[training_and_finetuning_processes_for_ai_models | AI training]] pipelines, with multiple [[pretraining_and_finetuning_in_ai_models | pretraining]] and [[finetuning_machine_learning_models | fine-tuning]] steps, reflects a current phase of development <a class="yt-timestamp" data-t="01:57:57">[01:57:57]</a>. However, there's a belief that as compute power and dataset sizes continue to scale, the need for intricate [[finetuning_machine_learning_models | fine-tuning]] steps for narrow tasks will diminish <a class="yt-timestamp" data-t="00:19:50">[00:19:50]</a>.

The trend suggests that larger, more general models (AGIs) trained on all available data will eventually outperform even the most specialized and [[finetuning_machine_learning_models | fine-tuned]] models for specific tasks <a class="yt-timestamp" data-t="00:24:46">[00:24:46]</a>, <a class="yt-timestamp" data-t="02:02:05">[02:02:05]</a>. This is supported by observations that general models (like Llama 2) provide a better starting point for [[finetuning_language_models_for_specific_tasks | fine-tuning]] than models trained on specialized data from scratch <a class="yt-timestamp" data-t="00:22:11">[00:22:11]</a>. Larger models also demonstrate a higher-level semantic understanding across different programming languages, suggesting they find a "compression" or fundamental logic underlying all code <a class="yt-timestamp" data-t="02:00:09">[02:00:09]</a>, <a class="yt-timestamp" data-t="02:01:05">[02:01:05]</a>. This "intelligence as compression" concept could extend to all human knowledge, leading to a single, powerful model that understands everything <a class="yt-timestamp" data-t="02:02:09">[02:02:09]</a>.

While current specialized models like Code Llama achieve state-of-the-art performance among publicly available models, they still lag behind closed-source models like GPT-4 <a class="yt-timestamp" data-t="01:31:53">[01:31:53]</a>. The ongoing secrecy surrounding proprietary datasets and model architectures remains a significant challenge to the "open" AI landscape <a class="yt-timestamp" data-t="01:48:50">[01:48:50]</a>, <a class="yt-timestamp" data-t="02:06:26">[02:06:26]</a>.