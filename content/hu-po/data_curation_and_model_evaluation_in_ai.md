---
title: Data Curation and Model Evaluation in AI
videoId: KSZiJ4k28b4
---

From: [[hu-po]] <br/> 

Dino V2, released by Meta AI Research, represents a significant advancement in unsupervised methods for training foundational computer vision models <a class="yt-timestamp" data-t="00:00:57">[00:00:57]</a>. This new version highlights a shift towards larger, multi-task models that are applicable to a wide variety of tasks, moving beyond specialized architectures for segmentation or classification <a class="yt-timestamp" data-t="00:02:04">[00:02:04]</a>. The backbones of these models are primarily [[challenges_and_innovations_in_ai_model_architecture_and_scaling | Vision Transformers]], indicating their growing supremacy over convolutional networks <a class="yt-timestamp" data-t="00:02:23">[00:02:23]</a>.

## The Era of Foundational Models
The development of these large foundation models necessitates significant team effort, with machine learning papers now often featuring 20 or more authors, reflecting the scale of resources required to train such models <a class="yt-timestamp" data-t="00:03:57">[00:03:57]</a>. These models aim to simplify image usage by producing all-purpose visual features that can be applied to any task—segmentation, classification, or bounding box detection—without fine-tuning <a class="yt-timestamp" data-t="00:04:45">[00:04:45]</a>.

### Challenges in AI Model Development
A significant challenge for individual researchers is the immense computational power required, with training runs often utilizing hardware like 12 A100 GPUs <a class="yt-timestamp" data-t="00:03:04">[00:03:04]</a>. The total cost of a system used for training, such as 20 nodes with 8 V100 32GB GPUs each, can exceed half a million dollars <a class="yt-timestamp" data-t="00:31:35">[00:31:35]</a>. This highlights a growing barrier to entry for independent research <a class="yt-timestamp" data-t="00:30:44">[00:30:44]</a>.

## [[data_set_curation_and_synthetic_data_utilization | Data Curation]]
A key aspect of Dino V2's success is its emphasis on [[model_architecture_and_data_quality_in_ai | data set curation]], especially for self-supervised methods <a class="yt-timestamp" data-t="00:05:18">[00:05:18]</a>.
Curated data from diverse sources is crucial, as uncurated data typically leads to a significant drop in model quality due to lack of control over data quality and diversity <a class="yt-timestamp" data-t="00:15:53">[00:15:53]</a>.

### Automatic Data Curation Pipeline
Meta AI built an automatic pipeline to filter and rebalance datasets, described as the LVD-142M dataset <a class="yt-timestamp" data-t="00:24:38">[00:24:38]</a> <a class="yt-timestamp" data-t="00:24:42">[00:24:42]</a>. This pipeline incorporates several components:
*   **Curated and Uncurated Data Sources:** Combining various datasets like ImageNet-22K, ImageNet-1K, Google Landmarks, and other fine-grained datasets <a class="yt-timestamp" data-t="00:25:45">[00:25:45]</a>.
*   **Deduplication:** Removing near-duplicate images based on embedding similarity using FAISS (Facebook AI Similarity Search) <a class="yt-timestamp" data-t="00:26:54">[00:26:54]</a> <a class="yt-timestamp" data-t="00:27:01">[00:27:01]</a>. This also involves removing duplicates from test or validation sets <a class="yt-timestamp" data-t="00:27:10">[00:27:10]</a>.
*   **Filtering:** Discarding unsafe or restricted URLs, blurring identifiable faces, and NSFW (Not Safe For Work) filtering <a class="yt-timestamp" data-t="00:26:13">[00:26:13]</a> <a class="yt-timestamp" data-t="00:26:22">[00:26:22]</a>.
*   **Retrieval System:** A self-supervised retrieval system that uses image embeddings (generated by a pre-trained Vision Transformer) and cosine similarity to find and augment data with images similar to curated sources <a class="yt-timestamp" data-t="00:23:33">[00:23:33]</a> <a class="yt-timestamp" data-t="00:27:22">[00:27:22]</a> <a class="yt-timestamp" data-t="00:27:30">[00:27:30]</a>. This system does not require external metadata or text, making it a pure image-based foundational model <a class="yt-timestamp" data-t="00:25:27">[00:25:27]</a> <a class="yt-timestamp" data-t="00:25:39">[00:25:39]</a>.

Comparison shows that a 142 million curated image dataset performs significantly better than an identically sized uncurated dataset (e.g., 73% vs. 59% accuracy) <a class="yt-timestamp" data-t="01:00:00">[01:00:00]</a>. Interestingly, ImageNet-22K, despite being 10 times smaller (14 million images), achieves similar or slightly better performance on ImageNet 1K than the larger LVD-142M dataset, suggesting data specificity can sometimes outweigh sheer scale <a class="yt-timestamp" data-t="01:01:10">[01:01:10]</a> <a class="yt-timestamp" data-t="01:01:25">[01:01:25]</a>.

## [[challenges_and_innovations_in_ai_model_architecture_and_scaling | Model Architecture and Training Innovations]]
Dino V2 models are large Vision Transformers (ViT), with the biggest "ViT-G" backbone having 1.1 billion parameters <a class="yt-timestamp" data-t="00:37:34">[00:37:34]</a> <a class="yt-timestamp" data-t="00:43:54">[00:43:54]</a>.

### Model Distillation
A key strategy employed is [[Selfimprovement in AI models | model distillation]], where a very large model is trained first, and then smaller models are trained to mimic its behavior, often outperforming smaller models trained from scratch <a class="yt-timestamp" data-t="00:47:48">[00:47:48]</a> <a class="yt-timestamp" data-t="00:08:00">[00:08:00]</a>. This approach saves significant training and compute budget <a class="yt-timestamp" data-t="00:08:28">[00:08:28]</a>. Surprisingly, a distilled model can sometimes even outperform its larger teacher model on specific benchmarks <a class="yt-timestamp" data-t="01:05:06">[01:05:06]</a> <a class="yt-timestamp" data-t="01:05:47">[01:05:47]</a>.

### Technical Contributions for Scaling
Several techniques were developed to accelerate and stabilize training at scale:
*   **Larger Batch Sizes:** Considered inherently better as they provide more stable gradients, leading to a straighter path through the loss landscape <a class="yt-timestamp" data-t="00:18:11">[00:18:11]</a>.
*   **Untying Head Weights:** Resolves issues of underfitting at the patch level and overfitting at the image level by allowing different objectives to have independent weights <a class="yt-timestamp" data-t="00:34:26">[00:34:26]</a>.
*   **Sinkhorn-Knopp Batch Normalization and Colleo Regularizer:** These normalization techniques encourage a uniform span of features within a batch and contribute to stability and performance <a class="yt-timestamp" data-t="00:36:05">[00:36:05]</a> <a class="yt-timestamp" data-t="00:36:32">[00:36:32]</a>.
*   **Adaptive Resolution Training:** Models are initially trained at a lower resolution (e.g., 224x224 pixels) and then fine-tuned at a higher resolution (e.g., 518x518 pixels) for a short period at the end of pre-training. This significantly reduces compute costs while retaining high performance <a class="yt-timestamp" data-t="00:39:45">[00:39:45]</a> <a class="yt-timestamp" data-t="01:06:00">[01:06:00]</a>.
*   **Efficient Stochastic Depth:** Skips computation of dropped residuals, saving memory and compute <a class="yt-timestamp" data-t="00:44:45">[00:44:45]</a>.
*   **Fully Sharded Data Parallel (FSDP):** A PyTorch implementation of data parallelism that distributes model replicas across GPUs, saving on cross-GPU communication costs and allowing the model size to be bounded by total GPU memory across nodes <a class="yt-timestamp" data-t="00:50:11">[00:50:11]</a> <a class="yt-timestamp" data-t="00:50:31">[00:50:31]</a>. Mixed precision training (using float16 for gradients and weights while keeping shards in float32) further reduces communication costs <a class="yt-timestamp" data-t="00:52:14">[00:52:14]</a>.
*   **Hardware-Specific Hyperparameters:** Model architectures are chosen to maximize compute efficiency based on GPU hardware specifics (e.g., embedding dimensions being multiples of 64 or 256) <a class="yt-timestamp" data-t="00:42:00">[00:42:00]</a>.

## Model Evaluation
Dino V2 aims to produce [[Accuracy of AI information retrieval | all-purpose visual features]] that can be used "out of the box" without fine-tuning, achieving strong performance on downstream tasks <a class="yt-timestamp" data-t="00:04:45">[00:04:45]</a> <a class="yt-timestamp" data-t="00:10:01">[01:10:01]</a>.

### Benchmarking and Performance
Models are evaluated across a wide range of computer vision benchmarks, including image classification (ImageNet), video action recognition, instance-level recognition, semantic segmentation, and monocular depth estimation <a class="yt-timestamp" data-t="00:43:40">[00:43:40]</a> <a class="yt-timestamp" data-t="01:07:45">[01:07:45]</a>. Evaluation often involves training linear classifiers on "frozen" features, meaning the feature encoder is not fine-tuned <a class="yt-timestamp" data-t="01:04:08">[01:04:08]</a>.

Dino V2 demonstrates competitive performance with the best openly available weakly supervised models like Clip and Eva-Clip <a class="yt-timestamp" data-t="01:15:02">[01:15:02]</a>. While not always state-of-the-art on every benchmark, its generalization capabilities, especially with frozen features, are noted <a class="yt-timestamp" data-t="01:14:40">[01:14:40]</a>. For instance, on domain generalization benchmarks, Dino V2's frozen features perform significantly better than OpenCLIP, indicating superior transferability across domains <a class="yt-timestamp" data-t="01:25:25">[01:25:25]</a>.

Qualitative results for depth estimation and semantic segmentation show Dino V2 producing much cleaner and smoother outputs compared to Clip, even on out-of-distribution examples like drawings or paintings <a class="yt-timestamp" data-t="01:40:41">[01:40:41]</a> <a class="yt-timestamp" data-t="01:42:00">[01:42:00]</a>.

### Emergent Properties
The models exhibit emergent properties, such as an understanding of object parts and scene geometry. Principal Component Analysis (PCA) on patch-level features shows that the model implicitly learns to separate foreground from background and even identifies specific body parts (e.g., head, legs) across different images and scales without explicit training for these tasks <a class="yt-timestamp" data-t="01:43:08">[01:43:08]</a> <a class="yt-timestamp" data-t="01:47:25">[01:47:25]</a>. This suggests that [[selfimprovement_in_ai_models | more emergent properties]] are likely to appear at larger scales of model and data <a class="yt-timestamp" data-t="01:55:12">[01:55:12]</a>.

## Conclusion
Dino V2 marks a significant step towards truly general-purpose computer vision models. Its focus on meticulously [[human_data_vs_synthetic_data_in_training_ai | curated data]] and robust training techniques allows it to achieve strong performance with frozen features, minimizing the need for task-specific fine-tuning <a class="yt-timestamp" data-t="01:53:02">[01:53:02]</a>. The open-source release of models and code by Meta AI contrasts with the secrecy of some other AI developers, fostering greater transparency and research in the field <a class="yt-timestamp" data-t="02:02:59">[02:02:59]</a>.