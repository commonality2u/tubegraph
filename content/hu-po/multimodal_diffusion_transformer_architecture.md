---
title: Multimodal Diffusion Transformer Architecture
videoId: yTXMK2TZOZc
---

From: [[hu-po]] <br/> 

The Multimodal Diffusion Transformer (MMD) is a novel [[Transformer architecture in diffusion models | Transformer-based architecture]] proposed for text-to-image generation within [[Diffusion Models and ControlNet | diffusion models]] <a class="yt-timestamp" data-t="08:56:00">[08:56:00]</a>. It builds upon the Diffusion Transformer (DiT) model <a class="yt-timestamp" data-t="09:59:00">[09:59:00]</a> <a class="yt-timestamp" data-t="09:59:00">[09:59:00]</a> <a class="yt-timestamp" data-t="09:59:00">[09:59:00]</a>.

## Core Architecture
The MMD architecture incorporates distinct features to handle different data modalities (text and image) <a class="yt-timestamp" data-t="09:00:00">[09:00:00]</a>:
*   **Separate Weights for Modalities** <a class="yt-timestamp" data-t="09:00:00">[09:00:00]</a>: The MMD uses separate sets of weights for the two modalities, effectively creating two independent Transformers for each modality <a class="yt-timestamp" data-t="09:38:00">[09:38:00]</a> <a class="yt-timestamp" data-t="09:40:00">[09:40:00]</a>. This allows each representation to operate within its own space while still interacting with the other <a class="yt-timestamp" data-t="09:46:00">[09:46:00]</a> <a class="yt-timestamp" data-t="09:48:00">[09:48:00]</a>.
*   **Concatenated Sequences for Attention** <a class="yt-timestamp" data-t="09:31:00">[09:31:00]</a>: Instead of using cross-attention, the MMD concatenates the image tokens (as a sequence of patches) and text tokens (as a sequence) <a class="yt-timestamp" data-t="09:32:00">[09:32:00]</a> <a class="yt-timestamp" data-t="09:59:00">[09:59:00]</a>. Self-attention is then applied to this combined sequence, allowing any part of the text sequence to attend to any part of the image sequence, and vice versa <a class="yt-timestamp" data-t="09:59:00">[09:59:00]</a> <a class="yt-timestamp" data-t="10:02:00">[10:02:00]</a>.
*   **Separate MLPs** <a class="yt-timestamp" data-t="09:46:00">[09:46:00]</a>: After the shared attention mechanism, the architecture splits the representations again, utilizing separate Multi-Layer Perceptrons (MLPs) for the image and text modalities <a class="yt-timestamp" data-t="09:40:00">[09:40:00]</a> <a class="yt-timestamp" data-t="10:04:00">[10:04:00]</a> <a class="yt-timestamp" data-t="10:11:00">[10:11:00]</a>.

This design is considered "symmetric" and "clean" <a class="yt-timestamp" data-t="10:01:00">[10:01:00]</a> <a class="yt-timestamp" data-t="10:04:00">[10:04:00]</a>. While this concatenated approach increases computational complexity and memory usage due to a larger sequence length, it enables richer interaction between modalities <a class="yt-timestamp" data-t="10:08:00">[10:08:00]</a> <a class="yt-timestamp" data-t="10:19:00">[10:19:00]</a> <a class="yt-timestamp" data-t="10:21:00">[10:21:00]</a> <a class="yt-timestamp" data-t="10:26:00">[10:26:00]</a> <a class="yt-timestamp" data-t="10:28:00">[10:28:00]</a>.

## Text Encoding Integration
The MMD architecture processes text conditioning `C` (user prompts or captions) through pre-trained, frozen text models <a class="yt-timestamp" data-t="09:00:00">[09:00:00]</a> <a class="yt-timestamp" data-t="09:53:00">[09:53:00]</a> <a class="yt-timestamp" data-t="09:56:00">[09:56:00]</a>. These include:
*   Clip G14 <a class="yt-timestamp" data-t="09:57:00">[09:57:00]</a> <a class="yt-timestamp" data-t="11:51:00">[11:51:00]</a>
*   Clip L14 <a class="yt-timestamp" data-t="09:57:00">[09:57:00]</a> <a class="yt-timestamp" data-t="11:51:00">[11:51:00]</a>
*   T5 XXL (a large text encoder from Google) <a class="yt-timestamp" data-t="09:57:00">[09:57:00]</a> <a class="yt-timestamp" data-t="11:51:00">[11:51:00]</a> <a class="yt-timestamp" data-t="11:53:00">[11:53:00]</a>

The use of an ensemble of text encoders generally improves model performance, especially if they are trained with different objectives or datasets <a class="yt-timestamp" data-t="01:35:55">[01:35:55]</a> <a class="yt-timestamp" data-t="01:36:02">[01:36:02]</a>.
A notable strategy for [[Training and implementation details of Transformerbased diffusion models | training and implementation details of Transformerbased diffusion models]] is the use of a high dropout rate (46%) on individual text encoders during training <a class="yt-timestamp" data-t="01:37:51">[01:37:51]</a> <a class="yt-timestamp" data-t="01:38:23">[01:38:23]</a>. This makes the model robust to the absence of certain encoders during inference. For instance, if the T5 XXL encoder (which has 4.7 billion parameters and requires significant VRAM) is not available, its embeddings can be replaced by zeros with minimal performance drops <a class="yt-timestamp" data-t="01:39:27">[01:39:27]</a> <a class="yt-timestamp" data-t="01:39:43">[01:39:43]</a>.

### Impact of Text Encoders
Experiments revealed specific contributions of each text encoder:
*   Removing the T5 XXL encoder has little effect on aesthetic quality but significantly impacts the model's ability to generate correctly spelled text <a class="yt-timestamp" data-t="01:39:50">[01:39:50]</a> <a class="yt-timestamp" data-t="01:39:59">[01:39:59]</a>. This suggests that T5, due to its different training objective, provides a more nuanced understanding of token relationships, particularly for spelling and precise semantic understanding (e.g., distinguishing "in" vs. "around" for objects) <a class="yt-timestamp" data-t="01:41:08">[01:41:08]</a> <a class="yt-timestamp" data-t="01:42:04">[01:42:04]</a> <a class="yt-timestamp" data-t="01:42:54">[01:42:54]</a>.

## Performance and Scaling
The size of the MMD model is parameterized by a single variable, `D`, which controls its depth (number of attention blocks), hidden size (width), and number of attention heads <a class="yt-timestamp" data-t="01:08:10">[01:08:10]</a> <a class="yt-timestamp" data-t="01:08:16">[01:08:16]</a> <a class="yt-timestamp" data-t="01:09:47">[01:09:47]</a>. This parameterization simplifies [[Performance and scalability of diffusion models with Transformers | scaling studies]] <a class="yt-timestamp" data-t="01:09:53">[01:09:53]</a>.
Studies show that increasing `D` (making the model larger) consistently improves performance, with larger models exhibiting lower validation loss <a class="yt-timestamp" data-t="01:11:31">[01:11:31]</a> <a class="yt-timestamp" data-t="01:11:49">[01:11:49]</a>. This [[scalable_diffusion_models_with_transformers | scaling trend]] indicates no sign of saturation, suggesting continued performance improvements with larger computational resources <a class="yt-timestamp" data-t="01:44:59">[01:44:59]</a> <a class="yt-timestamp" data-t="02:05:44">[02:05:44]</a>.

In [[multimodal_model_benchmarks | multimodal model benchmarks]], the MMD architecture generally outperforms other Transformer-based diffusion architectures such as original DiT, Cross DiT, and UViT on validation loss and FID scores <a class="yt-timestamp" data-t="01:24:07">[01:24:07]</a> <a class="yt-timestamp" data-t="01:24:20">[01:24:20]</a>.

The MMD is also used in a three-stage training pipeline:
1.  **Pre-training:** At a smaller image resolution <a class="yt-timestamp" data-t="01:45:34">[01:45:34]</a>.
2.  **Fine-tuning:** At a higher image resolution <a class="yt-timestamp" data-t="01:45:37">[01:45:37]</a>.
3.  **Direct Preference Optimization (DPO):** To align the model with human preferences for aesthetic quality <a class="yt-timestamp" data-t="01:45:53">[01:45:53]</a> <a class="yt-timestamp" data-t="01:46:25">[01:46:25]</a>. This final step uses a dataset of "party prompts" and applies LoRAs (low-rank adaptations) to fine-tune the model, resulting in more visually pleasing images <a class="yt-timestamp" data-t="01:46:37">[01:46:37]</a> <a class="yt-timestamp" data-t="01:47:00">[01:47:00]</a> <a class="yt-timestamp" data-t="01:47:11">[01:47:11]</a> <a class="yt-timestamp" data-t="01:47:14">[01:47:14]</a>. This shifts the model slightly away from merely matching the data distribution to generating images preferred by human evaluators <a class="yt-timestamp" data-t="01:48:08">[01:48:08]</a> <a class="yt-timestamp" data-t="01:48:59">[01:48:59]</a>.