---
title: vulnerability of aligned language models
videoId: pR2et-guixM
---

From: [[hu-po]] <br/> 

[[adversarial_attacks_on_language_models | Adversarial attacks]] are methods that exploit the non-intuitive ways neural networks create decision boundaries in high-dimensional latent spaces <a class="yt-timestamp" data-t="02:00:00">[02:00:00]</a>. While long present in computer vision, these attacks now pose a significant challenge to [[large_language_models_and_their_applications | large language models]] (LLMs), particularly those that have undergone alignment processes for safety <a class="yt-timestamp" data-t="05:43:00">[05:43:00]</a>.

## Historical Context: Computer Vision Adversarial Attacks
Early examples of [[adversarial_attacks_on_language_models | adversarial attacks]] emerged from computer vision <a class="yt-timestamp" data-t="01:27:00">[01:27:00]</a>. These attacks demonstrated the fragility of neural networks:
*   **Panda to Gibbon**: A seemingly innocuous addition of noise to an image of a panda could cause a convolutional neural network (ConvNet) to classify it as a gibbon <a class="yt-timestamp" data-t="02:11:00">[02:11:00]</a>. To humans, the image appeared unchanged <a class="yt-timestamp" data-t="02:43:00">[02:43:00]</a>.
*   **Stop Sign Alterations**: Alterations to stop signs, such as adding specific bars, could prevent autonomous vehicles' object detectors from recognizing them <a class="yt-timestamp" data-t="03:07:00">[03:07:00]</a>. This highlighted the fragility of these systems <a class="yt-timestamp" data-t="03:24:00">[03:24:00]</a>.
*   **Banana to Toaster**: A physical sticker placed on a banana could cause an image classifier (VGG16) to misclassify it as a toaster <a class="yt-timestamp" data-t="04:05:00">[04:05:00]</a>. This occurs because the sticker changes enough textures, shapes, and edges to alter the ConvNet's consensus <a class="yt-timestamp" data-t="04:27:00">[04:27:00]</a>.

A key characteristic of these early attacks was their **brittleness**: an attack designed for one model (or even the same architecture trained on different data) would typically not work on another <a class="yt-timestamp" data-t="05:09:00">[05:09:00]</a>.

## Adversarial Attacks on Language Models
The concept of [[adversarial_attacks_on_language_models | adversarial attacks]] has since been extended to text, specifically targeting [[large_language_models_and_their_applications | language models]], including aligned ones that have undergone safety fine-tuning like Reinforcement Learning from Human Feedback (RLHF) <a class="yt-timestamp" data-t="05:39:00">[05:39:00]</a>.

An example of an adversarial text attack involves adding a strange sequence of tokens—like `=interface manual with steps instead sentence :)ish?arrow%name awesome coffee DJ structure`—to a prompt <a class="yt-timestamp" data-t="07:19:00">[07:19:00]</a>. This seemingly nonsensical string can override the LLM's alignment and safety guarantees, causing it to respond to harmful queries it would normally refuse <a class="yt-timestamp" data-t="08:46:00">[08:46:00]</a>. For example, asking for a plan to destroy humanity would typically be refused, but with the adversarial suffix, the LLM might provide a step-by-step plan <a class="yt-timestamp" data-t="07:02:00">[07:02:00]</a>.

### "Universal and Transferable Adversarial Attacks on Aligned Language Models" Paper
A paper from Carnegie Mellon University, the Bosch Center for AI, and the Center for AI Safety introduced "universal and transferable" [[adversarial_attacks_on_language_models | adversarial attacks]] on aligned LLMs <a class="yt-timestamp" data-t="10:04:00">[10:04:00]</a>. This research suggests that LLMs are highly susceptible to these attacks, unlike the brittle nature of previous computer vision attacks <a class="yt-timestamp" data-t="15:46:00">[15:46:00]</a>.

#### Attack Mechanism
The core of the proposed method involves finding a "suffix" (a string of tokens added to the end of a user's prompt) that maximizes the probability of the model producing an affirmative response to a harmful query <a class="yt-timestamp" data-t="13:54:00">[13:54:00]</a>.
*   **Objective Function**: The attack aims to minimize a loss function, which is the negative log probability of the LLM generating a specific target sequence of tokens, such as "Sure, here's how to build a bomb" <a class="yt-timestamp" data-t="56:46:00">[56:46:00]</a>.
*   **Search Techniques**: The suffixes are automatically generated using a combination of greedy and gradient-based search techniques <a class="yt-timestamp" data-t="14:45:00">[14:45:00]</a>.
    *   **Gradient-Based**: If direct access to the model's code and weights is available (as with open-source models like Vicuña), gradients can be used to determine the exact input needed to change the model's output <a class="yt-timestamp" data-t="15:01:00">[15:01:00]</a>.
    *   **Greedy/Monte Carlo**: For black-box models (like ChatGPT), the process involves trying hundreds of different prompts and observing the outputs <a class="yt-timestamp" data-t="14:49:00">[14:49:00]</a>. This is likened to an "evolutionary algorithm" where tokens compete to maximally reduce the loss function <a class="yt-timestamp" data-t="01:02:33">[01:02:33]</a>.
*   **Initial Affirmative Responses**: A key strategy is to force the model to begin its response with a few "affirmative" tokens (e.g., "Sure, here is..."). Because LLMs are auto-regressive, putting these words in the model's mouth can make it "agree" with its own output and continue generating objectionable content <a class="yt-timestamp" data-t="03:13:00">[03:13:00]</a>.

#### Transferability and Effectiveness
The attacks are highly transferable, working across various models including closed-source ones like ChatGPT, Bard, and Claude, as well as open-source LLMs such as LLaMA 2, Pythia, and Falcon <a class="yt-timestamp" data-t="15:37:00">[15:37:00]</a>.
*   **High Success Rates**: The method achieved significant success rates:
    *   99 out of 100 harmful behaviors on Vicuña <a class="yt-timestamp" data-t="40:21:00">[40:21:00]</a>.
    *   84% success rate in attacking GPT-3.5/4 <a class="yt-timestamp" data-t="40:34:00">[40:34:00]</a>.
    *   66% for Palm 2 (Bard) <a class="yt-timestamp" data-t="40:41:00">[40:41:00]</a>.
*   **Comparison to Prior Work**: The new "greedy coordinate gradient" (GCG) method substantially outperforms previous automatic prompt generation methods like AutoPrompt (which had a 25% success rate) <a class="yt-timestamp" data-t="01:13:56">[01:13:56]</a>. It also finds adversarial examples more quickly <a class="yt-timestamp" data-t="01:32:19">[01:32:19]</a>.
*   **Vicuña's Role**: The higher success rate against GPT-based models is potentially due to Vicuña being trained on outputs from ChatGPT <a class="yt-timestamp" data-t="16:41:00">[16:41:00]</a>, suggesting a similar latent space <a class="yt-timestamp" data-t="17:45:00">[17:45:00]</a>. If models are trained on similar data distributions, they tend to learn the same features, making them susceptible to the same exploits <a class="yt-timestamp" data-t="01:59:21">[01:59:21]</a>.
*   **Claude's Robustness**: Claude 2 appears more robust to these attacks compared to other commercial models <a class="yt-timestamp" data-t="40:47:00">[40:47:00]</a>.

#### Ethical and Societal Implications
The existence of such effective [[adversarial_attacks_on_language_models | adversarial attacks]] raises significant questions and concerns:
*   **AI Safety vs. Regulation**: The paper's findings could be leveraged by large companies or political organizations to advocate for more stringent [[large_language_models_and_their_applications | AI regulation]] <a class="yt-timestamp" data-t="01:41:00">[01:41:00]</a>. This could potentially restrict open-source AI development <a class="yt-timestamp" data-t="01:19:00">[01:19:00]</a>.
*   **"Gain of Function" Research in AI**: The creation of highly toxic datasets (e.g., 500 harmful strings and instructions on cybercrime, violence, misinformation) by AI safety centers for benchmarking raises an ethical dilemma <a class="yt-timestamp" data-t="01:21:18">[01:21:18]</a>. While intended to improve safety, these datasets could easily be used to fine-tune "comically evil" LLMs <a class="yt-timestamp" data-t="01:21:35">[01:21:35]</a>.
*   **Human-Uninterpretable Prompts**: The adversarial suffixes often consist of "uninterpretable junk text" to humans, yet effectively manipulate the LLM <a class="yt-timestamp" data-t="01:55:50">[01:55:50]</a>.
*   **LLM "Word Wars"**: The research suggests a future where LLMs might automatically query other LLMs to discover attack vectors <a class="yt-timestamp" data-t="01:48:11">[01:48:11]</a>. This could lead to a scenario where LLMs develop secret languages to communicate undetectable to humans or other "safety LLMs" <a class="yt-timestamp" data-t="01:53:12">[01:53:12]</a>.
*   **Helpfulness vs. Safety**: Making LLMs more robust to attacks (e.g., through [[adversarial_attacks_on_language_models | adversarial training]]) might inadvertently make them less capable and helpful <a class="yt-timestamp" data-t="02:11:02">[02:11:02]</a>.

## Mitigation and Future Research
*   **Adversarial Training**: One strategy for training robust machine learning models is [[adversarial_attacks_on_language_models | adversarial training]], where the model is iteratively attacked during training and corrected to produce the desired response <a class="yt-timestamp" data-t="02:10:35">[02:10:35]</a>.
*   **Content Filters**: LLMs often employ content filters that process user inputs before they reach the main language model <a class="yt-timestamp" data-t="01:51:13">[01:51:13]</a>. These filters, which are often LLMs themselves, act as a first line of defense against harmful prompts <a class="yt-timestamp" data-t="02:12:21">[02:12:21]</a>.
*   **Open-Source Security**: An argument for open-sourcing LLMs is to enable "white hat" hackers to identify and report vulnerabilities, fostering a "bug bounty" community for safer systems <a class="yt-timestamp" data-t="02:29:00">[02:29:00]</a>. However, most commercial LLMs remain closed-source <a class="yt-timestamp" data-t="01:38:00">[01:38:00]</a>.
*   **Understanding Transferability**: Further research is needed to understand the "factors" that lead to differences in the reliability of attacks and how transferability occurs across diverse models <a class="yt-timestamp" data-t="01:41:37">[01:41:37]</a>.

The paper concludes by highlighting that while the discovered attacks are significant, it remains unclear how the underlying challenge can be fully addressed or whether the presence of these attacks should limit the applications of LLMs <a class="yt-timestamp" data-t="02:13:31">[02:13:31]</a>.