---
title: Complexity comparison of raster scan and multiscale approaches
videoId: -jG7S5g071Q
---

From: [[hu-po]] <br/> 

Visual Autoregressive Modeling (VARM) proposes a new generation paradigm for images that significantly improves upon traditional raster scan approaches, particularly in terms of computational complexity and inference speed. This model, recognized with a best paper award at the NeurIPS conference, reformulates autoregressive learning for images as "next-scale prediction" rather than "next-token prediction" <a class="yt-timestamp" data-t="03:39:42">[03:39:42]</a>.

## Traditional Raster Scan Autoregressive Models

In conventional autoregressive models for images, a visual tokenizer discretizes continuous images into a grid of two-dimensional tokens <a class="yt-timestamp" data-t="01:11:10">[01:11:10]</a>. These tokens are then flattened into a one-dimensional sequence, mimicking sequential language modeling <a class="yt-timestamp" data-t="01:15:09">[01:15:09]</a>. Common flattening orders include:
*   **Row-major raster scan** (left-to-right, top-to-down) <a class="yt-timestamp" data-t="01:15:40">[01:15:40]</a> <a class="yt-timestamp" data-t="01:15:56">[01:15:56]</a>.
*   Spiral or Z-curve orders <a class="yt-timestamp" data-t="01:12:09">[01:12:09]</a>.

A fundamental problem with this approach is that images inherently have bidirectional correlations, which contradicts the unidirectional dependency assumption of autoregressive models <a class="yt-timestamp" data-t="01:19:19">[01:19:19]</a>. For example, a model trained with a top-to-bottom scan cannot predict the top part of an image given only the bottom part <a class="yt-timestamp" data-t="01:19:30">[01:19:30]</a>. Flattening also disrupts the spatial locality inherent in image feature maps <a class="yt-timestamp" data-t="01:19:54">[01:19:54]</a>. To compensate for these issues, models often employ [[layerwise_scaling_and_architecture_tricks | advanced techniques]] like Rotary Position Embeddings (RoPE) <a class="yt-timestamp" data-t="01:16:46">[01:16:46]</a> <a class="yt-timestamp" data-t="01:17:19">[01:17:19]</a>.

### Time Complexity of Raster Scan Autoregressive Models
The computational cost of conventional autoregressive models is significantly high. For an N x N latent grid, a full autoregressive generation of N^2 tokens requires N^2 decoding iterations <a class="yt-timestamp" data-t="01:14:07">[01:14:07]</a>. Each step involves a gradually increasing sequence length. When predicting the *i*-th token, the attention mechanism has a quadratic complexity with respect to the sequence length (O(L^2) where L is the current sequence length). Summing these up for all tokens results in an overall time complexity of **O(N^6)** <a class="yt-timestamp" data-t="01:14:10">[01:14:10]</a> <a class="yt-timestamp" data-t="01:14:13">[01:14:13]</a>. This N^6 complexity is considered "absolutely brutal" for scaling <a class="yt-timestamp" data-t="01:14:15">[01:14:15]</a>.

## Multiscale Autoregressive Modeling (VARM)

VARM introduces [[analysis_of_multiscale_hierarchical_structures | next-scale prediction]], where a Transformer predicts the next higher-resolution token map conditioned on all previous ones <a class="yt-timestamp" data-t="01:18:05">[01:18:05]</a> <a class="yt-timestamp" data-t="01:18:07">[01:18:07]</a>. This aligns with how humans typically perceive and create images in a hierarchical, coarse-to-fine manner <a class="yt-timestamp" data-t="01:17:53">[01:17:53]</a> <a class="yt-timestamp" data-t="01:17:56">[01:17:56]</a>. This approach draws intuition from the layered structure of the human visual system and the inductive priors of convolutional neural networks (CNNs) <a class="yt-timestamp" data-t="01:19:17">[01:19:17]</a>.

VARM involves two main training stages:
1.  **Multiscale VQ-VAE Tokenization**: A multiscale VQ-VAE autoencoder encodes an image into K multiscale discrete token maps, each at an increasingly higher resolution <a class="yt-timestamp" data-t="01:12:22">[01:12:22]</a> <a class="yt-timestamp" data-t="01:12:24">[01:12:24]</a>. Critically, a **shared codebook** (vocabulary) is utilized across all scales, meaning tokens representing macro-level concepts (e.g., an entire parrot) and micro-level details (e.g., the texture of a beak) come from the same dictionary <a class="yt-timestamp" data-t="01:12:45">[01:12:45]</a> <a class="yt-timestamp" data-t="01:12:50">[01:12:50]</a>.
2.  **V-Transformer Training**: A Transformer (based on a vanilla GPT-2 architecture) is trained to predict these token maps autoregressively <a class="yt-timestamp" data-t="01:12:18">[01:12:18]</a> <a class="yt-timestamp" data-t="01:18:18]">[01:18:18]</a>. The autoregressive unit is an *entire token map* at a specific resolution, rather than a single token <a class="yt-timestamp" data-t="01:19:54">[01:19:54]</a>. This means a higher-resolution map (RK) is predicted based on all previous, lower-resolution maps (R1 to RK-1) <a class="yt-timestamp" data-t="01:18:41">[01:18:41]</a> <a class="yt-timestamp" data-t="01:18:44">[01:18:44]</a>.

### Time Complexity of Multiscale Autoregressive Models
VARM's architecture allows for parallel token generation within each resolution map <a class="yt-timestamp" data-t="01:19:51">[01:19:51]</a>. Unlike the raster scan, where predicting the next token requires the completion of the previous token in the 1D sequence, in VARM, all tokens within a given resolution map can be predicted simultaneously because they only depend on the *previous, coarser resolution maps*, not on other tokens in the same map <a class="yt-timestamp" data-t="01:19:56">[01:19:56]</a> <a class="yt-timestamp" data-t="01:20:01">[01:20:01]</a>. This is analogous to how CNN operations can be parallelized on GPUs <a class="yt-timestamp" data-t="01:20:05">[01:20:05]</a>.

This parallelization dramatically reduces the computational complexity. For an N x N latent grid, VARM achieves a time complexity of **O(N^4)** <a class="yt-timestamp" data-t="01:19:56">[01:19:56]</a> <a class="yt-timestamp" data-t="01:20:01">[01:20:01]</a>, a significant improvement over the O(N^6) of traditional raster scan methods <a class="yt-timestamp" data-t="01:14:31">[01:14:31]</a>.

## Comparison and Advantages

| Feature            | Traditional Raster Scan                                  | Multiscale Autoregressive (VARM)                                 |
| :----------------- | :------------------------------------------------------- | :--------------------------------------------------------------- |
| **Tokenization**   | Flattened 1D sequence (e.g., left-to-right, top-to-down) <a class="yt-timestamp" data-t="01:15:40">[01:15:40]</a> | Multiscale discrete token maps, preserving 2D structure <a class="yt-timestamp" data-t="01:12:24">[01:12:24]</a> |
| **Dependency**     | Unidirectional, relies on sequential order <a class="yt-timestamp" data-t="01:19:22">[01:19:22]</a>     | Hierarchical, predicts next *scale* based on previous scales <a class="yt-timestamp" data-t="01:18:07">[01:18:07]</a> |
| **Spatial Locality** | Compromised by flattening; requires position embeddings <a class="yt-timestamp" data-t="01:19:54">[01:19:54]</a> | Preserved; no flattening; no need for RoPE <a class="yt-timestamp" data-t="01:19:52">[01:19:52]</a> |
| **Parallelism**    | Limited; sequential prediction <a class="yt-timestamp" data-t="01:20:00">[01:20:00]</a>        | High; tokens within a resolution map can be generated in parallel <a class="yt-timestamp" data-t="01:19:51">[01:19:51]</a> |
| **Time Complexity**| **O(N^6)** <a class="yt-timestamp" data-t="01:14:13">[01:14:13]</a>                                  | **O(N^4)** <a class="yt-timestamp" data-t="01:20:01">[01:20:01]</a>                                   |
| **Inference Speed**| Slower (e.g., 45 seconds for Diffusion Transformer) <a class="yt-timestamp" data-t="01:13:40">[01:13:40]</a> | 20x faster (e.g., 1 second for VARM) <a class="yt-timestamp" data-t="01:00:00">[01:00:00]</a> <a class="yt-timestamp" data-t="01:13:59">[01:13:59]</a> |
| **Image Quality**  | Inferior FID/Inception scores <a class="yt-timestamp" data-t="01:13:31">[01:13:31]</a>                 | Superior FID/Inception scores (e.g., FID from 18 to 1) <a class="yt-timestamp" data-t="01:00:00">[01:00:00]</a> |
| **Architecture**   | Often uses [[layerwise_scaling_and_architecture_tricks | advanced architectural tricks]] (e.g., RoPE, SwiGLU MLP, RMSNorm) <a class="yt-timestamp" data-t="01:16:46">[01:16:46]</a> | Simpler, uses vanilla GPT-2 architecture, demonstrating effectiveness of the core idea <a class="yt-timestamp" data-t="01:16:18">[01:16:18]</a> <a class="yt-timestamp" data-t="01:16:38">[01:16:38]</a> |

VARM demonstrates strong performance on benchmarks like ImageNet 256x256, achieving superior Fr√©chet Inception Distance (FID) and Inception Scores (IS) with significantly faster inference speeds <a class="yt-timestamp" data-t="01:13:54">[01:13:54]</a>. This indicates that VARM is a more efficient and scalable model for [[largescale_image_segmentation | image generation]] <a class="yt-timestamp" data-t="01:14:51">[01:14:51]</a>. It also exhibits zero-shot generalization abilities for downstream tasks such as image inpainting, outpainting, and editing, without requiring fine-tuning <a class="yt-timestamp" data-t="01:12:42">[01:12:42]</a> <a class="yt-timestamp" data-t="01:13:06">[01:13:06]</a>. VARM also aligns with [[potential_applications_of_multiscale_autoregressive_models | scaling laws]], showing predictable decreases in test loss with increased model size, data, or compute <a class="yt-timestamp" data-t="01:15:08">[01:15:08]</a>.

## Potential Applications and Future Work

The principle behind VARM, which leverages multiscale hierarchies, is broadly applicable beyond 2D images <a class="yt-timestamp" data-t="01:24:46">[01:24:46]</a>. Researchers anticipate applying this approach to:
*   **Temporal dimension**: Generating videos using 3D pyramids and next-scale prediction, which offers inherent advantages in temporal consistency compared to diffusion-based generators like Sora <a class="yt-timestamp" data-t="01:17:57">[01:17:57]</a> <a class="yt-timestamp" data-t="01:18:00">[01:18:00]</a> <a class="yt-timestamp" data-t="01:18:02">[01:18:02]</a>.
*   **3D data**: Rethinking how 3D data modalities, such as [[multiresolution_hash_grids_for_3d_representation | Gaussian Splats]], are flattened into 1D sequences for Transformer input <a class="yt-timestamp" data-t="01:25:01">[01:25:01]</a> <a class="yt-timestamp" data-t="01:25:17">[01:25:17]</a>. This could lead to improved [[fast_differentiable_tile_rasterization_for_rendering | rendering]] and generation of 3D objects <a class="yt-timestamp" data-t="01:25:22">[01:25:22]</a>.
*   **Other data modalities**: Exploring improvements for data types beyond images and videos where flattening into 1D sequences is currently a suboptimal practice, such as proteins or graphs <a class="yt-timestamp" data-t="01:27:37">[01:27:37]</a> <a class="yt-timestamp" data-t="01:27:53">[01:27:53]</a>.

Further work also includes advancing the VQ-VAE tokenizer and applying VARM to tasks like text-to-image generation <a class="yt-timestamp" data-t="01:17:35">[01:17:35]</a> <a class="yt-timestamp" data-t="01:17:48">[01:17:48]</a>. The simplicity and effectiveness of VARM's core idea suggest it will likely be incorporated into future image and video generation models <a class="yt-timestamp" data-t="01:41:33">[01:41:33]</a> <a class="yt-timestamp" data-t="01:41:41">[01:41:41]</a>.