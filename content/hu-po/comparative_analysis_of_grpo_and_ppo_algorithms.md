---
title: Comparative Analysis of GRPO and PPO Algorithms
videoId: Ii_7-wsTjLo
---

From: [[hu-po]] <br/> 

The DeepSeek mathematical reasoning model utilizes a variant of Proximal Policy Optimization (PPO) called Grouped Proximal Policy Optimization (GRPO) <a class="yt-timestamp" data-t="06:10:08">[06:10:08]</a>. This section explores the similarities and key differences between these two reinforcement learning algorithms.

## Reinforcement Learning Fundamentals

At its core, reinforcement learning involves a policy (or agent, neural network, robot) interacting with an environment <a class="yt-timestamp" data-t="08:25:01">[08:25:01]</a>. The environment provides a reward signal, which is then used to generate a gradient update to refine the policy <a class="yt-timestamp" data-t="08:37:05">[08:37:05]</a>.

### On-Policy vs. Off-Policy Learning

A crucial distinction in reinforcement learning is between on-policy and off-policy methods <a class="yt-timestamp" data-t="08:21:07">[08:21:07]</a>.

*   **On-Policy:** In on-policy learning, the gradient update is always applied to the *same policy* that generated the interaction and observations <a class="yt-timestamp" data-t="09:07:07">[09:07:07]</a>.
*   **Off-Policy:** Off-policy methods store experiences in a replay buffer, and samples from this buffer are used for updates <a class="yt-timestamp" data-t="09:20:06">[09:20:06]</a>. This means a policy update might use examples generated by an *older version* of the policy <a class="yt-timestamp" data-t="09:30:03">[09:30:03]</a>. The greater the "distance" between the policy that collected the experience and the policy receiving the update, the more complex the learning becomes <a class="yt-timestamp" data-t="09:58:01">[09:58:01]</a>.

Both PPO and GRPO utilize `Pi Theta` (the current policy) and `Pi Theta old` (the old policy) <a class="yt-timestamp" data-t="10:24:06">[10:24:06]</a>. The ratio of `Pi Theta` to `Pi Theta old` represents whether the new policy has a higher chance of picking the "right" token compared to the old policy, indicating progress if the ratio is greater than 1 <a class="yt-timestamp" data-t="11:10:05">[11:10:05]</a>.

### Clipping Mechanism

Both PPO and GRPO objective functions incorporate a `clip` function <a class="yt-timestamp" data-t="11:46:00">[11:46:00]</a>. This function clips the policy ratio (`Pi Theta / Pi Theta old`) to keep it within a range of `1 - Epsilon` and `1 + Epsilon`, preventing extreme values that could destabilize training <a class="yt-timestamp" data-t="11:59:00">[11:59:00]</a>.

### KL Divergence Term

Both algorithms include a Kullbackâ€“Leibler (KL) Divergence term, represented by `beta DK` <a class="yt-timestamp" data-t="12:21:00">[12:21:00]</a>. This term measures the distance between the current policy (`Pi Theta`) and a reference policy (`P ref`), which in the DeepSeek paper is an SFT (Supervised Fine-Tuning) model <a class="yt-timestamp" data-t="12:30:00">[12:30:00]</a>. Its purpose is to prevent the policy from drifting too far from the reference policy, ensuring that updates refine the policy without "scrambling" it <a class="yt-timestamp" data-t="12:42:00">[12:42:00]</a>. The `beta` hyperparameter controls the importance of this KL Divergence term <a class="yt-timestamp" data-t="28:27:00">[28:27:00]</a>. A high `beta` makes the KL divergence very important, keeping the policy close to the reference, while a low `beta` allows more divergence <a class="yt-timestamp" data-t="41:31:00">[41:31:00]</a>.

## Key Differences: Advantage Estimation

The primary distinction between PPO and GRPO lies in how they estimate the "advantage" <a class="yt-timestamp" data-t="18:57:00">[18:57:00]</a>. The advantage function (`A` or `A hat`) represents the expected reward an agent can receive from a particular state and action <a class="yt-timestamp" data-t="15:30:00">[15:30:00]</a>.

### PPO's Value Function

In PPO, the advantage (`AF of T`) is typically computed using Generalized Advantage Estimation (GAE), which relies on a learned **value function** (`V of S`) <a class="yt-timestamp" data-t="19:35:00">[19:35:00]</a>.
The value function estimates the "goodness" of a state <a class="yt-timestamp" data-t="14:52:00">[14:52:00]</a>. PPO usually employs a separate neural network of comparable size to the policy model to learn this value function <a class="yt-timestamp" data-t="19:48:00">[19:48:00]</a>.

The use of a separate value network in PPO introduces:
*   **Substantial Memory and Computational Burden:** Two distinct networks (policy and value) must be loaded and run <a class="yt-timestamp" data-t="19:53:00">[19:53:00]</a>.
*   **Training Instabilities:** Having "dueling" neural networks like the policy and value networks can lead to instabilities during training, similar to issues seen in Generative Adversarial Networks (GANs) <a class="yt-timestamp" data-t="22:48:00">[22:48:00]</a>.

The reliance on a learned value function in PPO stems partly from the reinforcement learning landscape around 2015, a time when generative models like GANs, which utilize a discriminator network (analogous to a value network) alongside a generator, were influential <a class="yt-timestamp" data-t="21:35:00">[21:35:00]</a>. This inspired the use of a learned value network to estimate state value rather than relying on Monte Carlo estimation <a class="yt-timestamp" data-t="22:30:00">[22:30:00]</a>.

### GRPO's Group Reward Score

GRPO's innovation lies in its approach to advantage estimation. Instead of a separate value model, GRPO calculates the advantage (`A hat I of T`) based on **group reward scores** <a class="yt-timestamp" data-t="19:07:00">[19:07:00]</a>. This involves sampling multiple observations (denoted as `G` observations) from the policy <a class="yt-timestamp" data-t="20:46:00">[20:46:00]</a>. By observing the rewards from this group of sampled outputs, GRPO effectively obtains a Monte Carlo estimate of the value function, eliminating the need for a dedicated value network <a class="yt-timestamp" data-t="20:55:00">[20:55:00]</a>.

Key benefits of GRPO's approach:
*   **Reduced Computational Burden:** No need to run and maintain a second neural network for the value function <a class="yt-timestamp" data-t="20:33:00">[20:33:00]</a>.
*   **Potential for Faster Training:** By parallelizing the generation of multiple observations, GRPO can efficiently gather the necessary data for its advantage estimation <a class="yt-timestamp" data-t="1:07:51">[1:07:51]</a>.

## Summary of Differences

| Feature                 | PPO (Proximal Policy Optimization)                 | GRPO (Grouped Proximal Policy Optimization)        |
| :---------------------- | :------------------------------------------------- | :------------------------------------------------- |
| **Observation Sampling**  | Samples a single observation (`o`) per update <a class="yt-timestamp" data-t="07:49:00">[07:49:00]</a> | Samples a group of `G` observations (`ois`) <a class="yt-timestamp" data-t="07:53:00">[07:53:00]</a> |
| **Value Model**           | Employs a separate learned value network (`V of S`) to estimate advantage <a class="yt-timestamp" data-t="19:48:00">[19:48:00]</a> | Eliminates the need for a separate value model by using "group reward scores" from multiple samples <a class="yt-timestamp" data-t="20:46:00">[20:46:00]</a> |
| **Computational Overhead** | Higher, due to running two comparable neural networks <a class="yt-timestamp" data-t="19:53:00">[19:53:00]</a> | Lower, by removing the value network and using a Monte Carlo estimate from multiple outputs <a class="yt-timestamp" data-t="20:33:00">[20:33:00]</a> |

## Algorithmic Importance and Efficiency

The speaker emphasizes that while GRPO is an innovation, it doesn't mean PPO is "dead" <a class="yt-timestamp" data-t="17:58:00">[17:58:00]</a>. The field of AI often sees cyclical advancements where different algorithms or architectures (e.g., ConvNets vs. Vision Transformers) take turns outperforming each other based on specific applications or optimization efforts <a class="yt-timestamp" data-t="18:08:00">[18:08:00]</a>.

> "Don't think too much about the algorithm; think instead about the actual data and then the engineering effort." <a class="yt-timestamp" data-t="18:42:00">[18:42:00]</a>

This perspective is reinforced by the conflicting results on process vs. outcome supervision in reinforcement learning <a class="yt-timestamp" data-t="24:11:00">[24:11:00]</a>. While one paper suggests process supervision is better, DeepSeek's paper indicates it was unsuccessful <a class="yt-timestamp" data-t="24:36:00">[24:36:00]</a>. This suggests that the "best" approach can vary, and no single algorithm is universally superior <a class="yt-timestamp" data-t="25:16:00">[25:16:00]</a>.

DeepSeek's overall cost savings (e.g., 40x reduction) are attributed to an accumulation of many "little tricks" and optimizations, not just the elimination of the value model by GRPO <a class="yt-timestamp" data-t="1:08:41">[1:08:41]</a>. This includes low-level GPU code optimization, demonstrating advanced engineering prowess <a class="yt-timestamp" data-t="0:52:03">[0:52:03]</a>.