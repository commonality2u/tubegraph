---
title: GANbased image manipulation
videoId: ExfMg4v5DMA
---

From: [[hu-po]] <br/> 

## Introduction
DragGAN is a new paper titled "Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold" <a class="yt-timestamp" data-t="00:00:46">[00:00:46]</a>. This GAN-based paper was released on May 18th <a class="yt-timestamp" data-t="00:01:03">[00:01:03]</a>, <a class="yt-timestamp" data-t="00:01:06">[00:01:06]</a>. It stems from collaborations between institutions like the Max Planck Institute in Germany, MIT CSAIL, and Google AR/VR <a class="yt-timestamp" data-t="00:01:12">[00:01:12]</a>, <a class="yt-timestamp" data-t="00:01:14">[00:01:14]</a>, <a class="yt-timestamp" data-t="00:01:16">[00:01:16]</a>, <a class="yt-timestamp" data-t="00:01:18">[00:01:18]</a>, <a class="yt-timestamp" data-t="00:01:22">[00:01:22]</a>. The work is described as impressive, with its demo being "absolutely insane" <a class="yt-timestamp" data-t="00:01:27">[00:01:27]</a>, <a class="yt-timestamp" data-t="00:01:29">[00:01:29]</a>.

## Core Functionality
DragGAN allows users to drag content in any generated image <a class="yt-timestamp" data-t="00:02:54">[00:02:54]</a>. Users click a "handle point" (start point) and a "target point" (end point), and the approach precisely moves the handle point to its corresponding target point <a class="yt-timestamp" data-t="00:03:00">[00:03:00]</a>, <a class="yt-timestamp" data-t="00:03:03">[00:03:03]</a>, <a class="yt-timestamp" data-t="00:03:05">[00:03:05]</a>. This manipulation occurs along a straight vector between the two points <a class="yt-timestamp" data-t="00:03:17">[00:03:17]</a>, <a class="yt-timestamp" data-t="00:03:19">[00:03:19]</a>. While the handle point moves, the rest of the image is kept fixed, enabling control over spatial attributes like pose, shape, expression, and layout across diverse object categories <a class="yt-timestamp" data-t="00:03:22">[00:03:22]</a>, <a class="yt-timestamp" data-t="00:03:25">[00:03:25]</a>, <a class="yt-timestamp" data-t="00:03:27">[00:03:27]</a>, <a class="yt-timestamp" data-t="00:03:28">[00:03:28]</a>, <a class="yt-timestamp" data-t="00:03:30">[00:03:30]</a>, <a class="yt-timestamp" data-t="00:03:34">[00:03:34]</a>.

A key aspect of DragGAN is its ability to produce realistic outputs even for challenging scenarios, such as "hallucinating occluded content" (e.g., generating teeth inside a lion's mouth when it opens) and deforming shapes while consistently following the object's rigidity (e.g., bending a horse leg) <a class="yt-timestamp" data-t="00:08:15">[00:08:15]</a>, <a class="yt-timestamp" data-t="00:08:18">[00:08:18]</a>, <a class="yt-timestamp" data-t="00:08:20">[00:08:20]</a>, <a class="yt-timestamp" data-t="00:08:22">[00:08:22]</a>, <a class="yt-timestamp" data-t="00:08:24">[00:08:24]</a>, <a class="yt-timestamp" data-t="00:08:26">[00:08:26]</a>, <a class="yt-timestamp" data-t="00:20:48">[00:20:48]</a>, <a class="yt-timestamp" data-t="00:20:49">[00:20:49]</a>, <a class="yt-timestamp" data-t="00:20:51">[00:20:51]</a>, <a class="yt-timestamp" data-t="00:20:52">[00:20:52]</a>, <a class="yt-timestamp" data-t="00:20:54">[00:20:54]</a>, <a class="yt-timestamp" data-t="00:20:56">[00:20:56]</a>. This is performed on the "learned generative image manifold" of the GAN, meaning it operates in the embedding space, not directly in image space <a class="yt-timestamp" data-t="00:06:50">[00:06:50]</a>, <a class="yt-timestamp" data-t="00:06:52">[00:06:52]</a>, <a class="yt-timestamp" data-t="00:06:54">[00:06:54]</a>, <a class="yt-timestamp" data-t="00:20:40">[00:20:40]</a>, <a class="yt-timestamp" data-t="00:20:42">[00:20:42]</a>.

### Components
DragGAN consists of two main components <a class="yt-timestamp" data-t="00:06:08">[00:06:08]</a>, <a class="yt-timestamp" data-t="00:06:10">[00:06:10]</a>, <a class="yt-timestamp" data-t="01:51:50">[01:51:50]</a>:
1.  **Feature-based Motion Supervisor**: Drives the handle point towards the target position <a class="yt-timestamp" data-t="00:06:12">[00:06:12]</a>, <a class="yt-timestamp" data-t="00:06:14">[00:06:14]</a>, <a class="yt-timestamp" data-t="00:06:16">[00:06:16]</a>. This component uses a motion supervision loss to optimize the latent code and does not rely on any additional neural networks <a class="yt-timestamp" data-t="00:52:45">[00:52:45]</a>, <a class="yt-timestamp" data-t="00:52:47">[00:52:47]</a>, <a class="yt-timestamp" data-t="00:52:49">[00:52:49]</a>. The core idea is that intermediate features of the generator are discriminative enough for motion supervision <a class="yt-timestamp" data-t="00:53:39">[00:53:39]</a>, <a class="yt-timestamp" data-t="00:53:40">[00:53:40]</a>, <a class="yt-timestamp" data-t="00:53:41">[00:53:41]</a>.
2.  **Point Tracking Approach**: Leverages discriminative generator features to localize the position of the handle points <a class="yt-timestamp" data-t="00:06:18">[00:06:18]</a>, <a class="yt-timestamp" data-t="00:06:21">[00:06:21]</a>, <a class="yt-timestamp" data-t="00:06:23">[00:06:23]</a>, <a class="yt-timestamp" data-t="00:06:26">[00:06:26]</a>. This is achieved through a nearest neighbor search in the feature space <a class="yt-timestamp" data-t="01:12:29">[01:12:29]</a>, <a class="yt-timestamp" data-t="01:12:31">[01:12:31]</a>, <a class="yt-timestamp" data-t="01:12:33">[01:12:33]</a>. It also does not use additional neural networks <a class="yt-timestamp" data-t="01:12:21">[01:12:21]</a>, <a class="yt-timestamp" data-t="01:12:23">[01:12:23]</a>, <a class="yt-timestamp" data-t="01:12:25">[01:12:25]</a>, <a class="yt-timestamp" data-t="01:12:26">[01:12:26]</a>.

The process is iterative: motion supervision moves the points slightly, then point tracking updates their positions, and this repeats until the handle points reach their targets <a class="yt-timestamp" data-t="00:47:36">[00:47:36]</a>, <a class="yt-timestamp" data-t="00:47:41">[00:47:41]</a>, <a class="yt-timestamp" data-t="00:47:46">[00:47:46]</a>, <a class="yt-timestamp" data-t="00:47:48">[00:47:48]</a>, <a class="yt-timestamp" data-t="00:47:49">[00:47:49]</a>, <a class="yt-timestamp" data-t="00:47:51">[00:47:51]</a>, <a class="yt-timestamp" data-t="00:47:53">[00:47:53]</a>, <a class="yt-timestamp" data-t="00:47:55">[00:47:55]</a>, <a class="yt-timestamp" data-t="00:47:59">[00:47:59]</a>, <a class="yt-timestamp" data-t="00:48:01">[00:48:01]</a>, <a class="yt-timestamp" data-t="00:48:04">[00:48:04]</a>, <a class="yt-timestamp" data-t="00:50:40">[00:50:40]</a>, <a class="yt-timestamp" data-t="00:50:53">[00:50:53]</a>. This usually takes 20 to 200 iterations <a class="yt-timestamp" data-t="00:51:17">[00:51:17]</a>, <a class="yt-timestamp" data-t="00:51:18">[00:51:18]</a>, <a class="yt-timestamp" data-t="00:51:20">[00:51:20]</a>.

## Technical Details
DragGAN is implemented in PyTorch <a class="yt-timestamp" data-t="01:05:01">[01:05:01]</a>, <a class="yt-timestamp" data-t="01:19:05">[01:19:05]</a>, <a class="yt-timestamp" data-t="01:19:07">[01:19:07]</a> and uses the Adam Optimizer <a class="yt-timestamp" data-t="01:19:12">[01:19:12]</a>, <a class="yt-timestamp" data-t="01:19:13">[01:19:13]</a>. It is based on the StyleGAN2 architecture <a class="yt-timestamp" data-t="01:18:18">[01:18:18]</a>.
A 512-dimensional latent code (Z) is mapped to an intermediate latent code (W) which exists in the "W space" <a class="yt-timestamp" data-t="00:36:50">[00:36:50]</a>, <a class="yt-timestamp" data-t="00:36:53">[00:36:53]</a>, <a class="yt-timestamp" data-t="00:37:03">[00:37:03]</a>, <a class="yt-timestamp" data-t="00:37:04">[00:37:04]</a>, <a class="yt-timestamp" data-t="00:37:08">[00:37:08]</a>, <a class="yt-timestamp" data-t="00:37:10">[00:37:10]</a>. This W is then sent to the generator (G) to produce the output image <a class="yt-timestamp" data-t="00:38:08">[00:38:08]</a>, <a class="yt-timestamp" data-t="00:38:10">[00:38:10]</a>, <a class="yt-timestamp" data-t="00:38:12">[00:38:12]</a>, <a class="yt-timestamp" data-t="00:38:16">[00:38:16]</a>. W is copied and sent to different layers of the generator to control different levels of attributes <a class="yt-timestamp" data-t="00:40:40">[00:40:40]</a>, <a class="yt-timestamp" data-t="00:40:43">[00:40:43]</a>, <a class="yt-timestamp" data-t="00:40:44">[00:40:44]</a>, <a class="yt-timestamp" data-t="00:40:45">[00:40:45]</a>.
Using different W values for different layers creates the "W+ space," which is less constrained and more expressive, allowing for "out of distribution manipulations" <a class="yt-timestamp" data-t="00:42:05">[00:42:05]</a>, <a class="yt-timestamp" data-t="00:42:07">[00:42:07]</a>, <a class="yt-timestamp" data-t="00:42:10">[00:42:10]</a>, <a class="yt-timestamp" data-t="00:42:13">[00:42:13]</a>, <a class="yt-timestamp" data-t="00:42:20">[00:42:20]</a>, <a class="yt-timestamp" data-t="00:42:25">[00:42:25]</a>, <a class="yt-timestamp" data-t="00:42:27">[00:42:27]</a>, <a class="yt-timestamp" data-t="00:42:44">[00:42:44]</a>, <a class="yt-timestamp" data-t="00:42:47">[00:42:47]</a>, <a class="yt-timestamp" data-t="00:42:49">[00:42:49]</a>, <a class="yt-timestamp" data-t="00:42:50">[00:42:50]</a>. DragGAN uses W+ for better editability <a class="yt-timestamp" data-t="01:05:53">[01:05:53]</a>, <a class="yt-timestamp" data-t="01:05:57">[01:05:57]</a>. The first six layers of W primarily affect spatial attributes, while remaining layers affect appearance <a class="yt-timestamp" data-t="01:05:58">[01:05:58]</a>, <a class="yt-timestamp" data-t="01:05:59">[01:05:59]</a>, <a class="yt-timestamp" data-t="01:06:00">[01:06:00]</a>, <a class="yt-timestamp" data-t="01:06:01">[01:06:01]</a>, <a class="yt-timestamp" data-t="01:06:03">[01:06:03]</a>. Therefore, optimization focuses only on the first six layers to preserve appearance <a class="yt-timestamp" data-t="01:07:27">[01:07:27]</a>, <a class="yt-timestamp" data-t="01:07:28">[01:07:28]</a>, <a class="yt-timestamp" data-t="01:07:30">[01:07:30]</a>.

The datasets used for training include FFHQ (human faces, 1024x1024) <a class="yt-timestamp" data-t="01:19:22">[01:19:22]</a>, <a class="yt-timestamp" data-t="01:23:51">[01:23:51]</a>, AFHQ Cat (cats, 512x512) <a class="yt-timestamp" data-t="01:19:24">[01:19:24]</a>, <a class="yt-timestamp" data-t="01:23:51">[01:23:51]</a>, <a class="yt-timestamp" data-t="01:23:55">[01:23:55]</a>, LSUN Car (cars, 256x256) <a class="yt-timestamp" data-t="01:19:25">[01:19:25]</a>, <a class="yt-timestamp" data-t="01:23:55">[01:23:55]</a>, Landscapes HQ (landscapes, 256x256) <a class="yt-timestamp" data-t="01:24:06">[01:24:06]</a>, <a class="yt-timestamp" data-t="01:24:09">[01:24:09]</a>, Microscope (various sizes) <a class="yt-timestamp" data-t="01:24:09">[01:24:09]</a>, <a class="yt-timestamp" data-t="01:24:13">[01:24:13]</a>, and collections of lions, dogs, and elephants <a class="yt-timestamp" data-t="01:24:15">[01:24:15]</a>.

## Advantages and Comparisons

DragGAN offers notable advantages over existing approaches for [[image_synthesis_and_editing_using_gans | image manipulation]] <a class="yt-timestamp" data-t="00:08:50">[00:08:50]</a>, <a class="yt-timestamp" data-t="00:08:52">[00:08:52]</a>.

### Comparison to Previous GAN Control Methods
Existing approaches for GAN controllability often rely on manually annotated training data or prior 3D models, or latent space manipulation <a class="yt-timestamp" data-t="00:42:29">[00:42:29]</a>, <a class="yt-timestamp" data-t="00:42:31">[00:42:31]</a>, <a class="yt-timestamp" data-t="00:42:33">[00:42:33]</a>, <a class="yt-timestamp" data-t="00:42:36">[00:42:36]</a>. These methods tend to:
*   **Fail to generalize** to new object categories <a class="yt-timestamp" data-t="01:13:29">[01:13:29]</a>, <a class="yt-timestamp" data-t="01:13:31">[01:13:31]</a>.
*   **Control a limited range** of spatial attributes <a class="yt-timestamp" data-t="01:13:40">[01:13:40]</a>, <a class="yt-timestamp" data-t="01:13:43">[01:13:43]</a>.
*   **Lack precision and flexibility** in text-guided editing, unable to move objects by specific pixel counts <a class="yt-timestamp" data-t="01:14:16">[01:14:16]</a>, <a class="yt-timestamp" data-t="01:14:18">[01:14:18]</a>, <a class="yt-timestamp" data-t="01:14:19">[01:14:19]</a>, <a class="yt-timestamp" data-t="01:14:21">[01:14:21]</a>.

DragGAN, in contrast, offers flexible, precise, and generic control through its interactive point-based manipulation <a class="yt-timestamp" data-t="01:14:31">[01:14:31]</a>, <a class="yt-timestamp" data-t="01:14:33">[01:14:33]</a>, <a class="yt-timestamp" data-t="01:14:35">[01:14:35]</a>, <a class="yt-timestamp" data-t="01:14:36">[01:14:36]</a>, <a class="yt-timestamp" data-t="01:14:38">[01:14:38]</a>. It allows control of diverse spatial attributes agnostic to object categories <a class="yt-timestamp" data-t="01:14:51">[01:14:51]</a>, <a class="yt-timestamp" data-t="01:14:53">[01:14:53]</a>, <a class="yt-timestamp" data-t="01:14:55">[01:14:55]</a>.

### Comparison to User-Controllable LT
DragGAN's closest comparable approach is "User-Controllable LT" (from 2022) <a class="yt-timestamp" data-t="01:14:57">[01:14:57]</a>, <a class="yt-timestamp" data-t="01:14:59">[01:14:59]</a>, <a class="yt-timestamp" data-t="01:15:01">[01:15:01]</a>. While User-Controllable LT also featured dragging manipulation <a class="yt-timestamp" data-t="01:15:04">[01:15:04]</a>, <a class="yt-timestamp" data-t="01:15:08">[01:15:08]</a>, it generally produces less natural and less superior results <a class="yt-timestamp" data-t="01:07:46">[01:07:46]</a>, <a class="yt-timestamp" data-t="01:07:47">[01:07:47]</a>. User-Controllable LT struggles with faithfulness in moving points and often leads to undesired semantic changes (e.g., changing a dress to pants, or an ocean background to a forest) <a class="yt-timestamp" data-t="01:30:12">[01:30:12]</a>, <a class="yt-timestamp" data-t="01:30:14">[01:30:14]</a>, <a class="yt-timestamp" data-t="01:30:15">[01:30:15]</a>, <a class="yt-timestamp" data-t="01:30:16">[01:30:16]</a>. DragGAN, in contrast, maintains the overall semantic meaning of the image while allowing fine-grained spatial manipulation <a class="yt-timestamp" data-t="01:27:37">[01:27:37]</a>, <a class="yt-timestamp" data-t="01:27:39">[01:27:39]</a>, <a class="yt-timestamp" data-t="01:27:40">[01:27:40]</a>, <a class="yt-timestamp" data-t="01:27:43">[01:27:43]</a>, <a class="yt-timestamp" data-t="01:27:45">[01:27:45]</a>, <a class="yt-timestamp" data-t="01:27:46">[01:27:46]</a>, <a class="yt-timestamp" data-t="01:27:48">[01:27:48]</a>, <a class="yt-timestamp" data-t="01:27:50">[01:27:50]</a>, <a class="yt-timestamp" data-t="01:27:52">[01:27:52]</a>, <a class="yt-timestamp" data-t="01:27:54">[01:27:54]</a>, <a class="yt-timestamp" data-t="01:27:55">[01:27:55]</a>.

### Efficiency
DragGAN achieves efficient manipulation, taking only a few seconds on a single RTX 3090 GPU <a class="yt-timestamp" data-t="01:19:26">[01:19:26]</a>, <a class="yt-timestamp" data-t="01:19:28">[01:19:28]</a>. This efficiency is partly due to not relying on additional neural networks for motion supervision or point tracking <a class="yt-timestamp" data-t="01:19:22">[01:19:22]</a>, <a class="yt-timestamp" data-t="01:19:24">[01:19:24]</a>, <a class="yt-timestamp" data-t="01:19:26">[01:19:26]</a>, <a class="yt-timestamp" data-t="01:19:28">[01:19:28]</a>, <a class="yt-timestamp" data-t="00:52:51">[00:52:51]</a>, <a class="yt-timestamp" data-t="00:52:53">[00:52:53]</a>, <a class="yt-timestamp" data-t="00:52:55">[00:52:55]</a>.

### Comparison to Diffusion Models
Diffusion models, while enabling high-quality [[image_generation_using_advanced_mathematical_models | image generation]] and expressive image synthesis conditioned on text inputs <a class="yt-timestamp" data-t="01:11:11">[01:11:11]</a>, <a class="yt-timestamp" data-t="01:11:13">[01:11:13]</a>, <a class="yt-timestamp" data-t="01:11:14">[01:11:14]</a>, <a class="yt-timestamp" data-t="01:11:27">[01:11:27]</a>, <a class="yt-timestamp" data-t="01:11:28">[01:11:28]</a>, are generally slower due to requiring multiple denoising steps (iteratively removing noise) <a class="yt-timestamp" data-t="01:12:28">[01:12:28]</a>, <a class="yt-timestamp" data-t="01:12:29">[01:12:29]</a>, <a class="yt-timestamp" data-t="01:12:31">[01:12:31]</a>, <a class="yt-timestamp" data-t="01:12:33">[01:12:33]</a>, <a class="yt-timestamp" data-t="01:12:36">[01:12:36]</a>, <a class="yt-timestamp" data-t="01:12:39">[01:12:39]</a>. GANs, conversely, can generate an entire image in one step from a latent vector <a class="yt-timestamp" data-t="01:12:44">[01:12:44]</a>, <a class="yt-timestamp" data-t="01:12:46">[01:12:46]</a>, <a class="yt-timestamp" data-t="01:12:48">[01:12:48]</a>, <a class="yt-timestamp" data-t="01:12:50">[01:12:50]</a>, <a class="yt-timestamp" data-t="01:12:52">[01:12:52]</a>.
Natural language control with diffusion models also often lacks fine-grained control <a class="yt-timestamp" data-t="01:12:42">[01:12:42]</a>, <a class="yt-timestamp" data-t="01:12:44">[01:12:44]</a>.

### Comparison to Optical Flow Tracking (Pips, Raft)
Traditional point tracking methods like Optical Flow estimation (e.g., Raft and Pips) can harm efficiency and suffer from accumulation error, especially with GAN alias artifacts <a class="yt-timestamp" data-t="01:10:57">[01:10:57]</a>, <a class="yt-timestamp" data-t="01:10:58">[01:10:58]</a>, <a class="yt-timestamp" data-t="01:11:00">[01:11:00]</a>, <a class="yt-timestamp" data-t="01:11:04">[01:11:04]</a>, <a class="yt-timestamp" data-t="01:11:05">[01:11:05]</a>, <a class="yt-timestamp" data-t="01:11:07">[01:11:07]</a>, <a class="yt-timestamp" data-t="01:11:08">[01:11:08]</a>, <a class="yt-timestamp" data-t="01:11:10">[01:11:10]</a>. DragGAN's point tracking, using nearest neighbor search in GAN feature space, outperforms state-of-the-art methods like Raft and Pips despite its simplicity <a class="yt-timestamp" data-t="01:12:21">[01:12:21]</a>, <a class="yt-timestamp" data-t="01:12:23">[01:12:23]</a>, <a class="yt-timestamp" data-t="01:12:25">[01:12:25]</a>, <a class="yt-timestamp" data-t="01:12:26">[01:12:26]</a>, <a class="yt-timestamp" data-t="01:12:29">[01:12:29]</a>, <a class="yt-timestamp" data-t="01:12:31">[01:12:31]</a>, <a class="yt-timestamp" data-t="01:12:33">[01:12:33]</a>, <a class="yt-timestamp" data-t="01:35:32">[01:35:32]</a>, <a class="yt-timestamp" data-t="01:35:34">[01:35:34]</a>, <a class="yt-timestamp" data-t="01:35:35">[01:35:35]</a>, <a class="yt-timestamp" data-t="01:35:36">[01:35:36]</a>.

## Image Inversion and Masking
DragGAN can manipulate real images through [[zero_shot_image_editing_capabilities | GAN inversion]] techniques, where a real image is mapped into the GAN's latent space <a class="yt-timestamp" data-t="00:08:57">[00:08:57]</a>, <a class="yt-timestamp" data-t="00:09:01">[00:09:01]</a>, <a class="yt-timestamp" data-t="01:24:57">[01:24:57]</a>, <a class="yt-timestamp" data-t="01:24:59">[01:24:59]</a>, <a class="yt-timestamp" data-t="01:31:46">[01:31:46]</a>, <a class="yt-timestamp" data-t="01:31:48">[01:31:48]</a>.

Users can optionally define a binary mask (M) to denote a movable region, keeping the unmasked region fixed with a reconstruction loss <a class="yt-timestamp" data-t="00:45:58">[00:45:58]</a>, <a class="yt-timestamp" data-t="00:45:59">[00:45:59]</a>, <a class="yt-timestamp" data-t="00:46:02">[00:46:02]</a>, <a class="yt-timestamp" data-t="00:46:05">[00:46:05]</a>, <a class="yt-timestamp" data-t="00:46:07">[00:46:07]</a>, <a class="yt-timestamp" data-t="00:46:09">[00:46:09]</a>. However, since masking is applied in the feature map space rather than image space, unmasked regions may not remain perfectly intact <a class="yt-timestamp" data-t="01:05:06">[01:05:06]</a>, <a class="yt-timestamp" data-t="01:05:07">[01:05:07]</a>, <a class="yt-timestamp" data-t="01:05:09">[01:05:09]</a>, <a class="yt-timestamp" data-t="01:05:11">[01:05:11]</a>, <a class="yt-timestamp" data-t="01:05:13">[01:05:13]</a>, <a class="yt-timestamp" data-t="01:05:15">[01:05:15]</a>.

## Limitations
*   **Training Data Diversity**: Editing quality is still affected by the diversity of the training data. Creating human poses outside the training distribution can lead to artifacts <a class="yt-timestamp" data-t="01:50:55">[01:50:55]</a>, <a class="yt-timestamp" data-t="01:50:57">[01:50:57]</a>, <a class="yt-timestamp" data-t="01:50:59">[01:50:59]</a>, <a class="yt-timestamp" data-t="01:51:01">[01:51:01]</a>.
*   **Textureless Regions**: Handling points in textureless regions can sometimes suffer from more drift in tracking <a class="yt-timestamp" data-t="01:51:05">[01:51:05]</a>, <a class="yt-timestamp" data-t="01:51:07">[01:51:07]</a>. It is suggested to pick texture-rich handle points if possible <a class="yt-timestamp" data-t="01:51:10">[01:51:10]</a>, <a class="yt-timestamp" data-t="01:51:12">[01:51:12]</a>.

## Future Work
The researchers plan to extend [[techniques_for_personalizing_text_to_image_diffusion_models | point-based image editing]] to 3D generative models <a class="yt-timestamp" data-t="01:53:01">[01:53:01]</a>, <a class="yt-timestamp" data-t="01:53:03">[01:53:03]</a>. Other potential extensions include applying the technique to diffusion models or video manipulation <a class="yt-timestamp" data-t="01:53:35">[01:53:35]</a>, <a class="yt-timestamp" data-t="01:53:37">[01:53:37]</a>, <a class="yt-timestamp" data-t="01:53:40">[01:53:40]</a>, <a class="yt-timestamp" data-t="01:53:42">[01:53:42]</a>, <a class="yt-timestamp" data-t="01:53:45">[01:53:45]</a>. Adding regularization to the latent code could help keep images within the training distribution <a class="yt-timestamp" data-t="01:50:06">[01:50:06]</a>, <a class="yt-timestamp" data-t="01:50:07">[01:50:07]</a>, <a class="yt-timestamp" data-t="01:50:08">[01:50:08]</a>, <a class="yt-timestamp" data-t="01:50:11">[01:50:11]</a>, <a class="yt-timestamp" data-t="01:50:13">[01:50:13]</a>.