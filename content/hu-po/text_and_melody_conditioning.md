---
title: Text and melody conditioning
videoId: SodPUNBFeMY
---

From: [[hu-po]] <br/> 

MusicGen is a model developed by Meta AI Research that enables [[conditional_music_generation | conditional music generation]] based on textual descriptions and/or melodic features [00:00:39][00:00:44][00:06:43][02:10:32]. It operates over multiple streams of compressed, discrete music representations, referred to as tokens [00:04:42][00:04:47]. Unlike some prior work, MusicGen is a single-stage [[transformer_models_in_audio_generation | Transformer Language Model (LM)]] [00:05:27][00:05:28][00:06:26][01:19:20].

## Conditioning Methods

MusicGen offers flexibility in controlling the generated output by conditioning on different modalities [00:01:58][00:06:43][01:06:07].

### Text Conditioning
Text is a primary conditioning method, allowing users to describe the desired music, such as "cheerful song with acoustic guitars" [00:01:40][01:00:00]. This provides a natural language interface for controllability [01:02:59].

The model supports three main approaches for representing text:
*   **Pre-trained text encoders:** Specifically, the T5 encoder is used [01:06:53][01:08:16][01:21:19].
*   **Instruct-based language models:** Flan-T5, an instruct-based variant of T5, is also utilized, which has shown superior performance in some cases [01:07:00][01:08:18].
*   **Joint text-audio representation:** A joint text-audio representation, such as [[clap_text_to_audio_representation | CLAP]], can be used, which is claimed to provide better quality generations [01:07:05][01:08:19].

Text preprocessing techniques are also employed, including:
*   **Text normalization:** Stop words are omitted, and the remaining text is lemmatized [01:40:02][01:40:08].
*   **Concatenating annotations:** Musical key, tempo (BPM), and instrument tags can be added to the text description [01:40:19][01:41:07].
*   **Word Dropout:** A text augmentation strategy where specific words are randomly omitted [01:40:23][01:40:42].
*   **Condition merging:** Used with a probability of 0.25 [01:40:37].
*   **Text description dropout:** Applied with a probability of 0.5 [01:40:40].
*   **Guidance scale:** A parameter common in [[text_to_image_diffusion_models | text-to-image diffusion models]] that adjusts the strength of the text conditioning [01:42:42][01:43:07].

### Melody Conditioning
Beyond text, MusicGen can be conditioned on a melodic structure derived from another audio track [01:08:54][01:09:10]. This is considered a more natural approach for music creators [01:09:07].

The process involves:
1.  **Chromogram conversion:** The input melody (e.g., an MP3 file) is converted into a chromogram, which is a 2D image-like representation of audio [00:05:05][01:09:11][01:09:22].
2.  **Information bottleneck:** To make the chromogram tractable for conditioning, an "information bottleneck" is introduced by selecting the dominant time-frequency bin at each step using an ARG Max operation [01:10:27][01:10:30][01:42:02]. This simplifies the 2D chromogram into a 1D sequence [01:18:29].
3.  **Unsupervised approach:** MusicGen employs an unsupervised approach for melody conditioning, eliminating the need for costly supervised data [01:10:01][01:11:01].
4.  **Transformer input:** The processed chromogram (conditioning tensor `C`) is provided as a prefix to the Transformer input [01:17:38][01:17:53].

## Model Architecture and Training
MusicGen uses an auto-regressive [[transformer_models_in_audio_generation | Transformer-based decoder]] [01:18:53]. The language model operates on quantized units generated by an EnCodec audio tokenizer [01:13:08][01:15:30][01:16:15]. EnCodec is a convolutional autoencoder that quantizes a latent space using [[residual_vector_quantization | residual vector quantization (RVQ)]] [02:22:25][01:13:59][01:15:30][01:16:15].

The EnCodec model used in MusicGen:
*   Processes 32 kilohertz (kHz) monophonic audio [01:26:54].
*   Reduces the frame rate from 32,000 samples per second to 50 samples per second [01:28:02][01:28:13].
*   Uses four quantizers (quantization levels) for RVQ [01:28:57].
*   Has a codebook size of 2048 entries [01:29:02]. The first codebook is the most important as it contains the majority of the signal [01:36:30].

### Codebook Interleaving Patterns
A key contribution of MusicGen is its "codebook interleaving patterns" [01:13:54]. The challenge arises because the different quantization levels from RVQ are generally dependent on each other, making parallel prediction difficult [01:12:05][01:12:21][01:13:09]. MusicGen introduces a framework to model multiple parallel streams of acoustic tokens [01:05:53].

Various patterns exist, including:
*   **Flattening:** Concatenating all codebooks into a single, large vector to be predicted [00:49:50][00:50:09][02:04:03]. This offers the best scores but comes at a high computational cost, increasing the number of autoregressive steps significantly [02:00:00][02:04:10].
*   **Partial Delay:** A strategy where predictions for codebooks two, three, and four are delayed relative to the first codebook [02:00:00][02:00:02][02:03:20]. This allows for some parallel processing while maintaining a balance between speed and quality [01:03:13][01:03:41]. This pattern results in fewer autoregressive steps compared to flattening [02:00:29].
*   **Parallel Pattern:** Predicts all codebooks at the same time step in parallel [02:03:30].
*   **Valley Pattern:** Predicts the first codebook for all time steps sequentially, then predicts the remaining codebooks (two, three, and four) in parallel [02:03:41].

These patterns aim to save computational time during inference while maintaining quality [01:20:00][01:20:55].

## Evaluation and Findings
MusicGen is evaluated using both objective metrics and human studies [01:48:38].
*   **Objective Metrics:** Include Frechet Audio Distance (FAD), KL Divergence (measuring similarity between audio classification model probability distributions), and CLAP score (quantifying audio-text alignment) [01:48:42][01:50:08][01:51:56].
*   **Human Studies:** Raiders from Amazon Mechanical Turk evaluate the "overall quality" (OVL) and "relevance to the text input" (Rel) of generated samples on a scale of 1 to 100 [01:52:33][01:52:40][01:53:33].

Key findings from the evaluations:
*   **Performance:** MusicGen performs better than re-implemented baselines like Riffusion and MoosAI [01:57:01].
*   **Melody Conditioning Impact:** Adding melody conditioning tends to degrade objective metrics but does not significantly affect human ratings [01:58:30][01:58:36]. Human evaluators found that MusicGen trained with chromogram conditioning successfully generates music that follows a given melody [01:59:37][01:59:40].
*   **Model Size:** Scaling the model size (e.g., from 300 million to 3.3 billion parameters) results in slightly better objective scores, but humans barely perceive a difference in quality [02:01:15][02:01:20][02:01:27][02:01:40]. This suggests that a 300-million-parameter model may be sufficient for most use cases [02:01:55][02:26:45].
*   **Text Encoder Importance:** The choice of text encoder (T5, Flan-T5, CLAP) significantly impacts model performance, sometimes more so than the complex codebook patterns [02:15:28][02:16:02].
*   **Data:** MusicGen was trained on 20,000 hours of licensed music from internal datasets and Shutterstock/Pond5 music data [01:43:16][01:43:21][01:43:38].