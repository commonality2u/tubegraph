---
title: Introduction to ControlNet
videoId: Mp-HMQcB_M4
---

From: [[hu-po]] <br/> 

ControlNet is a neural network structure designed to enhance [[Diffusion Models and ControlNet | pre-trained large diffusion models]] by supporting additional input conditions <a class="yt-timestamp" data-t="00:55:55">[00:55:55]</a> <a class="yt-timestamp" data-t="00:57:57">[00:57:57]</a> <a class="yt-timestamp" data-t="01:08:51">[01:08:51]</a>. It has rapidly gained popularity, "taking the Internet by storm" shortly after its release in February 2023 <a class="yt-timestamp" data-t="01:16:16">[01:16:16]</a> <a class="yt-timestamp" data-t="01:21:21">[01:21:21]</a>. The project's GitHub repository quickly accumulated close to 7,000 stars within weeks of its launch <a class="yt-timestamp" data-t="01:32:32">[01:32:32]</a> <a class="yt-timestamp" data-t="01:41:41">[01:41:41]</a>. It was primarily developed by LV Min Jang and Manish Agrawala from Stanford <a class="yt-timestamp" data-t="04:41:41">[04:41:41]</a> <a class="yt-timestamp" data-t="04:44:44">[04:44:44]</a>.

## Core Functionality

[[Applications of ControlNet in Image Generation | ControlNet]] allows for fine-grained control over [[Diffusion Models and ControlNet | diffusion models]], which were previously useful for text-to-image generation but lacked precise control over the output <a class="yt-timestamp" data-t="02:08:08">[02:08:08]</a> <a class="yt-timestamp" data-t="02:27:27">[02:27:27]</a> <a class="yt-timestamp" data-t="02:30:30">[02:30:30]</a>. With ControlNet, users can specify elements like edges or poses to achieve a much more exact desired image <a class="yt-timestamp" data-t="02:32:32">[02:32:32]</a> <a class="yt-timestamp" data-t="02:37:37">[02:37:37]</a> <a class="yt-timestamp" data-t="03:31:31">[03:31:31]</a>. This capability has "opened the floodgates for all different types of content creation" <a class="yt-timestamp" data-t="03:15:15">[03:15:15]</a> <a class="yt-timestamp" data-t="03:18:18">[03:18:18]</a>.

### Limitations of Prompt-Based Control
While prompt-based control was a significant advancement, it often doesn't satisfy the need for precise image generation, especially for applications requiring specific image structures <a class="yt-timestamp" data-t="07:36:36">[07:36:36]</a> <a class="yt-timestamp" data-t="07:56:56">[07:56:56]</a> <a class="yt-timestamp" data-t="08:08:08">[08:08:08]</a>. ControlNet addresses this by enabling more fine-tuned control over [[Applications of ControlNet in Image Generation | image generation models]] <a class="yt-timestamp" data-t="08:13:13">[08:13:13]</a> <a class="yt-timestamp" data-t="08:38:38">[08:38:38]</a>.

## [[Technical Aspects of ControlNet Implementation | Technical Aspects of ControlNet Implementation]]

ControlNet operates by cloning the weights of a [[Diffusion Models and ControlNet | large diffusion model]] into two copies: a **trainable copy** and a **locked copy** <a class="yt-timestamp" data-t="07:09:09">[07:09:09]</a> <a class="yt-timestamp" data-t="07:11:11">[07:11:11]</a> <a class="yt-timestamp" data-t="04:54:54">[04:54:54]</a>.
*   The **locked copy** preserves the network's capabilities learned from billions of images <a class="yt-timestamp" data-t="01:18:18">[01:18:18]</a> <a class="yt-timestamp" data-t="01:19:19">[01:19:19]</a> <a class="yt-timestamp" data-t="04:50:50">[04:50:50]</a>.
*   The **trainable copy** is specifically trained on task-specific datasets to learn the conditional control <a class="yt-timestamp" data-t="01:22:22">[01:22:22]</a> <a class="yt-timestamp" data-t="01:25:25">[01:25:25]</a> <a class="yt-timestamp" data-t="04:50:50">[04:50:50]</a>.

### Zero Convolution
The trainable and locked neural network blocks are connected using a unique type of convolutional layer called **zero convolution** <a class="yt-timestamp" data-t="01:44:44">[01:44:44]</a> <a class="yt-timestamp" data-t="01:47:47">[01:47:47]</a> <a class="yt-timestamp" data-t="01:49:49">[01:49:49]</a>. These are 1x1 convolutional layers with both weights and biases initialized to zero <a class="yt-timestamp" data-t="01:53:53">[01:53:53]</a> <a class="yt-timestamp" data-t="02:51:51">[02:51:51]</a> <a class="yt-timestamp" data-t="02:56:56">[02:56:56]</a>.
*   **Initialization**: Because all weights and biases start at zero, in the first training step, the ControlNet causes no influence on the deep network features, meaning the original model's quality is perfectly preserved <a class="yt-timestamp" data-t="05:06:06">[05:06:06]</a> <a class="yt-timestamp" data-t="05:10:10">[05:10:10]</a> <a class="yt-timestamp" data-t="05:56:56">[05:56:56]</a>.
*   **Training**: Zero convolutions progressively grow from zeros to optimized parameters in a learned manner <a class="yt-timestamp" data-t="01:53:53">[01:53:53]</a> <a class="yt-timestamp" data-t="08:40:40">[08:40:40]</a>. This allows training to be as fast as fine-tuning a diffusion model, as it doesn't add new noise to deep features, unlike training layers from scratch <a class="yt-timestamp" data-t="01:13:13">[01:13:13]</a> <a class="yt-timestamp" data-t="01:17:17">[01:17:17]</a> <a class="yt-timestamp" data-t="01:21:21">[01:21:21]</a>.

### Architecture with Stable Diffusion
ControlNet is typically applied to a U-Net architecture, such as that used in [[Diffusion Models and ControlNet | Stable Diffusion]] <a class="yt-timestamp" data-t="01:12:12">[01:12:12]</a> <a class="yt-timestamp" data-t="01:13:13">[01:13:13]</a>.
*   **Inputs**: The standard [[Diffusion Models and ControlNet | Stable Diffusion]] U-Net is conditioned on time (diffusion step) and a text prompt (processed by a CLIP text encoder) <a class="yt-timestamp" data-t="01:03:54">[01:03:54]</a> <a class="yt-timestamp" data-t="01:04:04">[01:04:04]</a> <a class="yt-timestamp" data-t="01:05:25">[01:05:25]</a>.
*   **ControlNet Integration**: ControlNet parallels the U-Net structure. Its trainable copy receives the input condition (e.g., an edge map) which is first processed by a small convolutional network (named 'e') to convert it into the same latent space as the Stable Diffusion model's internal features (e.g., 512x512 images converted to 64x64 latent images) <a class="yt-timestamp" data-t="01:11:51">[01:11:51]</a> <a class="yt-timestamp" data-t="01:12:12">[01:12:12]</a> <a class="yt-timestamp" data-t="01:15:52">[01:15:52]</a>.
*   **Connections**: The output of ControlNet's trainable blocks (via zero convolutions) is added to the corresponding blocks in the locked Stable Diffusion model, particularly to the skip connections between the encoder and decoder parts of the U-Net <a class="yt-timestamp" data-t="01:13:00">[01:13:00]</a> <a class="yt-timestamp" data-t="01:13:45">[01:13:45]</a> <a class="yt-timestamp" data-t="01:14:15">[01:14:15]</a>.

## [[Technical Aspects of ControlNet Implementation | Training Strategies]]

Training ControlNet is efficient, requiring only about 23% more GPU memory and 34% more time per session compared to training a [[Diffusion Models and ControlNet | Stable Diffusion model]] alone <a class="yt-timestamp" data-t="01:17:37">[01:17:37]</a> <a class="yt-timestamp" data-t="01:17:40">[01:17:40]</a>.

### Small-Scale Training
For limited computation devices (e.g., consumer GPUs like Nvidia RTX 3070), partially breaking connections between the ControlNet and Stable Diffusion can accelerate convergence. This involves disconnecting links to some decoder blocks and only connecting the middle block initially, improving speed by a factor of 1.6 <a class="yt-timestamp" data-t="01:24:42">[01:24:42]</a> <a class="yt-timestamp" data-t="01:25:01">[01:25:01]</a> <a class="yt-timestamp" data-t="01:25:06">[01:25:06]</a>. Once the model shows reasonable association, these links can be reconnected for continued training to facilitate accurate control <a class="yt-timestamp" data-t="01:25:12">[01:25:12]</a> <a class="yt-timestamp" data-t="01:25:16">[01:25:16]</a>.

### Large-Scale Training
With powerful computation clusters (e.g., Nvidia A100s with 80GB memory) and large datasets (millions of training examples), a two-part training process is employed:
1.  Initially, the [[Diffusion Models and ControlNet | Stable Diffusion model]] weights are locked, and only the ControlNet is trained <a class="yt-timestamp" data-t="01:26:30">[01:26:30]</a> <a class="yt-timestamp" data-t="01:26:38">[01:26:38]</a>.
2.  After a sufficient number of iterations (e.g., 50,000 steps), all weights of the Stable Diffusion model are unlocked, and both models are jointly trained <a class="yt-timestamp" data-t="01:26:20">[01:26:20]</a> <a class="yt-timestamp" data-t="01:26:40">[01:26:40]</a> <a class="yt-timestamp" data-t="01:26:44">[01:26:44]</a>.

## [[Applications of ControlNet in Image Generation | Conditional Inputs and Applications]]

ControlNet can handle diverse forms of problem definitions and user controls by taking various image-based conditions as input <a class="yt-timestamp" data-t="01:31:31">[01:31:31]</a> <a class="yt-timestamp" data-t="01:32:32">[01:32:32]</a> <a class="yt-timestamp" data-t="01:42:42">[01:42:42]</a>.

### Examples of Conditional Inputs:
*   **Edge Maps**:
    *   **Canny Edge Maps**: Generated from images using the Canny Edge detector (developed in 1986) <a class="yt-timestamp" data-t="01:27:27">[01:27:27]</a> <a class="yt-timestamp" data-t="01:29:04">[01:29:04]</a>. Used to create 3 million edge-to-image caption pairs, often with random thresholds to diversify the dataset <a class="yt-timestamp" data-t="01:28:12">[01:28:12]</a> <a class="yt-timestamp" data-t="01:28:59">[01:28:59]</a>.
    *   **Hough Lines**: Uses the Hough Transform (developed in 1962/1972) to detect straight lines <a class="yt-timestamp" data-t="02:29:29">[02:29:29]</a> <a class="yt-timestamp" data-t="02:33:33">[02:33:33]</a> <a class="yt-timestamp" data-t="02:56:56">[02:56:56]</a>.
    *   **Holistically-Nested Edge Detection (HED)**: Another type of edge detector <a class="yt-timestamp" data-t="03:11:11">[03:11:11]</a> <a class="yt-timestamp" data-t="03:16:16">[03:16:16]</a>.
*   **User Scribbles**: Synthesized from HED boundary detection combined with strong data augmentations (e.g., random masking, morphological transformations) to simulate human drawings <a class="yt-timestamp" data-t="03:45:45">[03:45:45]</a> <a class="yt-timestamp" data-t="03:47:47">[03:47:47]</a>.
*   **Human Keypoints/Pose**: Learning-based pose estimation models identify human poses in images, creating datasets of pose-image caption pairs (e.g., 80K pairs) <a class="yt-timestamp" data-t="03:26:26">[03:26:26]</a> <a class="yt-timestamp" data-t="03:29:29">[03:29:29]</a> <a class="yt-timestamp" data-t="03:33:33">[03:33:33]</a>.
*   **Segmentation Maps**: Models take 2D images and determine class labels for every part of the image (e.g., road, car, building) <a class="yt-timestamp" data-t="03:50:50">[03:50:50]</a> <a class="yt-timestamp" data-t="03:54:54">[03:54:54]</a>. Common datasets include COCO (polygon-based) and AD20K (pixel-based) <a class="yt-timestamp" data-t="03:57:57">[03:57:57]</a> <a class="yt-timestamp" data-t="04:00:00">[04:00:00]</a>.
*   **Depth Maps**: Generated using monocular depth estimation models like MiDaS, which produce a depth image from a single 2D image <a class="yt-timestamp" data-t="04:02:02">[04:02:02]</a> <a class="yt-timestamp" data-t="04:11:11">[04:11:11]</a> <a class="yt-timestamp" data-t="04:15:15">[04:15:15]</a>.
*   **Normal Maps**: Indicate the orientation of surfaces in an image <a class="yt-timestamp" data-t="04:09:09">[04:09:09]</a> <a class="yt-timestamp" data-t="04:39:39">[04:39:39]</a>. These can be obtained from explicit datasets or approximated from depth maps <a class="yt-timestamp" data-t="04:40:40">[04:40:40]</a>.
*   **Cartoon Line Drawing**: Utilizes models to extract lines specifically from cartoon illustrations <a class="yt-timestamp" data-t="04:54:54">[04:54:54]</a> <a class="yt-timestamp" data-t="04:56:56">[04:56:56]</a>.

## [[Comparison of ControlNet with Other Techniques | Comparison with Other Techniques]]

ControlNet builds upon and differs from previous approaches:
*   **HyperNetworks**: ControlNet shares similarities with HyperNetworks, which originate from [[Metas Codellama Model Overview | neural language processing]] to influence the weights of larger networks <a class="yt-timestamp" data-t="02:22:50">[02:22:50]</a> <a class="yt-timestamp" data-t="02:23:03">[02:23:03]</a> <a class="yt-timestamp" data-t="02:33:33">[02:33:33]</a>. They have also been applied to image generation using Generative Adversarial Networks (GANs) and to change the artistic style of [[Diffusion Models and ControlNet | Stable Diffusion]] outputs <a class="yt-timestamp" data-t="02:50:50">[02:50:50]</a> <a class="yt-timestamp" data-t="02:53:53">[02:53:53]</a> <a class="yt-timestamp" data-t="02:57:57">[02:57:57]</a>.
*   **Image-to-Image Translation**: While ControlNet can appear to perform image-to-image translation, its motivation is distinct. Image-to-image translation aims to learn a mapping between image domains, whereas ControlNet focuses on controlling a [[Diffusion Models and ControlNet | diffusion model]] with task-specific conditions <a class="yt-timestamp" data-t="03:52:52">[03:52:52]</a> <a class="yt-timestamp" data-t="03:57:57">[03:57:57]</a> <a class="yt-timestamp" data-t="04:06:06">[04:06:06]</a>.

## Impact and Future

ControlNet is expected to be widely adopted in [[Diffusion Models and ControlNet | Stable Diffusion]] products and generative art tools <a class="yt-timestamp" data-t="05:07:07">[05:07:07]</a> <a class="yt-timestamp" data-t="05:14:14">[05:14:14]</a>. Its relatively simple concept—a "parasite shadow model" that influences a diffusion model through zero-initialized connections—makes it versatile. The ability to use any input as long as its dimensionality matches the internal features suggests the potential for combining multiple conditions and ControlNets in the future <a class="yt-timestamp" data-t="05:23:23">[05:23:23]</a> <a class="yt-timestamp" data-t="05:41:41">[05:41:41]</a> <a class="yt-timestamp" data-t="05:47:47">[05:47:47]</a> <a class="yt-timestamp" data-t="05:51:51">[05:51:51]</a>.