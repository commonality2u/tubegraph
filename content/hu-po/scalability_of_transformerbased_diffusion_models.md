---
title: Scalability of Transformerbased diffusion models
videoId: eTBG17LANcI
---

From: [[hu-po]] <br/> 

[[diffusion_models_and_transformers | Diffusion Transformers (DiTs)]], a novel class of [[diffusion_models_and_transformers | diffusion models]] based on the Transformer architecture, have demonstrated state-of-the-art image quality. Released on December 19th by researchers from UC Berkeley, New York University, and primarily funded by Meta AI (Fair team), this work suggests a significant shift in the architectural backbone for [[diffusion_models_and_transformers | diffusion models]] <a class="yt-timestamp" data-t="00:01:14">[00:01:14]</a>.

## Core Architecture and Innovation

Traditionally, [[diffusion_models_and_transformers | diffusion models]] have relied on the UNet architecture as their backbone <a class="yt-timestamp" data-t="00:07:28">[00:07:28]</a>. The key innovation of DiTs is the replacement of this convolutional UNet backbone with a Transformer that operates on latent patches <a class="yt-timestamp" data-t="00:05:08">[00:05:08]</a>. This approach aims to leverage the remarkable [[scaling_and_optimization_in_diffusion_models | scaling properties of Transformers]] observed in other domains like NLP <a class="yt-timestamp" data-t="00:14:51">[00:14:51]</a>.

The DiT architecture is based on [[diffusion_models_and_transformers | Vision Transformers (ViTs)]], which process images by breaking them into smaller patches and treating each patch as a token in a sequence <a class="yt-timestamp" data-t="00:09:57">[00:09:57]</a>. In DiTs, this process occurs in the latent space, meaning the diffusion process happens on lower-dimensional latent representations of the images <a class="yt-timestamp" data-t="00:05:14">[00:05:14]</a> <a class="yt-timestamp" data-t="00:11:04">[00:11:04]</a>. This makes the computation less prohibitive compared to operating directly in high-resolution pixel space <a class="yt-timestamp" data-t="00:27:28">[00:27:28]</a>.

### Key Architectural Features:
*   **Latent Patch Processing:** Images (e.g., 256x256x3) are converted to a latent representation (e.g., 32x32x4) by a VAE encoder, and then 'patchified' into smaller patches. For instance, a patch factor of P=2 (meaning 2x2 patches per dimension) applied to a 32x32 latent would result in 1064 tokens <a class="yt-timestamp" data-t="01:02:30">[01:02:30]</a> <a class="yt-timestamp" data-t="00:43:00">[00:43:00]</a>.
*   **Positional Embeddings:** Similar to ViTs, DiTs use frequency-based positional embeddings to inform the model about the original location of each patch <a class="yt-timestamp" data-t="00:29:42">[00:29:42]</a>.
*   **Conditional Information:** DiTs can incorporate additional conditional information such as the noise time step, class labels, or natural language prompts (relevant for [[text_to_image_diffusion_models | text-to-image diffusion models]]). These are added as additional tokens or via a multi-head cross-attention layer <a class="yt-timestamp" data-t="00:32:50">[00:32:50]</a> <a class="yt-timestamp" data-t="00:33:41">[00:33:41]</a>.
*   **Adaptive Normalization (AdaLN-Zero):** To ensure stable training, especially in deep Transformer architectures, DiTs utilize adaptive layer normalization layers. These layers normalize activations within a layer to keep numbers well-distributed and centered, allowing gradients to flow better <a class="yt-timestamp" data-t="00:34:06">[00:34:06]</a>. This is particularly crucial for larger and deeper networks <a class="yt-timestamp" data-t="00:40:08">[00:40:08]</a>.
*   **Residual Connections:** The architecture incorporates residual connections, an established technique to facilitate information flow between different abstraction layers of the neural network <a class="yt-timestamp" data-t="00:37:12">[00:37:12]</a>.

## Scalability and Performance

[[scaling_and_optimization_in_diffusion_models | DiTs]] were analyzed through the lens of forward pass complexity, measured in GFLOPs (giga floating-point operations) <a class="yt-timestamp" data-t="00:05:29">[00:05:29]</a>. A consistent finding is that DiTs with higher GFLOPs (achieved through increased Transformer depth, width, or number of input tokens from smaller patches) consistently lead to lower FID (Frechet Inception Distance) scores, indicating better image quality <a class="yt-timestamp" data-t="00:05:39">[00:05:39]</a> <a class="yt-timestamp" data-t="00:59:52">[00:59:52]</a>.

The paper presents various DiT models, from "Small" to "XL2", differing in their number of Transformer layers, multi-attention heads, and hidden dimensions <a class="yt-timestamp" data-t="00:41:50">[00:41:50]</a>. For example, DiT-Small has 12 Transformer layers, 6 heads, and a hidden dimension of 384, while DiT-XL2 boasts 28 layers, 16 heads, and a width of 1152 <a class="yt-timestamp" data-t="00:42:01">[00:42:01]</a>. The largest model, DiT-XL2, achieved a state-of-the-art FID of 2.27 on the 256x256 ImageNet generation benchmark <a class="yt-timestamp" data-t="00:11:24">[00:11:24]</a>.

Notably, DiT-XL2 outperforms prior UNet-based [[diffusion_models_and_transformers | diffusion models]] like ADM, even though ADM is a significantly larger model in terms of GFLOPs <a class="yt-timestamp" data-t="00:12:45">[00:12:45]</a>. This highlights that architectural choices can be more impactful than simply scaling up existing designs <a class="yt-timestamp" data-t="00:13:38">[00:13:38]</a>.

## Training Regimen

The training for these models is computationally intensive. The DiT models were implemented in Jax and trained on Google TPU V3 pods <a class="yt-timestamp" data-t="00:51:15">[00:51:15]</a>. The largest model (DiT-XL2) achieved 5.7 iterations per second on a 256-TPU V3 pod <a class="yt-timestamp" data-t="00:52:46">[00:52:46]</a>. A rough estimate for training a single model to 400,000 steps on such a pod indicates a cost of approximately $10,000 <a class="yt-timestamp" data-t="00:53:57">[00:53:57]</a> <a class="yt-timestamp" data-t="00:55:10">[00:55:10]</a>. For multiple models and extended training, this cost can easily exceed $100,000 <a class="yt-timestamp" data-t="00:55:31">[00:55:31]</a>.

Despite the complexity, the training process used surprisingly simple hyperparameters: a constant learning rate of 1e-4 and no explicit weight decay (though an exponential moving average of weights was maintained) <a class="yt-timestamp" data-t="00:45:05">[00:45:05]</a> <a class="yt-timestamp" data-t="00:47:26">[00:47:26]</a>. The VAE model used for latent space conversion was an off-the-shelf, pre-trained model from [[text_to_image_diffusion_models | Stable Diffusion]] <a class="yt-timestamp" data-t="00:49:03">[00:49:03]</a>. The models were evaluated with 250 sampling steps, a balance between speed and quality <a class="yt-timestamp" data-t="00:50:20">[00:50:20]</a>.

## Implications and Future Outlook

The results demonstrate that the conventional UNet bias is not essential for high-performance [[diffusion_models_and_transformers | diffusion models]], and standard Transformer designs are highly effective replacements <a class="yt-timestamp" data-t="00:09:09">[00:09:09]</a>. This architectural unification allows [[diffusion_models_and_transformers | diffusion models]] to benefit from the extensive research and [[scaling_and_optimization_in_diffusion_models | scaling properties of Transformers]] <a class="yt-timestamp" data-t="00:09:19">[00:09:19]</a>.

Despite the impressive image quality, some semantic inconsistencies remain in the generated images, where high-frequency details (texture) are accurate, but macro-scale semantic information (e.g., animal anatomy, object structure) can still exhibit "weirdness" <a class="yt-timestamp" data-t="01:11:11">[01:11:11]</a>. However, the speed of improvement in [[diffusion_models_and_transformers | diffusion models]] is accelerating rapidly <a class="yt-timestamp" data-t="01:14:31">[01:14:31]</a>. Researchers anticipate that continued [[scaling_and_optimization_in_diffusion_models | scaling and optimization]] of DiTs could lead to capabilities like generating 4K video from text prompts in the near future <a class="yt-timestamp" data-t="01:14:56">[01:14:56]</a>. This would fundamentally change content creation and interaction with AI <a class="yt-timestamp" data-t="01:15:09">[01:15:09]</a>.