---
title: test time scaling in machine learning
videoId: Xk8FtcSlFxs
---

From: [[hu-po]] <br/> 

Test time scaling refers to the practice of increasing the amount of computational effort spent during the inference or "test time" phase of a machine learning model to enhance the accuracy of its final output <a class="yt-timestamp" data-t="02:26:00">[02:26:00]</a>, <a class="yt-timestamp" data-t="03:37:00">[03:37:00]</a>. This approach leverages additional compute at the point of prediction to improve performance, rather than solely relying on increased compute during the model's pre-training phase <a class="yt-timestamp" data-t="09:51:00">[09:51:00]</a>.

## Contrasting with Pre-training Scaling

Historically, performance improvements in [[large_language_models_llms_and_scaling | LLMs]] were largely attributed to pre-training [[large_language_models_llms_and_scaling | scaling]] laws, where increasing compute, data, and model size led to smarter models <a class="yt-timestamp" data-t="07:07:00">[07:07:00]</a>, <a class="yt-timestamp" data-t="07:17:00">[07:17:00]</a>, <a class="yt-timestamp" data-t="14:46:00">[14:46:00]</a>. This requires significant capital expenditure (capex) on large GPU clusters and data centers <a class="yt-timestamp" data-t="09:40:00">[09:40:00]</a>, <a class="yt-timestamp" data-t="10:07:00">[10:07:00]</a>.

In contrast, test time compute is generally easier and more cost-effective to [[large_language_models_llms_and_scaling | scale]] because it occurs at the edge, often on a user's device like a cell phone or a robot <a class="yt-timestamp" data-t="10:18:00">[10:18:00]</a>, <a class="yt-timestamp" data-t="10:29:00">[10:29:00]</a>, <a class="yt-timestamp" data-t="10:58:00">[10:58:00]</a>. This suggests that allocating compute at test time can yield a greater "bank for your buck" compared to pre-training <a class="yt-timestamp" data-t="09:27:00">[09:27:00]</a>.

## Methods of Test Time Scaling

Test time scaling often involves increasing the "reasoning trace" or the number of tokens a model generates internally before providing a final answer <a class="yt-timestamp" data-t="03:59:00">[03:59:00]</a>, <a class="yt-timestamp" data-t="04:43:00">[04:43:00]</a>.

### Simple Test Time Scaling

*   **Ignoring End of Thinking Tokens:** One method involves modifying special "think tokens" (e.g., in DeepSeek models) to force the model to continue reasoning beyond its typical stopping point. By simply replacing the end-of-thinking token with an extra token weight, models can be compelled to continue their reasoning process, leading to significant performance boosts on tasks like math and [[setting_up_a_coding_environment_for_machine_learning | coding]] <a class="yt-timestamp" data-t="04:07:00">[04:07:00]</a>, <a class="yt-timestamp" data-t="04:22:00">[04:22:00]</a>. There is a clear relationship: increasing thinking time (in tokens) correlates directly with accuracy <a class="yt-timestamp" data-t="04:48:00">[04:48:00]</a>, <a class="yt-timestamp" data-t="04:50:00">[04:50:00]</a>.
*   **"Let's Think Step-by-Step" Prompting:** This is analogous to earlier findings with [[large_language_models_llms_and_scaling | LLMs]] where prompts like "let's think step by step" or offering a monetary reward (e.g., "$200 if you get this answer correct") could magically improve performance by encouraging more elaborate reasoning <a class="yt-timestamp" data-t="05:02:00">[05:02:00]</a>, <a class="yt-timestamp" data-t="05:09:00">[05:09:00]</a>.

### Test Time Strategies (Advanced)

Beyond simply extending the reasoning trace, more sophisticated "test time strategies" involve exploring and filtering potential solutions generated by the model. These are sometimes referred to as TTS, though the term can be confused with "test time scaling" <a class="yt-timestamp" data-t="07:46:00">[07:46:00]</a>.

*   **Sampling and Reranking:** OpenAI's approach for competitive programming involves dividing problems into subtasks, sampling thousands of solutions for each, and then employing clustering and reranking to select the best solution <a class="yt-timestamp" data-t="08:00:00">[08:00:00]</a>, <a class="yt-timestamp" data-t="08:09:00">[08:09:00]</a>. This means a model might try to solve the same problem 10,000 times to find a correct answer <a class="yt-timestamp" data-t="08:24:00">[08:24:00]</a>.
*   **Best of N:** A policy model generates *n* responses, and then a scoring or voting method (like a process reward model, PRM) is used to select the best one <a class="yt-timestamp" data-t="15:58:00">[15:58:00]</a>, <a class="yt-timestamp" data-t="16:08:00">[16:08:00]</a>.
*   **Beam Search:** A more advanced variant where the model looks ahead multiple steps in its generation process, considering several high-probability paths simultaneously <a class="yt-timestamp" data-t="16:27:00">[16:27:00]</a>. The "beam width" determines how many paths are considered, and the "depth" indicates how far ahead the search goes <a class="yt-timestamp" data-t="16:44:00">[16:44:00]</a>, <a class="yt-timestamp" data-t="16:51:00">[16:51:00]</a>.
*   **Diverse Verifier Tree Search (DVTS):** An even fancier variant of beam search <a class="yt-timestamp" data-t="17:18:00">[17:18:00]</a>.

## Effectiveness of Test Time Scaling

*   **Model Size Impact:** Test time scaling has a more substantial impact on models with weaker reasoning abilities (smaller models) <a class="yt-timestamp" data-t="18:36:00">[18:36:00]</a>. For example, a 1.5B parameter model can see a massive performance gain (e.g., from 31% to 76%) with compute-optimal TTS <a class="yt-timestamp" data-t="19:51:00">[19:51:00]</a>, <a class="yt-timestamp" data-t="19:58:00">[19:58:00]</a>. However, for larger models (e.g., 72B parameters), the gain is limited (e.g., from 83% to 91%) <a class="yt-timestamp" data-t="20:09:00">[20:09:00]</a>, <a class="yt-timestamp" data-t="20:21:00">[20:21:00]</a>.
*   **Problem Dependence:** The optimal TTS method can vary depending on the difficulty of the problem. Best of N might be better for easy problems, while beam search excels at harder ones <a class="yt-timestamp" data-t="24:49:00">[24:49:00]</a>, <a class="yt-timestamp" data-t="25:10:00">[25:10:00]</a>. This suggests that these methods might not represent universal scaling laws and could lead to over-engineering <a class="yt-timestamp" data-t="25:52:00">[25:52:00]</a>, <a class="yt-timestamp" data-t="26:01:00">[26:01:00]</a>.

## Role of Reinforcement Learning (RL) and Distillation

*   **Internalizing Search Strategies:** When larger models are trained with [[second_order_optimization_methods_in_machine_learning | RL]] (e.g., DeepSeek models using RL), they appear to internalize or "bake in" the ability to select correct reasoning paths <a class="yt-timestamp" data-t="20:50:00">[20:50:00]</a>, <a class="yt-timestamp" data-t="20:54:00">[20:54:00]</a>, <a class="yt-timestamp" data-t="22:55:00">[22:55:00]</a>. This reduces the need for complicated, human-engineered test time strategies like diverse verifier tree search <a class="yt-timestamp" data-t="22:12:00">[22:12:00]</a>, <a class="yt-timestamp" data-t="22:20:00">[22:20:00]</a>, <a class="yt-timestamp" data-t="22:57:00">[22:57:00]</a>. This trend simplifies the overall inference pipeline <a class="yt-timestamp" data-t="23:58:00">[23:58:00]</a>.
*   **[[quantization_in_machine_learning_models | Distillation]]:** Knowledge [[quantization_in_machine_learning_models | distillation]] is a powerful tool to transfer intelligence from large, powerful "teacher" models (potentially trained with [[second_order_optimization_methods_in_machine_learning | RL]] in giant data centers) into smaller, more efficient "student" models that can run on edge devices like cell phones or Raspberry Pis <a class="yt-timestamp" data-t="13:31:00">[13:31:00]</a>, <a class="yt-timestamp" data-t="15:01:00">[15:01:01]</a>, <a class="yt-timestamp" data-t="31:30:00">[31:30:00]</a>, <a class="yt-timestamp" data-t="32:20:00">[32:20:00]</a>.
    *   **Architecture Agnostic:** A key advantage of [[quantization_techniques_in_machine_learning | distillation]] is its architecture agnosticism; the teacher and student models can have completely different architectures <a class="yt-timestamp" data-t="31:46:00">[31:46:00]</a>, <a class="yt-timestamp" data-t="32:16:00">[32:16:00]</a>.
    *   **Pipeline Distillation:** Instead of distilling from a single large model, it's possible to distill an entire complex pipeline (e.g., a multimodal RAG pipeline involving multiple models and databases) into a single, smaller, optimized model <a class="yt-timestamp" data-t="36:21:00">[36:21:00]</a>, <a class="yt-timestamp" data-t="39:11:00">[39:11:00]</a>. This suggests a future where giant companies train large, complex models with [[second_order_optimization_methods_in_machine_learning | RL]] and then [[quantization_in_machine_learning_models | distill]] their capabilities into consumer-facing models <a class="yt-timestamp" data-t="30:38:00">[30:38:00]</a>.

## [[selfimproving_machine_learning_models | Self-Improving Machine Learning Models]]

The concept of "Transcendence" suggests that a student model can generalize slightly beyond the difficulty of the data provided by its teacher during training <a class="yt-timestamp" data-t="45:52:00">[45:52:00]</a>, <a class="yt-timestamp" data-t="46:03:00">[46:03:00]</a>. This is leveraged in self-improvement frameworks where models generate their own training data <a class="yt-timestamp" data-t="46:07:00">[46:07:00]</a>, <a class="yt-timestamp" data-t="49:09:00">[49:09:00]</a>.

*   **Data Filtering:** For self-improvement to work, filtering the self-generated [[loading_and_preparing_datasets_for_machine_learning_tasks | data set]] is crucial <a class="yt-timestamp" data-t="44:49:00">[44:49:00]</a>, <a class="yt-timestamp" data-t="53:30:00">[53:30:00]</a>. Without filtering, self-generated data degrades over successive rounds, leading to a collapse in the self-improvement process <a class="yt-timestamp" data-t="53:31:00">[53:31:00]</a>.
    *   **Length Filtering:** Removing outputs shorter than a predefined threshold <a class="yt-timestamp" data-t="51:01:00">[51:01:00]</a>.
    *   **Majority Voting:** Filtering data based on consensus among predictions from multiple models <a class="yt-timestamp" data-t="51:07:00">[51:07:00]</a>. This is akin to peer review in the scientific method, where quality is maintained by collective agreement <a class="yt-timestamp" data-t="51:22:00">[51:22:00]</a>.
*   **Implications for Subjective Domains:** The effectiveness of majority voting suggests that [[selfimproving_machine_learning_models | self-improving machine learning models]] might not require external reward signals from the environment <a class="yt-timestamp" data-t="54:50:00">[54:50:00]</a>. This opens the possibility of achieving superhuman performance in subjective domains like philosophy or poetry, where models could generate ideas and then judge each other's outputs, iteratively improving based on collective consensus <a class="yt-timestamp" data-t="56:17:00">[56:17:00]</a>, <a class="yt-timestamp" data-t="57:14:00">[57:14:00]</a>.

## Reasoning Beyond Token Space

Traditional reasoning models, like DeepSeek, are limited to "token space," producing longer sequences of words (tokens) as their reasoning trace <a class="yt-timestamp" data-t="41:58:00">[41:58:00]</a>, <a class="yt-timestamp" data-t="58:10:00">[58:10:00]</a>. This is analogous to a "word cell" who thinks primarily in words <a class="yt-timestamp" data-t="00:59:00">[00:59:00]</a>.

Future trends indicate a move towards more complex reasoning mechanisms:

*   **Tool Use:** Models will be able to perform external calls to tools like calculators or Python interpreters and integrate the results back into their reasoning chain <a class="yt-timestamp" data-t="42:36:00">[42:36:00]</a>, <a class="yt-timestamp" data-t="42:47:00">[42:47:00]</a>. This moves beyond simple token generation.
*   **Multimodal Visualization of Thought:** This involves conditioning image generation models based on verbal thought, creating images that are then tokenized and fed back into the reasoning process <a class="yt-timestamp" data-t="58:50:00">[58:50:00]</a>, <a class="yt-timestamp" data-t="59:31:00">[59:31:00]</a>. This is akin to a "shape rotator" who thinks in images and spatial terms <a class="yt-timestamp" data-t="01:00:59">[01:00:59]</a>.
*   **Latent Reasoning ("Latent Weavers"):** Instead of verbalizing thoughts, models can reason entirely within their continuous latent space <a class="yt-timestamp" data-t="01:03:00">[01:03:00]</a>, <a class="yt-timestamp" data-t="01:03:16">[01:03:16]</a>.
    *   **Recurrent Depth Approach:** Models like those described in "Scaling Up Test Time Compute with Latent Reasoning" use recurrent blocks to iterate computations within a high-dimensional latent vector space <a class="yt-timestamp" data-t="01:04:19">[01:04:19]</a>. This allows for variable computational depth, potentially deeper than fixed-depth Transformers <a class="yt-timestamp" data-t="01:10:50">[01:10:50]</a>.
    *   **Benefits:** This approach avoids the constraints of a fixed vocabulary, allowing for potentially richer and more efficient reasoning <a class="yt-timestamp" data-t="01:19:08">[01:19:08]</a>. It's particularly well-suited for edge devices (cell phones, robots) due to better scaling properties of recurrent architectures like LSTMs <a class="yt-timestamp" data-t="01:17:35">[01:17:35]</a>, <a class="yt-timestamp" data-t="01:21:08">[01:21:08]</a>. Recurrent Depth Networks also perform more FLOPs per parameter, reducing communication costs between accelerators during training, which could facilitate more distributed training paradigms <a class="yt-timestamp" data-t="01:23:02">[01:23:02]</a>, <a class="yt-timestamp" data-t="01:23:54">[01:23:54]</a>.
    *   **Challenges:** Reasoning in latent space is less interpretable, as it's not easily translated into human-understandable tokens <a class="yt-timestamp" data-t="01:12:26">[01:12:26]</a>. However, visualizations can sometimes show patterns like convergence to a fixed point, looping behaviors, or "sliders" that might indicate internal counting mechanisms <a class="yt-timestamp" data-t="01:13:06">[01:13:06]</a>, <a class="yt-timestamp" data-t="01:14:09">[01:14:09]</a>.
    *   **LSTM/RNN Revival:** This shift points to a potential return to architectures like LSTMs or RNNs, which have a "constant interaction with memory" rather than the quadratically [[large_language_models_llms_and_scaling | scaling]] attention mechanism of Transformers <a class="yt-timestamp" data-t="01:16:44">[01:16:44]</a>, <a class="yt-timestamp" data-t="01:18:09">[01:18:09]</a>. While LSTMs might be "lossy" (forgetting some information over very long sequences), their computational efficiency (100 times faster for thinking) could outweigh this for extended reasoning traces <a class="yt-timestamp" data-t="01:33:00">[01:33:00]</a>, <a class="yt-timestamp" data-t="01:34:00">[01:34:00]</a>.