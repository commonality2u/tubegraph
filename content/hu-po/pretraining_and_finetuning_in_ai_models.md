---
title: Pretraining and finetuning in AI models
videoId: viiB3JmK21M
---

From: [[hu-po]] <br/> 

Multimodal Large Language Models (MLLMs), also known as Vision Language Models (VLMs), are foundation models designed to consume image and text data and produce text output <a class="yt-timestamp" data-t="00:16:10">[00:16:10]</a>. The development of performant MLLMs involves critical architectural components and data choices, typically following a two-stage [[finetuning_and_training_curriculums_in_ai_models|training curriculum]] that includes [[training_and_finetuning_processes_for_ai_models|pre-training]] and [[finetuning_and_training_curriculums_in_ai_models|fine-tuning]] <a class="yt-timestamp" data-t="00:09:00">[00:09:00]</a> <a class="yt-timestamp" data-t="00:11:41">[00:11:41]</a>.

## Training Pipeline: Pre-training and Fine-tuning

The standard curriculum for MLLMs involves two distinct stages:
1.  **Pre-training**: This initial stage uses a large, diverse dataset to train the model on general knowledge and broad multimodal understanding <a class="yt-timestamp" data-t="00:11:41">[00:11:41]</a> <a class="yt-timestamp" data-t="01:18:51">[01:18:51]</a>.
2.  **Fine-tuning (or Instruction Tuning)**: Following pre-training, the model undergoes supervised [[finetuning machine learning models|fine-tuning]] on smaller, more specific datasets tailored to particular tasks, such as visual question answering (VQA) <a class="yt-timestamp" data-t="01:19:01">[01:19:01]</a> <a class="yt-timestamp" data-t="01:19:17">[01:19:17]</a>.

### Key Aspects of the Training Process

The [[training_and_finetuning_processes_for_ai_models|training]] procedure involves decisions on which parts of the model to train at what stage <a class="yt-timestamp" data-t="00:22:03">[00:22:03]</a>. Unlike many VLM papers that freeze pre-trained image encoders, models like Apple's MM1 are [[training_and_finetuning_processes_for_ai_models|pre-trained]] and [[finetuning_machine_learning_models|fine-tuned]] with both the language model and visual encoders entirely unfrozen, allowing gradients to propagate through the entire architecture <a class="yt-timestamp" data-t="01:01:14">[01:01:14]</a> <a class="yt-timestamp" data-t="01:35:57">[01:35:57]</a>. This approach requires substantial computational resources <a class="yt-timestamp" data-t="00:22:42">[00:22:42]</a>.

### Data Choices for Pre-training and Fine-tuning

Data composition is crucial for achieving state-of-the-art results <a class="yt-timestamp" data-t="00:09:36">[00:09:36]</a> <a class="yt-timestamp" data-t="00:21:44">[00:21:44]</a>.

For **pre-training**, a careful mix of data types is used:
*   **Captioned Images**: These typically have short text highly relevant to the image <a class="yt-timestamp" data-t="00:53:41">[00:53:41]</a>. Examples include CC3M (2 billion text-image pairs) <a class="yt-timestamp" data-t="00:52:57">[00:52:57]</a>. Synthetic captions, like those from VCap 300M, augment these short captions into longer paragraphs, significantly improving performance (up to 4% boost) <a class="yt-timestamp" data-t="00:42:40">[00:42:40]</a> <a class="yt-timestamp" data-t="00:58:47">[00:58:47]</a>. Captioning data is most important for zero-shot performance <a class="yt-timestamp" data-t="00:19:20">[00:19:20]</a>.
*   **Interleaved Image-Text Documents**: These contain longer, more diverse text, with images loosely related to the surrounding text, similar to news articles <a class="yt-timestamp" data-t="00:53:49">[00:53:49]</a> <a class="yt-timestamp" data-t="01:35:16">[01:35:16]</a>. This data type is instrumental for few-shot and text-only performance <a class="yt-timestamp" data-t="00:19:19">[00:19:19]</a> <a class="yt-timestamp" data-t="00:54:27">[00:54:27]</a>.
*   **Text-Only Data**: Including text-only data (e.g., 2 trillion tokens) ensures the Language Model (LLM) component doesn't "forget" how to read text and maintains strong text-based performance <a class="yt-timestamp" data-t="00:53:31">[00:53:31]</a> <a class="yt-timestamp" data-t="00:57:02">[00:57:02]</a>.

For **fine-tuning**, datasets are more task-specific, often generated by advanced models like GPT-4 Vision <a class="yt-timestamp" data-t="01:19:31">[01:19:31]</a>. This means many current MLLMs are, in essence, "distilled" from GPT-4 Vision's capabilities <a class="yt-timestamp" data-t="01:20:22">[01:20:22]</a>.

### Model Architectures and Components

1.  **Image Encoder**: This component processes visual input. It can be pre-trained with:
    *   **Contrastive Losses (e.g., CLIP)**: These emphasize semantic knowledge by pushing similar representations closer and dissimilar ones further apart in embedding space <a class="yt-timestamp" data-t="00:33:32">[00:33:32]</a> <a class="yt-timestamp" data-t="00:34:01">[00:34:01]</a>.
    *   **Reconstructive Losses (e.g., DINOv2)**: These aim to explicitly capture all parts of an image, leading to more nuanced, low-level features <a class="yt-timestamp" data-t="00:33:46">[00:33:46]</a> <a class="yt-timestamp" data-t="00:36:34">[00:36:34]</a>.

    The image resolution has the highest impact on model performance, followed by the image encoder's model size and training data composition <a class="yt-timestamp" data-t="00:42:10">[00:42:10]</a> <a class="yt-timestamp" data-t="00:26:01">[00:26:01]</a>. While increasing image resolution from 224 to 336 pixels yields a 3% boost, and increasing encoder capacity gives a 1% boost, synthetic captions provide a greater boost (4%) <a class="yt-timestamp" data-t="00:58:56">[00:58:56]</a>.

2.  **Vision Language Connector (Projector)**: This module bridges the visual features from the image encoder to the language model, converting them into "visual tokens" compatible with the LLM <a class="yt-timestamp" data-t="00:25:07">[00:25:07]</a> <a class="yt-timestamp" data-t="00:32:16">[00:32:16]</a>. Interestingly, extensive studies suggest that the specific architectural design of this connector has a comparatively negligible impact on performance <a class="yt-timestamp" data-t="00:29:28">[00:29:28]</a> <a class="yt-timestamp" data-t="00:49:40">[00:49:40]</a> <a class="yt-timestamp" data-t="01:37:44">[01:37:44]</a>. This aligns with the "bitter lesson" in AI, where scale and data often outweigh architectural novelty <a class="yt-timestamp" data-t="00:34:07">[00:34:07]</a> <a class="yt-timestamp" data-t="00:50:09">[00:50:09]</a> <a class="yt-timestamp" data-t="01:32:27">[01:32:27]</a>.

3.  **Language Model (LLM)**: Typically a decoder-only model that autoregressively predicts the next tokens <a class="yt-timestamp" data-t="00:26:27">[00:26:27]</a>. The output of the model is always text <a class="yt-timestamp" data-t="00:26:40">[00:26:40]</a>.

### Scaling Laws and Performance

Scaling laws generally hold true:
*   **Model Size**: Models up to 30 billion parameters demonstrate improved performance <a class="yt-timestamp" data-t="00:11:28">[00:11:28]</a>. Smaller models (e.g., 3 billion parameters) are often used for ablation studies due to computational cost, with the assumption that findings generalize to larger models <a class="yt-timestamp" data-t="00:23:05">[00:23:05]</a>.
*   **Image Resolution**: Larger image resolutions lead to better performance <a class="yt-timestamp" data-t="00:18:35">[00:18:35]</a> <a class="yt-timestamp" data-t="01:33:19">[01:33:19]</a>.
*   **Image Token Count**: Higher numbers of image tokens (patches from the image fed to the LLM) generally result in better performance, though this increases computational burden and consumes more context length <a class="yt-timestamp" data-t="00:45:15">[00:45:15]</a> <a class="yt-timestamp" data-t="00:46:28">[00:46:28]</a>.

The ability to say "no" to incorrect user prompts, rather than hallucinating, is a sign of a well-[[finetuning_machine_learning_models|tuned]] model <a class="yt-timestamp" data-t="01:14:05">[01:14:05]</a>.

### Mixture of Experts (MoE) Models

MoE models scale the total number of parameters while keeping the activated parameters constant <a class="yt-timestamp" data-t="01:10:42">[01:10:42]</a>. For instance, an MoE model might have 64 experts but only use two per token (top-2 gating) <a class="yt-timestamp" data-t="01:13:30">[01:13:30]</a>. While increasing experts improves performance, there's a trade-off with increased memory requirements during inference <a class="yt-timestamp" data-t="01:14:00">[01:14:00]</a>. Load balancing techniques are used during [[training_and_finetuning_processes_for_ai_models|training]] to ensure all experts are utilized equally and none become dominant <a class="yt-timestamp" data-t="01:14:15">[01:14:15]</a>.

### [[SelfImprovement in AI Models|Self-Improvement]] and Data Ownership

The reliance on synthetic data generated by other models (e.g., GPT-4 Vision) raises complex questions about copyright and data ownership, creating a "gordian knot" of data dependencies across models and companies <a class="yt-timestamp" data-t="01:21:45">[01:21:45]</a>. This interconnectedness makes strict regulation challenging <a class="yt-timestamp" data-t="01:21:55">[01:21:55]</a>.

### [[technical_aspects_of_ai_model_training_and_finetuning|Technical Aspects]]

*   **Frameworks**: Apple's MM1 models are trained using `axlearn`, a library built on Jax and XLA <a class="yt-timestamp" data-t="01:02:01">[01:02:01]</a>. Evaluation uses Luther AI's LM Evaluation Harness <a class="yt-timestamp" data-t="01:36:57">[01:36:57]</a>. Experiment tracking uses Weights & Biases <a class="yt-timestamp" data-t="01:40:07">[01:40:07]</a>.
*   **Hardware**: The paper does not disclose the specific hardware used for [[training_and_finetuning_processes_for_ai_models|training]] <a class="yt-timestamp" data-t="01:50:00">[01:50:00]</a>.
*   **Hyperparameter Tuning**: Hyperparameter sweeps (e.g., for learning rate and weight decay) are often conducted on smaller models to save resources, with the assumption that optimal values transfer to larger models <a class="yt-timestamp" data-t="01:08:01">[01:08:01]</a>. However, the correlation between validation loss and downstream task performance is not always strong <a class="yt-timestamp" data-t="01:05:00">[01:05:00]</a>.
*   **Decoding**: Outputs are generated using greedy decoding, selecting the best possible token at each step <a class="yt-timestamp" data-t="01:36:26">[01:36:26]</a>.

## Conclusion

The continuous improvement observed with increased scale suggests that the field is not hitting a wall, and future models will likely achieve even better performance as hardware and data capabilities advance <a class="yt-timestamp" data-t="01:49:19">[01:49:19]</a>. The transparency of companies like Apple in publishing detailed papers contributes to the open-source community's ability to develop and improve models <a class="yt-timestamp" data-t="01:50:50">[01:50:50]</a> <a class="yt-timestamp" data-t="01:51:19">[01:51:19]</a>.