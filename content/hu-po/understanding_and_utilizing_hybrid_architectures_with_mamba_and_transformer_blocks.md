---
title: Understanding and utilizing hybrid architectures with Mamba and Transformer blocks
videoId: aWMv8W_UgJU
---

From: [[hu-po]] <br/> 

Recent advancements in AI architecture have led to the exploration of [[hybrid_architectures]] that combine different model types, such as [[Mamba models and their applications | Mamba]] and [[Transformer | Transformer]] blocks <a class="yt-timestamp" data-t="06:11:12">[06:11:12]</a>. This approach aims to leverage the strengths of each architecture to potentially achieve better performance or efficiency.

## Motivation for Hybrid Architectures
While traditional [[Transformer]] models, like [[GPT-2 | GPT-2]], rely on stacked [[Transformer | Transformer]] blocks <a class="yt-timestamp" data-t="05:44:03">[05:44:03]</a>, empirical findings suggest that mixing [[Mamba models and their applications | Mamba]] and [[Transformer | Transformer]] blocks can yield superior results <a class="yt-timestamp" data-t="06:38:29">[06:38:29]</a>. This strategy has been observed in recent research, including models like Jamba <a class="yt-timestamp" data-t="06:44:59">[06:44:59]</a>.

### Comparison of Recurrent Networks and Transformers
[[comparisons_between_mambas_and_transformers | Mamba models]] are described as linear attention models <a class="yt-timestamp" data-t="02:55:00">[02:55:00]</a>, sharing similarities with recurrent neural networks (RNNs) or Long Short-Term Memory (LSTM) networks <a class="yt-timestamp" data-t="02:56:06">[02:56:06]</a>. They typically maintain a hidden state that is passed forward, implicitly providing a form of memory <a class="yt-timestamp" data-t="02:56:06">[02:56:06]</a>. This characteristic provides a stronger inductive bias, making them particularly effective for time series problems <a class="yt-timestamp" data-t="02:56:06">[02:56:06]</a>.

A key advantage of [[Mamba models and their applications | Mamba models]] is their efficiency in inference, as their computational complexity scales linearly with input length <a class="yt-timestamp" data-t="02:56:06">[02:56:06]</a>. This contrasts with [[Transformer | Transformers]], which use quadratic self-attention, leading to higher computational costs with longer sequences <a class="yt-timestamp" data-t="02:55:00">[02:55:00]</a>. While [[Transformer | Transformers]] benefit from a less inductive bias that allows them to learn broadly <a class="yt-timestamp" data-t="02:56:00">[02:56:00]</a>, combining them with [[Mamba models and their applications | Mamba models]] seeks to harness the best of both worlds.

## Implementation Example: Hybridizing GPT-2 with Mamba 2
An experimental project involved taking Karpathy's `nanogpt` [[GPT-2 | GPT-2]] repository <a class="yt-timestamp" data-t="04:49:03">[04:49:03]</a> and integrating [[Mamba 2 | Mamba 2]] blocks from the state-spaces [[Mamba | Mamba]] repository <a class="yt-timestamp" data-t="05:06:09">[05:06:09]</a>. The goal was to create a decoder-only [[Transformer | Transformer]] that alternates between standard [[Transformer | Transformer]] blocks and [[Mamba | Mamba]] blocks <a class="yt-timestamp" data-t="05:44:03">[05:44:03]</a>.

Initially, a standard [[GPT-2 | GPT-2]] configuration with 12 [[Transformer | Transformer]] blocks resulted in a model with 124 million parameters <a class="yt-timestamp" data-t="02:26:00">[02:26:00]</a>. By alternating between [[Mamba | Mamba]] and [[Transformer | Transformer]] blocks, the resulting hybrid model's parameter count was reduced to 103 million parameters <a class="yt-timestamp" data-t="02:27:09">[02:27:09]</a>.

## Development and Deployment Considerations

### Dependency Management
Setting up a development environment for such hybrid architectures involves careful dependency management. Two primary approaches are commonly used:

*   **Conda:** For local development, Conda allows for the creation of isolated Python environments. Commands include `conda create -n carpa_mbathi python=3.11` to create an environment and `conda activate carpa_mbathi` to activate it <a class="yt-timestamp" data-t="03:08:00">[03:08:00]</a>. Specific versions of libraries like PyTorch are installed via `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121` <a class="yt-timestamp" data-t="03:00:00">[03:00:00]</a>, ensuring compatibility with the local CUDA version (e.g., 12.2 or 12.1) <a class="yt-timestamp" data-t="03:17:00">[03:17:00]</a>. Dependencies like `tiktoken` and `datasets` are also installed <a class="yt-timestamp" data-t="03:51:00">[03:51:00]</a>. It is recommended to "pin" dependency versions to ensure reproducibility <a class="yt-timestamp" data-t="03:51:00">[03:51:00]</a>.
*   **Docker:** For consistent deployments across different machines, Docker provides a containerized environment <a class="yt-timestamp" data-t="00:59:00">[00:59:00]</a>. A `Dockerfile` defines the environment, starting from a base CUDA-enabled Ubuntu image <a class="yt-timestamp" data-t="01:00:00">[01:00:00]</a>. All necessary libraries, including Python, PyTorch, and the [[Mamba | Mamba]] repository itself (cloned and installed in editable mode), are set up within the container <a class="yt-timestamp" data-t="01:02:00">[01:02:00]</a>. A `.dockerignore` file prevents unnecessary files from being copied into the image, keeping it lean <a class="yt-timestamp" data-t="01:04:00">[01:04:00]</a>.

### GPU Utilization
Monitoring GPU usage is crucial for development. Tools like `nvidia-smi` <a class="yt-timestamp" data-t="00:33:00">[00:33:00]</a> and `gnome-system-monitor` <a class="yt-timestamp" data-t="00:57:00">[00:57:00]</a> provide insights into VRAM usage, power consumption, and GPU utilization. The amount of VRAM (Video RAM) is a critical factor when selecting a GPU for machine learning, as it determines the size of models and batches that can be loaded <a class="yt-timestamp" data-t="00:46:00">[00:46:00]</a>. Optimizing batch size is essential to fit models within available GPU memory <a class="yt-timestamp" data-t="00:52:00">[00:52:00]</a>. For example, a batch size of 32 utilized 22 GB of VRAM on an RTX 3090 GPU <a class="yt-timestamp" data-t="00:55:00">[00:55:00]</a>.

### Data Loading and Preprocessing
Adapting data loaders for specific tasks is often the most complex part of a machine learning project <a class="yt-timestamp" data-t="01:32:00">[01:32:00]</a>. For the Abstract Reasoning Corpus (ARC) challenge <a class="yt-timestamp" data-t="08:47:00">[08:47:00]</a>, data is stored in JSON format, with each task having demonstration input/output pairs and test input/output pairs <a class="yt-timestamp" data-t="01:38:00">[01:38:00]</a>.

A custom PyTorch `Dataset` is needed to handle the variable lengths of these examples <a class="yt-timestamp" data-t="02:09:00">[02:09:00]</a>. The inputs (demonstration inputs and outputs, and test inputs) are concatenated and flattened into a single sequence, with the test output serving as the target <a class="yt-timestamp" data-t="02:11:00">[02:11:00]</a>. Since `DataLoader` requires all tensors in a batch to be of equal size, padding with zeros is necessary to ensure uniform sequence lengths, based on a defined maximum sequence length <a class="yt-timestamp" data-t="02:27:00">[02:27:00]</a>. The raw data, representing colors on a grid, are effectively indices, making them compatible with the token ID format expected by the model's embedding layer <a class="yt-timestamp" data-t="02:51:00">[02:51:00]</a>.

## Challenges and Future Directions
While the conceptual framework for hybrid architectures is promising, practical implementation can face challenges:
*   **Variable Sequence Lengths:** Handling different input sizes requires padding, which can lead to inefficient use of memory if not managed carefully <a class="yt-timestamp" data-t="03:11:00">[03:11:00]</a>.
*   **Model Compatibility:** Specific architectural components, like `causal_conv1d` in [[Mamba | Mamba]] blocks, may have strict requirements (e.g., input strides being multiples of 8) <a class="yt-timestamp" data-t="02:55:00">[02:55:00]</a>, necessitating careful adjustment of model dimensions or input processing.
*   **Training Script Adaptation:** Modifying existing training scripts, especially those designed for language modeling, to accommodate new data formats or different loss functions can be complex <a class="yt-timestamp" data-t="03:00:00">[03:00:00]</a>.

Future work could involve fine-tuning pre-trained language models on a textual representation of the ARC challenge, which would leverage existing general intelligence rather than training from scratch <a class="yt-timestamp" data-t="02:38:00">[02:38:00]</a>. This would involve transforming the grid-based puzzles into a tokenized sequence format similar to conversation turns (e.g., "demonstration input: ... output: ... test input: ...").