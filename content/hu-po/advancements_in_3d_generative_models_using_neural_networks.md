---
title: Advancements in 3D generative models using neural networks
videoId: IsRHGf2rGCs
---

From: [[hu-po]] <br/> 

The field of [[generative_3d_technology | generative 3D]] modeling is experiencing rapid advancements, with new neural network architectures and techniques constantly emerging <a class="yt-timestamp" data-t="01:49:00">[01:49:00]</a>. A key focus for researchers is the creation of 3D assets from various inputs, including text and images <a class="yt-timestamp" data-t="02:44:00">[02:44:00]</a>.

## Core Challenges and Solutions
A primary obstacle in developing foundational [[generative_3d_technology | 3D generative models]] is the [[challenges_and_limitations_in_3d_generation | limited availability of 3D data]] <a class="yt-timestamp" data-t="07:53:00">[07:53:00]</a>. Unlike vast repositories of 2D images and videos, high-quality, diverse 3D datasets are scarce <a class="yt-timestamp" data-t="09:00:00">[09:00:00]</a>. Popular datasets like Objaverse often exhibit an object-centric bias, limiting their utility for complex scenes <a class="yt-timestamp" data-t="08:09:00">[08:09:00]</a>.

To circumvent this, many approaches leverage existing [[video_diffusion_models_in_generative_3d | video diffusion models]] or image diffusion models as knowledge sources for [[data_generation_for_ai_models | 3D data generation]] <a class="yt-timestamp" data-t="09:10:00">[09:10:00]</a>, creating synthetic multiview datasets <a class="yt-timestamp" data-t="09:28:00">[09:28:00]</a>.

## Key Generative Approaches

### Video Diffusion Models for 3D Generation
Several recent works adapt pre-trained [[video_diffusion_models_in_generative_3d | video diffusion models]] to synthesize consistent multi-view data for [[generative_3d_technology | 3D generative models]]:

*   **V Fusion 3D (Meta AI):** This model fine-tunes a pre-trained [[video_diffusion_models_in_generative_3d | video diffusion model]] (like Emu) on 3D data to generate a large-scale synthetic multi-view dataset <a class="yt-timestamp" data-t="07:41:00">[07:41:00]</a>. This dataset is then used to train a feed-forward [[3d_implicit_functions_and_generative_models | 3D generative model]] that produces objects from a single image in seconds <a class="yt-timestamp" data-t="11:45:00">[11:45:00]</a>. The internal 3D representation utilized is a [[3d_implicit_functions_and_generative_models | Neural Radiance Field (NeRF)]] with a triplane encoder, trained using reconstruction losses like L2 and LPIPS <a class="yt-timestamp" data-t="17:01:00">[17:01:00]</a>.
*   **SV3D (Stability AI):** Similar to V Fusion 3D, SV3D adapts a high-resolution image-conditioned [[video_diffusion_models_in_generative_3d | video diffusion model]] for novel multi-view synthesis <a class="yt-timestamp" data-t="21:03:00">[21:03:03]</a>. It leverages the temporal consistency of video diffusion models to ensure spatial 3D consistency in generated images <a class="yt-timestamp" data-t="22:20:00">[22:20:00]</a>. This also helps mitigate issues like the "Yanis effect," where different views of an object might appear inconsistent <a class="yt-timestamp" data-t="23:01:00">[23:01:00]</a>. The output is fed into a NeRF, from which [[generative_ai_for_highquality_meshes | high-quality meshes]] can be produced via marching cubes and DMTet <a class="yt-timestamp" data-t="31:58:00">[31:58:00]</a>. Optimization uses a soft-masked Score Distillation Sampling (SDS) loss <a class="yt-timestamp" data-t="32:04:00">[32:04:00]</a>.

### Gaussian Splatting for Dynamic Scenes
While NeRFs are prevalent, [[3d_implicit_functions_and_generative_models | Gaussian Splatting]] is emerging as an alternative 3D representation, especially for dynamic content:

*   **Gaussian Flow Splatting:** This method introduces "Gaussian Flow," which connects the dynamics of 3D Gaussians with pixel velocities between consecutive frames, enabling supervision from optical flow <a class="yt-timestamp" data-t="36:50:00">[36:50:00]</a>. This approach is beneficial for content with rich motions and aims to solve color drifting issues in 4D (3D + time) scenes <a class="yt-timestamp" data-t="37:21:00">[37:21:00]</a>.
*   **GRM (Large Gaussian Reconstruction Model):** GRM focuses on efficiently reconstructing 3D assets from sparse-view images using pixel-aligned Gaussians <a class="yt-timestamp" data-t="02:52:02">[02:52:02]</a>. It employs a feed-forward Transformer-based model that translates input pixels into 3D Gaussians <a class="yt-timestamp" data-t="02:53:06">[02:53:06]</a>. The results from this model show significantly higher quality and cleaner representations compared to many NeRF-based methods, especially in fine details <a class="yt-timestamp" data-t="03:09:47">[03:09:47]</a>. The paper suggests that Gaussian Splats have a higher representational capacity than NeRF MLPs <a class="yt-timestamp" data-t="03:10:05">[03:10:05]</a>.

### Latent Space Diffusion and Hybrid Approaches
Many models utilize [[generative_latent_spaces_in_ai | generative latent spaces]] for efficient 3D generation:

*   **Compressed 3D:** This approach encodes 3D models into a compact triplane latent space within an autoencoder framework <a class="yt-timestamp" data-t="02:21:42">[02:21:42]</a>. A diffusion model is then trained on this refined latent space, conditioned by both image embeddings and shape embeddings <a class="yt-timestamp" data-t="02:22:05">[02:22:05]</a>. It aims for [[generative_ai_for_highquality_meshes | high-quality 3D mesh generation]] from a single image rapidly <a class="yt-timestamp" data-t="02:27:07">[02:27:07]</a>.
*   **LN3Diff (Scalable Latent Neural Field Diffusion):** LN3Diff introduces a two-stage process. First, a convolutional encoder maps a monocular image to a 3D latent (similar to a triplane), which is then decoded by a Transformer into a high-capacity [[3d_implicit_functions_and_generative_models | 3D neural field]] (NeRF) <a class="yt-timestamp" data-t="01:57:46">[01:57:46]</a>. In the second stage, a diffusion model is trained on this 3D-aware latent space, enabling fast conditional 3D generation without per-instance optimization <a class="yt-timestamp" data-t="02:03:52">[02:03:52]</a>.
*   **MV-Edit (Generic 3D Diffusion Adapter):** MV-Edit functions as a 3D counterpart to 2D image editing models. It uses a training-free 3D adapter to extend pre-trained image diffusion models for 3D-aware diffusion, enabling multi-view images to be jointly denoised <a class="yt-timestamp" data-t="01:06:05">[01:06:05]</a>. This allows for text-guided 3D-to-3D editing and texture generation <a class="yt-timestamp" data-t="01:13:41">[01:13:41]</a>.

### Compositional and Preference-Aligned Generation

*   **ComboVerse:** This framework addresses the challenge of generating complex 3D assets containing multiple objects <a class="yt-timestamp" data-t="02:13:03">[02:13:03]</a>. It works by first decomposing an input image into individual objects, inpainting missing parts, and reconstructing separate 3D models <a class="yt-timestamp" data-t="02:15:57">[02:15:57]</a>. These individual 3D models are then spatially combined and optimized using a spatially-aware Score Distillation Sampling (SDS) loss to match the input image's composition <a class="yt-timestamp" data-t="02:19:33">[02:19:33]</a>.
*   **Dream Reward:** Recognizing that current 3D generation models often produce results misaligned with human preferences, Dream Reward introduces a framework to learn and improve 3D models from human feedback <a class="yt-timestamp" data-t="03:36:00">[03:36:00]</a>. It collects a dataset of 25,000 expert comparisons to build "Reward 3D," a general-purpose text-to-3D human preference reward model <a class="yt-timestamp" data-t="03:53:00">[03:53:00]</a>. This reward model is then used to optimize multi-view diffusion models, akin to Reinforcement Learning from Human Feedback (RLHF) in language models <a class="yt-timestamp" data-t="02:38:00">[02:38:00]</a>.

## Evaluation and Future Directions
[[evaluation_of_3d_generative_techniques | Evaluation of 3D generative techniques]] is challenging, with many papers claiming "state-of-the-art" based on comparisons to older methods <a class="yt-timestamp" data-t="02:01:00">[02:01:00]</a>. Human preference studies offer a more reliable gauge of quality <a class="yt-timestamp" data-t="02:45:37">[02:45:37]</a>.

Looking ahead, [[future_potential_and_direction_for_generative_3d_technology | the future of generative 3D technology]] will likely involve:
*   **Dominance of Video Models:** Utilizing large-scale [[video_diffusion_models_in_generative_3d | video diffusion models]] like Sora will provide a superior source of signal for 3D generation due to their inherent object consistency and temporal coherence <a class="yt-timestamp" data-t="03:13:59">[03:13:59]</a>.
*   **Evolution of Representations:** Expect a continued shift from traditional meshes and textures to more flexible and scalable representations like 4D Gaussian Splatting, which can directly model dynamic scenes <a class="yt-timestamp" data-t="03:38:58">[03:38:58]</a>.
*   **Human Feedback Integration:** Techniques inspired by RLHF, such as preference tuning, will become crucial for aligning generated 3D content with subjective human aesthetic preferences and intentions <a class="yt-timestamp" data-t="03:35:00">[03:35:00]</a>.
*   **Real-time and On-Device Generation:** Future models will aim for on-device, real-time generation, enabling applications in VR/AR where users can interactively create and manipulate 3D content through voice, gestures, or even brain signals <a class="yt-timestamp" data-t="03:31:00">[03:31:00]</a>.
*   **Addressing Data Biases:** Moving beyond object-centric datasets like Objaverse to larger, more diverse datasets synthesized from video will reduce biases and enable the generation of complex, multi-object scenes <a class="yt-timestamp" data-t="02:23:00">[02:23:00]</a>.
*   **Direct-to-Representation Diffusion:** Expect models that directly diffuse into explicit 3D representations (like Gaussian Splats) rather than relying on multi-step, over-engineered pipelines involving 2D diffusion intermediate steps <a class="yt-timestamp" data-t="03:33:00">[03:33:00]</a>.
*   **Hardware Advancements:** The continuous development of specialized AI hardware will enable larger, more capable models, pushing the boundaries of resolution, detail, and complexity in 3D content <a class="yt-timestamp" data-t="01:40:00">[01:40:00]</a>.

The field is still in its rapid growth phase, with significant breakthroughs expected in the coming years before reaching a plateau in capabilities <a class="yt-timestamp" data-t="03:38:00">[03:38:00]</a>.