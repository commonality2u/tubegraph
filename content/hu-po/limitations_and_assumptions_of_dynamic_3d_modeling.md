---
title: Limitations and assumptions of dynamic 3D modeling
videoId: hDuy1TgD8I4
---

From: [[hu-po]] <br/> 

Dynamic 3D modeling, particularly using [[Dynamic 3D Gaussian methodology | Dynamic 3D Gaussians]], presents several limitations and relies on specific assumptions that constrain its applicability in various real-world scenarios.

## Data Capture Limitations

The effectiveness of current [[Dynamic 3D Gaussian methodology | Dynamic 3D Gaussians]] models is heavily dependent on specialized data capture environments:

*   **Reliance on Geodesic Domes** The primary method used for data collection involves a giant geodesic dome studio, approximately 5.49 meters in radius and 4.15 meters in height, designed to allow increased access to its edges (<a class="yt-timestamp" data-t="00:03:25">[00:03:25]</a>, <a class="yt-timestamp" data-t="00:37:39">[00:37:39]</a>).
*   **Synchronized Multi-Camera Arrays** These domes house a large number of cameras, for instance, 480 cameras synchronized to a central clock system (<a class="yt-timestamp" data-t="00:04:05">[00:04:05]</a>). For this specific work, 27 training cameras and 4 testing cameras were used (<a class="yt-timestamp" data-t="00:37:54">[00:37:54]</a>, <a class="yt-timestamp" data-t="02:02:19">[02:02:19]</a>).
*   **Precise Camera Calibration** A critical assumption is that the exact position and intrinsic parameters (e.g., focal length, distortions) of every single camera are known and perfectly calibrated beforehand using methods like a checkerboard (<a class="yt-timestamp" data-t="00:04:24">[00:04:24]</a>, <a class="yt-timestamp" data-t="00:58:06">[00:58:06]</a>, <a class="yt-timestamp" data-t="00:59:44">[00:59:44]</a>, <a class="yt-timestamp" data-t="01:17:55">[01:17:55]</a>, <a class="yt-timestamp" data-t="02:27:58">[02:27:58]</a>).
*   **Uniform Camera Distribution** The cameras are ideally positioned roughly in a hemisphere around the area of interest, which provides comprehensive coverage essential for reconstruction (<a class="yt-timestamp" data-t="02:02:26">[02:02:26]</a>, <a class="yt-timestamp" data-t="02:02:58">[02:02:58]</a>).
*   **Not Suitable for Monocular Video** The method is not designed for off-the-shelf monocular video (e.g., from an iPhone) due to the lack of this precise multi-camera calibration and synchronized viewpoints (<a class="yt-timestamp" data-t="00:50:51">[00:50:51]</a>, <a class="yt-timestamp" data-t="00:59:29">[00:59:29]</a>, <a class="yt-timestamp" data-t="02:14:00">[02:14:00]</a>, <a class="yt-timestamp" data-t="02:21:27">[02:21:27]</a>, <a class="yt-timestamp" data-t="02:28:07">[02:28:07]</a>).

## Model Assumptions and Constraints

The [[Dynamic 3D Gaussian methodology | Dynamic 3D Gaussians]] model itself operates under several simplifying assumptions:

*   **Fixed Gaussian Attributes Over Time** A key insight is that all attributes of the gaussians—such as their number, color, opacity, and size—remain constant over time. Only their position and orientation are allowed to vary (<a class="yt-timestamp" data-t="01:17:47">[01:17:47]</a>, <a class="yt-timestamp" data-t="01:29:29">[01:29:29]</a>, <a class="yt-timestamp" data-t="02:24:56">[02:24:56]</a>).
    *   This assumption works well for rigid or semi-rigid objects like humans throwing a ball (<a class="yt-timestamp" data-t="01:03:05">[01:03:05]</a>, <a class="yt-timestamp" data-t="01:07:08">[01:07:08]</a>).
    *   However, it struggles with deformable objects or scenes involving fluids like smoke or water, where opacity and size *would* change (<a class="yt-timestamp" data-t="01:18:08">[01:18:08]</a>, <a class="yt-timestamp" data-t="02:28:51">[02:28:51]</a>).
*   **Local Rigidity Priors** The model enforces local rigidity constraints, assuming that nearby gaussians should move and rotate together as a rigid body (<a class="yt-timestamp" data-t="01:12:16">[01:12:16]</a>, <a class="yt-timestamp" data-t="01:20:00">[01:20:00]</a>, <a class="yt-timestamp" data-t="02:25:07">[02:25:07]</a>).
    *   This is crucial for accurate long-term tracking (<a class="yt-timestamp" data-t="01:35:05">[01:35:05]</a>).
    *   However, scenes without local rigidity, such as turbulent water, would likely not be modeled well (<a class="yt-timestamp" data-t="01:12:45">[01:12:45]</a>).
*   **View-Dependent Color Excluded** For simplicity, the model does not include view-dependent color using spherical harmonics (<a class="yt-timestamp" data-t="01:10:47">[01:10:47]</a>). This means the color of a gaussian does not change based on the viewing angle, which limits the ability to perform realistic re-lighting or complex compositional scene synthesis (<a class="yt-timestamp" data-t="01:11:11">[01:11:11]</a>, <a class="yt-timestamp" data-t="01:28:47">[01:28:47]</a>).
*   **Lack of Direct Physics Engine Integration** While [[Dynamic 3D Gaussian methodology | Dynamic 3D Gaussians]] offers potential for physics-based interactions due to explicit object notions, the current paper doesn't integrate a physics engine directly (<a class="yt-timestamp" data-t="02:08:50">[02:08:50]</a>). This means it cannot naturally simulate collisions or other complex physical phenomena (<a class="yt-timestamp" data-t="02:11:00">[02:11:00]</a>).
*   **Limited to Initially Visible Scene Parts** The method is only able to track parts of the scene that are visible in the initial frame and would fail to reconstruct new objects entering the scene later (<a class="yt-timestamp" data-t="01:05:57">[01:05:57]</a>, <a class="yt-timestamp" data-t="02:21:11">[02:21:11]</a>, <a class="yt-timestamp" data-t="02:28:59">[02:28:59]</a>).
*   **Dependency on Pseudoground Truth for Foreground/Background** To improve tracking in areas of uniform color (e.g., a shirt matching a background), the model renders a foreground/background mask, which is obtained by frame differences with a reference frame where no foreground objects are present (<a class="yt-timestamp" data-t="01:57:17">[01:57:17]</a>, <a class="yt-timestamp" data-t="02:03:55">[02:03:55]</a>). This assumes consistent lighting in the reference frame, which is often not true in real-world scenarios (<a class="yt-timestamp" data-t="02:04:07">[02:04:07]</a>).

## Performance and Generalization

*   **Sensitivity to Uniform Colors** The model struggles to track [[Dynamic 3D Gaussian methodology | Dynamic 3D Gaussians]] in large areas of near-uniform color, as they tend to "move freely around the area of similar color" due to insufficient restrictions (<a class="yt-timestamp" data-t="01:33:53">[01:33:53]</a>, <a class="yt-timestamp" data-t="01:56:55">[01:56:55]</a>).
*   **Hyperparameter Tuning** The method relies on specific hyperparameter values (e.g., Lambda W = 2000, K = 20 for K-nearest neighbors) that are specific to the tested scenes and might require re-tuning for different data sets (<a class="yt-timestamp" data-t="01:42:56">[01:42:56]</a>, <a class="yt-timestamp" data-t="01:47:38">[01:47:38]</a>, <a class="yt-timestamp" data-t="02:27:13">[02:27:13]</a>). The interplay between competing losses (e.g., local rigidity vs. local rotational similarity) can necessitate finding "magical values" for these parameters (<a class="yt-timestamp" data-t="01:51:15">[01:51:15]</a>).
*   **Accumulated Error in Tracking** The short-term nature of rigidity and rotation losses (only applied between current and directly preceding time steps) means that errors can accumulate and lead to "drift" over longer time horizons (<a class="yt-timestamp" data-t="01:48:20">[01:48:20]</a>, <a class="yt-timestamp" data-t="01:51:45">[01:51:45]</a>). The long-term isometry loss is introduced to mitigate this, but it also has its own limitations for deformable scenes (<a class="yt-timestamp" data-t="01:51:15">[01:51:15]</a>).
*   **Not Purely "Correspondence-Free"** While the method states it takes no explicit correspondence or flow as input (<a class="yt-timestamp" data-t="01:39:59">[01:39:59]</a>, <a class="yt-timestamp" data-t="01:41:13">[01:41:13]</a>, <a class="yt-timestamp" data-t="01:53:20">[01:53:20]</a>), the underlying information for correspondence is implicitly available through the known positions of multiple calibrated cameras viewing the scene from different perspectives (<a class="yt-timestamp" data-t="00:56:48">[00:56:48]</a>).

Despite these limitations, the approach delivers excellent real-time rendering performance (850 frames per second) and accurate tracking, setting a promising direction for future research in 3D modeling and tracking (<a class="yt-timestamp" data-t="02:21:12">[02:21:12]</a>, <a class="yt-timestamp" data-t="02:21:52">[02:21:52]</a>, <a class="yt-timestamp" data-t="02:29:16">[02:29:16]</a>).