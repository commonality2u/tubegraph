---
title: comparison of direct coding prompts to multi agent frameworks
videoId: tFwYD1UPIfM
---

From: [[hu-po]] <br/> 

This article explores the effectiveness of multi-agent AI frameworks, specifically [[meta_gpt_multi_agent_collaborative_framework | MetaGPT]], in generating functional code and documentation, contrasting it with the results obtained from direct prompts to large language models (LLMs) like GPT-4. It delves into the practical outcomes of [[AI agents and automation | AI agents]] mimicking human software development roles and the implications for traditional software engineering workflows.

## Introduction to Multi-Agent Frameworks and MetaGPT

[[meta_gpt_multi_agent_collaborative_framework | MetaGPT]], short for "Meta Programming for Multi-Agent Collaborative Framework," is a GitHub repository that gained significant traction for its approach to automated task solving through multi-agent systems driven by large language models (LLMs) <a class="yt-timestamp" data-t="01:01:04">[01:01:04]</a>. The project had accumulated over 20,000 stars by late 2023 <a class="yt-timestamp" data-t="01:1:18">[01:1:18]</a>. The core concept behind MetaGPT is to incorporate efficient human workflows as a metaprogramming approach into LLMs <a class="yt-timestamp" data-t="02:59:30">[02:59:30]</a>. Metaprogramming, in this context, refers to programming the level above traditional programming, similar to "metagaming" where one plays the game of playing a game <a class="yt-timestamp" data-t="06:40:00">[06:40:00]</a>.

The framework is organized into two distinct layers <a class="yt-timestamp" data-t="03:52:00">[03:52:00]</a>:
*   **Foundational Component Layer:** Handles agent operations and system-wide communication, including environment, memory, roles, actions, and tools <a class="yt-timestamp" data-t="03:54:00">[03:54:00]</a>.
*   **Collaboration Layer:** Facilitates agent coordination through mechanisms like knowledge sharing and workflow encapsulation <a class="yt-timestamp" data-t="03:59:00">[03:59:00]</a>. Knowledge sharing allows agents to exchange information, contributing to a shared knowledge base <a class="yt-timestamp" data-t="03:30:00">[03:30:00]</a>.

Agents within MetaGPT are guided by specialized role prompts, known as "anchor agents," which come with specific capabilities like thinking, reflection, and knowledge accumulation <a class="yt-timestamp" data-t="03:04:00">[03:04:00]</a>. These roles interact with the environment through subscription and publication methods, reminiscent of a pub/sub messaging framework <a class="yt-timestamp" data-t="03:26:00">[03:26:00]</a>.

## MetaGPT's Approach to Software Development

[[meta_gpt_multi_agent_collaborative_framework | MetaGPT]] encodes standard operating procedures (SOPs) into prompts to enhance structured coordination and mandate modular outputs, empowering agents with domain expertise comparable to human professionals <a class="yt-timestamp" data-t="08:03:00">[08:03:00]</a>. This approach mirrors human software development teams by assigning diverse roles to various [[AI agents and automation | agents]], leveraging an "assembly line paradigm" <a class="yt-timestamp" data-t="08:56:00">[08:56:00]</a>.

The framework simulates a typical software team structure:
*   **Product Manager:** Determines tasks and creates requirement documents (PRD) <a class="yt-timestamp" data-t="13:21:00">[13:21:00]</a>.
*   **Architect:** Determines system design <a class="yt-timestamp" data-t="13:25:00">[13:25:00]</a>.
*   **Project Manager:** Outlines steps from analysis to delivery and promotes teamwork <a class="yt-timestamp" data-t="13:56:00">[13:56:00]</a>.
*   **Engineer:** Writes the code <a class="yt-timestamp" data-t="13:19:00">[13:19:19]</a>.
*   **QA Engineer:** Validates outputs and minimizes compounded errors <a class="yt-timestamp" data-t="08:54:00">[08:54:00]</a>.

The process is sequential, moving from product manager to architect, then project manager, engineer, and finally QA engineer <a class="yt-timestamp" data-t="09:33:00">[09:33:00]</a>, aiming to effectively deconstruct complex multi-agent collaborative problems <a class="yt-timestamp" data-t="09:40:00">[09:40:00]</a>.

## Practical Application: AI Cat Toy Frontend

To test [[meta_gpt_multi_agent_collaborative_framework | MetaGPT]]'s capabilities, a task was given: "write a Gradio front end for a robotic AI cat toy" <a class="yt-timestamp" data-t="39:58:00">[39:58:00]</a>. The process generated several documents and code files:

*   **Product Requirement Document (PRD):** Generated by the Product Manager, it included requirements like an intuitive and user-friendly Gradio front end, control over toy movements, customizable behavior, scheduled play sessions, and monitoring <a class="yt-timestamp" data-t="41:59:00">[41:59:00]</a>. It also generated hypothetical competitor names and a "quadrant chart" for competitive analysis, though the axes chosen for the chart were deemed "meaningless" as they simply stated "more user friendly" vs. "less user friendly" and "more feature rich" vs. "less feature rich" <a class="yt-timestamp" data-t="43:40:00">[43:40:00]</a>.
*   **System Design:** The Architect generated a system design document that suggested using Gradio for the frontend and Flask for backend services <a class="yt-timestamp" data-t="48:12:00">[48:12:00]</a>. This choice was questioned as Flask would be "overkill" for a local Raspberry Pi backend <a class="yt-timestamp" data-t="48:51:00">[48:51:00]</a>. The file structure proposed seemed arbitrary, and the API design using "control params," "customize params," etc., was largely "meaningless" <a class="yt-timestamp" data-t="50:22:00">[50:22:00]</a>.
*   **Code Generation:** The Engineer produced Python code. However, the code was found to be problematic:
    *   It specified an extremely outdated version of Gradio (1.77 from April 2021), aligning with GPT-4's knowledge cut-off date <a class="yt-timestamp" data-t="52:47:00">[52:47:00]</a>.
    *   The `ControlParams`, `CustomizeParams`, etc., were generic dictionaries without specific schemas, described as "nonsense" <a class="yt-timestamp" data-t="57:44:00">[57:44:00]</a>.
    *   It created separate Gradio interfaces for different functionalities rather than consolidating them into one <a class="yt-timestamp" data-t="58:38:00">[58:38:00]</a>.
    *   Inconsistencies were found in time formatting, with some parts using basic strings and others using `datetime` objects <a class="yt-timestamp" data-t="01:06:40">[01:06:40]</a>.
    *   The `utils` class with only static methods was deemed a poor design pattern <a class="yt-timestamp" data-t="01:07:49">[01:07:49]</a>.
    *   The Flask integration was syntactically incorrect (missing `request` import) and unorthodox (routes defined within the `__init__` function) <a class="yt-timestamp" data-t="01:15:05">[01:15:05]</a>.
*   **Missing QA Engineer Output:** No test file was created by the QA engineer, indicating an incomplete workflow <a class="yt-timestamp" data-t="01:10:10">[01:10:10]</a>.

The process of running this demo cost approximately $0.87 in API tokens <a class="yt-timestamp" data-t="01:47:00">[01:47:00]</a>, with subsequent requests using more tokens due to accumulating context <a class="yt-timestamp" data-t="01:06:00">[01:06:00]</a>.

## Direct Prompting with Large Language Models (LLMs)

In contrast to [[meta_gpt_multi_agent_collaborative_framework | MetaGPT]]'s multi-agent approach, directly prompting LLMs yielded more immediately functional code:

*   **GPT-4:** When asked directly to "write a basic python script for a gradio interface for a robot cat toy," GPT-4 provided a working script that included concepts like "direction" (forward, backward, left, right) and "speed" (slider input) <a class="yt-timestamp" data-t="01:00:11">[01:00:11]</a>. This code was functional and more specific than MetaGPT's output <a class="yt-timestamp" data-t="01:01:09">[01:01:09]</a>.
*   **Claude and Bard:** Similarly, Claude and Bard also produced working (though sometimes slightly confused) Gradio interfaces directly from the prompt, offering features like "walk," "meow," "sleep" actions (Claude) <a class="yt-timestamp" data-t="01:04:25">[01:04:25]</a> and identifying relevant concepts like GPIO and sensors <a class="yt-timestamp" data-t="01:05:03">[01:05:03]</a>.

LLMs were also able to critically review the MetaGPT-generated Flask code, identifying missing imports, unorthodox class structure, potential port conflicts with Gradio, and lack of error handling <a class="yt-timestamp" data-t="01:15:00">[01:15:00]</a>. This suggests that LLMs can provide actionable feedback and suggest improvements, indicating that an iterative loop (e.g., feeding the code back for review) might be more effective than a one-pass waterfall model <a class="yt-timestamp" data-t="01:18:12">[01:18:12]</a>.

## Evaluation and Benchmarking in AI Coding

[[meta_gpt_multi_agent_collaborative_framework | MetaGPT]]'s performance is evaluated using [[evaluation of AI coding through benchmarks | benchmarks]] such as HumanEval and MBPP <a class="yt-timestamp" data-t="01:34:44">[01:34:44]</a>.
*   **HumanEval:** A problem-solving dataset from "Evaluating Large Language Models Trained on Code," measuring functional correctness for synthesizing programs from docstrings (e.g., writing a function based on its description) <a class="yt-timestamp" data-t="01:34:51">[01:34:51]</a>.
*   **MBPP (Mostly Basic Python Problems):** A dataset of 1,000 crowdsourced Python programming problems designed to be solvable by entry-level programmers <a class="yt-timestamp" data-t="01:35:26">[01:35:26]</a>.

The use of these [[evaluation of AI coding through benchmarks | benchmarks]] is problematic because they primarily evaluate the ability to write individual functions rather than to design and implement an entire software system <a class="yt-timestamp" data-t="01:36:31">[01:36:31]</a>. While MetaGPT claims to outperform GPT-4 on these benchmarks, the practical demonstration suggests that its multi-agent system, despite generating extensive "context" (thousands of tokens), results in less functional code than a direct prompt to GPT-4 for a system-level task <a class="yt-timestamp" data-t="01:38:11">[01:38:11]</a>. This raises questions about the relevance of the chosen benchmarks for evaluating complex, collaborative software engineering capabilities <a class="yt-timestamp" data-t="01:58:06">[01:58:06]</a>.

## Implications for Software Engineering and AI Agent Development

The comparison between direct LLM prompts and multi-agent frameworks like [[meta_gpt_multi_agent_collaborative_framework | MetaGPT]] highlights potential inefficiencies in traditional software development methodologies when applied to AI-driven processes. The argument is made that roles such as Product Manager, Architect, and Project Manager, which generate extensive documentation and context, may add unnecessary "fluff" and "confusion" to the process, leading to less effective code from the engineer <a class="yt-timestamp" data-t="01:25:00">[01:25:00]</a>.

The traditional "waterfall" structure, with its rigid roles and hierarchical flow, is criticized as potentially being a result of human bureaucracy and career progression incentives rather than the most optimal way to develop software <a class="yt-timestamp" data-t="01:58:51">[01:58:51]</a>. In an AI-driven future, where LLMs can directly generate functional code from high-level requirements and even critique their own output, the need for intermediary management and documentation roles may diminish <a class="yt-timestamp" data-t="01:31:00">[01:31:00]</a>. The ideal interaction with [[AI agents and automation | AI agents]] might shift towards direct, iterative conversations with fewer hierarchical layers, leading to more efficient and actionable outcomes <a class="yt-timestamp" data-t="01:13:15">[01:13:15]</a>.