---
title: Role of vision transformers in DINOv2
videoId: KSZiJ4k28b4
---

From: [[hu-po]] <br/> 

[[introduction_to_dinov2|DINOv2]], released by [[meta_ai_research | Meta AI Research]] in May 2023, represents a significant advancement in computer vision, focusing on the development of foundational models through unsupervised learning <a class="yt-timestamp" data-t="00:00:57">[00:00:57]</a>. This project builds upon its predecessor, DINO, by scaling up pre-training in terms of data, model size, and computational efficiency <a class="yt-timestamp" data-t="00:06:07">[00:06:07]</a>. A key aspect of [[introduction_to_dinov2|DINOv2]]'s success is its exclusive reliance on [[vision_transformers_and_their_efficiency | Vision Transformers (ViTs)]] as its backbone architecture <a class="yt-timestamp" data-t="00:02:21">[00:02:21]</a>.

## Vision Transformers as Foundational Models
The field of computer vision is transitioning from specialized architectures for tasks like segmentation or classification towards more generalized, multi-task foundational models <a class="yt-timestamp" data-t="00:01:40">[00:01:40]</a>, <a class="yt-timestamp" data-t="00:02:02">[00:02:02]</a>. [[vision_transformers_and_their_efficiency | Vision Transformers]] are at the forefront of this shift, demonstrating "supremacy" over older convolutional neural networks (ConvNets) as encoders <a class="yt-timestamp" data-t="00:02:23">[00:02:23]</a>, <a class="yt-timestamp" data-t="00:02:27">[00:02:27]</a>. [[introduction_to_dinov2|DINOv2]] leverages large [[vision_transformers_and_their_efficiency | ViTs]] (e.g., 1 billion parameters) designed to be task-agnostic, capable of generating all-purpose visual features usable across a huge variety of tasks without fine-tuning <a class="yt-timestamp" data-t="00:02:40">[00:02:40]</a>, <a class="yt-timestamp" data-t="00:04:45">[00:04:45]</a>, <a class="yt-timestamp" data-t="00:10:03">[00:10:03]</a>.

## Unsupervised Pre-training and Data Curation
[[introduction_to_dinov2|DINOv2]] employs an unsupervised method for training its [[vision_transformers_and_their_efficiency | ViTs]], specifically self-supervised learning <a class="yt-timestamp" data-t="00:01:32">[00:01:32]</a>, <a class="yt-timestamp" data-t="00:15:16">[00:15:16]</a>. This approach learns features from images alone, similar to pretext tasks in natural language processing that require no supervision <a class="yt-timestamp" data-t="00:10:20">[00:10:20]</a>, <a class="yt-timestamp" data-t="00:15:21">[00:15:21]</a>. Unlike some other foundational models like CLIP, [[introduction_to_dinov2|DINOv2]] is a pure image-only model, meaning its training does not involve text-guided pre-training or external metadata <a class="yt-timestamp" data-t="00:11:25">[00:11:25]</a>, <a class="yt-timestamp" data-t="00:25:27">[00:25:27]</a>. The researchers argue that text captions can limit the information retained about an image, particularly complex pixel-level details <a class="yt-timestamp" data-t="00:11:32">[00:11:32]</a>.

A crucial aspect of training these large [[vision_transformers_and_their_efficiency | ViTs]] is data quality. [[introduction_to_dinov2|DINOv2]] was trained on a small but diverse corpus of 142 million curated images, referred to as the LVD-142M dataset <a class="yt-timestamp" data-t="00:20:06">[00:20:06]</a>, <a class="yt-timestamp" data-t="00:24:42">[00:24:42]</a>. This contrasts with previous efforts that often used uncurated data, which typically leads to a significant drop in quality <a class="yt-timestamp" data-t="00:15:53">[00:15:53]</a>, <a class="yt-timestamp" data-t="00:57:54">[00:57:54]</a>. The data curation process for [[introduction_to_dinov2|DINOv2]] involved an automatic pipeline using image embeddings and cosine similarity for deduplication and retrieval, without manual annotation <a class="yt-timestamp" data-t="00:21:54">[00:21:54]</a>, <a class="yt-timestamp" data-t="00:25:27">[00:25:27]</a>, <a class="yt-timestamp" data-t="00:27:01">[00:27:01]</a>.

## Scaling and Distillation of ViTs
Training a [[vision_transformers_and_their_efficiency | ViT]] with 1 billion parameters is computationally intensive, requiring systems with multiple powerful GPUs (e.g., 12 A100s or 20 nodes with 8 V100 32GB GPUs) <a class="yt-timestamp" data-t="00:03:04">[00:03:04]</a>, <a class="yt-timestamp" data-t="00:30:20">[00:30:20]</a>. To make these models more practical for various applications, [[introduction_to_dinov2|DINOv2]] employs a distillation strategy. A very large [[vision_transformers_and_their_efficiency | ViT]] is trained first, and then smaller ViT models are distilled from it to mimic the larger model's outputs <a class="yt-timestamp" data-t="00:07:45">[00:07:45]</a>, <a class="yt-timestamp" data-t="00:08:06">[00:08:06]</a>, <a class="yt-timestamp" data-t="00:53:38">[00:53:38]</a>. This approach is highly efficient in terms of training and compute budget <a class="yt-timestamp" data-t="00:08:29">[00:08:29]</a>. Surprisingly, distilled models in [[introduction_to_dinov2|DINOv2]] sometimes even outperform their larger teacher models on certain benchmarks <a class="yt-timestamp" data-t="01:05:06">[01:05:06]</a>, <a class="yt-timestamp" data-t="01:05:47">[01:05:47]</a>.

## Technical Innovations for ViT Training
Several technical contributions in [[introduction_to_dinov2|DINOv2]] aimed at accelerating and stabilizing [[vision_transformers_and_their_efficiency | ViT]] training at scale <a class="yt-timestamp" data-t="00:07:07">[00:07:07]</a>, <a class="yt-timestamp" data-t="00:17:33">[00:17:33]</a>:

*   **Faster and Memory-Efficient Attention**: The researchers implemented their own version of Flash Attention to optimize memory usage and speed in self-attention layers, which are critical for [[vision_transformers_and_their_efficiency | Transformers]] due to their quadratic memory growth with sequence length <a class="yt-timestamp" data-t="00:41:46">[00:41:46]</a>, <a class="yt-timestamp" data-t="00:41:50">[00:41:50]</a>.
*   **Hardware-Specific Architecture Design**: Hyperparameters for the ViT architecture, such as embedding dimension and number of heads, were chosen to maximize compute efficiency based on GPU hardware specifics <a class="yt-timestamp" data-t="00:42:00">[00:42:00]</a>, <a class="yt-timestamp" data-t="00:43:14">[00:43:14]</a>.
*   **Efficient Stochastic Depth**: An improved implementation of stochastic depth skips computation for dropped residuals, leading to memory and compute savings <a class="yt-timestamp" data-t="00:44:45">[00:44:45]</a>.
*   **Fully Sharded Data Parallel (FSDP)**: Pytorch's FSDP was used to split model replicas across GPUs, overcoming single GPU memory limitations and reducing cross-GPU communication costs, a common bottleneck in large-scale training <a class="yt-timestamp" data-t="00:50:11">[00:50:11]</a>, <a class="yt-timestamp" data-t="00:50:40">[00:50:40]</a>.
*   **Mixed Precision Training**: While weights are stored in float32, broadcasting and gradient reduction are performed in float16 precision to further reduce communication costs <a class="yt-timestamp" data-t="00:52:15">[00:52:15]</a>.
*   **Adaptive Resolution Training**: [[introduction_to_dinov2|DINOv2]] trains [[vision_transformers_and_their_efficiency | ViTs]] at lower resolutions initially, then switches to higher resolutions (e.g., 518x518) for a short period at the end of pre-training <a class="yt-timestamp" data-t="00:39:45">[00:39:45]</a>. This curriculum learning approach significantly reduces compute cost while maintaining high performance, especially for pixel-level tasks where small objects would disappear at low resolutions <a class="yt-timestamp" data-t="00:38:45">[00:38:45]</a>, <a class="yt-timestamp" data-t="01:11:15">[01:11:15]</a>.

## Emergent Properties of ViT Features
[[introduction_to_dinov2|DINOv2]]'s self-supervised training enables its [[vision_transformers_and_their_efficiency | ViTs]] to learn rich, emergent properties. Principal Component Analysis (PCA) applied to the patch features of the [[vision_transformers_and_their_efficiency | ViT]] reveals that the model implicitly learns to separate foreground from background <a class="yt-timestamp" data-t="01:44:19">[01:44:19]</a>. Furthermore, it demonstrates an emergent ability to identify and differentiate object parts (e.g., head, legs) across various images, even those with different poses, styles, or unusual objects like statues <a class="yt-timestamp" data-t="01:31:53">[01:31:53]</a>, <a class="yt-timestamp" data-t="01:46:50">[01:46:50]</a>, <a class="yt-timestamp" data-t="01:47:28">[01:47:28]</a>. This capability is seen as a sign of "emergent intelligence" in foundational models <a class="yt-timestamp" data-t="01:47:35">[01:47:35]</a>. The model's features also exhibit robustness to style variations and large changes in pose <a class="yt-timestamp" data-t="01:48:27">[01:48:27]</a>.

## Performance Across Downstream Tasks
The [[vision_transformers_and_their_efficiency | ViT]] features from [[introduction_to_dinov2|DINOv2]] are highly "transferable" and can be used as "frozen features" for new tasks without the need for fine-tuning the feature encoder <a class="yt-timestamp" data-t="02:00:51">[02:00:51]</a>, <a class="yt-timestamp" data-t="02:04:14">[02:04:14]</a>. This implies that pushing gradients into the encoder would make it worse, suggesting the model is so powerful that it's found a deeply optimal local minimum <a class="yt-timestamp" data-t="02:07:34">[02:07:34]</a>, <a class="yt-timestamp" data-t="02:09:29">[02:09:29]</a>.

[[introduction_to_dinov2|DINOv2]]'s [[vision_transformers_and_their_efficiency | ViT]] models were evaluated on a wide range of computer vision benchmarks, including:
*   **Image Classification**: Competitive with or outperforming other self-supervised models on ImageNet classification, and showing strong generalization across various domain generalization benchmarks <a class="yt-timestamp" data-t="01:14:40">[01:14:40]</a>, <a class="yt-timestamp" data-t="01:25:23">[01:25:23]</a>, <a class="yt-timestamp" data-t="01:28:44">[01:28:44]</a>.
*   **Video Action Recognition**: Sets a new state-of-the-art among self-supervised approaches, particularly on complex datasets like SSV2 <a class="yt-timestamp" data-t="01:27:11">[01:27:11]</a>.
*   **Instance Recognition**: Outperforms other models on Landmark recognition benchmarks (Paris and Oxford) and art image datasets (Met) <a class="yt-timestamp" data-t="01:32:44">[01:32:44]</a>.
*   **Semantic Segmentation**: Achieves strong results, even on out-of-distribution examples like drawings or paintings <a class="yt-timestamp" data-t="01:42:47">[01:42:47]</a>, <a class="yt-timestamp" data-t="01:43:32">[01:43:32]</a>.
*   **Monocular Depth Estimation**: Produces significantly smoother and cleaner depth estimations compared to models like CLIP <a class="yt-timestamp" data-t="01:38:38">[01:38:38]</a>, <a class="yt-timestamp" data-t="01:40:31">[01:40:31]</a>.

These results demonstrate that [[introduction_to_dinov2|DINOv2]]'s [[vision_transformers_and_their_efficiency | ViT]] features are robust and transferable across different domains and tasks, often performing on par with or better than weekly supervised or other self-supervised models <a class="yt-timestamp" data-t="01:15:02">[01:15:02]</a>.