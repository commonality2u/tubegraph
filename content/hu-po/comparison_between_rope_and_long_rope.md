---
title: Comparison between rope and long rope
videoId: PFxi6SmozZ4
---

From: [[hu-po]] <br/> 

[[long_rope_paper_and_its_significance | Long Rope]] is a method that significantly extends the context length of Large Language Models (LLMs), building upon and remixing concepts from **Rope (Rotary Position Embeddings)** and subsequent advancements like Position Interpolation (PI), NTK, and Yarn <a class="yt-timestamp" data-t="03:36:38">[03:36:38]</a>. The development of Long Rope aims to achieve state-of-the-art context lengths, similar to those seen in models like Gemini 1.5 Pro, which has a 10 million token context window <a class="yt-timestamp" data-t="02:24:07">[02:24:07]</a>, <a class="yt-timestamp" data-t="02:51:00">[02:51:00]</a>.

## Rotary Position Embeddings (Rope)

Rotary Position Embeddings (RoPE), often simply referred to as Rope, originated from the 2021 "RoFormer" paper by Jo ye Technology Co and Shenzhen <a class="yt-timestamp" data-t="01:12:57">[01:12:57]</a>, <a class="yt-timestamp" data-t="01:13:08">[01:13:08]</a>. It is a type of positional embedding used in Transformer models, notably in Llama models <a class="yt-timestamp" data-t="02:33:33">[02:33:33]</a>.

### Purpose and Mechanism
The core idea behind Rope is to model the dependency between elements at different positions in a sequence. It encodes the absolute position within a rotation matrix while incorporating the relative position dependency in the self-attention formulation <a class="yt-timestamp" data-t="01:13:30">[01:13:30]</a>, <a class="yt-timestamp" data-t="01:16:03">[01:16:03]</a>.

Rope works by rotating the semantic embedding vectors (e.g., word embeddings) by some angle that depends on their position in the sequence <a class="yt-timestamp" data-t="01:19:18">[01:19:18]</a>, <a class="yt-timestamp" data-t="01:20:47">[01:20:47]</a>. This rotation modifies the dot product between query (Q) and key (K) vectors in the self-attention mechanism, which is central to how Transformers process information <a class="yt-timestamp" data-t="01:19:50">[01:19:50]</a>. The rotation is achieved using sine and cosine functions, often conceptualized through Euler's identity, which links exponential functions to trigonometric functions in the complex plane <a class="yt-timestamp" data-t="01:26:54">[01:26:54]</a>.

### Key Properties
*   **Flexibility with respect to sequence length**: Rope is designed to be continuous, allowing for evaluation at arbitrary positions and thus adaptable to varying sequence lengths <a class="yt-timestamp" data-t="01:28:28">[01:28:28]</a>, <a class="yt-timestamp" data-t="03:29:40">[03:29:40]</a>.
*   **Decaying inter-token dependency**: The dot product (similarity) between tokens decreases as their relative distance increases. This property aligns with the intuition that closer tokens should have stronger connections <a class="yt-timestamp" data-t="01:16:30">[01:16:30]</a>, <a class="yt-timestamp" data-t="01:23:23">[01:23:23]</a>, <a class="yt-timestamp" data-t="02:10:00">[02:10:00]</a>.
*   **Relative position encoding**: It enables the self-attention mechanism to inherently understand relative positions <a class="yt-timestamp" data-t="01:25:00">[01:25:00]</a>.

Rope position embeddings are largely "hand-designed" or engineered, meaning their mathematical formulation (e.g., the base frequency of 10,000) is determined by human intuition and experimentation rather than being learned by the model <a class="yt-timestamp" data-t="01:17:58">[01:17:58]</a>, <a class="yt-timestamp" data-t="01:45:00">[01:45:00]</a>.

## Long Rope

Long Rope, a 2024 paper from Microsoft Research, is presented as a "remix on a remix on a remix" of position embeddings <a class="yt-timestamp" data-t="03:36:38">[03:36:38]</a>. It extends the context window of pre-trained LLMs to an unprecedented 2048k tokens (or 256k initially) <a class="yt-timestamp" data-t="03:28:40">[03:28:40]</a>.

### Innovations and Mechanism
Long Rope introduces three key innovations:

1.  **Exploiting Non-Uniformities via Evolutionary Search**: Previous methods like Yarn made arbitrary choices for how to interpolate different frequency-based dimensions of Rope (e.g., splitting dimensions into three groups with different interpolation strategies) <a class="yt-timestamp" data-t="01:18:41">[01:18:41]</a>, <a class="yt-timestamp" data-t="01:40:50">[01:40:50]</a>. Long Rope replaces these human-designed rules with an efficient evolutionary search algorithm. This search discovers optimal, non-uniform rescale factors (Lambda I) for Rope's rotation angles across different dimensions and token positions, guided by perplexity as the fitness metric <a class="yt-timestamp" data-t="01:41:17">[01:41:17]</a>, <a class="yt-timestamp" data-t="01:58:11">[01:58:11]</a>.
2.  **Progressive Extension Strategy**: To fine-tune LLMs for extremely long contexts, Long Rope uses a progressive extension strategy. Models are first fine-tuned on smaller context lengths (e.g., 256k) and then gradually extended to longer contexts (e.g., 2048k) in subsequent fine-tuning stages <a class="yt-timestamp" data-t="01:04:04">[01:04:04]</a>, <a class="yt-timestamp" data-t="01:03:01">[01:03:01]</a>.
3.  **Non-Interpolation for Initial Tokens**: Long Rope hypothesizes that the initial `n_hat` tokens in the input sequence should undergo *less* or no interpolation <a class="yt-timestamp" data-t="01:01:17">[01:01:17]</a>. This is because these initial tokens often receive large attention scores and are crucial for maintaining performance <a class="yt-timestamp" data-t="01:01:38">[01:01:38]</a>. The optimal `n_hat` value (e.g., 256 tokens) is also determined through the evolutionary search <a class="yt-timestamp" data-t="01:02:40">[01:02:40]</a>.

### Performance
Long Rope demonstrates significant improvements over previous methods like PI and Yarn.
*   **Perplexity**: It maintains low perplexity even at very large context window sizes (up to 2048k tokens), where other models' perplexity "explodes" <a class="yt-timestamp" data-t="01:13:52">[01:13:52]</a>.
*   **Pass Key Retrieval Accuracy**: It achieves nearly 100% accuracy in "needle in a haystack" tasks (finding a specific key in a long document) across extended context lengths, while other models fail <a class="yt-timestamp" data-t="01:13:59">[01:13:59]</a>, <a class="yt-timestamp" data-t="01:15:01">[01:15:01]</a>.
*   **Benchmark Performance**: While there is a slight loss in performance on traditional short-context benchmarks (e.g., MMLU, HellaSwag), this is considered acceptable given the massive increase in context length capabilities <a class="yt-timestamp" data-t="01:13:56">[01:13:56]</a>.

Long Rope allows for an 8x context window extension without any fine-tuning and much larger extensions (2048k) with progressive fine-tuning, all while retaining the original model architecture <a class="yt-timestamp" data-t="01:09:06">[01:09:06]</a>, <a class="yt-timestamp" data-t="01:21:23">[01:21:23]</a>.

## Comparison Summary

| Feature              | Rope (Rotary Position Embeddings)                                                                                                                                                                                                                                 | [[long_rope_paper_and_its_significance | Long Rope]]                                                                                                                                                                                                                                                                                                                                                                                                                   |
| :------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Origin**           | RoFormer paper (2021), Jo ye Technology Co. <a class="yt-timestamp" data-t="01:13:30">[01:13:30]</a>                                                                                                                                                          | Microsoft Research (2024) <a class="yt-timestamp" data-t="03:36:38">[03:36:38]</a>                                                                                                                                                                                                                                                                                                                                                                |
| **Base Concept**     | Introduces a position-dependent rotation to embedding vectors, leveraging sine/cosine functions for relative position encoding. <a class="yt-timestamp" data-t="01:36:02">[01:36:02]</a>                                                                            | Builds on Rope by optimizing its extension to longer contexts. <a class="yt-timestamp" data-t="03:36:38">[03:36:38]</a>                                                                                                                                                                                                                                                                                                                         |
| **Extension to Long Contexts** | Has inherent flexibility but requires specific interpolation strategies (like PI, NTK, Yarn) for practical long context extension. <a class="yt-timestamp" data-t="01:28:28">[01:28:28]</a>                                                                | Proposes a refined method for extending Rope by leveraging evolutionary search to find optimal scaling factors and a progressive fine-tuning strategy. <a class="yt-timestamp" data-t="03:36:38">[03:36:38]</a>, <a class="yt-timestamp" data-t="01:04:04">[01:04:04]</a>, <a class="yt-timestamp" data-t="01:58:11">[01:58:11]</a> |
| **Parameter Design** | Hand-engineered or empirically derived parameters (e.g., base frequency of 10,000 in RoPE <a class="yt-timestamp" data-t="01:45:00">[01:45:00]</a>, or arbitrary frequency bins in Yarn <a class="yt-timestamp" data-t="01:40:50">[01:40:50]</a>).                               | Automatically discovers optimal "non-uniform" rescale factors and an `n_hat` (initial tokens not interpolated) through an efficient evolutionary search. <a class="yt-timestamp" data-t="01:58:11">[01:58:11]</a>, <a class="yt-timestamp" data-t="01:01:17">[01:01:17]</a>                                         |
| **Performance Gain** | A foundational improvement for Transformers' positional understanding.                                                                                                                                                                                      | Significantly outperforms previous Rope extension methods (PI, Yarn) in perplexity and "needle in a haystack" tasks at extreme context lengths (up to 2048k tokens). <a class="yt-timestamp" data-t="01:13:52">[01:13:52]</a>, <a class="yt-timestamp" data-t="01:13:59">[01:13:59]</a>             |
| **Training**         | Can be used directly or with simple fine-tuning.                                                                                                                                                                                                            | Achieves 8x context extension without fine-tuning, and massive extensions (2048k) with progressive fine-tuning on RedPajama dataset <a class="yt-timestamp" data-t="01:21:23">[01:21:23]</a>, <a class="yt-timestamp" data-t="01:14:15">[01:14:15]</a>.                             |
| **Hypothesis (General ML)** | Represents a "hand-designed" feature, which may eventually be surpassed by purely learned approaches in deep learning (Rich Sutton's "Bitter Lesson" <a class="yt-timestamp" data-t="01:22:55">[01:22:55]</a>).                                             | While using search, still operates within the paradigm of hand-designed positional embeddings, pushing the limits of that approach through automated parameter selection. <a class="yt-timestamp" data-t="01:47:22">[01:47:22]</a>                                                                                                                                                                                                                                      |

The emergence of Long Rope suggests a continued effort to achieve extremely long context windows in LLMs, possibly hinting at techniques used by closed models like Gemini 1.5 <a class="yt-timestamp" data-t="02:24:07">[02:24:07]</a>, <a class="yt-timestamp" data-t="03:31:50">[03:31:50]</a>. However, the reliance on increasingly complex heuristic designs through methods like Long Rope leads to questions about whether future breakthroughs will come from further engineering or from allowing models to learn these positional encodings naturally through increased scale and computation <a class="yt-timestamp" data-t="01:27:11">[01:27:11]</a>, <a class="yt-timestamp" data-t="01:47:22">[01:47:22]</a>.