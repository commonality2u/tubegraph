---
title: Comparison with other multimodal AI models
videoId: BJ98vicRYHg
---

From: [[hu-po]] <br/> 

The Large Language and Vision Assistant (LLaVA) is considered a leading open-source vision language model, offering a robust alternative to proprietary models like [[multimodal_large_language_models_vs_vision_language_models | GPT-4 Vision]] and similar offerings from Google <a class="yt-timestamp" data-t="00:03:16">[00:03:16]</a>.

## Performance Benchmarks
LLaVA 1.5, the more recent iteration, achieves state-of-the-art (SOTA) results across 11 different [[multimodal_model_benchmarks | benchmarks]] <a class="yt-timestamp" data-t="00:06:05">[00:06:05]</a> <a class="yt-timestamp" data-t="01:00:15">[01:00:15]</a>. This performance is achieved with only public data and approximately one day of training on a single A100 node <a class="yt-timestamp" data-t="00:06:13">[00:06:13]</a>.

Key comparisons include:
*   **GPT-4 Vision**: LLaVA demonstrates impressive [[multimodal_capabilities_in_language_models | multimodal chat abilities]], sometimes exhibiting behaviors akin to [[multimodal_large_language_models_vs_vision_language_models | GPT-4 Vision]] on unseen images and instructions <a class="yt-timestamp" data-t="00:18:38">[00:18:38]</a>. On a synthetic [[multimodal_capabilities_in_language_models | multimodal instruction following data set]], LLaVA achieves an 85% relative score compared to [[multimodal_large_language_models_vs_vision_language_models | GPT-4]] <a class="yt-timestamp" data-t="00:19:27">[00:19:27]</a>. When fine-tuned on the ScienceQA [[multimodal_model_benchmarks | benchmark]], LLaVA and [[multimodal_large_language_models_vs_vision_language_models | GPT-4]] achieve a new SOTA accuracy of 92% <a class="yt-timestamp" data-t="00:19:41">[00:19:41]</a>, surpassing prior models like GPT-3.5 Chain of Thought (75%) and Multimodal Chain of Thought (91%) <a class="yt-timestamp" data-t="00:20:14">[00:20:14]</a>.

*   **BLIP-2 and OpenFlamingo**: LLaVA 1.5 shows significant improvements over models like BLIP-2 and OpenFlamingo <a class="yt-timestamp" data-t="00:31:08">[00:31:08]</a> <a class="yt-timestamp" data-t="00:57:51">[00:57:51]</a>. In a comparison across multiple benchmarks (GQA, MME, MM-Vet), LLaVA with 7 billion parameters (7B) outperforms InstructBLIP and continues to improve with added data, prompt formatting, and resolution increases <a class="yt-timestamp" data-t="01:04:10">[01:04:10]</a>. LLaVA 1.5 even surpasses models like Flamingo with billions of trainable parameters for cross-modal connection <a class="yt-timestamp" data-t="01:34:06">[01:34:06]</a>.

*   **InstructBLIP**: LLaVA 1.5 surpasses InstructBLIP in performance, partially due to LLaVA's fine-tuning on academic task-oriented VQA data with simple response formatting <a class="yt-timestamp" data-t="01:14:02">[01:14:02]</a> <a class="yt-timestamp" data-t="01:30:37">[01:30:37]</a>. InstructBLIP's limitations might stem from ambiguous prompts and insufficient fine-tuning on diverse response formats <a class="yt-timestamp" data-t="01:09:46">[01:09:46]</a>.

## Architectural and Training Philosophy
LLaVA's success comes from a relatively simple [[building_multimodal_models | architecture]] and an efficient training recipe. It connects a pre-trained vision encoder (OpenAI's [[alternative_uses_for_ai_models | CLIP]] ViT-L/14) with a large language model (LLaMA, specifically Vicuna 1.5, which is based on [[comparison_of_language_models_in_coding_tasks | LLaMA]] 1 and fine-tuned on [[multimodal_large_language_models_vs_vision_language_models | GPT-4]] answers) using a simple Multi-Layer Perceptron (MLP) projection layer <a class="yt-timestamp" data-t="00:48:47">[00:48:47]</a> <a class="yt-timestamp" data-t="00:59:21">[00:59:21]</a>.

In contrast to models that train specialized visual resamplers on hundreds of millions or billions of uses, LLaVA uses the simplest [[building_multimodal_models | architecture design]] for LLMs and only requires training a simple fully connected projection layer on top of merely 600K image pairs <a class="yt-timestamp" data-t="01:03:15">[01:03:15]</a>. This highlights a shift in machine learning research from complex model architectures to the effective use of pre-trained models and strategic data mixture <a class="yt-timestamp" data-t="01:05:42">[01:05:42]</a> <a class="yt-timestamp" data-t="01:06:30">[01:06:30]</a>.

While LLaVA leverages pre-trained models like [[alternative_uses_for_ai_models | CLIP]] (trained by OpenAI) and Vicuna (based on LLaMA from Facebook, fine-tuned on [[multimodal_large_language_models_vs_vision_language_models | GPT-4]] answers), its own fine-tuning process is remarkably efficient <a class="yt-timestamp" data-t="00:17:14">[00:17:14]</a> <a class="yt-timestamp" data-t="01:00:42">[01:00:42]</a>. The final model is fine-tuned for only about one day on an 8-GPU A100 server rack <a class="yt-timestamp" data-t="01:01:04">[01:01:04]</a>, making SOTA LMM research more accessible <a class="yt-timestamp" data-t="01:11:10">[01:11:10]</a>.

## Potential Future Comparisons
The LLaVA research team suggests exploring connections with other powerful vision models like Segment Anything Model (SAM) <a class="yt-timestamp" data-t="01:34:26">[01:34:26]</a>. This could involve replacing [[alternative_uses_for_ai_models | CLIP]] with SAM's vision encoder or even using an ensemble of multiple vision encoders to feed into the language model <a class="yt-timestamp" data-t="01:34:36">[01:34:36]</a>.

Comparing LLaVA's approach (combining pre-trained vision and language components) with end-to-end models like DeepMind's Gate or RT (Robotic Transformer) raises questions about long-term scaling <a class="yt-timestamp" data-t="01:43:19">[01:43:19]</a>. While end-to-end training might theoretically yield better results with infinite compute <a class="yt-timestamp" data-t="01:48:03">[01:48:03]</a>, the practical efficiency and strong performance of LLaVA's composite approach offer a compelling argument for its continued relevance <a class="yt-timestamp" data-t="01:49:06">[01:49:06]</a>. The organizational incentives within large companies may also favor approaches that maximize results with less compute, such as LLaVA's model <a class="yt-timestamp" data-t="01:51:08">[01:51:08]</a>.