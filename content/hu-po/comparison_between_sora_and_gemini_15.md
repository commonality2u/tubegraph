---
title: Comparison between Sora and Gemini 15
videoId: dPonS4kISPM
---

From: [[hu-po]] <br/> 

This article explores two prominent AI models, Sora and Gemini 1.5, which were released in close succession by OpenAI and Google DeepMind, respectively. While they serve different purposes, both represent significant advancements in their fields <a class="yt-timestamp" data-t="00:02:21">[00:02:21]</a>.

## Sora: State-of-the-Art Video Generation

Sora is OpenAI's new state-of-the-art video generation model <a class="yt-timestamp" data-t="00:02:56">[00:02:56]</a> <a class="yt-timestamp" data-t="01:36:25">[01:36:25]</a>. OpenAI released a "technical report" rather than a traditional paper, featuring videos demonstrating the model's capabilities <a class="yt-timestamp" data-t="00:03:07">[00:03:07]</a> <a class="yt-timestamp" data-t="01:37:51">[01:37:51]</a>.

### Capabilities and Impact
Sora generates videos that are significantly longer, of higher quality, and feature superior motion compared to previous models <a class="yt-timestamp" data-t="00:05:50">[00:05:50]</a> <a class="yt-timestamp" data-t="01:37:36">[01:37:36]</a>. This level of video generation quality is seen as a "step function" improvement, potentially enabling new advancements in 3D generative content, such as text-to-3D models like Gaussian Splats or NeRFs <a class="yt-timestamp" data-t="00:06:42">[00:06:42]</a> <a class="yt-timestamp" data-t="00:07:54">[00:07:54]</a>. It is expected to lead to significant improvements in generative 3D within approximately six months <a class="yt-timestamp" data-t="00:07:56">[00:07:56]</a>.

### Key Contributors and Their Backgrounds
Sam Altman, CEO of OpenAI, credited three individuals for Sora's development:
*   **Tim Brooks**: A research scientist at OpenAI and a University of California Berkeley PhD <a class="yt-timestamp" data-t="00:08:29">[00:08:29]</a>. He is known for his work on [[text_to_image_models | InstructPix2Pix]], a diffusion model for image editing that uses stable diffusion as a generator and GPT-3 as a prompt engineer to create training datasets <a class="yt-timestamp" data-t="00:09:03">[00:09:03]</a> <a class="yt-timestamp" data-t="00:11:15">[00:11:15]</a>.
*   **Bill Peebles (William Peebles)**: Also a research scientist at OpenAI and a Berkeley PhD <a class="yt-timestamp" data-t="00:12:27">[00:12:27]</a> <a class="yt-timestamp" data-t="00:12:35">[00:12:35]</a>. He is known for the "Scalable Diffusion Models with Transformers" paper, which replaces the U-Net backbone in latent diffusion models with a Transformer <a class="yt-timestamp" data-t="00:13:02">[00:13:02]</a> <a class="yt-timestamp" data-t="00:13:13">[00:13:13]</a> <a class="yt-timestamp" data-t="00:16:08">[00:16:08]</a>. This architecture is called a Diffusion Transformer (DiT) <a class="yt-timestamp" data-t="00:16:26">[00:16:26]</a>.
*   **Aditya Ramesh (Model Mechanic)**: A more senior figure with extensive citations, known for papers on image generation, including "Hierarchical Text-Conditional Image Generation with CLIP Latents" and "Zero-Shot Text-to-Image Generation" <a class="yt-timestamp" data-t="00:19:42">[00:19:42]</a> <a class="yt-timestamp" data-t="00:20:14">[00:20:14]</a>. His work includes using Transformers that autoregressively model images and text as tokens in a single data stream, often employing a discrete variational autoencoder (VQ-VAE) <a class="yt-timestamp" data-t="00:21:39">[00:21:39]</a> <a class="yt-timestamp" data-t="00:21:49">[00:21:49]</a>.

### Underlying Architecture and Data
Sora is largely believed to be a latent diffusion model, where denoising occurs in a "SpaceTime latent space" <a class="yt-timestamp" data-t="00:44:00">[00:44:00]</a>. It likely utilizes a Transformer-based autoencoder that can encode entire videos at once, rather than frame by frame <a class="yt-timestamp" data-t="00:44:08">[00:44:08]</a>. The model transforms visual data into "SpaceTime patches" or "visual tokens," which are then fed into a Transformer. This patching allows Sora to train on and generate videos of variable resolutions and lengths <a class="yt-timestamp" data-t="00:31:50">[00:31:50]</a> <a class="yt-timestamp" data-t="00:32:01">[00:32:01]</a>.

A key technique used in training is "recaptioning," where language models like GPT-4V expand short user prompts into longer, more detailed captions to augment the training data <a class="yt-timestamp" data-t="00:35:37">[00:35:37]</a> <a class="yt-timestamp" data-t="01:39:39">[01:39:39]</a>. There is speculation that Sora's training data includes video game footage, which might explain some "video game-looking" artifacts in generated content, as it would implicitly learn the physics models present in those games <a class="yt-timestamp" data-t="00:37:26">[00:37:26]</a> <a class="yt-timestamp" data-t="02:05:00">[02:05:00]</a>. However, OpenAI itself likely did not explicitly generate synthetic data using game engines due to the specialized skill sets required <a class="yt-timestamp" data-t="00:38:38">[00:38:38]</a>.

### Limitations and "World Simulators"
While impressive, Sora is not a perfect "world simulator" <a class="yt-timestamp" data-t="00:41:01">[00:41:01]</a>. It exhibits "weirdness," such as illogical physics or perspective errors <a class="yt-timestamp" data-t="00:42:19">[00:42:19]</a>. An example highlighted is a person's legs "flipping" unnaturally in a generated video <a class="yt-timestamp" data-t="00:42:34">[00:42:34]</a>. The "physics" it learns are implicit, derived from the vast amount of video data it consumes, which can include unrealistic video game physics or drawn content <a class="yt-timestamp" data-t="01:24:09">[01:24:09]</a> <a class="yt-timestamp" data-t="01:25:21">[01:25:21]</a>.

## Gemini 1.5: Multimodal Understanding and Long Context

Gemini 1.5 is Google DeepMind's latest multimodal model, focused on understanding rather than generation <a class="yt-timestamp" data-t="01:40:10">[01:40:10]</a>.

### Capabilities and Benchmarks
Gemini 1.5 is described as a "highly compute-efficient multimodal mixture of experts" <a class="yt-timestamp" data-t="00:45:46">[00:45:46]</a>. It excels at reasoning over extremely long contexts, achieving near-perfect recall on long context retrieval tasks <a class="yt-timestamp" data-t="00:46:53">[00:46:53]</a>. It can consume up to 10 million tokens, a generational leap over existing models like GPT-4 Turbo, which has a 128k token limit <a class="yt-timestamp" data-t="00:47:15">[00:47:15]</a> <a class="yt-timestamp" data-t="00:47:50">[00:47:50]</a> <a class="yt-timestamp" data-t="01:40:24">[01:40:24]</a>.

This capability is demonstrated through the "Needle in the Haystack" benchmark, where the model must find specific information (the "needle") within a vast amount of data (the "haystack") <a class="yt-timestamp" data-t="00:49:05">[00:49:05]</a>. For example, it can analyze an entire novel (like Les Mis√©rables) and answer questions about specific images from the book, correlating visual tokens with text <a class="yt-timestamp" data-t="00:48:12">[00:48:12]</a> <a class="yt-timestamp" data-t="00:50:45">[00:50:45]</a>. It also demonstrates state-of-the-art speech recognition, marginally outperforming Whisper V3 <a class="yt-timestamp" data-t="00:56:31">[00:56:31]</a>.

### Technical Speculation
Google DeepMind's technical report provides limited detail on the model's architecture or training data <a class="yt-timestamp" data-t="00:51:11">[00:51:11]</a> <a class="yt-timestamp" data-t="01:41:20">[01:41:20]</a>. However, two key aspects are mentioned or speculated upon:

*   **Mixture of Experts (MoE)**: Gemini 1.5 is explicitly described as a "mixture of experts" model <a class="yt-timestamp" data-t="00:57:52">[00:57:52]</a>. Given a recent Google DeepMind paper on MoE for deep reinforcement learning, it's hypothesized that Gemini 1.5 uses "Soft MoE" <a class="yt-timestamp" data-t="00:58:15">[00:58:15]</a> <a class="yt-timestamp" data-t="01:00:50">[01:00:50]</a>. Soft MoE involves a router network sending weighted averages of input tokens to different "experts" (e.g., MLPs), rather than hard assignments <a class="yt-timestamp" data-t="01:00:59">[01:00:59]</a>.

*   **Ring Attention**: The unprecedented long context window of Gemini 1.5 is hypothesized to stem from a technique similar to "Ring Attention" <a class="yt-timestamp" data-t="01:05:11">[01:05:11]</a>. A separate paper, "World Model on Million-Length Video and Language with Ring Attention," released around the same time, details a Transformer that scales context size arbitrarily and achieves strong [[Performance analysis and benchmarks | performance]] on needle retrieval tasks with multimodal data <a class="yt-timestamp" data-t="01:05:25">[01:05:25]</a> <a class="yt-timestamp" data-t="01:08:01">[01:08:01]</a>. Ring Attention operates by splitting a sequence into blocks, distributing them across a "ring of hosts" (GPUs), and shuffling key-value blocks between hosts to compute attention efficiently <a class="yt-timestamp" data-t="01:10:52">[01:10:52]</a> <a class="yt-timestamp" data-t="01:12:01">[01:12:01]</a>.

*   **Multimodality**: Gemini 1.5 consumes multiple modalities (text, image, audio, video) by turning raw data into sequences of tokens <a class="yt-timestamp" data-t="00:46:36">[00:46:36]</a>. Specific "new tokens" (e.g., `end of text`, `start of image`) are introduced to explicitly signal changes in modality to the model <a class="yt-timestamp" data-t="01:09:42">[01:09:42]</a>.

### Compute and Data Transparency
Gemini 1.5 Pro was trained on "multiple 4096-chip PODs of Google TPU V4 accelerators" <a class="yt-timestamp" data-t="00:51:37">[00:51:37]</a>. Google's TPUs are custom in-house GPUs designed for AI workloads <a class="yt-timestamp" data-t="00:51:43">[00:51:43]</a>. Like OpenAI, Google DeepMind remains vague about its training data, citing "a variety of multimodal and multilingual data" <a class="yt-timestamp" data-t="00:52:14">[00:52:14]</a>. This lack of transparency is noted as a concerning trend in AI, potentially due to legal concerns about copyrighted material <a class="yt-timestamp" data-t="00:53:16">[00:53:16]</a>.

## Overall Comparison and Impact

Both Sora and Gemini 1.5 represent significant milestones. Their release timing was notable, with OpenAI's Sora announcement closely following Google's Gemini 1.5, interpreted as a strategic move by Sam Altman to "steal their thunder" <a class="yt-timestamp" data-t="00:03:49">[00:03:49]</a> <a class="yt-timestamp" data-t="01:45:02">[01:45:02]</a>.

While Sora's video generation is visually stunning and creates immediate hype, Gemini 1.5's technical advancements in long context understanding and perfect recall are argued to be more fundamentally "transformative" <a class="yt-timestamp" data-t="01:44:06">[01:44:06]</a> <a class="yt-timestamp" data-t="01:44:08">[01:44:08]</a>. Gemini's ability to process massive amounts of information could significantly disrupt fields reliant on Retrieval Augmented Generation (RAG) systems, as a single model could soon handle an individual's entire digital history as context <a class="yt-timestamp" data-t="01:47:21">[01:47:21]</a>. Sora, on the other hand, is poised to revolutionize video and 3D content creation <a class="yt-timestamp" data-t="01:48:51">[01:48:51]</a>.

In essence, Gemini 1.5 is seen as technically more impressive, while Sora generated more public hype due to its visual modality <a class="yt-timestamp" data-t="01:47:08">[01:47:08]</a>. Both models underscore the increasing importance of scale (compute and data) in achieving state-of-the-art AI capabilities, primarily by tech giants like OpenAI and Google <a class="yt-timestamp" data-t="00:35:01">[00:35:01]</a> <a class="yt-timestamp" data-t="01:50:30">[01:50:30]</a>.