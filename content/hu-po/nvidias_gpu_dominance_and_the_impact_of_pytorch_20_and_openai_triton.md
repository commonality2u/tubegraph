---
title: Nvidias GPU dominance and the impact of PyTorch 20 and OpenAI Triton
videoId: t21REMsFJ_4
---

From: [[hu-po]] <br/> 

Over the past decade, the landscape of machine learning software development has seen significant changes, with many frameworks relying heavily on [[the_role_of_cuda_and_tensorflow_in_machine_learning_software_development | Nvidia's CUDA]] and performing best on Nvidia GPUs <a class="yt-timestamp" data-t="02:04:00">[02:04:00]</a>. This has given Nvidia a dominant position in the field, largely due to its software "moat" <a class="yt-timestamp" data-t="00:00:02">[00:00:02]</a> <a class="yt-timestamp" data-t="01:08:56">[01:08:56]</a>. However, the arrival of [[advancements_in_pytorch_20_and_its_potential_ability_to_operate_on_various_hardware | PyTorch 2.0]] and OpenAI's Triton is challenging this dominance <a class="yt-timestamp" data-t="02:32:00">[02:32:00]</a> <a class="yt-timestamp" data-t="01:04:49">[01:04:49]</a>.

## The Rise of PyTorch and Decline of TensorFlow

Historically, [[the_role_of_cuda_and_tensorflow_in_machine_learning_software_development | Google's TensorFlow]] was a frontrunner in the machine learning framework ecosystem and Google seemed poised to control the industry due to its first-mover advantage and the development of TPUs <a class="yt-timestamp" data-t="06:10:00">[06:10:00]</a> <a class="yt-timestamp" data-t="06:18:00">[06:18:00]</a>. However, [[advancements_in_pytorch_20_and_its_potential_ability_to_operate_on_various_hardware | PyTorch]] ultimately gained dominance, largely due to its increased flexibility and usability compared to [[the_role_of_cuda_and_tensorflow_in_machine_learning_software_development | TensorFlow]] <a class="yt-timestamp" data-t="08:33:00">[08:33:00]</a> <a class="yt-timestamp" data-t="07:05:00">[07:05:00]</a>.

[[the_role_of_cuda_and_tensorflow_in_machine_learning_software_development | TensorFlow]] was designed with a compiled code mindset, requiring users to create a graph and then compile it before execution, similar to C-based workflows <a class="yt-timestamp" data-t="08:42:00">[08:42:00]</a>. This graph-based approach made debugging and understanding code more challenging <a class="yt-timestamp" data-t="09:43:00">[09:43:00]</a>. In contrast, [[advancements_in_pytorch_20_and_its_potential_ability_to_operate_on_various_hardware | PyTorch]] offers a more Python-like, "eager mode" workflow where code is executed line by line, making it easier to debug and use <a class="yt-timestamp" data-t="09:01:00">[09:01:00]</a> <a class="yt-timestamp" data-t="09:08:00">[09:08:00]</a>. Despite [[the_role_of_cuda_and_tensorflow_in_machine_learning_software_development | TensorFlow]] now having an eager mode, the research community and most large tech firms have largely adopted [[advancements_in_pytorch_20_and_its_potential_ability_to_operate_on_various_hardware | PyTorch]] <a class="yt-timestamp" data-t="10:00:00">[10:00:00]</a> <a class="yt-timestamp" data-t="10:07:00">[10:07:00]</a>. Many generative AI models making headlines are based on [[advancements_in_pytorch_20_and_its_potential_ability_to_operate_on_various_hardware | PyTorch]] <a class="yt-timestamp" data-t="10:08:00">[10:08:00]</a>.

## The Memory Wall and Hardware Optimization

In machine learning model training, the two major time components are compute and memory <a class="yt-timestamp" data-t="10:41:00">[10:41:00]</a>. Historically, compute time (waiting for matrix multiplies) was the dominant factor <a class="yt-timestamp" data-t="12:03:00">[12:03:00]</a>. However, with advancements in Nvidia's GPUs, particularly architectural changes like the tensor core and the use of lower precision floating-point formats (e.g., FP16, FP8, and even FP4), compute speed has rapidly increased <a class="yt-timestamp" data-t="12:13:00">[12:13:00]</a> <a class="yt-timestamp" data-t="12:20:00">[12:20:00]</a> <a class="yt-timestamp" data-t="12:55:00">[12:55:00]</a>.

Today, the primary bottleneck is memory bandwidth <a class="yt-timestamp" data-t="14:42:00">[14:42:00]</a>. GPUs spend most of their time waiting for data to transfer between different memory caches (from RAM to GPU global memory, L2, L1, and registers) <a class="yt-timestamp" data-t="10:52:00">[10:52:00]</a>. This "memory wall" means that despite high computational power, GPU utilization can be as low as 60%, with the rest of the time spent idly waiting for data <a class="yt-timestamp" data-t="31:13:00">[31:13:00]</a> <a class="yt-timestamp" data-t="32:01:00">[32:01:00]</a>.

Modern deep learning models have soared in size, with large language models requiring hundreds of gigabytes or even terabytes for their weights alone <a class="yt-timestamp" data-t="01:07:01">[01:07:01]</a>. This makes it challenging to fit models on consumer GPUs <a class="yt-timestamp" data-t="17:11:00">[17:11:00]</a>. Putting more memory closer to the compute units is costly <a class="yt-timestamp" data-t="18:53:00">[18:53:00]</a>, and the cost per gigabyte for DRAM, which followed Moore's Law for decades, has plateaued since 2012 <a class="yt-timestamp" data-t="29:14:00">[29:14:00]</a>. Nvidia uses HBM memory for high bandwidth, but it is expensive <a class="yt-timestamp" data-t="30:42:00">[30:42:00]</a>.

### Operator Fusion
To mitigate memory bottlenecks, a key optimization method called "operator fusion" is used <a class="yt-timestamp" data-t="37:09:00">[37:09:00]</a>. Instead of executing each operation separately and writing intermediate results back to memory (as in [[advancements_in_pytorch_20_and_its_potential_ability_to_operate_on_various_hardware | PyTorch]]'s eager mode), multiple operations are fused into a single pass <a class="yt-timestamp" data-t="37:11:00">[37:11:00]</a>. This reduces memory reads and writes, similar to how a compiler optimizes code <a class="yt-timestamp" data-t="37:31:00">[37:31:00]</a>.

## PyTorch 2.0's Innovations and Impact
[[advancements_in_pytorch_20_and_its_potential_ability_to_operate_on_various_hardware | PyTorch 2.0]], released for early testing in January 2023, introduces a compiled solution that supports graph execution <a class="yt-timestamp" data-t="00:01:29">[00:01:29]</a> <a class="yt-timestamp" data-t="04:00:00">[04:00:00]</a> <a class="yt-timestamp" data-t="48:24:00">[48:24:00]</a>. This dramatically improves performance, showing an 86% improvement for training on Nvidia's A100 GPUs and 26% on CPUs for inference <a class="yt-timestamp" data-t="48:48:00">[48:48:00]</a>. These benefits are expected to extend to other [[hardware_for_ai_training_and_deployment | GPUs]] and accelerators from various companies like AMD, Intel, Tesla, Google, and Amazon <a class="yt-timestamp" data-t="04:09:00">[04:09:00]</a> <a class="yt-timestamp" data-t="49:04:00">[49:04:00]</a>.

Key components of [[advancements_in_pytorch_20_and_its_potential_ability_to_operate_on_various_hardware | PyTorch 2.0]] include:
*   **PrimTorch**: Reduces the number of operators from over 2,000 to a core set of 250, making it simpler for non-Nvidia backends to implement [[advancements_in_pytorch_20_and_its_potential_ability_to_operate_on_various_hardware | PyTorch]] <a class="yt-timestamp" data-t="54:50:00">[54:50:00]</a>.
*   **TorchDynamo**: A robust compiler that ingests any [[advancements_in_pytorch_20_and_its_potential_ability_to_operate_on_various_hardware | PyTorch]] user script and generates an optimized graph <a class="yt-timestamp" data-t="55:38:00">[55:38:00]</a>. It can handle partial graph capture and dynamic shapes, making it highly adaptable <a class="yt-timestamp" data-t="01:00:16">[01:00:16]</a> <a class="yt-timestamp" data-t="02:02:00">[02:02:00]</a>.
*   **TorchInductor**: A Python-native compiler that generates fast code for multiple accelerator backends, further optimizing the graph by fusing operators and planning memory <a class="yt-timestamp" data-t="01:02:14">[01:02:14]</a> <a class="yt-timestamp" data-t="01:03:09">[01:03:09]</a>.

This shift means that [[advancements_in_pytorch_20_and_its_potential_ability_to_operate_on_various_hardware | PyTorch 2.0]] adopts a hybrid approach, combining the usability of eager mode with the performance benefits of compilation <a class="yt-timestamp" data-t="00:58:20">[00:58:20]</a>.

## OpenAI Triton's Role in Disrupting CUDA
OpenAI's Triton is a particularly disruptive element <a class="yt-timestamp" data-t="01:04:49">[01:04:49]</a>. Integrated into the [[advancements_in_pytorch_20_and_its_potential_ability_to_operate_on_various_hardware | PyTorch]] Inductor stack, Triton directly generates PTX code for Nvidia GPUs, effectively **skipping Nvidia's closed-source CUDA libraries** like cuBLAS <a class="yt-timestamp" data-t="01:05:06">[01:05:06]</a>. Instead, it leverages [[opensource_ai_and_its_implications | open-source]] alternatives like Cutlass <a class="yt-timestamp" data-t="01:05:28">[01:05:28]</a>.

Historically, efficiently utilizing [[the_role_of_cuda_and_tensorflow_in_machine_learning_development | CUDA]] required deep understanding of hardware architecture, often necessitating CUDA experts to optimize and parallelize code <a class="yt-timestamp" data-t="01:06:46">[01:06:46]</a>. For example, the initial implementation of NeRF (Neural Radiance Fields) was significantly optimized (100x faster) by CUDA experts <a class="yt-timestamp" data-t="01:07:01">[01:07:01]</a> <a class="yt-timestamp" data-t="01:07:10">[01:07:10]</a> <a class="yt-timestamp" data-t="01:07:28">[01:07:28]</a>. Triton bridges this gap by enabling higher-level languages to achieve performance comparable to lower-level languages, making it much more usable for typical machine learning researchers <a class="yt-timestamp" data-t="01:07:52">[01:07:52]</a>.

## Implications for Nvidia's Dominance

The combined efforts of [[advancements_in_pytorch_20_and_its_potential_ability_to_operate_on_various_hardware | PyTorch 2.0]] and OpenAI Triton significantly weaken Nvidia's "software moat" <a class="yt-timestamp" data-t="01:08:56">[01:08:56]</a>. By providing a portable software stack that can run efficiently on diverse [[hardware_for_ai_training_and_deployment | AI hardware]], these developments introduce competition into a market previously dominated by Nvidia <a class="yt-timestamp" data-t="04:54:00">[04:54:00]</a> <a class="yt-timestamp" data-t="01:08:31">[01:08:31]</a>.

Other companies are increasingly developing their own [[hardware_developments_in_machine_learning_including_innovations_by_companies_like_tesla_google_and_apple | in-house AI hardware]], such as Apple with its M1 chips, Tesla with its Dojo chips, and Google with its TPUs <a class="yt-timestamp" data-t="04:19:00">[04:19:00]</a> <a class="yt-timestamp" data-t="04:27:00">[04:27:00]</a> <a class="yt-timestamp" data-t="04:39:00">[04:39:00]</a>. As [[challenges_and_innovations_in_ai_model_architecture_and_scaling | model architectures]] continue to evolve and become more dependent on specific hardware for efficient training and inference, the ability to swap out hardware while maintaining the same framework becomes crucial <a class="yt-timestamp" data-t="04:44:00">[04:44:00]</a> <a class="yt-timestamp" data-t="04:50:00">[04:50:00]</a> <a class="yt-timestamp" data-t="04:57:00">[04:57:00]</a>.

This shift allows [[hardware_for_ai_training_and_deployment | AI hardware]] startups and custom ASICs (Application-Specific Integrated Circuits) to make a dent in Nvidia's market share by dramatically reducing the effort required to build a compiler stack for new hardware <a class="yt-timestamp" data-t="01:08:46">[01:08:46]</a>. The focus is moving from ease of use afforded by Nvidia's software to the architecture and economics of the chip solution itself <a class="yt-timestamp" data-t="04:46:00">[04:46:00]</a>.

While [[challenges_and_innovations_in_ai_model_architecture_and_scaling | model architectures]] are still expected to undergo "sharp turns" in the future (e.g., from [[vision_transformers_vs_convolutional_networks | Vision Transformers]] to new Transformers) <a class="yt-timestamp" data-t="02:30:00">[02:30:00]</a> <a class="yt-timestamp" data-t="04:06:00">[04:06:00]</a>, the increasing interoperability at the framework level signals a more open and competitive future for [[hardware_for_ai_training_and_deployment | AI hardware]] <a class="yt-timestamp" data-t="04:39:00">[04:39:00]</a> <a class="yt-timestamp" data-t="04:47:00">[04:47:00]</a>.