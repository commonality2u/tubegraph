---
title: Model ensembling techniques
videoId: MVWYTFs9M-s
---

From: [[hu-po]] <br/> 

Model ensembling, also known as a mixture model, is a strategy that uses multiple models to improve final performance <a class="yt-timestamp" data-t="01:36:00">[01:36:00]</a>. This technique is particularly popular in Kaggle competitions, where participants aim to squeeze out the last one or two percent of performance <a class="yt-timestamp" data-t="01:42:49">[01:42:49]</a>. In such competitions, many different versions of the same model might be trained on slightly different subsets of the data, leading to more variety in the final output given the same input <a class="yt-timestamp" data-t="01:53:07">[01:53:07]</a>. Generally, picking the best output from several models is superior to relying on a single "best" model <a class="yt-timestamp" data-t="02:08:44">[02:08:44]</a>.

## GPT-4's Ensemble Structure
Recent reports, including a tweet by Sumit based on a podcast by George Hotz, suggest that [[GPT4 ensemble model structure | GPT4]] is not a single model <a class="yt-timestamp" data-t="00:49:10">[00:49:10]</a>. Instead, [[GPT4 ensemble model structure | GPT4]] is reportedly an [[Ensemble Learning for Language Models | ensemble]] of eight models, each with approximately 220 billion parameters <a class="yt-timestamp" data-t="01:01:07">[01:01:07]</a>. These models are likely slightly different in terms of what they were [[Training and finetuning processes for AI models | fine-tuned]] on <a class="yt-timestamp" data-t="01:10:48">[01:10:48]</a>. This structure explains [[Performance and efficiency in machine learning models | GPT4]]'s performance and also the high inference costs frequently mentioned by Sam Altman <a class="yt-timestamp" data-t="02:20:00">[02:20:00]</a>.

### Inference Cost Implications
When a user interacts with [[GPT4 ensemble model structure | GPT4]], it is suggested that 16 inferences are performed <a class="yt-timestamp" data-t="03:17:01">[03:17:01]</a>. In contrast, Google's Bard performs inference on just one model <a class="yt-timestamp" data-t="03:09:47">[03:09:47]</a>. This means [[GPT4 ensemble model structure | OpenAI]]'s inference cost could be 16 times higher than Bard's for a single query <a class="yt-timestamp" data-t="03:31:02">[03:31:02]</a>. The final output presented to the user is the result of these 16 different models, likely with a value function model selecting the best among them <a class="yt-timestamp" data-t="03:26:07">[03:26:07]</a>.

## Historical Precedent at OpenAI
The use of [[Ensemble Learning for Language Models | model ensembling]] is not new for [[GPT4 ensemble model structure | OpenAI]]. A 2021 paper for Codex, an [[AI model architecture and parallelism strategies | OpenAI]] model, describes a similar strategy <a class="yt-timestamp" data-t="04:15:28">[04:15:28]</a>. For solving "leak code problems," Codex would generate 100 samples per token and then filter them down <a class="yt-timestamp" data-t="04:40:02">[04:40:02]</a>. Additionally, the paper mentions [[finetuning machine learning models | fine-tuning]] Codex on training problems to produce "a set of supervised [[finetuning machine learning models | fine-tuned]] models," referred to as "Codex S" or "a set of Codex models" <a class="yt-timestamp" data-t="04:54:19">[04:54:19]</a>. This suggests that [[Ensemble Learning for Language Models | ensembling]] was likely applied to GPT models given the timing of this paper <a class="yt-timestamp" data-t="05:14:52">[05:14:52]</a>.

## Relevance to Model Performance
The use of [[Ensemble Learning for Language Models | model ensembling]] provides context for the observed "random jump in performance" seen with ChatGPT, explaining much of the hype surrounding it <a class="yt-timestamp" data-t="05:37:25">[05:37:25]</a>. By combining eight slightly different models, [[GPT4 ensemble model structure | OpenAI]] effectively created what appears to be a single, more powerful model <a class="yt-timestamp" data-t="05:46:17">[05:46:17]</a>. This strategy contributes significantly to the model's overall performance.

## Related Concepts in Self-Supervised Learning
The principles of ensembling and diverse model outputs are also relevant in other areas of machine learning, such as [[Self-supervised learning from images with a joint embedding predictive architecture | self-supervised learning for image representations]]. While not directly an ensemble, the concept of [[Self-supervised learning from images with a joint embedding predictive architecture | Joint Embedding Predictive Architectures]] (JEPA) emphasizes predicting representations from different parts of an image <a class="yt-timestamp" data-t="09:15:46">[09:15:46]</a>. This approach aims to produce highly semantic image representations without relying on handcrafted data augmentations <a class="yt-timestamp" data-t="07:34:04">[07:34:04]</a>, a key difference from traditional invariance-based methods that introduce biases <a class="yt-timestamp" data-t="01:18:24">[01:18:24]</a>.

JEPA's "multi-block masking strategy" samples large target blocks and uses spatially distributed context blocks to predict representations <a class="yt-timestamp" data-t="10:04:40">[10:04:40]</a>. This method predicts missing information in an abstract representation space rather than pixel or token space <a class="yt-timestamp" data-t="11:17:28">[11:17:28]</a>. This distinction is crucial for learning more semantic features, as it avoids wasting model capacity on unnecessary pixel-level details like texture and exact color <a class="yt-timestamp" data-t="02:24:26">[02:24:26]</a>. This also significantly reduces total computation needed for [[Training and finetuning processes for AI models | self-supervised training]] <a class="yt-timestamp" data-t="03:22:15">[03:22:15]</a>.

In conclusion, [[Ensemble Learning for Language Models | model ensembling techniques]] are powerful methods to boost [[Performance and efficiency in machine learning models | performance]] and address [[Challenges and strategies in model training and performance | challenges in model training]], even impacting the cost and perceived capabilities of advanced [[AI model architecture and parallelism strategies | AI models]] like [[GPT4 ensemble model structure | GPT4]]. The underlying principles, such as leveraging multiple perspectives or abstract representations, extend across various [[Challenges and Solutions in Training Multimodal Models | AI architectures]] and [[Challenges and Solutions in Training Multimodal Models | modalities]].