---
title: PyTorch 20 and OpenAI Triton
videoId: t21REMsFJ_4
---

From: [[hu-po]] <br/> 

The landscape of machine learning software development has seen significant changes, with many frameworks relying heavily on NVIDIA's CUDA and performing best on NVIDIA GPUs <a class="yt-timestamp" data-t="02:04:00">[02:04:00]</a>. However, the arrival of [[developments_in_deep_learning_hardware | PyTorch 2.0]] and [[open_source_ai_models_and_accessibility | OpenAI]]'s Triton is challenging NVIDIA's dominant position, particularly its software moat <a class="yt-timestamp" data-t="02:32:00">[02:32:00]</a>, <a class="yt-timestamp" data-t="05:33:00">[05:33:00]</a>.

## PyTorch's Rise to Dominance

For the past decade, NVIDIA GPUs have been the standard for deep learning, primarily due to their [[developments_in_deep_learning_hardware | CUDA]] software <a class="yt-timestamp" data-t="02:11:00">[02:11:00]</a>. Initially, [[comparison_between_tensorflow_and_pytorch | Google's TensorFlow]] was a frontrunner in the machine learning framework ecosystem, holding around 40% market share in 2017, but it has since significantly decreased <a class="yt-timestamp" data-t="03:14:00">[03:14:00]</a>, <a class="yt-timestamp" data-t="05:55:00">[05:55:00]</a>. In contrast, [[comparison_between_tensorflow_and_pytorch | PyTorch]] has grown from about 30% to nearly 50% market share by 2022 <a class="yt-timestamp" data-t="03:25:00">[03:25:00]</a>, winning the "framework race" <a class="yt-timestamp" data-t="03:33:00">[03:33:00]</a>. By 2020, [[comparison_between_tensorflow_and_pytorch | PyTorch]] was mentioned in almost 80% of major machine learning conferences <a class="yt-timestamp" data-t="06:58:00">[06:58:00]</a>.

The primary reason for [[comparison_between_tensorflow_and_pytorch | PyTorch]]'s success is its increased flexibility and usability compared to [[comparison_between_tensorflow_and_pytorch | TensorFlow]] <a class="yt-timestamp" data-t="08:36:00">[08:36:00]</a>. [[comparison_between_tensorflow_and_pytorch | TensorFlow]] was designed with a compiled code mindset, requiring a graph to be compiled before execution, similar to C-based workflows <a class="yt-timestamp" data-t="08:42:00">[08:42:00]</a>. This graph-based approach made debugging challenging because results weren't visible until compilation <a class="yt-timestamp" data-t="09:43:00">[09:43:00]</a>. [[comparison_between_tensorflow_and_pytorch | PyTorch]], on the other hand, offers a more Python-like, eager execution workflow where code is read and executed line by line, making it easier to use and debug <a class="yt-timestamp" data-t="09:01:00">[09:01:00]</a>, <a class="yt-timestamp" data-t="09:55:00">[09:55:00]</a>. Despite [[comparison_between_tensorflow_and_pytorch | TensorFlow]] now having an eager mode by default, [[comparison_between_tensorflow_and_pytorch | PyTorch]] has become the preferred choice for the research community and most large tech firms <a class="yt-timestamp" data-t="10:00:00">[10:00:00]</a>. Notably, nearly every prominent generative AI model is based on [[comparison_between_tensorflow_and_pytorch | PyTorch]] <a class="yt-timestamp" data-t="10:08:00">[10:08:00]</a>.

### The Memory Wall

Modern deep learning models are increasingly bottlenecked by memory bandwidth rather than raw computational power <a class="yt-timestamp" data-t="14:46:00">[14:46:00]</a>. Historically, compute time for matrix multiplications was the dominant factor, but NVIDIA's GPUs have advanced rapidly, making this less of a concern <a class="yt-timestamp" data-t="12:03:00">[12:03:00]</a>. Today, the majority of time during [[training_and_finetuning_processes_for_ai_models | model training]] and inference is spent waiting for data to be transferred between different memory levels <a class="yt-timestamp" data-t="18:32:00">[18:32:00]</a>.

The "memory wall" is evident in the increasing cost of DRAM, which now comprises 50% of total server costs <a class="yt-timestamp" data-t="29:46:00">[29:46:00]</a>. While GPU FLOPS have increased significantly, memory bandwidth has not kept pace, leading to low FLOPS utilization <a class="yt-timestamp" data-t="14:35:00">[14:35:00]</a>, <a class="yt-timestamp" data-t="31:13:00">[31:13:00]</a>. For example, NVIDIA's A100 GPUs often have very low FLOPS utilization without heavy optimization, with Tensor Cores being idle up to 40% of the time waiting for data <a class="yt-timestamp" data-t="31:20:00">[31:20:00]</a>, <a class="yt-timestamp" data-t="31:41:00">[31:41:00]</a>. This issue is exacerbated by the growing size of models, which can require hundreds of gigabytes or even terabytes for weights alone <a class="yt-timestamp" data-t="17:01:00">[17:01:00]</a>.

### Operator Fusion and PyTorch's Compiler Approach

To mitigate memory bottlenecks in eager execution, a technique called "operator fusion" is employed <a class="yt-timestamp" data-t="37:09:00">[37:09:00]</a>. Instead of writing each intermediate result to memory, multiple operations are "fused" and computed in one pass, minimizing memory reads and writes <a class="yt-timestamp" data-t="37:11:00">[37:11:00]</a>. This is analogous to how compilers optimize code in graph-based frameworks like [[comparison_between_tensorflow_and_pytorch | TensorFlow]] <a class="yt-timestamp" data-t="37:22:00">[37:22:00]</a>. [[comparison_between_tensorflow_and_pytorch | PyTorch]] has natively implemented many fused operators, growing its operator count to over 2,000 <a class="yt-timestamp" data-t="41:10:00">[41:10:00]</a>.

[[developments_in_deep_learning_hardware | PyTorch 2.0]], established under the [[meta_ai_research | PyTorch Foundation]] to ensure open development and governance, introduces a compiled solution that supports graph execution <a class="yt-timestamp" data-t="48:09:00">[48:09:00]</a>, <a class="yt-timestamp" data-t="48:29:00">[48:29:00]</a>. This shift aims to better utilize diverse hardware resources <a class="yt-timestamp" data-t="48:35:00">[48:35:00]</a> and dramatically reduces compute time and cost for [[training_and_finetuning_processes_for_ai_models | model training]] <a class="yt-timestamp" data-t="48:58:00">[48:58:00]</a>. [[meta_ai_research | Meta]]'s significant contribution to [[comparison_between_tensorflow_and_pytorch | PyTorch]] is driven by a desire for higher FLOPS utilization and increased software portability to other hardware, fostering competition in the space <a class="yt-timestamp" data-t="49:39:00">[49:39:00]</a>, <a class="yt-timestamp" data-t="49:54:00">[49:54:00]</a>.

[[developments_in_deep_learning_hardware | PyTorch 2.0]] also brings advancements in [[training_and_finetuning_processes_for_ai_models | distributed training]] with better API support for data parallelism, sharding, pipeline parallelism, and tensor parallelism <a class="yt-timestamp" data-t="50:50:00">[50:50:00]</a>. It supports dynamic shapes natively, making varying sequence lengths for Large Language Models (LLMs) easier to handle <a class="yt-timestamp" data-t="52:02:00">[52:02:00]</a>.

The backend integration of [[developments_in_deep_learning_hardware | PyTorch 2.0]] involves several key components:
*   **PrimTorch:** Reduces the 2,000+ [[comparison_between_tensorflow_and_pytorch | PyTorch]] operators to a core set of 250 primitive operators, simplifying the implementation of non-NVIDIA backends <a class="yt-timestamp" data-t="54:50:00">[54:50:00]</a>.
*   **Torch Dynamo:** Ingests any [[comparison_between_tensorflow_and_pytorch | PyTorch]] user script, including those calling third-party libraries, and generates an optimized computational graph <a class="yt-timestamp" data-t="55:38:00">[55:38:00]</a>. It lowers complex operations to the 250 primitive operations <a class="yt-timestamp" data-t="56:02:00">[56:02:00]</a> and works for over 99% of 7,000 tested [[comparison_between_tensorflow_and_pytorch | PyTorch]] models <a class="yt-timestamp" data-t="56:45:00">[56:45:00]</a>. Dynamo enables partial graph capture, guarded graph capture (recompiles only when necessary), and just-in-time recapture <a class="yt-timestamp" data-t="59:19:00">[59:19:00]</a>.
*   **Torch Inductor:** A Python-native deep learning compiler that takes the FX graph (with 50 primitive operators) from Dynamo, schedules and fuses operators, and performs memory planning to minimize memory transfers <a class="yt-timestamp" data-t="01:02:14">[01:02:14]</a>, <a class="yt-timestamp" data-t="01:02:41">[01:02:41]</a>. It then generates code for various accelerator backends, including CPUs, GPUs, and other AI accelerators <a class="yt-timestamp" data-t="01:03:34">[01:03:34]</a>.

## OpenAI Triton

[[open_source_ai_models_and_accessibility | OpenAI]]'s Triton is a disruptive force that challenges NVIDIA's closed-source [[developments_in_deep_learning_hardware | CUDA]] software moat <a class="yt-timestamp" data-t="01:04:49">[01:04:49]</a>. Triton takes Python code (or input from the [[developments_in_deep_learning_hardware | PyTorch Inductor]] stack) and converts it into an LLVM intermediate representation, generating code directly <a class="yt-timestamp" data-t="01:04:56">[01:04:56]</a>. For NVIDIA GPUs, it generates PTX code, *bypassing* NVIDIA's closed-source [[developments_in_deep_learning_hardware | CUDA]] libraries like cuBLAS in favor of open-source alternatives like Cutlass <a class="yt-timestamp" data-t="01:05:14">[01:05:14]</a>.

[[developments_in_deep_learning_hardware | CUDA]] is traditionally used by accelerated computing specialists, requiring a deep understanding of hardware architecture <a class="yt-timestamp" data-t="01:06:09">[01:06:09]</a>, <a class="yt-timestamp" data-t="01:06:46">[01:06:46]</a>. This often means machine learning researchers rely on [[developments_in_deep_learning_hardware | CUDA]] experts to optimize their code <a class="yt-timestamp" data-t="01:06:51">[01:06:51]</a>. Triton bridges this gap by enabling higher-level languages to achieve performance comparable to lower-level languages, making GPU kernel writing more accessible to the typical ML researcher <a class="yt-timestamp" data-t="01:07:52">[01:07:52]</a>. It automates memory coalescing, shared memory management, and scheduling within streaming multiprocessors <a class="yt-timestamp" data-t="01:08:04">[01:08:04]</a>.

While Triton currently supports NVIDIA GPUs, it is expanding to support multiple other hardware vendors, including AMD and Intel <a class="yt-timestamp" data-t="01:08:30">[01:08:30]</a>. This open-source project dramatically reduces the time needed to build an AI compiler stack for new hardware, thereby opening up the market for AI hardware and custom ASICs <a class="yt-timestamp" data-t="01:08:40">[01:08:40]</a>. NVIDIA's lack of focus on usability in their software stack allowed [[open_source_ai_models_and_accessibility | OpenAI]] and [[meta_ai_research | Meta]] to create a portable software stack <a class="yt-timestamp" data-t="01:09:02">[01:09:02]</a>.

### Impact on the AI Hardware Landscape

The combination of [[developments_in_deep_learning_hardware | PyTorch 2.0]] and [[open_source_ai_models_and_accessibility | OpenAI]] Triton is creating a more open and competitive environment for [[developments_in_deep_learning_hardware | AI hardware]]. Companies like Apple, Tesla (with their Dojo chips), and Google (with TPUs) are increasingly developing their own in-house hardware for deep learning <a class="yt-timestamp" data-t="04:13:00">[04:13:00]</a>. This trend suggests that model development may become more hardware-specific, with companies designing models optimized for their unique chips <a class="yt-timestamp" data-t="04:41:00">[04:41:00]</a>. However, [[developments_in_deep_learning_hardware | PyTorch 2.0]] and Triton aim to allow hardware to be swapped out while maintaining the same framework <a class="yt-timestamp" data-t="04:45:00">[04:45:00]</a>, <a class="yt-timestamp" data-t="04:50:00">[04:50:00]</a>.

The emerging pattern is a hybrid approach between eager and graph-based execution. Initially, code runs in eager mode, but once functions are called multiple times, the compiler automatically builds and runs an optimized graph <a class="yt-timestamp" data-t="00:58:20">[00:58:20]</a>. This compilation process is hardware-aware, optimizing for specific configurations of GPUs or TPUs <a class="yt-timestamp" data-t="01:01:29">[01:01:29]</a>. This convergence of features, also seen in Google's Jax framework <a class="yt-timestamp" data-t="00:52:15">[00:52:15]</a>, signifies a significant shift in the deep learning software stack.