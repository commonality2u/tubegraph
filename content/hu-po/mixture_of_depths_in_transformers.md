---
title: Mixture of Depths in Transformers
videoId: Teru_qIdB8Y
---

From: [[hu-po]] <br/> 

Mixture of Depths (MoD) is a neural network architecture trick developed by Google DeepMind and Mila that dynamically allocates compute in Transformer-based language models (LMs) <a class="yt-timestamp" data-t="00:02:14">[00:02:14]</a>, <a class="yt-timestamp" data-t="00:02:25">[00:02:25]</a>. It is seen as a pun on [[soft_mixture_of_experts_in_transformer_architecture | Mixture of Experts]] (MoE) <a class="yt-timestamp" data-t="00:02:36">[00:02:36]</a>, which is another popular network architecture trick <a class="yt-timestamp" data-t="00:02:41">[00:02:41]</a>. While [[soft_mixture_of_experts_in_transformer_architecture | Mixture of Experts]] routes data to different experts to improve quality, MoD intelligently routes data to skip unnecessary computations, aiming for compute efficiency <a class="yt-timestamp" data-t="00:03:00">[00:03:00]</a>, <a class="yt-timestamp" data-t="00:03:07">[00:03:07]</a>, <a class="yt-timestamp" data-t="00:03:14">[00:03:14]</a>.

## Core Mechanism

Traditional Transformer-based LMs distribute Floating Point Operations (FLOPS), a measure of compute, uniformly across input sequences <a class="yt-timestamp" data-t="00:03:31">[00:03:31]</a>, <a class="yt-timestamp" data-t="00:04:02">[00:04:02]</a>. The majority of computation in Transformers occurs in the attention mechanism, which treats every token equally <a class="yt-timestamp" data-t="00:04:10">[00:04:10]</a>, <a class="yt-timestamp" data-t="00:04:18">[00:04:18]</a>. MoD allows Transformers to dynamically allocate FLOPS to specific positions in a sequence <a class="yt-timestamp" data-t="00:04:51">[00:04:51]</a>.

### Transformer Block Components
A Transformer block primarily consists of two parts <a class="yt-timestamp" data-t="00:05:32">[00:05:32]</a>:
1.  **Multi-head Causal Self-Attention**: This mechanism treats any part of the sequence as equally relevant, making its computation quadratic with respect to the input sequence length (O(N^2)) <a class="yt-timestamp" data-t="00:04:41">[00:04:41]</a>, <a class="yt-timestamp" data-t="00:14:23">[00:14:23]</a>. It is considered one of the weakest links in terms of computational efficiency <a class="yt-timestamp" data-t="00:11:45">[00:11:45]</a>.
2.  **Feed-Forward Network (MLP)**: A multi-layer perceptron <a class="yt-timestamp" data-t="00:05:50">[00:05:50]</a>.

### Dynamic Allocation with Static Computation Graphs
MoD enforces a total compute budget by limiting the number of tokens that participate in self-attention and MLP computations <a class="yt-timestamp" data-t="00:05:22">[00:05:22]</a>, <a class="yt-timestamp" data-t="00:22:30">[00:22:30]</a>. The tokens to be processed are determined by the network using a top-K routing mechanism <a class="yt-timestamp" data-t="00:06:04">[00:06:04]</a>. This ensures a static computation graph, which is crucial for efficient hardware utilization <a class="yt-timestamp" data-t="00:06:17">[00:06:17]</a>, <a class="yt-timestamp" data-t="00:14:45">[00:14:45]</a>. A static graph means the sizes of all tensors and operations are known beforehand, allowing for optimization for specific hardware <a class="yt-timestamp" data-t="00:21:44">[00:21:44]</a>.

MoD allows tokens to either undergo computation in a standard Transformer block or pass through a residual (skip) connection, remaining unchanged and saving compute <a class="yt-timestamp" data-t="00:23:30">[00:23:30]</a>, <a class="yt-timestamp" data-t="00:23:54">[00:23:54]</a>. These skip connections originated from the "Deep Residual Learning for Image Recognition" paper (ResNet) in 2015 <a class="yt-timestamp" data-t="00:24:11">[00:24:11]</a>. In MoD, a router decides whether a token goes through the full Transformer block or skips it <a class="yt-timestamp" data-t="00:23:40">[00:23:40]</a>, <a class="yt-timestamp" data-t="00:28:47">[00:28:47]</a>.

The dynamic nature comes from the identity of the K tokens being fluid, allowing FLOPS to be expended non-uniformly across time (sequence position) and model depth <a class="yt-timestamp" data-t="00:07:19">[00:07:19]</a>, <a class="yt-timestamp" data-t="00:07:25">[00:07:25]</a>. This makes compute predictable in total but dynamic and context-sensitive at the token level <a class="yt-timestamp" data-t="00:07:34">[00:07:34]</a>.

## Routing Schemes

The routing mechanism for MoD is crucial, as random routing significantly underperforms <a class="yt-timestamp" data-t="00:46:09">[00:46:09]</a>, <a class="yt-timestamp" data-t="00:46:12">[00:46:12]</a>. MoD employs learned routing schemes:
*   **Token Choice Routing**: The token produces a probability distribution and "chooses" its computational path <a class="yt-timestamp" data-t="00:46:57">[00:46:57]</a>. If a path's capacity is exceeded, surplus tokens are dropped, leading to wasted compute on underutilized hardware slots <a class="yt-timestamp" data-t="00:48:10">[00:48:10]</a>, <a class="yt-timestamp" data-t="00:49:09">[00:49:09]</a>.
*   **Expert Choice Routing**: The path (or "expert") chooses the top K tokens based on their preference weights <a class="yt-timestamp" data-t="00:47:11">[00:47:11]</a>, <a class="yt-timestamp" data-t="00:50:24">[00:50:24]</a>. This method ensures perfect load balance as K tokens are guaranteed to be shuttled to each path, avoiding empty slots <a class="yt-timestamp" data-t="00:47:21">[00:47:21]</a>. MoD primarily uses expert choice routing <a class="yt-timestamp" data-t="00:51:42">[00:51:42]</a>.

A router weight is a scalar produced by a linear projection of the token embedding <a class="yt-timestamp" data-t="00:53:22">[00:53:22]</a>. These router weights are parameters (W_theta) that are trained via gradient descent, similar to other network weights <a class="yt-timestamp" data-t="00:53:50">[00:53:50]</a>, <a class="yt-timestamp" data-t="01:44:13">[01:44:13]</a>. This adds a slight increase to the total parameter count compared to a vanilla Transformer <a class="yt-timestamp" data-t="01:01:27">[01:01:27]</a>.

During autoregressive sampling (inference), the non-causal nature of the top-K operation (requiring knowledge of future tokens) presents a challenge <a class="yt-timestamp" data-t="00:55:36">[00:55:36]</a>, <a class="yt-timestamp" data-t="02:13:01">[02:13:01]</a>. This is resolved by using a small auxiliary MLP predictor (a second router) that predicts whether a token will be among the top K without needing to see future tokens <a class="yt-timestamp" data-t="00:57:52">[00:57:52]</a>, <a class="yt-timestamp" data-t="02:21:56">[02:21:56]</a>. This auxiliary predictor has a stop gradient, preventing its loss from affecting the primary language modeling objective <a class="yt-timestamp" data-t="00:58:37">[00:58:37]</a>, <a class="yt-timestamp" data-t="00:59:00">[00:59:00]</a>.

## Performance and Results

MoD models learn to dynamically allocate compute efficiently <a class="yt-timestamp" data-t="00:07:40">[00:07:40]</a>, <a class="yt-timestamp" data-t="00:07:41">[00:07:41]</a>.
*   **Training and Inference Efficiency**: MoD Transformers match baseline performance for equivalent FLOPS and wall-clock training times <a class="yt-timestamp" data-t="00:07:51">[00:07:51]</a>, while requiring only a fraction of the FLOPS for a forward pass (inference) <a class="yt-timestamp" data-t="00:07:55">[00:07:55]</a>. This can result in up to 50% faster inference speed <a class="yt-timestamp" data-t="00:08:39">[00:08:39]</a>, <a class="yt-timestamp" data-t="01:41:50">[01:41:50]</a>.
*   **Compute Savings**: By setting a block's capacity to T/2 (processing half the tokens), self-attention becomes 25% as FLOP-intensive because of its quadratic (O(N^2)) complexity <a class="yt-timestamp" data-t="00:44:20">[00:44:20]</a>, <a class="yt-timestamp" data-t="01:40:51">[01:40:51]</a>.
*   **Qualitative Improvement**: MoD Transformers can achieve lower training loss than "isoflop" optimal vanilla Transformers (models using the same amount of compute), indicating better performance <a class="yt-timestamp" data-t="01:02:00">[01:02:00]</a>, <a class="yt-timestamp" data-t="01:09:16">[01:09:16]</a>. This suggests that the ability to skip certain levels of abstraction in the hierarchy of features leads to a qualitative improvement <a class="yt-timestamp" data-t="00:32:58">[00:32:58]</a>, <a class="yt-timestamp" data-t="01:42:00">[01:42:00]</a>.
*   **Model Scaling**: It is empirically found that it is better to add depth (more layers) than to add width (more neurons per layer) when adding FLOPS to a model <a class="yt-timestamp" data-t="01:12:25">[01:12:25]</a>, <a class="yt-timestamp" data-t="01:13:01">[01:13:01]</a>.
*   **KV Cache**: MoD can significantly improve the KV cache size during autoregressive sampling <a class="yt-timestamp" data-t="01:32:59">[01:32:59]</a>. By reducing the number of tokens processed by attention, more space is available in the KV cache, which stores Keys (K) and Values (V) from previous time steps to avoid recalculation <a class="yt-timestamp" data-t="01:14:05">[01:14:05]</a>.

Routing analysis shows that some tokens engage with every block throughout the Transformer's depth, while others skip blocks whenever possible <a class="yt-timestamp" data-t="01:15:12">[01:15:12]</a>. Tokens that engage more frequently tend to be correlated with output predictions that have higher entropy, possibly corresponding to more difficult predictions <a class="yt-timestamp" data-t="01:15:21">[01:15:21]</a>.

## MoD Variants

Two primary variants for integrating MoD with [[soft_mixture_of_experts_in_transformer_architecture | Mixture of Experts]] (MoE) architectures are proposed:
*   **Staged MoD**: This approach places a router before the entire Transformer block (including self-attention and MLP) to route tokens around or towards the block <a class="yt-timestamp" data-t="01:17:18">[01:17:18]</a>, <a class="yt-timestamp" data-t="01:20:02">[01:20:02]</a>. This is advantageous because it allows tokens to skip the computationally expensive self-attention step <a class="yt-timestamp" data-t="01:17:34">[01:17:34]</a>.
*   **Integrated MoD**: This approach integrates a "noop" (no-operation) expert alongside conventional MLP experts within an existing MoE architecture <a class="yt-timestamp" data-t="01:17:26">[01:17:26]</a>, <a class="yt-timestamp" data-t="01:18:41">[01:18:41]</a>. The existing MoE router then decides whether to send tokens to a standard expert or the noop expert. This simplifies the routing machinery, but tokens still pass through the self-attention mechanism, losing some of the computational benefits of skipping attention <a class="yt-timestamp" data-t="01:17:37">[01:17:37]</a>, <a class="yt-timestamp" data-t="01:20:33">[01:20:33]</a>.

## Future Directions

The MoD concept can be extended further:
*   **Granular Routing**: Instead of just routing for the entire self-attention mechanism, tokens could be routed specifically for Queries (Q), Keys (K), and Values (V) <a class="yt-timestamp" data-t="01:30:27">[01:30:27]</a>. For example, a token might be valuable as a Key but not necessarily for its Query component <a class="yt-timestamp" data-t="01:33:02">[01:33:02]</a>.
*   **Diverse Computations**: MoD could enable routing between even more types of computations beyond simple skipping, such as directing some tokens to memory lookup or tool use modules <a class="yt-timestamp" data-t="01:43:56">[01:43:56]</a>, <a class="yt-timestamp" data-t="01:34:40">[01:34:40]</a>. This provides a "knob" for adjusting available computation types and their relative costs <a class="yt-timestamp" data-t="01:34:50">[01:34:50]</a>.
*   **Long-term Memory**: There is potential to extend the idea into long-term memory, where certain tokens could be extremely valuable as Keys regardless of their immediate utility as Queries <a class="yt-timestamp" data-t="01:39:41">[01:39:41]</a>, <a class="yt-timestamp" data-t="01:39:50">[01:39:50]</a>.

MoD is considered a simple, generic modification that can be integrated with other techniques, opening doors for many extensions <a class="yt-timestamp" data-t="01:35:29">[01:35:29]</a>.