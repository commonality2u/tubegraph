---
title: Monocular Depth Estimation using Diffusion Models
videoId: WoiI_Pn9yHw
---

From: [[hu-po]] <br/> 

Monocular depth estimation is a fundamental computer vision task that involves recovering 3D depth information from a single 2D image <a class="yt-timestamp" data-t="09:37:06">[09:37:06]</a>. This task is geometrically ill-posed and requires robust scene understanding <a class="yt-timestamp" data-t="09:41:06">[09:41:06]</a>. The impressive advancements in monocular depth estimators have mirrored the growth in model capacity, from Convolutional Neural Networks (CNNs) to large Transformer architectures <a class="yt-timestamp" data-t="10:47:06">[10:47:06]</a>. Historically, these models struggled with unfamiliar content (out-of-distribution data) because their knowledge of the visual world was limited by their training datasets <a class="yt-timestamp" data-t="11:06:06">[11:06:06]</a>.

The rise of large [[Scaling and Training Techniques for Diffusion Models | generative diffusion models]], trained on internet-scale data, has opened new avenues for more generalizable depth estimation <a class="yt-timestamp" data-t="12:20:06">[12:20:06]</a>. These models, often referred to as [[Scaling and Training Techniques for Diffusion Models | foundation models]], have learned comprehensive representations of the visual world, which can be leveraged for tasks like depth estimation <a class="yt-timestamp" data-t="23:57:06">[23:57:06]</a>.

## Marigold: Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation

A significant paper in this domain, "Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation," (referred to as Marigold) was released on December 4, 2023, by ETH Zurich <a class="yt-timestamp" data-t="03:20:06">[03:20:06]</a>. Marigold proposes a method for [[Applications and Limitations of Monocular Depth | monocular depth estimation]] by repurposing [[Scaling and Training Techniques for Diffusion Models | diffusion models]], specifically [[Comparisons with diffusion models | Stable Diffusion models]] <a class="yt-timestamp" data-t="03:51:06">[03:51:06]</a>. The core idea is to leverage the excellent image priors learned by these models, which provide a strong understanding of the visual world, to derive depth information <a class="yt-timestamp" data-t="04:00:06">[04:00:06]</a>.

Depth, in this context, is represented as a single-channel image, unlike standard RGB images with three channels <a class="yt-timestamp" data-t="04:20:06">[04:20:06]</a>. In the visualization, blue typically represents objects far from the camera, while red signifies objects close to it <a class="yt-timestamp" data-t="05:02:06">[05:02:06]</a>. The term "monocular" emphasizes that depth is estimated from a single image, contrasting with traditional methods that relied on camera geometry from stereo or multi-view setups <a class="yt-timestamp" data-t="05:15:06">[05:15:06]</a>.

Marigold achieves state-of-the-art performance across a wide range of datasets, with over 20% performance gains in specific cases <a class="yt-timestamp" data-t="00:16:12">[00:16:12]</a>. The model is fine-tuned efficiently in a couple of days on a single consumer GPU <a class="yt-timestamp" data-t="01:03:23">[01:03:23]</a>.

### Technical Approach

Marigold poses [[visualizing_depth_and_saliency_in_diffusion_processes | monocular depth estimation]] as a conditional denoising diffusion generation task <a class="yt-timestamp" data-t="02:09:06">[02:09:06]</a>. This means it learns to predict depth by reversing a process where Gaussian noise is progressively added to ground truth depth maps <a class="yt-timestamp" data-t="02:20:06">[02:20:06]</a>.

1.  **Affine-Invariant Depth Estimation**: The method focuses on [[causality_in_depth_representation_in_diffusion_models | affine-invariant depth estimation]], meaning it captures relative distances between objects (e.g., this pixel is closer than that pixel) rather than true metric distances in meters or millimeters <a class="yt-timestamp" data-t="13:27:06">[13:27:06]</a>. This simplifies the problem by abstracting away the exact scale, but it means the output doesn't directly provide real-world measurements <a class="yt-timestamp" data-t="14:49:06">[14:49:06]</a>. The depth values are normalized to a range of -1 to 1 <a class="yt-timestamp" data-t="00:43:34">[00:43:34]</a>.
2.  **Frozen Variational Autoencoder (VAE)**: Marigold utilizes a frozen VAE (a component of [[Comparisons with diffusion models | Stable Diffusion]]) to encode both the input RGB image and the corresponding depth map into a low-dimensional latent space <a class="yt-timestamp" data-t="30:00:06">[30:00:06]</a>.
    *   **"Deep Learning Magic"**: Crucially, the VAE, originally trained solely on real RGB images (like those in the LAION 5B dataset <a class="yt-timestamp" data-t="32:11:06">[32:11:06]</a>, which doesn't contain depth images <a class="yt-timestamp" data-t="33:25:06">[33:25:06]</a>), is repurposed to encode single-channel depth images <a class="yt-timestamp" data-t="32:28:06">[32:28:06]</a>. This is achieved by duplicating the depth channel across three dimensions to simulate an RGB image <a class="yt-timestamp" data-t="34:06:06">[34:06:06]</a>. Remarkably, this "shouldn't work" but does, allowing for accurate latent representation of depth <a class="yt-timestamp" data-t="32:37:06">[32:37:06]</a>.
3.  **Adapted Denoising UNet**: The denoising UNet (the core of the diffusion model) is fine-tuned <a class="yt-timestamp" data-t="18:41:06">[18:41:06]</a>. It takes as input the concatenated latent codes of both the noisy depth map and the original RGB image <a class="yt-timestamp" data-t="37:06:06">[37:06:06]</a>.
    *   To accommodate the expanded input, the UNet's input channels are doubled, and its initial weights are halved to prevent activation magnitude inflation <a class="yt-timestamp" data-t="38:47:06">[38:47:06]</a>. This unconventional modification also "magically works" <a class="yt-timestamp" data-t="39:20:06">[39:20:06]</a>.
4.  **Training on Synthetic Data**: The model is fine-tuned on synthetic datasets like HyperSim and Virtual KITTI <a class="yt-timestamp" data-t="01:00:06">[01:00:06]</a>. Synthetic data offers perfectly dense, complete, and clean ground truth depth maps, unlike real-world depth sensors which suffer from missing values, noise, and artifacts due to physical constraints and material properties (e.g., shiny surfaces, shadowing effects) <a class="yt-timestamp" data-t="47:18:06">[47:18:06]</a>.
5.  **Inference Scheme**:
    *   An input RGB image `X` is encoded into its latent representation `ZX` using the frozen VAE <a class="yt-timestamp" data-t="52:57:06">[52:57:06]</a>.
    *   The process starts with pure latent noise `Zt` (sampled from a Gaussian distribution).
    *   The denoising UNet iteratively removes noise from `Zt`, conditioned on `ZX` and the current time step <a class="yt-timestamp" data-t="53:05:06">[53:05:06]</a>.
    *   The denoised latent depth is then decoded back into a 3-channel image using the frozen VAE decoder, and the three channels are averaged to obtain the final single-channel depth map <a class="yt-timestamp" data-t="54:13:06">[54:13:06]</a>.
6.  **Accelerated Inference and Ensembling**:
    *   Marigold uses a non-Markovian sampling approach, such as [[Comparisons with diffusion models | DDIM (Denoising Diffusion Implicit Models)]], which allows skipping steps in the reverse diffusion process, significantly accelerating inference <a class="yt-timestamp" data-t="56:42:06">[56:42:06]</a>.
    *   A test-time ensembling scheme is employed, where multiple depth predictions (generated from different initial noise samples) are combined, typically by taking the median <a class="yt-timestamp" data-t="01:01:46">[01:01:46]</a>. This ensemble improves accuracy, reducing absolute relative error by up to 9.5% with 20 predictions <a class="yt-timestamp" data-t="01:09:18">[01:09:18]</a>.
    *   The model also uses multi-resolution noise during training, which superimposes Gaussian noise images of different scales to recover both high-frequency textures and larger object details <a class="yt-timestamp" data-t="00:55:07">[00:55:07]</a>.

Marigold's success confirms the hypothesis that a comprehensive representation of the visual world, such as that captured by [[Comparisons with diffusion models | large-scale diffusion models]], is the cornerstone of effective [[visualizing_depth_and_saliency_in_diffusion_processes | monocular depth estimation]] <a class="yt-timestamp" data-t="01:06:01">[01:06:01]</a>.

## Patch Fusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation

Another paper, "Patch Fusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation," was also published on December 4, 2023, by King Abdullah University of Science and Technology <a class="yt-timestamp" data-t="01:16:06">[01:16:06]</a>. This paper addresses the challenge of performing depth estimation on high-resolution images, common in modern consumer cameras <a class="yt-timestamp" data-t="01:13:16">[01:13:16]</a>.

The core technique of [[Patchbased Depth Estimation Techniques | Patch Fusion]] involves:
1.  **Tile-Based Processing**: Cutting a high-resolution input image into multiple overlapping patches <a class="yt-timestamp" data-t="01:16:19">[01:16:19]</a>.
2.  **Independent Depth Prediction**: Running a depth prediction model independently on each of these patches <a class="yt-timestamp" data-t="01:16:40">[01:16:40]</a>.
3.  **Fusion with Consistency**: Fusing the independently predicted depth maps from the patches. Because the patches are processed independently, their depth predictions may not be globally consistent (e.g., overlapping regions might show different depth values) <a class="yt-timestamp" data-t="01:15:00">[01:15:00]</a>. The paper introduces a "consistency-aware training" mechanism that uses overlapping patches to ensure alignment <a class="yt-timestamp" data-t="01:15:17">[01:15:17]</a>.

While [[Patchbased Depth Estimation Techniques | Patch Fusion]] demonstrates impressive fine-grained detail in its depth maps, particularly on synthetic 4K images from Unreal Engine <a class="yt-timestamp" data-t="01:19:33">[01:19:33]</a>, its approach is seen as less innovative than Marigold's. It relies on older techniques like [[Finetuning Pretrained Models for Depth Estimation | ConvNets]] trained from scratch across multiple networks (coarse network, fine network, guided fusion network), making it unnecessarily complicated and potentially less efficient for generalized use <a class="yt-timestamp" data-t="01:17:28">[01:17:28]</a>. The presenter noted that the "trick" of cutting images into patches to capture fine details could be combined with Marigold's more generalizable approach for even better results <a class="yt-timestamp" data-t="01:18:55">[01:18:55]</a>.

## Applications and Limitations

[[Applications and Limitations of Monocular Depth | Monocular depth estimation using diffusion models]] is rapidly advancing to a point where it can potentially replace traditional, often expensive, depth sensors like Lidars and structured light sensors <a class="yt-timestamp" data-t="01:06:51">[01:06:51]</a>. These hardware sensors often produce noisy, incomplete depth maps due to environmental factors and material properties <a class="yt-timestamp" data-t="01:06:51">[01:06:51]</a>. AI models can now generate cleaner and more complete depth predictions than the raw sensor data <a class="yt-timestamp" data-t="01:06:58">[01:06:58]</a>.

However, challenges remain:
*   **Affine Invariance**: The primary limitation of Marigold is its affine-invariant depth output. For applications requiring true metric distances (e.g., [[Generative 3D models using video diffusion | SLAM]], [[Challenges in 3D model generation using diffusion models | 3D model generation]], photogrammetry), an additional step would be needed to scale the relative depth values to real-world units, often by making an assumption about the scene's scale <a class="yt-timestamp" data-t="01:31:52">[01:31:52]</a>.
*   **Inference Speed**: While faster than traditional methods, the iterative nature and ensembling techniques can still make inference slow for real-time, latency-sensitive applications like autonomous vehicles <a class="yt-timestamp" data-t="01:10:04">[01:10:04]</a>. However, advancements like Latent Consistency Models (LCM-LoRA) could drastically speed up diffusion model inference, making them more viable for such applications <a class="yt-timestamp" data-t="01:42:40">[01:42:40]</a>.

Despite these limitations, the rapid progress in [[visualizing_depth_and_saliency_in_diffusion_processes | monocular depth estimation]] suggests a future where high-quality depth information can be readily obtained from standard RGB cameras, potentially disrupting markets for specialized depth-sensing hardware <a class="yt-timestamp" data-t="01:07:06">[01:07:06]</a>.