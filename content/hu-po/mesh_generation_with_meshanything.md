---
title: Mesh generation with MeshAnything
videoId: SkvyrgSzigo
---

From: [[hu-po]] <br/> 

The "MeshAnything" paper, titled "Artist Created Mesh Generation with Autoregressive Transformers," presents a distinctive approach to [[generative_3d_models_and_techniques | generative 3D model generation]], particularly focusing on creating meshes with superior topology compared to other methods <a class="yt-timestamp" data-t="00:36:32">[00:36:32]</a>.

## Artist-Created Meshes and Topology
MeshAnything specifically aims to produce "artist-created meshes" <a class="yt-timestamp" data-t="00:36:39">[00:36:39]</a>, which refers to meshes manually crafted by digital artists in software like Blender <a class="yt-timestamp" data-t="00:36:50">[00:36:50]</a>. This contrasts with meshes generated by scanning objects or photogrammetry, such as those derived from drones <a class="yt-timestamp" data-t="00:37:03">[00:37:03]</a>.

The key advantage of artist-created meshes is their "good topology" <a class="yt-timestamp" data-t="00:36:50">[00:36:50]</a>, <a class="yt-timestamp" data-t="00:38:00">[00:38:00]</a>. Good topology features clean contour lines <a class="yt-timestamp" data-t="00:38:08">[00:38:08]</a>, which are crucial for creating clean animations <a class="yt-timestamp" data-t="00:38:11">[00:38:11]</a> and simplifying texturing processes, especially in complex areas like the mouth <a class="yt-timestamp" data-t="00:38:28">[00:38:28]</a>. Meshes from photogrammetry often exhibit disorganized topology with artifacts, making animation and texturing challenging <a class="yt-timestamp" data-t="00:38:13">[00:38:13]</a>.

> [!INFO] Importance of Meshes in 3D Industry
> The current 3D industry relies predominantly on mesh-based pipelines due to their efficiency and controllability <a class="yt-timestamp" data-t="00:38:58">[00:38:58]</a>. Tools like Unreal Engine and Unity, and CGI pipelines, are built around meshes <a class="yt-timestamp" data-t="00:39:31">[00:39:31]</a>. While many [[generative_models_for_3d_assets | 3D asset generation]] methods output alternative representations like NeRFs, Gaussians, or SDFs, these must be converted to meshes for use in existing software <a class="yt-timestamp" data-t="00:39:09">[00:39:09]</a>. This conversion, often via Marching Cubes, loses control over topology <a class="yt-timestamp" data-t="00:39:48">[00:39:48]</a>, resulting in dense, inefficient meshes <a class="yt-timestamp" data-t="00:52:58">[00:52:58]</a>, <a class="yt-timestamp" data-t="00:53:00">[00:53:00]</a>.

## Autoregressive Generation with Mesh Vocabulary
MeshAnything employs an autoregressive Transformer for [[generative_models_for_3d_assets | mesh generation]] <a class="yt-timestamp" data-t="00:49:07">[00:49:07]</a>. The core innovation lies in directly generating the mesh by producing "mesh tokens" <a class="yt-timestamp" data-t="00:50:01">[00:50:01]</a>, <a class="yt-timestamp" data-t="00:50:18">[00:50:18]</a> one at a time <a class="yt-timestamp" data-t="00:51:12">[00:51:12]</a>.

To achieve this token-based generation, MeshAnything utilizes a Vector Quantized Variational Autoencoder (VQVAE) to learn a "mesh vocabulary" <a class="yt-timestamp" data-t="00:48:27">[00:48:27]</a>, <a class="yt-timestamp" data-t="00:48:31">[00:48:31]</a>. This VQVAE is trained on existing "artist-created meshes" from datasets like Objaverse and ShapeNet <a class="yt-timestamp" data-t="00:43:10">[00:43:10]</a>. The VQVAE effectively discretizes continuous 3D shapes into a finite codebook of shape tokens <a class="yt-timestamp" data-t="00:48:54">[00:48:54]</a>, which the Transformer can then generate sequentially <a class="yt-timestamp" data-t="00:49:10">[00:49:10]</a>.

### Data Augmentation and Noise Resistance
MeshAnything incorporates a clever data augmentation technique: intentionally corrupting the shape quality of artist-created meshes during training <a class="yt-timestamp" data-t="00:43:17">[00:43:17]</a>. By adding noise to vertex positions or reordering points <a class="yt-timestamp" data-t="00:51:39">[00:51:39]</a>, <a class="yt-timestamp" data-t="00:51:56">[00:51:56]</a>, the model learns to map these "shitty meshes" back to their high-quality counterparts with good topology <a class="yt-timestamp" data-t="00:58:48">[00:58:48]</a>. This "noise-resistant decoder" allows the Transformer to produce higher-quality meshes even when generating potentially low-quality token sequences <a class="yt-timestamp" data-t="00:59:15">[00:59:15]</a>, thereby enhancing its understanding of topology <a class="yt-timestamp" data-t="00:52:10">[00:52:10]</a>.

## Input and Pipeline
Instead of relying on multiple multi-view images like [[mesh_generation_with_meshformer | MeshFormer]] <a class="yt-timestamp" data-t="00:56:28">[00:56:28]</a>, MeshAnything starts with a point cloud <a class="yt-timestamp" data-t="00:56:31">[00:56:31]</a>. Point clouds can be obtained from LiDAR scanning or photogrammetry using a cell phone <a class="yt-timestamp" data-t="00:56:35">[00:56:35]</a>. This point cloud is encoded into shape features, which are then projected into the learned mesh token space for the decoder-only Transformer to generate the mesh <a class="yt-timestamp" data-t="00:49:19">[00:49:19]</a>.

## Future Outlook on 3D Representations
The approach taken by MeshAnything challenges the speaker's prior belief that mesh formats might become obsolete in favor of [[comparison_between_meshbased_and_gaussianbased_3d_representations | alternative 3D representations]] like NeRFs or Gaussian Splats <a class="yt-timestamp" data-t="00:54:07">[00:54:07]</a>. If high-quality meshes with good topology can be directly generated, it might be more practical to continue using existing 3D tools and pipelines that rely on meshes <a class="yt-timestamp" data-t="00:54:38">[00:54:38]</a>, rather than undertaking the difficult task of refactoring the entire 3D industry stack to support new representations <a class="yt-timestamp" data-t="00:55:04">[00:55:04]</a>. This suggests that meshes could maintain their status as the standard 3D representation due to the significant accumulated momentum <a class="yt-timestamp" data-t="00:55:02">[00:55:02]</a>.