---
title: Visual Autoregressive Modeling
videoId: -jG7S5g071Q
---

From: [[hu-po]] <br/> 

Visual Autoregressive Modeling (VAR) is a novel generative paradigm for image generation that redefines how autoregressive learning is applied to images <a class="yt-timestamp" data-t="00:07:42">[00:07:42]</a>. Instead of sequential next-token prediction, VAR employs a next-scale or next-resolution prediction approach <a class="yt-timestamp" data-t="00:07:47">[00:07:47]</a> <a class="yt-timestamp" data-t="00:09:00">[00:09:00]</a> <a class="yt-timestamp" data-t="00:18:02">[00:18:02]</a>. This paper won a best paper award at the prestigious Neural Information Processing Systems (NeurIPS) conference <a class="yt-timestamp" data-t="00:04:02">[00:04:02]</a> <a class="yt-timestamp" data-t="00:04:06">[00:04:06]</a>.

## Background and Problem Statement

Traditional autoregressive models for images, mirroring sequential language modeling, discretize continuous images into 2D token grids, which are then flattened into a 1D sequence for autoregressive learning <a class="yt-timestamp" data-t="00:15:05">[00:15:05]</a> <a class="yt-timestamp" data-t="00:15:09">[00:15:09]</a> <a class="yt-timestamp" data-t="00:31:02">[00:31:02]</a>. This flattening typically uses a row-major raster scan (left-to-right, top-to-down), or other methods like spiral or Z-curve orders <a class="yt-timestamp" data-t="00:15:40">[00:15:40]</a> <a class="yt-timestamp" data-t="00:42:06">[00:42:06]</a> <a class="yt-timestamp" data-t="00:42:09">[00:42:09]</a>.

However, this approach introduces several [[challenges_with_autoregressive_decoders | challenges]]:
*   **Inherent Directionality Mismatch**: Language has a natural 1D, unidirectional flow, which is a strong inductive prior for language models <a class="yt-timestamp" data-t="00:31:17">[00:31:17]</a>. Images, conversely, possess bidirectional correlation; a token's neighbors are closely related in all directions <a class="yt-timestamp" data-t="00:43:21">[00:43:21]</a>. This raster scan order contradicts the bidirectional nature of images <a class="yt-timestamp" data-t="00:43:22">[00:43:22]</a>.
*   **Loss of Spatial Locality**: Flattening a 2D grid into a 1D sequence disrupts the inherent spatial locality <a class="yt-timestamp" data-t="00:44:01">[00:44:01]</a>. For example, a token and its four immediate spatial neighbors are closely correlated, but this relationship is compromised in a linear sequence <a class="yt-timestamp" data-t="00:44:08">[00:44:08]</a>.
*   **Reliance on Position Embeddings**: To mitigate the loss of spatial information, these models often rely on [[continuous_and_discrete_data_in_generative_models | rotary position embeddings]] to implicitly remind the network of spatial relationships <a class="yt-timestamp" data-t="00:16:38">[00:16:38]</a> <a class="yt-timestamp" data-t="00:17:19">[00:17:19]</a>.
*   **Computational Cost**: Full autoregressive generation of N² tokens requires O(N²) decoding iterations and O(N⁶) total computations, which is computationally expensive <a class="yt-timestamp" data-t="01:14:07">[01:14:07]</a> <a class="yt-timestamp" data-t="01:14:10">[01:14:10]</a> <a class="yt-timestamp" data-t="01:35:16">[01:35:16]</a>.

## VAR's Hierarchical Approach

VAR proposes a new strategy based on the observation that humans perceive and create images hierarchically, from coarse to fine <a class="yt-timestamp" data-t="00:17:53">[00:17:53]</a> <a class="yt-timestamp" data-t="00:19:50">[00:19:50]</a>. This multiscale, coarse-to-fine inductive prior is similar to how convolutional neural networks (CNNs) and the human visual system (e.g., V1, V2, V3 layers) operate <a class="yt-timestamp" data-t="00:19:17">[00:19:17]</a> <a class="yt-timestamp" data-t="00:20:06">[00:20:06]</a> <a class="yt-timestamp" data-t="00:21:04">[00:21:04]</a>.

Instead of next-token prediction, the VAR Transformer predicts the next *higher resolution token map* conditioned on all previous (lower resolution) ones <a class="yt-timestamp" data-t="00:18:05">[00:18:05]</a> <a class="yt-timestamp" data-t="00:50:39">[00:50:39]</a>. This means starting with a single token representing the whole image, then progressively predicting entire image representations at slightly higher resolutions <a class="yt-timestamp" data-t="00:18:36">[00:18:36]</a>.

### VQ-VAE for Multiscale Tokenization

VAR utilizes a two-stage training process:
1.  **Multiscale VQ-VAE Training**: In the first stage, a multiscale [[the_role_of_variational_autoencoders_in_latent_diffusion | VQ-VAE]] (Vector Quantized Variational AutoEncoder) encodes an image into multiple token maps at increasingly higher resolutions <a class="yt-timestamp" data-t="00:45:00">[00:45:00]</a> <a class="yt-timestamp" data-t="00:45:03">[00:45:03]</a>. The VQ-VAE takes a continuous image, uses an encoder (a CNN) to convert it into a continuous feature map, and then a quantizer selects the closest discrete vector from a predefined codebook (vocabulary) for each part of the feature map <a class="yt-timestamp" data-t="00:32:27">[00:32:27]</a> <a class="yt-timestamp" data-t="00:34:24">[00:34:24]</a> <a class="yt-timestamp" data-t="00:35:12">[00:35:12]</a>. This effectively discretizes the image into tokens <a class="yt-timestamp" data-t="00:36:58">[00:36:58]</a>. The codebook is shared across all scales, meaning the same vocabulary of 4,096 possible token values is used for both coarse and fine resolutions <a class="yt-timestamp" data-t="00:29:06">[00:29:06]</a> <a class="yt-timestamp" data-t="01:02:44">[01:02:44]</a>. This VQ-VAE training is self-supervised, using the image itself as the learning signal <a class="yt-timestamp" data-t="00:46:51">[00:46:51]</a>.
2.  **VAR Transformer Training**: Once the VQ-VAE is trained, the VAR Transformer is trained on these multiscale tokens <a class="yt-timestamp" data-t="00:47:40">[00:47:40]</a>. It predicts the tokens of a higher resolution map (RK) based on all previous (lower resolution) maps (R1 to RK-1) <a class="yt-timestamp" data-t="00:50:39">[00:50:39]</a>. A block-wise causal attention mask ensures that each RK can only attend to R less than or equal to K <a class="yt-timestamp" data-t="00:51:04">[00:51:04]</a>. Crucially, all tokens within a given resolution map (RK) can be generated in parallel, unlike traditional raster scan autoregressive models <a class="yt-timestamp" data-t="00:59:03">[00:59:03]</a> <a class="yt-timestamp" data-t="01:00:57">[01:00:57]</a>.

## Performance and Advantages

VAR demonstrates significant improvements over previous methods:
*   **Quantitative Results**: On the ImageNet 256x256 benchmark, VAR drastically improves the Fréchet Inception Distance (FID) from 18 to 1 and the Inception Score (IS) from 80 to 350 <a class="yt-timestamp" data-t="00:09:50">[00:09:50]</a> <a class="yt-timestamp" data-t="00:09:53">[00:09:53]</a> <a class="yt-timestamp" data-t="00:09:57">[00:09:57]</a>. An FID score of 1.78 is considered a lower bound, making VAR's score of 1.0 highly impressive <a class="yt-timestamp" data-t="01:03:32">[01:03:32]</a> <a class="yt-timestamp" data-t="01:03:34">[01:03:34]</a>.
*   **Inference Speed**: VAR achieves a 20x faster inference speed compared to Diffusion Transformer models <a class="yt-timestamp" data-t="00:09:59">[00:09:59]</a> <a class="yt-timestamp" data-t="01:14:43">[01:14:43]</a>. This is due to the parallel token generation within each resolution map <a class="yt-timestamp" data-t="01:00:01">[01:00:01]</a>.
*   **Computational Complexity**: The time complexity for generating an image with N² tokens is significantly reduced to O(N⁴) for VAR, compared to O(N⁶) for conventional autoregressive models that use raster scans <a class="yt-timestamp" data-t="01:01:54">[01:01:54]</a> <a class="yt-timestamp" data-t="01:14:07">[01:14:07]</a> <a class="yt-timestamp" data-t="01:14:10">[01:14:10]</a>.
*   **Zero-Shot Generalization**: VAR showcases strong zero-shot generalization abilities in downstream tasks like image inpainting, outpainting, and editing, meaning it can perform these tasks without specific fine-tuning <a class="yt-timestamp" data-t="01:12:42">[01:12:42]</a> <a class="yt-timestamp" data-t="01:28:42">[01:28:42]</a>.
*   **Simplicity**: The model achieves these results using a vanilla [[the_role_of_variational_autoencoders_in_latent_diffusion | VQ-VAE]] architecture and a standard GPT-2 Transformer, without relying on advanced techniques like Rotary Position Embeddings, SwiGLU MLP, or RMSNorm <a class="yt-timestamp" data-t="01:08:42">[01:08:42]</a> <a class="yt-timestamp" data-t="01:08:47">[01:08:47]</a> <a class="yt-timestamp" data-t="01:08:50">[01:08:50]</a>. This indicates that the core innovation in the inductive prior is the primary driver of performance.
*   **Scaling Laws**: VAR demonstrates predictable scaling laws, where increasing model parameters, training tokens, or optimal training compute leads to a predictable decrease in test loss <a class="yt-timestamp" data-t="01:15:05">[01:15:05]</a> <a class="yt-timestamp" data-t="01:15:10">[01:15:10]</a>.

## Future Work and Applications

The paper outlines several promising directions for future work:
*   **Advanced Tokenization**: Further improvements could be made by advancing the VQ-VAE tokenizer itself <a class="yt-timestamp" data-t="01:17:34">[01:17:34]</a>.
*   **Downstream Tasks**: Applying VAR to more complex tasks such as text-to-image generation is a high priority for exploration <a class="yt-timestamp" data-t="01:17:47">[01:17:47]</a>.
*   **3D and Video Generation**: The multiscale approach is naturally extensible to 3D data and videos <a class="yt-timestamp" data-t="01:17:57">[01:17:57]</a>. By formulating a similar 3D next-scale prediction, VAR could generate [[video_diffusion_models_in_generative_3d | videos]], potentially offering inherent advantages in temporal consistency compared to diffusion-based generators like [[stateoftheart_video_generation_and_multimodal_models | Sora]] <a class="yt-timestamp" data-t="01:18:00">[01:18:00]</a> <a class="yt-timestamp" data-t="01:18:04">[01:18:04]</a>. This concept could also apply to [[motion_modeling_in_ai | motion modeling]].
*   **Other Data Modalities**: The core idea of rethinking 1D flattening for high-dimensional data could be applied to other modalities like proteins or graphs <a class="yt-timestamp" data-t="01:18:50">[01:18:50]</a>.

## Controversy

The first author, Kairui Tong, faced a legal battle with ByteDance, where he interned <a class="yt-timestamp" data-t="00:05:01">[00:05:01]</a> <a class="yt-timestamp" data-t="00:05:21">[00:05:21]</a>. Allegations included maliciously disrupting or poisoning internal model training by altering code, leading to significant resource wastage <a class="yt-timestamp" data-t="00:05:34">[00:05:34]</a> <a class="yt-timestamp" data-t="00:05:39">[00:05:39]</a>.

## Why it Won Best Paper

VAR's success as a best paper award winner can be attributed to several factors:
*   **Strong Results**: The significant quantitative improvements on established benchmarks (FID, Inception Score) are a key factor <a class="yt-timestamp" data-t="01:23:23">[01:23:23]</a>.
*   **Promising Future Directions**: The clear and broad applicability to various tasks and data modalities highlights its potential for further research and development <a class="yt-timestamp" data-t="01:23:24">[01:23:24]</a>.
*   **Well-Written and Clear Figures**: The paper is praised for its clarity, educational value, and effective figures, making the complex concepts understandable <a class="yt-timestamp" data-t="01:23:25">[01:23:25]</a> <a class="yt-timestamp" data-t="01:23:31">[01:23:31]</a>.
*   **Elegant and Intuitive Idea**: The core idea of next-scale prediction is both simple and intuitively pleasing, grounded in how humans perceive images and analogous to principles found in CNNs <a class="yt-timestamp" data-t="00:23:59">[00:23:59]</a> <a class="yt-timestamp" data-t="01:20:57">[01:20:57]</a>. Papers that introduce simple yet highly effective concepts often win top awards <a class="yt-timestamp" data-t="00:24:15">[00:24:15]</a>.