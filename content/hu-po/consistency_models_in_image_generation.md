---
title: Consistency Models in Image Generation
videoId: 8b6NhnNYtpg
---

From: [[hu-po]] <br/> 

[[openai_and_consistency_models | Consistency Models]] are a new family of generative models that support efficient single-step image generation while maintaining high sample quality and avoiding adversarial training challenges found in other models <a class="yt-timestamp" data-t="01:16:15">[01:16:15]</a><a class="yt-timestamp" data-t="01:13:28">[01:13:28]</a>. They were developed by [[openai_and_consistency_models | OpenAI]], with notable contributions from Ilya Sutskever <a class="yt-timestamp" data-t="01:01:02">[01:01:02]</a>.

## Background and Motivation
[[diffusion_models_and_image_generation | Diffusion Models]] have achieved significant breakthroughs in image, audio, and [[stateoftheart_video_generation_and_multimodal_models | video generation]] <a class="yt-timestamp" data-t="00:05:33">[00:05:33]</a>. However, their primary limitation is a slow sampling speed, as they rely on an iterative generation process <a class="yt-timestamp" data-t="00:05:39">[00:05:39]</a><a class="yt-timestamp" data-t="01:10:59">[01:10:59]</a>. This iterative refinement means running the same network repeatedly to progressively denoise an image, which can take 10 to 2,000 times more compute than single-step models <a class="yt-timestamp" data-t="01:12:44">[01:12:44]</a><a class="yt-timestamp" data-t="00:05:50">[00:05:50]</a><a class="yt-timestamp" data-t="01:04:40">[01:04:40]</a>.

Traditional [[comparison_of_ganbased_methods_for_image_manipulation | Generative Adversarial Networks (GANs)]] offer fast one-step inference <a class="yt-timestamp" data-t="01:14:51">[01:14:51]</a>. However, [[comparison_of_ganbased_methods_for_image_manipulation | GANs]] are prone to unstable training and issues like mode collapse because of their adversarial training nature <a class="yt-timestamp" data-t="01:11:30">[01:11:30]</a><a class="yt-timestamp" data-t="01:11:33">[01:11:33]</a>. [[openai_and_consistency_models | Consistency Models]] aim to bridge this gap by offering high-quality generation with fast, single-step inference, without requiring adversarial training <a class="yt-timestamp" data-t="00:06:13">[00:06:13]</a><a class="yt-timestamp" data-t="01:11:30">[01:11:30]</a>.

## How Consistency Models Work

[[openai_and_consistency_models | Consistency Models]] build upon the concept of probability flow Ordinary Differential Equations (ODEs) <a class="yt-timestamp" data-t="01:15:12">[01:15:12]</a><a class="yt-timestamp" data-t="00:02:22">[00:02:22]</a>. A probability flow ODE describes how a given probability distribution evolves or changes over time, smoothly converting data to noise <a class="yt-timestamp" data-t="00:03:12">[00:03:12]</a><a class="yt-timestamp" data-t="02:22:10">[02:22:10]</a><a class="yt-timestamp" data-t="02:22:25">[02:22:25]</a>.

The core idea is to learn a mapping from any point on an ODE trajectory (from noisy image to clean image) directly to its origin, the clean image <a class="yt-timestamp" data-t="00:03:41">[00:03:41]</a><a class="yt-timestamp" data-t="00:04:49">[00:04:49]</a>. This mapping is performed by a neural network, denoted as F_theta, which takes a noisy image `xt` and its corresponding time `t` as input, and produces the original clean image `x0` <a class="yt-timestamp" data-t="00:03:54">[00:03:54]</a><a class="yt-timestamp" data-t="00:05:00">[00:05:00]</a>.

### Key Properties
*   **Self-Consistency**: The models are trained to be "consistent" for points on the same trajectory, meaning regardless of where on the trajectory an input point `xt` is sampled (e.g., heavily noisy or slightly noisy), the model should map it to the same initial clean image <a class="yt-timestamp" data-t="00:04:09">[00:04:09]</a><a class="yt-timestamp" data-t="01:48:44">[01:48:44]</a>. This means `F(Xt, t) = F(Xt', t')` for any `t` and `t'` on the same trajectory <a class="yt-timestamp" data-t="01:57:27">[01:57:27]</a>.
*   **Flexible Sampling**: Unlike [[diffusion_models_and_image_generation | Diffusion Models]] with fixed steps, [[openai_and_consistency_models | Consistency Models]] inherently support fast one-step generation <a class="yt-timestamp" data-t="00:06:20">[00:06:20]</a><a class="yt-timestamp" data-t="00:09:50">[00:09:50]</a>. They also allow for multi-step sampling, which can trade off compute for higher quality, similar to a "quality dial" <a class="yt-timestamp" data-t="00:06:27">[00:06:27]</a><a class="yt-timestamp" data-t="01:08:07">[01:08:07]</a>. This means one can run the denoiser for many steps for the highest quality or for one step for maximum speed <a class="yt-timestamp" data-t="01:13:37">[01:13:37]</a>.

### Mathematical Underpinnings
The process involves learning the trajectory of probability distributions over time <a class="yt-timestamp" data-t="02:24:53">[02:24:53]</a>. Starting with a pure Gaussian noise distribution at time `T` (the end of the diffusion process), the model aims to map it backwards in time to the original data distribution at time `0` (the clean image) <a class="yt-timestamp" data-t="01:15:56">[01:15:56]</a><a class="yt-timestamp" data-t="00:03:19">[00:03:19]</a>.

The evolution of the image `xt` at time `t` is described by a stochastic differential equation (SDE), which accounts for both drift (mean, `mu`) and diffusion (variance, `sigma`) influenced by Brownian motion <a class="yt-timestamp" data-t="01:29:27">[01:29:27]</a><a class="yt-timestamp" data-t="01:31:05">[01:31:05]</a>. This SDE can be converted into a probability flow ODE, which does not involve stochastic terms <a class="yt-timestamp" data-t="01:32:31">[01:32:31]</a>.

A "score function" (nabla log p_t(x)) approximates the gradient of the log-probability density of the data distribution, indicating the direction to move from noise to data <a class="yt-timestamp" data-t="01:34:41">[01:34:41]</a>. This score function is approximated by a neural network called the "score model" <a class="yt-timestamp" data-t="01:38:07">[01:38:07]</a><a class="yt-timestamp" data-t="01:50:51">[01:50:51]</a>. [[openai_and_consistency_models | Consistency Models]] often employ skip connections, similar to ResNets, to aid in learning these complex mappings <a class="yt-timestamp" data-t="02:02:10">[02:02:10]</a><a class="yt-timestamp" data-t="02:02:12">[02:02:12]</a>.

## Training Methods

[[openai_and_consistency_models | Consistency Models]] can be trained in two ways:

1.  **Consistency Distillation (CD)**: This method leverages pre-trained [[diffusion_models_and_image_generation | Diffusion Models]] to distill their knowledge into a single-step sampler <a class="yt-timestamp" data-t="00:08:04">[00:08:04]</a><a class="yt-timestamp" data-t="02:20:17">[02:20:17]</a>. By querying the pre-trained model for intermediate steps of a diffused image, pairs of adjacent points on the ODE trajectory are generated <a class="yt-timestamp" data-t="02:10:07">[02:10:07]</a><a class="yt-timestamp" data-t="02:11:10">[02:11:10]</a>. The [[openai_and_consistency_models | Consistency Model]] is then trained to minimize the difference between its outputs for these pairs <a class="yt-timestamp" data-t="02:13:12">[02:13:12]</a>. This process uses numerical ODE solvers like Euler or Heun methods, which approximate solutions to differential equations <a class="yt-timestamp" data-t="02:05:03">[02:05:03]</a><a class="yt-timestamp" data-t="02:05:07">[02:05:07]</a>.
2.  **Consistency Training (CT)**: This is a standalone generative model training approach that does not rely on any pre-trained [[diffusion_models_and_image_generation | Diffusion Model]] <a class="yt-timestamp" data-t="00:08:17">[00:08:17]</a><a class="yt-timestamp" data-t="02:25:55">[02:25:55]</a>. It seeks to directly estimate the score function needed for the probability flow ODE. This makes [[openai_and_consistency_models | Consistency Models]] an independent family of generative models, distinct from [[diffusion_models_and_image_generation | Diffusion Models]] <a class="yt-timestamp" data-t="01:52:32">[01:52:32]</a><a class="yt-timestamp" data-t="02:28:24">[02:28:24]</a>.

Both training methods employ an exponential moving average (EMA) of model parameters (weights) to create a "target network," which helps stabilize optimization, a technique also seen in deep reinforcement learning <a class="yt-timestamp" data-t="02:18:52">[02:18:52]</a><a class="yt-timestamp" data-t="02:19:50">[02:19:50]</a><a class="yt-timestamp" data-t="02:29:08">[02:29:08]</a>.

## Evaluation and Performance
[[openai_and_consistency_models | Consistency Models]] are evaluated using metrics like Fr√©chet Inception Distance (FID) and Inception Score, which measure image quality and diversity <a class="yt-timestamp" data-t="02:34:00">[02:34:00]</a><a class="yt-timestamp" data-t="02:37:00">[02:37:00]</a>. They have been tested on datasets such as CIFAR-10 <a class="yt-timestamp" data-t="02:36:58">[02:36:58]</a>, ImageNet <a class="yt-timestamp" data-t="02:37:03">[02:37:03]</a>, and LSUN Bedroom/Cat <a class="yt-timestamp" data-t="02:37:04">[02:37:04]</a>.

*   **Metric Function**: The choice of metric function (e.g., L1, L2, Learned Perceptual Image Patch Similarity (LPIPS)) to calculate the loss greatly impacts performance <a class="yt-timestamp" data-t="02:39:10">[02:39:10]</a><a class="yt-timestamp" data-t="02:47:00">[02:47:00]</a>. LPIPS is preferred as it better captures perceptual similarity by encoding images with a pre-trained image encoder before computing distance <a class="yt-timestamp" data-t="02:44:24">[02:44:24]</a>.
*   **ODE Solvers**: Higher-order ODE solvers like Heun generally provide better accuracy compared to simpler methods like Euler, but can be slower <a class="yt-timestamp" data-t="02:49:57">[02:49:57]</a><a class="yt-timestamp" data-t="01:43:02">[01:43:02]</a>.

While [[openai_and_consistency_models | Consistency Models]] (both distillation and standalone training) outperform other single-step non-adversarial generative models, their performance is generally lower than state-of-the-art [[comparison_of_ganbased_methods_for_image_manipulation | GANs]] <a class="yt-timestamp" data-t="01:14:50">[01:14:50]</a><a class="yt-timestamp" data-t="02:18:29">[02:18:29]</a><a class="yt-timestamp" data-t="02:32:00">[02:32:00]</a>. However, they avoid the unstable training issues associated with [[comparison_of_ganbased_methods_for_image_manipulation | GANs]] <a class="yt-timestamp" data-t="01:11:30">[01:11:30]</a>.

## Applications and Future Directions
[[openai_and_consistency_models | Consistency Models]] support various zero-shot [[data_generation_for_ai_models | data editing]] capabilities without explicit training for these tasks <a class="yt-timestamp" data-t="00:07:42">[00:07:42]</a>. These include:
*   Image inpainting <a class="yt-timestamp" data-t="01:11:42">[01:11:42]</a>
*   Colorization (e.g., converting grayscale images to color) <a class="yt-timestamp" data-t="01:11:42">[01:11:42]</a>
*   Super-resolution <a class="yt-timestamp" data-t="01:11:42">[01:11:42]</a>
*   Stroke-guided image generation <a class="yt-timestamp" data-t="01:17:01">[01:17:01]</a>
*   Interpolation between samples by traversing the latent space <a class="yt-timestamp" data-t="01:27:04">[01:27:04]</a>

The striking similarities between [[openai_and_consistency_models | Consistency Models]] and techniques used in [[continuous_and_discrete_data_in_generative_models | deep reinforcement learning]] (like target/online networks) suggest exciting prospects for cross-pollination of ideas and methods across these fields <a class="yt-timestamp" data-t="02:18:52">[02:18:52]</a><a class="yt-timestamp" data-t="02:19:39">[02:19:39]</a>.