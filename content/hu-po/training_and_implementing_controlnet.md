---
title: Training and implementing ControlNet
videoId: Mp-HMQcB_M4
---

From: [[hu-po]] <br/> 

ControlNet is a neural network structure that has "taken the Internet by storm" due to its ability to add conditional control to pre-trained large diffusion models like Stable Diffusion <a class="yt-timestamp" data-t="00:01:14">[00:01:14]</a>. Released in February 2023, its GitHub repository quickly garnered close to 7,000 stars, indicating rapid adoption <a class="yt-timestamp" data-t="00:01:36">[00:01:36]</a>. This system addresses the challenge of precisely controlling diffusion models, which traditionally rely on text prompts <a class="yt-timestamp" data-t="00:02:29">[00:02:29]</a>.

## Core Functionality and Advantages
[[ControlNet overview | ControlNet]] allows users to specify visual cues, such as edges or poses, to achieve much more fine-grained control over the final output image from a diffusion model <a class="yt-timestamp" data-t="00:02:38">[00:02:38]</a>. This capability enables new forms of content creation, including generating images from "coloring book pictures" (edge maps) or turning a picture of a backpack into various styles while maintaining its structure <a class="yt-timestamp" data-t="00:02:55">[00:02:55]</a>, <a class="yt-timestamp" data-t="00:03:20">[00:03:20]</a>.

Key advantages of ControlNet include:
*   **Robustness**: It learns task-specific conditions effectively, even with small [[training_and_finetuning_processes_for_ai_models | training datasets]] <a class="yt-timestamp" data-t="00:06:08">[00:06:08]</a>.
*   **Speed**: Training [[ControlNet overview | ControlNet]] is as fast as [[training_and_finetuning_processes_for_ai_models | fine-tuning]] a diffusion model <a class="yt-timestamp" data-t="00:06:11">[00:06:11]</a>.
*   **Accessibility**: It can be trained on personal devices with GPUs <a class="yt-timestamp" data-t="00:06:16">[00:06:16]</a>.
*   **Flexibility**: It supports various conditional inputs like Edge Maps, Segmentation Maps, and Key Points <a class="yt-timestamp" data-t="00:06:43">[00:06:43]</a>.

## Architecture and [[Technical setup and implementation details | Technical Implementation]]
ControlNet is based on the paper "Adding Conditional Control to Text-to-Image Diffusion Models" by Lvmin Zhang and Maneesh Agrawala from Stanford <a class="yt-timestamp" data-t="00:04:21">[00:04:21]</a>, <a class="yt-timestamp" data-t="00:04:44">[00:04:44]</a>. The core innovation lies in its network structure.

### Locked and Trainable Copies
ControlNet clones the weights of a large pre-trained diffusion model into two copies for each neural network block:
1.  **Locked Copy**: Preserves the original model's capabilities, learned from billions of images. Its parameters (Theta) remain fixed during initial training <a class="yt-timestamp" data-t="00:17:18">[00:17:18]</a>, <a class="yt-timestamp" data-t="00:43:56">[00:43:56]</a>.
2.  **Trainable Copy**: This copy (Theta C) is trained on task-specific datasets to learn the conditional control <a class="yt-timestamp" data-t="00:17:22">[00:17:22]</a>, <a class="yt-timestamp" data-t="00:43:58">[00:43:58]</a>. This approach prevents overfitting on small datasets and maintains the quality of the large models <a class="yt-timestamp" data-t="00:45:00">[00:45:00]</a>.

### Zero Convolution
The trainable and locked neural network blocks are connected using a unique type of convolutional layer called a "zero convolution" <a class="yt-timestamp" data-t="00:17:45">[00:17:45]</a>.
*   **Definition**: A zero convolution is a 1x1 convolutional layer with both weights and biases initialized to zero <a class="yt-timestamp" data-t="00:25:39">[00:25:39]</a>, <a class="yt-timestamp" data-t="00:45:19">[00:45:19]</a>.
*   **Functionality**: Since all weights and biases are initially zero, in the first training step, the zero convolution does not add new noise or influence the deep network features <a class="yt-timestamp" data-t="00:50:57">[00:50:57]</a>, <a class="yt-timestamp" data-t="00:51:10">[00:51:10]</a>. This ensures that the original model's quality is perfectly preserved <a class="yt-timestamp" data-t="00:51:16">[00:51:16]</a>. As training progresses, the weights progressively grow from zeros to optimized parameters <a class="yt-timestamp" data-t="00:17:53">[00:17:53]</a>, <a class="yt-timestamp" data-t="01:08:42">[01:08:42]</a>.
*   **Benefits**: This initialization strategy allows for faster training, similar to [[Finetuning and Training Curriculums in AI Models | fine-tuning]], by not requiring gradients to "overwrite" randomly initialized noise <a class="yt-timestamp" data-t="01:18:17">[01:18:17]</a>, <a class="yt-timestamp" data-t="00:19:03">[00:19:03]</a>. It also saves GPU memory <a class="yt-timestamp" data-t="01:17:20">[01:17:20]</a>.

### Integration with Stable Diffusion (UNet)
ControlNet is designed to work with the UNet architecture of Stable Diffusion <a class="yt-timestamp" data-t="01:02:50">[01:02:50]</a>, <a class="yt-timestamp" data-t="01:09:12">[01:09:12]</a>.
*   **Parallel Structure**: ControlNet creates a parallel network that mirrors the encoder blocks and the middle block of the Stable Diffusion UNet <a class="yt-timestamp" data-t="01:05:48">[01:05:48]</a>, <a class="yt-timestamp" data-t="01:17:57">[01:17:57]</a>.
*   **Conditional Inputs**: The diffusion model (e.g., Stable Diffusion 1.5) receives a text prompt (encoded by CLIP) <a class="yt-timestamp" data-t="01:28:50">[01:28:50]</a> and a diffusion time step (encoded by positional encoding) <a class="yt-timestamp" data-t="01:34:03">[01:34:03]</a>.
*   **Latent Space Operations**: Stable Diffusion operates in a latent space, converting 512x512 images into smaller 64x64 latent images <a class="yt-timestamp" data-t="01:11:36">[01:11:36]</a>, <a class="yt-timestamp" data-t="01:19:25">[01:19:25]</a>. ControlNet requires its image-based conditions (e.g., Canny Edge map) to be converted to this same 64x64 feature space <a class="yt-timestamp" data-t="01:11:53">[01:11:53]</a>.
*   **Condition Encoding**: A "Tiny Network e" (a small convolutional network) is used to encode image space conditions into these 64x64 feature maps for input into ControlNet <a class="yt-timestamp" data-t="01:13:51">[01:13:51]</a>.

### [[Training and finetuning processes for AI models | Training Processes]] and Strategies
The overall [[training_and_finetuning_processes_for_ai_models | training]] objective for [[ControlNet overview | ControlNet]] is to predict the noise in the latent image, given the current noisy latent image, time step, text prompt, and the task-specific conditional input <a class="yt-timestamp" data-t="01:20:44">[01:20:44]</a>, <a class="yt-timestamp" data-t="01:20:50">[01:20:50]</a>.

#### Dataset Preparation
ControlNet leverages various types of image-based conditions generated from large datasets:
*   **Canny Edge**: 3 million Edge-to-image caption pairs were obtained by applying Canny Edge detection with random thresholds to existing image-caption datasets <a class="yt-timestamp" data-t="01:27:51">[01:27:51]</a>, <a class="yt-timestamp" data-t="01:28:10">[01:28:10]</a>.
*   **Hough Lines**: 600k Edge-to-image caption pairs using a deep Hough transform <a class="yt-timestamp" data-t="01:29:56">[01:29:56]</a>, [[Finetuning and Training Curriculums in AI Models | fine-tuned]] from the Canny Edge model <a class="yt-timestamp" data-t="01:30:47">[01:30:47]</a>.
*   **HED Boundary**: Another type of edge detector, trained from the Canny Edge model for 150 GPU hours <a class="yt-timestamp" data-t="01:31:07">[01:31:07]</a>, <a class="yt-timestamp" data-t="01:32:16">[01:32:16]</a>.
*   **User Scribbles**: Synthesized from HED boundary detection with data augmentations (masking, morphological transformations) <a class="yt-timestamp" data-t="01:31:47">[01:31:47]</a>, <a class="yt-timestamp" data-t="01:31:50">[01:31:50]</a>.
*   **Human Pose**: 80k to 200k pose-image caption pairs obtained by using a learning-based pose estimation method to detect human key points in images <a class="yt-timestamp" data-t="01:32:43">[01:32:43]</a>, <a class="yt-timestamp" data-t="01:34:10">[01:34:10]</a>.
*   **Semantic Segmentation**: Datasets like COCO (polygons) and ADE20K (pixel-level) were used to create segmentation map pairs <a class="yt-timestamp" data-t="01:35:05">[01:35:05]</a>, <a class="yt-timestamp" data-t="01:36:09">[01:36:09]</a>.
*   **Depth**: 3 million depth-image caption pairs obtained using Midas, a monocular depth estimation model <a class="yt-timestamp" data-t="01:37:00">[01:37:00]</a>, <a class="yt-timestamp" data-t="01:37:11">[01:37:11]</a>.
*   **Normal Maps**: 225k normal-image caption pairs from the DIODE dataset, with some also approximated from Midas depth maps <a class="yt-timestamp" data-t="01:39:06">[01:39:06]</a>, <a class="yt-timestamp" data-t="01:40:26">[01:40:26]</a>.
*   **Cartoon Line Drawing**: 1 million line art cartoon-captioned pairs <a class="yt-timestamp" data-t="01:40:53">[01:40:53]</a>, <a class="yt-timestamp" data-t="01:41:10">[01:41:10]</a>.

#### Training Strategies
*   **Prompt Masking**: During [[training_and_finetuning_processes_for_ai_models | training]], 50% of text prompts are randomly replaced with empty strings <a class="yt-timestamp" data-t="01:23:37">[01:23:37]</a>. This encourages the Stable Diffusion model's encoder to learn more semantic information directly from the input control maps <a class="yt-timestamp" data-t="01:23:55">[01:23:55]</a>.
*   **Small-Scale Training**: For devices with limited computation (e.g., RTX 3070 laptops) <a class="yt-timestamp" data-t="01:25:06">[01:25:06]</a>, connections between ControlNet and specific decoder blocks (1, 2, 3, 4) can be disconnected, connecting only to the middle block <a class="yt-timestamp" data-t="01:24:51">[01:24:51]</a>, <a class="yt-timestamp" data-t="01:25:01">[01:25:01]</a>. This accelerates convergence by a factor of 1.6 and reduces computation <a class="yt-timestamp" data-t="01:25:03">[01:25:03]</a>, <a class="yt-timestamp" data-t="01:25:35">[01:25:35]</a>. Once reasonable association is achieved, these links can be reconnected for continued training <a class="yt-timestamp" data-t="01:25:15">[01:25:15]</a>.
*   **Large-Scale Training**: For powerful computational clusters (e.g., Nvidia A100 with 80GB memory) and large datasets (millions of pairs) <a class="yt-timestamp" data-t="01:25:50">[01:25:50]</a>, <a class="yt-timestamp" data-t="01:26:04">[01:26:04]</a>:
    1.  **Initial Phase**: ControlNet is trained for a sufficient number of iterations while the weights of the Stable Diffusion model are locked <a class="yt-timestamp" data-t="01:26:17">[01:26:17]</a>.
    2.  **Joint Training**: After initial [[training_and_finetuning_processes_for_ai_models | training]] (e.g., 50,000 steps), all Stable Diffusion weights are unlocked, and both ControlNet and Stable Diffusion are jointly [[Finetuning and Training Curriculums in AI Models | fine-tuned]] <a class="yt-timestamp" data-t="01:26:20">[01:26:20]</a>, <a class="yt-timestamp" data-t="01:26:40">[01:26:40]</a>. This approach is similar to "locked image tuning" strategies <a class="yt-timestamp" data-t="01:26:50">[01:26:50]</a>.

## [[Applications and implications of ControlNet | Applications and Implications]]
The [[applications_and_implications_of_controlnet | impact of ControlNet]] is significant for generative AI, particularly in creating more controllable and precise visual content.
*   **Content Creation**: It "opened the floodgates for all different types of content creation" <a class="yt-timestamp" data-t="00:03:16">[00:03:16]</a>.
*   **Control over Diffusion Models**: It allows feeding in precise controls like edge maps, segmentation maps, or human poses to dictate the output of diffusion models <a class="yt-timestamp" data-t="00:03:40">[00:03:40]</a>, <a class="yt-timestamp" data-t="00:06:43">[00:06:43]</a>.
*   **"Sudden Convergence"**: During [[training_and_finetuning_processes_for_ai_models | training]], the model exhibits a "sudden convergence" phenomenon, where it abruptly learns to adapt to input conditions once the zero convolutions' magnitudes sufficiently grow <a class="yt-timestamp" data-t="01:43:56">[01:43:56]</a>, <a class="yt-timestamp" data-t="01:58:39">[01:58:39]</a>.

The ability to train [[ControlNet overview | ControlNet]] on personal devices and achieve competitive results with commercial models trained on large computational clusters demonstrates its efficiency and potential for broader adoption <a class="yt-timestamp" data-t="02:21:42">[02:21:42]</a>, <a class="yt-timestamp" data-t="01:53:52">[01:53:52]</a>. While text-to-image models provided a "step function improvement" <a class="yt-timestamp" data-t="00:03:29">[00:03:29]</a>, ControlNet pushes beyond prompt-based control, offering the fine-tuned precision needed for [[applications_and_implications_of_controlnet | practical applications]] <a class="yt-timestamp" data-t="00:08:00">[00:08:00]</a>, <a class="yt-timestamp" data-t="00:08:13">[00:08:13]</a>. Its simple yet powerful concept of a "parasite shadow model" connected to each layer of the diffusion model opens "sky's the limit" possibilities for combining multiple conditions and future developments in [[control_net_for_animation_generation | generative AI]] <a class="yt-timestamp" data-t="02:07:04">[02:07:04]</a>, <a class="yt-timestamp" data-t="02:05:27">[02:05:27]</a>.