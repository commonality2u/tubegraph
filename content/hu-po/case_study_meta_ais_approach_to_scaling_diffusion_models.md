---
title: Case study Meta AIs approach to scaling diffusion models
videoId: eTBG17LANcI
---

From: [[hu-po]] <br/> 

A recent paper, "Scalable Diffusion Models with Transformers," published on December 19th, introduces a new class of [[Diffusion models and Transformers | diffusion models]] with Transformer backbones, achieving state-of-the-art image quality <a class="yt-timestamp" data-t="00:01:14">[00:01:14]</a>. The research originates from UC Berkeley and New York University, with significant compute budget provided by Meta AI's FAIR team <a class="yt-timestamp" data-t="00:01:36">[00:01:36]</a>. Authors include William Pebbles and Saining Xie <a class="yt-timestamp" data-t="00:02:06">[00:02:06]</a>.

## Core Innovation: Diffusion Transformers (DiT)
The central idea is to replace the commonly used U-Net backbone in [[Conditional diffusion models for neural networks | diffusion models]] with a Transformer architecture <a class="yt-timestamp" data-t="00:05:08">[00:05:08]</a>. This approach leverages the [[Scalability of Transformerbased diffusion models | scalability of Transformer-based diffusion models]] observed in other domains <a class="yt-timestamp" data-t="00:09:09">[00:09:09]</a>. The paper demonstrates that the "U-Net bias" is not crucial for the performance of [[Conditional diffusion models for neural networks | diffusion models]], which can be effectively replaced with standard Transformer designs <a class="yt-timestamp" data-t="00:09:09">[00:09:09]</a>.

### Architecture and Latent Space Operations
DiT models operate on a sequence of latent patches <a class="yt-timestamp" data-t="00:05:22">[00:05:22]</a>.
*   **Latent Diffusion**: The diffusion process occurs in the latent space <a class="yt-timestamp" data-t="00:05:14">[00:05:14]</a>. Input images (e.g., 256x256x3) are first encoded into a much lower-dimensional latent representation (e.g., 32x32x4) using an off-the-shelf convolutional Variational Autoencoder (VAE) encoder from Stable Diffusion <a class="yt-timestamp" data-t="00:29:10">[00:29:10]</a>, <a class="yt-timestamp" data-t="00:49:09">[00:49:09]</a>.
*   **Patchification**: This latent representation is then divided into smaller patches, similar to how Vision Transformers (ViTs) process images <a class="yt-timestamp" data-t="00:10:08">[00:10:08]</a>, <a class="yt-timestamp" data-t="00:28:46">[00:28:46]</a>. The paper experimented with patch sizes of P=2, 4, and 8, where a smaller P results in more patches and thus more input tokens to the Transformer <a class="yt-timestamp" data-t="00:31:52">[00:31:52]</a>.
*   **Positional Embeddings**: Positional embeddings are added to each patch token to convey its original spatial location within the latent image <a class="yt-timestamp" data-t="00:29:42">[00:29:42]</a>.
*   **Conditioning**: DiT models are conditioned on information such as the noise time step (T) and class labels (C). This is achieved by feeding vector embeddings of T and C as additional tokens into the Transformer <a class="yt-timestamp" data-t="00:33:12">[00:33:12]</a>.
*   **Adaptive Layer Normalization (AdaLN)**: The Transformer blocks incorporate adaptive normalization layers (AdaLN) to condition the model <a class="yt-timestamp" data-t="00:34:09">[00:34:09]</a>. This helps in maintaining well-distributed and well-shaped numbers within the neural network activations, aiding gradient flow and learning <a class="yt-timestamp" data-t="00:34:40">[00:34:40]</a>.
*   **Model Sizes**: DiT models are scaled by increasing Transformer depth (number of layers, N) and width (hidden dimension, D), as well as the number of multi-attention heads. For example, the largest model, DiT-XL, has 28 layers, 16 heads, and a hidden dimension of 1152 <a class="yt-timestamp" data-t="00:41:50">[00:41:50]</a>.

## Scaling Analysis and Results
The research analyzes the [[Implications of AI model scaling and convergence | scalability of diffusion Transformers]] through the lens of Giga-FLOPs (G-FLOPs) for forward pass complexity, finding it a more accurate measure of complexity than parameter counts, especially for image models <a class="yt-timestamp" data-t="00:05:26">[00:05:26]</a>, <a class="yt-timestamp" data-t="00:17:10">[00:17:10]</a>.

### Key Findings:
*   **Consistent Improvement**: DiTs with higher G-FLOPs (due to increased depth, width, or more input tokens from smaller patches) consistently achieve lower FID (Fr√©chet Inception Distance) scores, indicating better image quality <a class="yt-timestamp" data-t="00:05:39">[00:05:39]</a>, <a class="yt-timestamp" data-t="00:11:16">[00:11:16]</a>. This aligns with the general trend in machine learning where bigger models trained on more data yield better results <a class="yt-timestamp" data-t="00:05:57">[00:05:57]</a>.
*   **DiT-XL/2 Performance**: The largest model, DiT-XL/2 (XL size with patch factor P=2), significantly outperforms all prior [[Diffusion models and Transformers | diffusion models]] on class-conditional ImageNet benchmarks, achieving a state-of-the-art FID of 2.27 on 256x256 image generation <a class="yt-timestamp" data-t="00:11:32">[00:11:32]</a>, <a class="yt-timestamp" data-t="01:02:05">[01:02:05]</a>.
*   **Compute Efficiency**: Larger DiT models are also found to be more compute-efficient <a class="yt-timestamp" data-t="01:00:31">[01:00:31]</a>.
*   **Patch Size Impact**: Smaller patch sizes (e.g., P=2) generally lead to better performance, as they provide more global information and allow for capturing finer details at each patch <a class="yt-timestamp" data-t="00:56:01">[00:56:01]</a>.

## Training Environment and Costs
*   **Framework and Hardware**: All models were implemented in JAX and trained on Google's TPU v3 pods <a class="yt-timestamp" data-t="00:51:15">[00:51:15]</a>. This is notable given Meta's usual use of PyTorch and Nvidia GPUs <a class="yt-timestamp" data-t="00:52:19">[00:52:19]</a>.
*   **Training Time**: The DiT-XL model trains at 5.7 iterations per second on a TPU v3 256-pod <a class="yt-timestamp" data-t="00:52:46">[00:52:46]</a>. Training models for 400,000 steps took approximately 19 hours <a class="yt-timestamp" data-t="00:54:30">[00:54:30]</a>.
*   **Estimated Costs**: Rough calculations suggest that training a single DiT model for 400,000 steps on a 256-TPU pod could cost around $10,000, implying over $100,000 for the 12 models featured in one of their charts <a class="yt-timestamp" data-t="00:55:10">[00:55:10]</a>. The largest models were trained for up to 7 million steps <a class="yt-timestamp" data-t="01:01:47">[01:01:47]</a>.
*   **Hyperparameters**: The models were trained with a constant learning rate of 1e-4 and no weight decay (though an exponential moving average of weights was maintained) <a class="yt-timestamp" data-t="00:45:05">[00:45:05]</a>, <a class="yt-timestamp" data-t="00:47:26">[00:47:26]</a>. Many training hyperparameters were retained directly from the ADM model <a class="yt-timestamp" data-t="00:48:18">[00:48:18]</a>, suggesting significant room for further optimization.
*   **Sampling Steps**: While FID was evaluated at 250 sampling steps, the paper also explores the impact of varying steps (16 to 1024), finding diminishing returns beyond a certain point <a class="yt-timestamp" data-t="01:03:01">[01:03:01]</a>.

## Qualitative Results
The generated images exhibit "extremely crisp" details and high semantic correctness. Examples like parrots, puppies, and huskies show accurate features such as the number of legs, wings, and subtle details like a dog's wrist <a class="yt-timestamp" data-t="00:03:22">[00:03:22]</a>. However, some "weirdness" remains at higher-level semantic details, such as illogical connections in a boat's structure or disproportionate animal tails, despite highly realistic textures <a class="yt-timestamp" data-t="01:10:40">[01:10:40]</a>, <a class="yt-timestamp" data-t="01:12:31">[01:12:31]</a>.

## Future Implications
The paper highlights that [[Scaling and optimization in diffusion models | scaling and optimization in diffusion models]] should continue by scaling DiT to larger models and token counts <a class="yt-timestamp" data-t="01:15:13">[01:15:13]</a>. The accelerating pace of improvement in [[using_diffusion_models_for_visual_world_understanding | diffusion models]] suggests that high-resolution video generation from text prompts could become a reality in the near future, potentially transforming content creation globally <a class="yt-timestamp" data-t="01:14:45">[01:14:45]</a>. The work also has implications for [[Applications of diffusion models in small model and neural field generation | applications of diffusion models in small model and neural field generation]].