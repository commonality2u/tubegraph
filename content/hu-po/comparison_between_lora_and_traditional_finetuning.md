---
title: Comparison between LoRA and traditional finetuning
videoId: vjEPXSCbmDE
---

From: [[hu-po]] <br/> 

**Low-Rank Adaptation (LoRA)** is a [[lora_technique_for_model_adaptation | technique]] that has gained popularity for [[finetuning_machine_learning_models | fine-tuning existing large models]], particularly [[finetuning_language_models_for_specific_tasks | large language models]] (LLMs) and image generation models like Stable Diffusion <a class="yt-timestamp" data-t="00:01:09">[00:01:09]</a> <a class="yt-timestamp" data-t="00:01:27">[00:01:27]</a> <a class="yt-timestamp" data-t="00:01:34">[00:01:34]</a>. Developed by Microsoft in October 2021 <a class="yt-timestamp" data-t="00:01:40">[00:01:40]</a>, LoRA addresses the challenges of traditional full [[finetuning_machine_learning_models | fine-tuning]] by significantly reducing the number of trainable parameters and computational requirements <a class="yt-timestamp" data-t="00:03:37">[00:03:37]</a>.

## Traditional Finetuning (Full Finetuning)

Traditional [[finetuning_machine_learning_models | fine-tuning]] involves updating all parameters of a pre-trained model to adapt it to a particular domain or task <a class="yt-timestamp" data-t="00:02:28">[00:02:28]</a>. This approach is common in natural language processing (NLP), where large-scale pre-training on general domain data is followed by adaptation to specific tasks <a class="yt-timestamp" data-t="00:02:26">[00:02:26]</a> <a class="yt-timestamp" data-t="00:06:44">[00:06:44]</a>.

### Challenges of Full Finetuning
As models become larger, such as GPT-3 with 175 billion parameters <a class="yt-timestamp" data-t="00:03:32">[00:03:32]</a> <a class="yt-timestamp" data-t="00:08:43">[00:08:43]</a>, full [[finetuning_machine_learning_models | fine-tuning]] becomes less feasible due to:
*   **Prohibitive Cost**: Deploying independent instances of fully [[finetuning_machine_learning_models | fine-tuned]] models is extremely expensive <a class="yt-timestamp" data-t="00:03:34">[00:03:34]</a> <a class="yt-timestamp" data-t="01:16:43">[01:16:43]</a>.
*   **Memory Requirements**: [[technical_aspects_of_ai_model_training_and_finetuning | Training]] large models requires substantial GPU memory, for example, 1.2 terabytes for GPT-3 with Adam optimizer <a class="yt-timestamp" data-t="00:50:53">[00:50:53]</a>. This includes storing optimizer states for all parameters <a class="yt-timestamp" data-t="00:22:00">[00:22:00]</a> <a class="yt-timestamp" data-t="00:24:05">[00:24:05]</a>.
*   **Storage Burden**: Each [[finetuning_machine_learning_models | fine-tuned]] model contains as many parameters as the original model, making storing and deploying many independent instances challenging <a class="yt-timestamp" data-t="00:22:00">[00:22:00]</a> <a class="yt-timestamp" data-t="00:22:27">[00:22:27]</a>. The checkpoint size for GPT-3 can be 30 gigabytes <a class="yt-timestamp" data-t="00:51:30">[00:51:30]</a>.

## Low-Rank Adaptation (LoRA)

[[lora_technique_for_model_adaptation | LoRA]] tackles these issues by freezing the pre-trained model weights and injecting trainable rank decomposition matrices into each layer of the [[application_of_lora_in_transformer_architectures | Transformer architecture]] <a class="yt-timestamp" data-t="00:03:40">[00:03:40]</a> <a class="yt-timestamp" data-t="00:04:07">[00:04:07]</a>. This means only a small number of additional parameters (the "LoRA modules") are trained <a class="yt-timestamp" data-t="00:05:06">[00:05:06]</a>.

### Core Mechanism
The fundamental idea behind [[lora_technique_for_model_adaptation | LoRA]] is based on the hypothesis that the change in weights during model adaptation (ΔW) has a low intrinsic rank <a class="yt-timestamp" data-t="00:10:33">[00:10:33]</a> <a class="yt-timestamp" data-t="00:37:04">[00:37:04]</a>. Instead of directly training ΔW (which would be the same size as the original weight matrix W₀), LoRA represents ΔW as a product of two smaller matrices, B and A, where ΔW = BA <a class="yt-timestamp" data-t="00:38:41">[00:38:41]</a> <a class="yt-timestamp" data-t="00:39:02">[00:39:02]</a>.
*   W₀ (pre-trained weight matrix) has dimensions D x K.
*   B has dimensions D x R.
*   A has dimensions R x K.
*   R (the rank) is chosen to be much smaller than D or K <a class="yt-timestamp" data-t="00:41:14">[00:41:14]</a>.

During training, W₀ is frozen and does not receive gradient updates <a class="yt-timestamp" data-t="00:40:08">[00:40:08]</a>. Only matrices A and B (which contain trainable parameters) are optimized <a class="yt-timestamp" data-t="00:40:10">[00:40:10]</a>. Matrix A is initialized with random Gaussian values, while B is initialized with zeros, making ΔW zero at the start of training <a class="yt-timestamp" data-t="00:41:34">[00:41:34]</a> <a class="yt-timestamp" data-t="00:41:48">[00:41:48]</a>. The output of the forward pass is modified to be (W₀ + BA)X <a class="yt-timestamp" data-t="00:40:36">[00:40:36]</a>. A scaling factor (Alpha/R) is applied to ΔWX, which helps reduce the need to retune hyperparameters when varying R <a class="yt-timestamp" data-t="00:41:57">[00:41:57]</a> <a class="yt-timestamp" data-t="00:42:21">[00:42:21]</a>.

### Benefits of LoRA

1.  **Reduced Trainable Parameters**: LoRA can reduce the number of trainable parameters by up to 10,000 times compared to full [[finetuning_machine_learning_models | fine-tuning]] for models like GPT-3 175B <a class="yt-timestamp" data-t="00:35:29">[00:35:29]</a>. For GPT-3, trainable parameters can be as small as 0.01% of the total model parameters <a class="yt-timestamp" data-t="01:16:17">[01:16:17]</a>. This is achieved by encoding the task-specific parameter increment (ΔΦ) with a much smaller set of parameters (Θ) <a class="yt-timestamp" data-t="00:24:46">[00:24:46]</a>.
2.  **Lower GPU Memory Requirement**: LoRA reduces GPU memory requirements by up to three times, especially when using adaptive optimizers like Adam <a class="yt-timestamp" data-t="00:32:03">[00:32:03]</a>. This is because optimizer states are not maintained for the frozen pre-trained parameters <a class="yt-timestamp" data-t="00:21:58">[00:21:58]</a> <a class="yt-timestamp" data-t="00:24:22">[00:24:22]</a> <a class="yt-timestamp" data-t="00:50:51">[00:50:51]</a>. For GPT-3, VRAM usage during training can be reduced from 1.2 terabytes to 350 gigabytes <a class="yt-timestamp" data-t="00:50:53">[00:50:53]</a>.
3.  **No Additional Inference Latency**: When deployed, the learned LoRA weights (BA) can be explicitly computed and added directly to the original pre-trained weights (W₀ + BA) <a class="yt-timestamp" data-t="00:45:26">[00:45:26]</a>. This results in no additional inference latency compared to a fully [[finetuning_machine_learning_models | fine-tuned]] model, as the model size remains the same <a class="yt-timestamp" data-t="00:05:50">[00:05:50]</a> <a class="yt-timestamp" data-t="00:33:37">[00:33:37]</a> <a class="yt-timestamp" data-t="01:19:37">[01:19:37]</a>. This contrasts with "adapter layers" which add sequential compute steps, increasing latency <a class="yt-timestamp" data-t="00:29:53">[00:29:53]</a> <a class="yt-timestamp" data-t="00:30:52">[00:30:52]</a>.
4.  **Faster Training Throughput**: LoRA offers a 25% speed-up during [[technical_aspects_of_ai_model_training_and_finetuning | training]] compared to full [[finetuning_machine_learning_models | fine-tuning]] because gradients are not calculated for the vast majority of parameters <a class="yt-timestamp" data-t="00:52:03">[00:52:03]</a>.
5.  **Efficient Task Switching**: For multi-task scenarios, a pre-trained model can be shared, and multiple small LoRA modules can be built for different tasks <a class="yt-timestamp" data-t="00:12:11">[00:12:11]</a>. Switching between tasks is efficient: the existing BA can be subtracted from W₀, and a new BA for another task can be added <a class="yt-timestamp" data-t="00:45:46">[00:45:46]</a> <a class="yt-timestamp" data-t="00:51:44">[00:51:44]</a>. This significantly reduces storage requirements and task switching overhead <a class="yt-timestamp" data-t="00:12:25">[00:12:25]</a>. The checkpoint size for a LoRA module can be 10,000 times smaller (e.g., 35 megabytes vs. 30 gigabytes for GPT-3) <a class="yt-timestamp" data-t="00:51:27">[00:51:27]</a>.

### Application of LoRA
While initially focused on language models like GPT-3, [[lora_technique_for_model_adaptation | LoRA]]'s principles apply to any dense layers in deep learning models <a class="yt-timestamp" data-t="00:33:42">[00:33:42]</a> <a class="yt-timestamp" data-t="01:49:45">[01:49:45]</a>. It has been successfully applied to image generation models (e.g., diffusion models like Stable Diffusion) <a class="yt-timestamp" data-t="00:01:34">[00:01:34]</a> <a class="yt-timestamp" data-t="00:35:37">[00:35:37]</a>. In [[application_of_lora_in_transformer_architectures | Transformer architectures]], LoRA is typically applied to the query (WQ) and value (WV) projection matrices within the self-attention module <a class="yt-timestamp" data-t="00:46:50">[00:46:50]</a> <a class="yt-timestamp" data-t="00:50:00">[00:50:00]</a> <a class="yt-timestamp" data-t="01:03:46">[01:03:46]</a>.

### LoRA vs. Other Parameter-Efficient Methods

LoRA stands apart from other [[finetuning_pretrained_models_with_minimal_additional_parameters | parameter-efficient adaptation methods]] such as Adapter layers and Prefix Tuning:
*   **Adapter Layers**: These involve inserting new, small layers between existing layers and training only those <a class="yt-timestamp" data-t="00:29:16">[00:29:16]</a>. While they reduce trainable parameters, they introduce additional inference latency because they add sequential compute steps <a class="yt-timestamp" data-t="00:29:53">[00:29:53]</a> <a class="yt-timestamp" data-t="00:30:22">[00:30:22]</a>.
*   **Prefix Tuning**: This method optimizes a specific "prompt" or input layer activations by adding special tokens to the input sequence <a class="yt-timestamp" data-t="00:29:22">[00:29:22]</a> <a class="yt-timestamp" data-t="01:01:15">[01:01:15]</a>. This approach can be difficult to optimize and reduces the effective sequence length available for the task <a class="yt-timestamp" data-t="00:31:30">[00:31:30]</a> <a class="yt-timestamp" data-t="01:01:40">[01:01:40]</a>.

LoRA performs on par with or better than full [[finetuning_machine_learning_models | fine-tuning]] and other methods in model quality, despite having fewer trainable parameters <a class="yt-timestamp" data-t="00:05:40">[00:05:40]</a> <a class="yt-timestamp" data-t="00:58:24">[00:58:24]</a> <a class="yt-timestamp" data-t="01:11:49">[01:11:49]</a>. Its effectiveness is particularly pronounced in larger models like GPT-2 and GPT-3, possibly because larger models have more model capacity, leading to a higher likelihood of low-rank structure in their weight updates <a class="yt-timestamp" data-t="01:05:52">[01:05:52]</a> <a class="yt-timestamp" data-t="01:07:26">[01:07:26]</a>.

### Determining the Rank (R)
The rank `r` is a crucial hyperparameter for LoRA. Instead of a theoretical calculation, `r` is often chosen based on a predetermined "parameter budget" for the LoRA modules <a class="yt-timestamp" data-t="01:25:34">[01:25:34]</a> <a class="yt-timestamp" data-t="01:25:58">[01:25:58]</a>. Studies have shown that even a very small rank, such as `r=1`, can achieve competitive performance on some datasets, suggesting that the intrinsic rank of weight updates is indeed very low <a class="yt-timestamp" data-t="01:29:56">[01:29:56]</a> <a class="yt-timestamp" data-t="01:30:30">[01:30:30]</a> <a class="yt-timestamp" data-t="01:40:03">[01:40:03]</a>.

## Limitations of LoRA

*   **Batching Challenges**: It is not straightforward to batch inputs from different tasks that require different A and B matrices in a single forward pass <a class="yt-timestamp" data-t="01:16:43">[01:16:43]</a> <a class="yt-timestamp" data-t="01:17:10">[01:17:10]</a> <a class="yt-timestamp" data-t="01:17:14">[01:17:14]</a>.
*   **Optimal Configuration**: The optimal choice of which weight matrices to adapt with LoRA (e.g., Query, Key, Value, or MLP layers) can vary significantly depending on the model architecture and the specific task <a class="yt-timestamp" data-t="00:34:30">[00:34:30]</a> <a class="yt-timestamp" data-t="01:27:29">[01:27:29]</a>. This suggests further research is needed to determine the best application strategy <a class="yt-timestamp" data-t="00:50:04">[00:50:04]</a>.
*   **Task/Language Dependence**: While LoRA performs well on many tasks, a small rank might not suffice for every task or dataset, particularly if the downstream tasks are in a different language than the pre-training data <a class="yt-timestamp" data-t="01:30:51">[01:30:51]</a>.

## Conclusion

LoRA represents a significant advancement in [[finetuning_machine_learning_models | fine-tuning]] large models by enabling [[finetuning_pretrained_models_with_minimal_additional_parameters | finetuning pretrained models with minimal additional parameters]] <a class="yt-timestamp" data-t="00:05:40">[00:05:40]</a>. Its ability to drastically reduce memory consumption, storage needs, and training time, while maintaining or even improving model quality and introducing no additional inference latency after merging, makes it a highly efficient and practical [[technical_aspects_of_ai_model_training_and_finetuning | technical aspects of AI model training and finetuning]] solution <a class="yt-timestamp" data-t="00:51:57">[00:51:57]</a> <a class="yt-timestamp" data-t="00:33:37">[00:33:37]</a>. The principle of low-rank updates suggests an inherent compressibility in the necessary changes for adaptation, providing a glimpse into the underlying mechanisms of deep learning <a class="yt-timestamp" data-t="01:21:06">[01:21:06]</a> <a class="yt-timestamp" data-t="01:50:48">[01:50:48]</a>.