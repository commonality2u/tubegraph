---
title: Thermodynamic gradient descent
videoId: Hxv_y_bI3zA
---

From: [[hu-po]] <br/> 

"Thermodynamic Gradient Descent" (TGD) is an optimization method leveraging novel hardware paradigms to improve the training of neural networks <a class="yt-timestamp" data-t="02:38:00">[02:38:00]</a>. This approach centers around a paper titled "Thermodynamic Natural Gradient Descent," released on May 22, 2024, by researchers at Normal Computing <a class="yt-timestamp" data-t="02:45:00">[02:45:00]</a> <a class="yt-timestamp" data-t="02:52:00">[02:52:00]</a> <a class="yt-timestamp" data-t="02:55:00">[02:55:00]</a>. It proposes a new hybrid digital-analog algorithm for training neural networks that is equivalent to [[natural_gradient_descent_vs_stochastic_gradient_descent | Natural Gradient Descent]] (NGD) <a class="yt-timestamp" data-t="03:50:00">[03:50:00]</a>.

## Limitations of Current Hardware and Optimizers

The current landscape of deep learning predominantly relies on conventional digital hardware <a class="yt-timestamp" data-t="06:21:00">[06:21:00]</a>. This hardware is facing fundamental limits due to the "ending of both Moore's Law and Denard's Law" <a class="yt-timestamp" data-t="04:41:00">[04:41:00]</a> <a class="yt-timestamp" data-t="05:18:00">[05:18:00]</a>. As computational needs continue to grow, this presents an opportunity for specialized, unconventional hardware <a class="yt-timestamp" data-t="04:50:00">[04:50:00]</a> <a class="yt-timestamp" data-t="05:20:00">[05:20:00]</a>.

Currently, most deep learning models use relatively simplistic first-order optimizers such as [[natural_gradient_descent_vs_stochastic_gradient_descent | Stochastic Gradient Descent]] (SGD), Adam, and their variants <a class="yt-timestamp" data-t="06:31:00">[06:31:00]</a> <a class="yt-timestamp" data-t="06:37:00">[06:37:00]</a>. While these are popular for training large AI models, digital computers are not efficient at second-order training methods <a class="yt-timestamp" data-t="06:48:00">[06:48:00]</a> <a class="yt-timestamp" data-t="07:01:00">[07:01:00]</a>.

## Thermodynamic/Probabilistic Computers

Thermodynamic computers, sometimes referred to as [[latent_diffusion_model_for_neural_networks | probabilistic computers]], operate between traditional digital computers and quantum computers <a class="yt-timestamp" data-t="07:26:00">[07:26:00]</a> <a class="yt-timestamp" data-t="07:30:00">[07:30:00]</a> <a class="yt-timestamp" data-t="08:13:00">[08:13:00]</a>.
*   **Digital computers** (CPUs, GPUs) are limited to ones and zeros, implemented with transistors and silicon <a class="yt-timestamp" data-t="07:34:00">[07:34:00]</a> <a class="yt-timestamp" data-t="07:40:00">[07:40:00]</a>.
*   **Quantum computers** rely on quantum states of matter and wave functions, but are still in early stages of development <a class="yt-timestamp" data-t="07:45:00">[07:45:00]</a> <a class="yt-timestamp" data-t="07:51:00">[07:51:00]</a> <a class="yt-timestamp" data-t="07:58:00">[07:58:00]</a>.
*   **Thermodynamic computers** utilize the "vibration of matter" and natural physical processes (thermodynamic processes involving heat) to perform computations <a class="yt-timestamp" data-t="08:21:00">[08:21:00]</a> <a class="yt-timestamp" data-t="08:52:00">[08:52:00]</a> <a class="yt-timestamp" data-t="09:00:00">[09:00:00]</a>. They are particularly well-suited for tasks involving randomness, such as probabilistic and generative AI <a class="yt-timestamp" data-t="09:11:00">[09:11:00]</a> <a class="yt-timestamp" data-t="09:13:00">[09:13:00]</a>.

### Key Players and Hardware Details
Normal Computing AI is a startup focused on probabilistic AI, launching from stealth to make a "big splash" <a class="yt-timestamp" data-t="05:37:00">[05:37:00]</a> <a class="yt-timestamp" data-t="05:50:00">[05:50:00]</a> <a class="yt-timestamp" data-t="06:01:00">[06:01:00]</a>. Unlike some other companies in the space, such as Extropic, Normal Computing publishes their work and open-sources code on GitHub <a class="yt-timestamp" data-t="09:35:00">[09:35:00]</a> <a class="yt-timestamp" data-t="10:57:00">[10:57:00]</a> <a class="yt-timestamp" data-t="10:59:00">[10:59:00]</a> <a class="yt-timestamp" data-t="11:32:00">[11:32:00]</a>.

Thermodynamic computers use a component called a Stochastic Processing Unit (SPU) <a class="yt-timestamp" data-t="25:36:00">[25:36:00]</a>. These SPUs require thermal insulation and isolation from external factors to prevent interference with the atomic vibrations used for computation <a class="yt-timestamp" data-t="27:00:00">[27:00:00]</a> <a class="yt-timestamp" data-t="27:20:00">[27:20:00]</a> <a class="yt-timestamp" data-t="27:37:00">[27:37:00]</a>. This is a challenge, similar to quantum computers <a class="yt-timestamp" data-t="27:38:00">[27:38:00]</a>. The current hardware is in the proof-of-concept stage, often built with custom setups (e.g., 8020 aluminum framing) and not yet ready for production-scale tasks like training LLaMA 4 <a class="yt-timestamp" data-t="30:11:00">[30:11:00]</a> <a class="yt-timestamp" data-t="30:20:00">[30:20:00]</a> <a class="yt-timestamp" data-t="30:32:00">[30:32:00]</a>.

The hybrid system involves Digital-to-Analog Converters (DAC) and Analog-to-Digital Converters (ADC) to facilitate communication between the digital GPU and the analog SPU <a class="yt-timestamp" data-t="28:00:00">[28:00:00]</a> <a class="yt-timestamp" data-t="28:07:00">[28:07:00]</a>. Interestingly, the FPGAs used for this conversion feature chips from Texas Instruments, highlighting a potential geopolitical advantage for US-made chips over those from TSMC in Taiwan <a class="yt-timestamp" data-t="31:16:00">[31:16:00]</a> <a class="yt-timestamp" data-t="31:34:00">[31:34:00]</a> <a class="yt-timestamp" data-t="32:26:00">[32:26:00]</a> <a class="yt-timestamp" data-t="32:50:00">[32:50:00]</a>.

## Optimization Methods in Deep Learning

### Loss Landscape
In deep learning, the goal is to minimize an objective function, often called the "loss function" <a class="yt-timestamp" data-t="33:31:00">[33:31:00]</a> <a class="yt-timestamp" data-t="35:36:00">[35:36:00]</a>. This can be visualized as a "loss landscape," where each point represents the loss value for a specific combination of neural network parameters <a class="yt-timestamp" data-t="13:36:00">[13:36:00]</a> <a class="yt-timestamp" data-t="14:16:00">[14:16:00]</a>. Gradient descent aims to find the "global minimum" of this landscape <a class="yt-timestamp" data-t="14:19:00">[14:19:00]</a> <a class="yt-timestamp" data-t="14:27:00">[14:27:00]</a>.

In the context of statistics, minimizing the objective function is analogous to minimizing the [[introduction_to_relative_entropy_and_kl_divergence | Kullback-Leibler (KL) Divergence]] between the true data distribution and the distribution learned by the neural network <a class="yt-timestamp" data-t="36:17:00">[36:17:00]</a> <a class="yt-timestamp" data-t="36:20:00">[36:20:00]</a> <a class="yt-timestamp" data-t="37:12:00">[37:12:00]</a>.

### First Order vs. Second Order Optimization
*   **First-order methods** (like SGD) follow the "direction of steepest descent" defined by the negative gradient, which is effectively the first derivative or "slope" of the loss landscape at a given point <a class="yt-timestamp" data-t="17:45:00">[17:45:00]</a> <a class="yt-timestamp" data-t="37:30:00">[37:30:00]</a> <a class="yt-timestamp" data-t="37:51:00">[37:51:00]</a>. These methods take small steps based on local information <a class="yt-timestamp" data-t="20:45:00">[20:45:00]</a>.
*   **Second-order methods** (like [[natural_gradient_descent_vs_stochastic_gradient_descent | Natural Gradient Descent]]) use the second derivatives of a function, such as the Hessian matrix, or approximations like the Fisher Information Matrix, to understand the "curvature" of the loss landscape <a class="yt-timestamp" data-t="18:07:00">[18:07:00]</a> <a class="yt-timestamp" data-t="18:10:00">[18:10:00]</a> <a class="yt-timestamp" data-t="39:08:00">[39:08:00]</a> <a class="yt-timestamp" data-t="41:04:00">[41:04:00]</a> <a class="yt-timestamp" data-t="43:04:00">[43:04:00]</a>. This allows for more efficient navigation of complex loss surfaces and larger, better gradient steps, leading to faster convergence in fewer iterations <a class="yt-timestamp" data-t="18:50:00">[18:50:00]</a> <a class="yt-timestamp" data-t="20:38:00">[20:38:00]</a> <a class="yt-timestamp" data-t="23:18:00">[23:18:00]</a> <a class="yt-timestamp" data-t="43:49:00">[43:49:00]</a>.

However, a single iteration of NGD is generally more computationally expensive than SGD or Adam when performed on digital computers <a class="yt-timestamp" data-t="23:57:00">[23:57:00]</a> <a class="yt-timestamp" data-t="24:01:00">[24:01:00]</a>. This is because calculating the Fisher Information Matrix can be difficult, and its computation involves cubic runtime complexity (O(N^3)) and quadratic memory cost (O(N^2)) with respect to the number of parameters (N) <a class="yt-timestamp" data-t="39:30:00">[39:30:00]</a> <a class="yt-timestamp" data-t="40:07:00">[40:07:00]</a> <a class="yt-timestamp" data-t="52:26:00">[52:26:00]</a>. This "cubic runtime is absolutely gnarly" <a class="yt-timestamp" data-t="54:48:00">[54:48:00]</a>, which is why NGD has rarely been used in practice for large-scale training <a class="yt-timestamp" data-t="02:28:00">[02:28:00]</a> <a class="yt-timestamp" data-t="24:05:00">[24:05:00]</a>.

## Thermodynamic Natural Gradient Descent (TNGD)

The core innovation of Normal Computing's paper is demonstrating that NGD can have similar computational complexity per iteration to first-order methods when employing appropriate hardware: thermodynamic computers <a class="yt-timestamp" data-t="03:34:00">[03:34:00]</a> <a class="yt-timestamp" data-t="03:50:00">[03:50:00]</a>. Using this hybrid digital-analog system, TNGD can perform NGD with linear runtime and memory scaling (O(N)), drastically reducing its computational overhead <a class="yt-timestamp" data-t="52:53:00">[52:53:00]</a> <a class="yt-timestamp" data-t="53:01:00">[53:01:00]</a> <a class="yt-timestamp" data-t="11:10:00">[11:10:00]</a> <a class="yt-timestamp" data-t="54:03:00">[54:03:00]</a> <a class="yt-timestamp" data-t="11:08:00">[11:08:00]</a>.

### Hybrid Digital-Analog Algorithm
The TNGD algorithm operates in a hybrid loop:
1.  A traditional **GPU** stores the model architecture, calculates the first-order gradient, and components of the Fisher Information Matrix (Jacobian and Hessian) <a class="yt-timestamp" data-t="26:12:00">[26:12:00]</a> <a class="yt-timestamp" data-t="26:31:00">[26:31:00]</a> <a class="yt-timestamp" data-t="28:51:00">[28:51:00]</a>. These are sent to the SPU via a DAC <a class="yt-timestamp" data-t="28:00:00">[28:00:00]</a>.
2.  The **SPU** (stochastic processing unit), the analog thermodynamic computer, takes these inputs and "evolves" by allowing its internal particles to vibrate and dissipate energy <a class="yt-timestamp" data-t="28:21:00">[28:21:00]</a> <a class="yt-timestamp" data-t="28:28:00">[28:28:00]</a> <a class="yt-timestamp" data-t="55:07:00">[55:07:00]</a>. This physical process effectively implements the parameter update and calculates the natural gradient <a class="yt-timestamp" data-t="12:28:00">[12:28:00]</a> <a class="yt-timestamp" data-t="26:45:00">[26:45:00]</a>. The SPU's dynamics are related to an Ornstein-Uhlenbeck process, which is a stationary Gaussian Markov process (akin to Brownian motion) <a class="yt-timestamp" data-t="12:30:00">[12:30:00]</a> <a class="yt-timestamp" data-t="12:44:00">[12:44:00]</a>.
3.  The SPU settles into an equilibrium state defined by a Boltzmann distribution, which provides an estimate of the natural gradient <a class="yt-timestamp" data-t="55:28:00">[55:28:00]</a> <a class="yt-timestamp" data-t="55:31:00">[55:31:00]</a> <a class="yt-timestamp" data-t="56:03:00">[56:03:00]</a>.
4.  Samples are taken from the SPU and sent back to the GPU (via an ADC) for averaging and updating the model parameters <a class="yt-timestamp" data-t="58:01:00">[58:01:00]</a>.

A critical finding is that samples can be taken even before equilibrium is fully reached "without harming performance significantly" <a class="yt-timestamp" data-t="58:23:00">[58:23:00]</a> <a class="yt-timestamp" data-t="58:27:00">[58:27:00]</a>. This is possible because the "clock speed of nature" (the speed at which atoms vibrate and heat dissipates) is "incredibly fast" and significantly faster than a GPU's clock speed <a class="yt-timestamp" data-t="59:07:00">[59:07:00]</a> <a class="yt-timestamp" data-t="59:15:00">[59:15:00]</a>.

The delay time between the GPU and SPU, due to their asynchronous operation and different clock speeds, is "not necessarily detrimental to the performance and can in fact improve it" <a class="yt-timestamp" data-t="01:00:11">[01:00:11]</a> <a class="yt-timestamp" data-t="01:00:14">[01:00:14]</a> <a class="yt-timestamp" data-t="01:00:17">[01:00:17]</a> <a class="yt-timestamp" data-t="01:00:25">[01:00:25]</a>.
Additionally, the system allows for a "smooth interpolation between SGD and NGD" <a class="yt-timestamp" data-t="01:01:11">[01:01:11]</a>. The amount of time the SPU is allowed to evolve (τ) acts as a toggle: if τ is zero, it effectively performs SGD; if τ goes to infinity, it performs NGD <a class="yt-timestamp" data-t="01:01:50">[01:01:50]</a> <a class="yt-timestamp" data-t="01:01:53">[01:01:53]</a> <a class="yt-timestamp" data-t="01:01:55">[01:01:55]</a>.

### Momentum Effect
A fascinating finding is that the delay in the thermodynamic computer creates a "momentum effect" <a class="yt-timestamp" data-t="01:12:32">[01:12:32]</a> <a class="yt-timestamp" data-t="01:12:35">[01:12:35]</a>.
*   In traditional optimizers like Adam, momentum terms are explicitly added to "overcome local minimas" in the loss landscape <a class="yt-timestamp" data-t="01:13:00">[01:13:00]</a> <a class="yt-timestamp" data-t="01:13:08:00">[01:13:08:00]</a> <a class="yt-timestamp" data-t="01:13:12:00">[01:13:12:00]</a>. They encode a history of past steps to continue movement even if the immediate gradient is small or zero <a class="yt-timestamp" data-t="01:13:30">[01:13:30]</a> <a class="yt-timestamp" data-t="01:14:18">[01:14:18]</a>.
*   In the SPU, when new data is fed, there is "a little bit of leftover kind of like thermal heat or kind of like vibrational information from the previous batch" <a class="yt-timestamp" data-t="01:15:30">[01:15:30]</a> <a class="yt-timestamp" data-t="01:15:37">[01:15:37]</a>. This residual thermal information acts as a non-explicit momentum, propagating across iterations and allowing TGD to potentially outperform even pure NGD <a class="yt-timestamp" data-t="01:12:49">[01:12:49]</a> <a class="yt-timestamp" data-t="01:12:51">[01:12:51]</a> <a class="yt-timestamp" data-t="01:16:07">[01:16:07]</a>.

## Experimental Results and Future Implications

The paper demonstrates TGD's superiority over state-of-the-art digital first and second-order training methods on classification tasks and language model fine-tuning <a class="yt-timestamp" data-t="04:13:00">[04:13:00]</a> <a class="yt-timestamp" data-t="04:21:00">[04:21:00]</a>.
*   **Classification**: TGD was tested on MNIST classification using a convolutional network (ConvNet). While results showed TGD achieving slightly lower loss faster than Atom, the error bars were thick, making the difference less compelling <a class="yt-timestamp" data-t="01:08:20">[01:08:20]</a> <a class="yt-timestamp" data-t="01:08:48">[01:08:48]</a> <a class="yt-timestamp" data-t="01:09:50">[01:09:50]</a>.
*   **Language Model Fine-tuning**: TGD was used to fine-tune a LoRA for DistilBERT on the Stanford Question Answering Dataset (SQuAD) <a class="yt-timestamp" data-t="01:10:19">[01:10:19]</a> <a class="yt-timestamp" data-t="01:10:28">[01:10:28]</a>. Pure TGD showed worse performance than Atom, but a hybrid approach (using natural gradient estimates with the Atom update rule) yielded the best results <a class="yt-timestamp" data-t="01:10:56">[01:10:56]</a> <a class="yt-timestamp" data-t="01:11:04">[01:11:04]</a>.

It's important to note that these experiments were conducted on "toy problems" and the results were *simulated* rather than run on actual physical hardware <a class="yt-timestamp" data-t="01:08:14">[01:08:14]</a> <a class="yt-timestamp" data-t="01:18:10">[01:18:10]</a> <a class="yt-timestamp" data-t="01:19:12">[01:19:12]</a>. This requires significant GPU resources (e.g., an Nvidia A100 with 80GB RAM) to simulate the thermodynamic process <a class="yt-timestamp" data-t="01:21:37">[01:21:37]</a> <a class="yt-timestamp" data-t="01:22:50">[01:22:50]</a>.

Despite these early limitations, TGD "greatly reduces the computational overhead typically associated with second order methods for arbitrary model architectures" <a class="yt-timestamp" data-t="01:16:46">[01:16:46]</a>. The practical implications, however, rely on the "future availability of analog thermodynamic computers" <a class="yt-timestamp" data-t="01:17:06">[01:17:06]</a>. This technology is still in its infancy, decades away from widespread adoption or consumer use <a class="yt-timestamp" data-t="01:35:34">[01:35:34]</a>. The slowest part of the current hybrid system is the conversion between analog and digital information <a class="yt-timestamp" data-t="01:04:46">[01:04:46]</a> <a class="yt-timestamp" data-t="01:05:01">[01:05:01]</a>.

If these chips prove scalable and effective, they could carve out a significant market for specialized AI training hardware, potentially even leading to fully thermodynamic computing in the distant future <a class="yt-timestamp" data-t="01:05:11">[01:05:11]</a> <a class="yt-timestamp" data-t="01:05:51">[01:05:51]</a> <a class="yt-timestamp" data-t="01:38:13">[01:38:13]</a> <a class="yt-timestamp" data-t="01:44:48">[01:44:48]</a>. They could also contribute to reducing reliance on offshore chip manufacturing due to US-based fabrication capabilities <a class="yt-timestamp" data-t="01:33:50">[01:33:50]</a> <a class="yt-timestamp" data-t="01:34:04">[01:34:04]</a>.