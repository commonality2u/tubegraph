---
title: Comparison of RT2 with Previous Robotics Methodologies
videoId: Utv0XpXUGQE
---

From: [[hu-po]] <br/> 

[[rt2_robotic_transformer_for_vision_language_action_models | RT2]] (Robotic Transformer 2) represents a significant departure from traditional robotics methodologies by integrating [[large_language_models_in_robotics | large language models]] directly into robotic control. This approach leverages internet-scale data to achieve improved generalization and emergent capabilities, a stark contrast to older, more modular systems <a class="yt-timestamp" data-t="01:25:26">[01:25:26]</a>.

## Traditional Robotics Architectures
Historically, robotics relied on modular designs, with different components handling specific tasks.
*   **Modular Design ([[challenges_of_robotics_integration | ROS]]):** The Robot Operating System ([[challenges_of_robotics_integration | ROS]]), founded in 2007, exemplified this approach <a class="yt-timestamp" data-t="01:28:44">[01:28:44]</a>. In a [[challenges_of_robotics_integration | ROS]]-based robot, various computers and modules operate in a publisher-subscriber model <a class="yt-timestamp" data-t="01:31:56">[01:31:56]</a>. For instance, a vision module subscribes to a camera module, processes images, and publishes information (e.g., object distance), which other modules then consume <a class="yt-timestamp" data-t="01:32:32">[01:32:32]</a>.
    *   **Perceived Benefit:** Modularity was believed to be the path to AGI (Artificial General Intelligence), allowing individual pieces to be improved separately and combined over time <a class="yt-timestamp" data-t="01:42:55">[01:42:55]</a>.
    *   **Challenges:** Despite the perceived ease of debugging, modular systems like [[challenges_of_robotics_integration | ROS]] can become "very hairy" due to complexity, race conditions, and stale data propagation <a class="yt-timestamp" data-t="01:50:57">[01:50:57]</a>.

*   **Separate Vision and Control Models:** Previous robotics methods often used [[large_language_models_in_robotics | language models]] for high-level planning, such as breaking down a command into primitive actions <a class="yt-timestamp" data-t="01:46:17">[01:46:17]</a>. These primitives would then be executed by separate low-level controllers, which did not benefit from the rich semantic knowledge of internet-scale models <a class="yt-timestamp" data-t="01:49:18">[01:49:18]</a>.

*   **Pre-trained Visual Representations:** Before the advent of large-scale [[integration_of_vision_language_models_in_robotics | Vision Language Models]], it was common for only the vision model to utilize deep learning <a class="yt-timestamp" data-t="03:54:19">[03:54:19]</a>. These convolutional networks (CNNs) were typically pre-trained on datasets like ImageNet, which provided a visual representation to downstream robotic tasks <a class="yt-timestamp" data-t="03:59:43">[03:59:43]</a>. However, these models were often specific to detection or pose estimation, providing precise but limited answers <a class="yt-timestamp" data-t="04:33:13">[04:33:13]</a>.

*   **Camera Calibration:** Traditional robot control systems relied on precisely calibrated cameras to determine object locations in 3D space. This ensured high accuracy for control policies <a class="yt-timestamp" data-t="03:51:24">[03:51:24]</a>. However, camera calibration is a "huge pain" and impractical for home robots where camera stability cannot be guaranteed <a class="yt-timestamp" data-t="03:53:32">[03:53:32]</a>.

## [[rt2_robotic_transformer_for_vision_language_action_models | RT2]]'s End-to-End Approach
[[rt2_robotic_transformer_for_vision_language_action_models | RT2]] contrasts these older methods by pursuing an end-to-end, unified model for robotic control.
*   **Single, End-to-End Model:** [[rt2_robotic_transformer_for_vision_language_action_models | RT2]] aims to be a single, end-to-end trained model that directly maps robot observations (images, sensors) to actions (commands sent to robot joints) <a class="yt-timestamp" data-t="05:04:47">[05:04:47]</a>. This means the model directly consumes visual inputs and outputs low-level robot actions <a class="yt-timestamp" data-t="01:56:06">[01:56:06]</a>.
    *   While described as a "single model," [[rt2_robotic_transformer_for_vision_language_action_models | RT2]] still incorporates a Vision Transformer (ViT) within its architecture, which is pre-trained on other data <a class="yt-timestamp" data-t="02:48:00">[02:48:00]</a>. The definition of "end-to-end" and "single model" is becoming "more fuzzy" <a class="yt-timestamp" data-t="02:52:50">[02:52:50]</a>.

*   **Tokenizing Actions as Text:** A core innovation in [[rt2_robotic_transformer_for_vision_language_action_models | RT2]] is representing robot actions as text tokens <a class="yt-timestamp" data-t="02:56:09">[02:56:09]</a>. These action tokens are incorporated directly into the model's training set, treated in the same way as natural [[large_language_models_in_robotics | language models]] tokens <a class="yt-timestamp" data-t="06:49:03">[06:49:03]</a>. This allows the model to leverage the benefits of [[large_language_models_in_robotics | large-scale pre-training]] on web data <a class="yt-timestamp" data-t="05:33:04">[05:33:04]</a>.

*   **Leveraging Web-Scale Pre-training:** Instead of millions of robotic interaction trials, [[rt2_robotic_transformer_for_vision_language_action_models | RT2]] benefits from being fine-tuned from [[large_language_models_in_robotics | Vision Language Models]] pre-trained on billions of tokens and images from the web <a class="yt-timestamp" data-t="01:42:07">[01:42:07]</a>. This allows [[rt2_robotic_transformer_for_vision_language_action_models | RT2]] to acquire "emergent capabilities" <a class="yt-timestamp" data-t="01:20:41">[01:20:41]</a>, such as generalization to novel objects and interpreting commands not present in robot training data <a class="yt-timestamp" data-t="01:27:06">[01:27:06]</a>.

*   **Emergent Semantic Reasoning:** [[rt2_robotic_transformer_for_vision_language_action_models | RT2]] exhibits emergent semantic reasoning, meaning it can perform logical, common-sense tasks and behaviors not explicitly trained for <a class="yt-timestamp" data-t="04:50:54">[04:50:54]</a>. This includes capabilities like identifying the smallest object, the one closest to another, or even which drink is best for someone tired <a class="yt-timestamp" data-t="01:34:56">[01:34:56]</a>.
    *   This generalization primarily occurs in semantic understanding and visual interpretation, not in acquiring new robotic motions <a class="yt-timestamp" data-t="02:52:03">[02:52:03]</a>. The model's physical skills are still limited to the distribution of skills seen in the robot data, typically pick-and-place variations <a class="yt-timestamp" data-t="02:50:52">[02:50:52]</a>.

*   **[[chain_of_thought_prompting_in_robotics | Chain of Thought prompting]]:** By augmenting commands with [[chain_of_thought_prompting_in_robotics | Chain of Thought prompting]], [[rt2_robotic_transformer_for_vision_language_action_models | RT2]] can perform multi-stage semantic reasoning <a class="yt-timestamp" data-t="01:47:33">[01:47:33]</a>. This allows the model to "plan its actions in natural language first," leading to more sophisticated command execution <a class="yt-timestamp" data-t="01:47:33">[01:47:33]</a> <a class="yt-timestamp" data-t="01:54:55">[01:54:55]</a>.

*   **No Calibrated Camera Required:** The end-to-end nature of [[rt2_robotic_transformer_for_vision_language_action_models | RT2]] means it "does not require a calibrated camera" <a class="yt-timestamp" data-t="03:42:47">[03:42:47]</a>. This is crucial for real-world home robots, as the system can adapt even if the camera alignment shifts <a class="yt-timestamp" data-t="03:54:06">[03:54:06]</a>.

*   **Cloud Robotics:** To address the challenge of running large models (up to 55 billion parameters) on robot hardware, [[rt2_robotic_transformer_for_vision_language_action_models | RT2]] is deployed in a multi-TPU cloud service <a class="yt-timestamp" data-t="04:17:51">[04:17:51]</a>. The robot queries this service over the network for real-time inference <a class="yt-timestamp" data-t="04:19:10">[04:19:10]</a>. This "Cloud Robotics" approach means computation is offloaded, making robots cheaper and potentially more secure as the model never resides locally <a class="yt-timestamp" data-t="04:54:33">[04:54:33]</a>.

The shift towards single, end-to-end models like [[rt2_robotic_transformer_for_vision_language_action_models | RT2]] and away from modular approaches signifies a "strategic position" for robotic learning to benefit directly from advancements in other fields, particularly [[large_language_models_in_robotics | Vision Language Models]] <a class="yt-timestamp" data-t="03:31:07">[03:31:07]</a>.