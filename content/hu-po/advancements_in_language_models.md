---
title: Advancements in language models
videoId: oQqOiwUhJkA
---

From: [[hu-po]] <br/> 

The release of OpenAI's latest model, "Strawberry," also known as O1, has sparked significant discussion within the AI community. While some hail it as a breakthrough, others view it as an overhyped iteration of existing techniques <a class="yt-timestamp" data-t="03:10:00">[03:10:00]</a>. The prevailing sentiment is that O1 represents the first production-grade integration of traditional reinforcement learning (RL) with [[llm_large_language_models_development | Large Language Models (LLMs)]] <a class="yt-timestamp" data-t="01:24:22">[01:24:22]</a>.

## The "Strawberry" (O1) Model
Strawberry is OpenAI's latest model, though its exact nature remains somewhat ambiguous; it's described as a collection of different things rather than a single model <a class="yt-timestamp" data-t="02:14:00">[02:14:00]</a>. It has achieved better benchmark scores compared to GPT-4o <a class="yt-timestamp" data-t="02:39:00">[02:39:00]</a>.

Some AI YouTubers, like One Little Coder and David Shapiro, have criticized O1, suggesting it is merely "glorified Chain of Thought" <a class="yt-timestamp" data-t="02:50:00">[02:50:00]</a>. This contention arises partly from a recent incident where an AI personality claimed a top open-source model using prompt engineering and Chain of Thought with Anthropic's Sonet 3.5, only for it to be revealed as a glorified Chain of Thought application <a class="yt-timestamp" data-t="03:30:00">[03:30:00]</a>.

### Secrecy and Misdirection
OpenAI maintains a high degree of secrecy regarding O1:
*   **Model Size** The actual size of O1 preview or O1 is unknown <a class="yt-timestamp" data-t="05:36:00">[05:36:00]</a>. Some speculate that a small model achieving high performance would be truly impressive <a class="yt-timestamp" data-t="05:57:00">[05:57:00]</a>.
*   **Hidden Chain of Thought** OpenAI has decided not to show the raw Chain of Thought to users <a class="yt-timestamp" data-t="06:41:00">[06:41:00]</a>. This is considered "creepy and Orwellian" as the FBI reportedly gets to see it, but users do not <a class="yt-timestamp" data-t="06:44:00">[06:44:00]</a>. One potential reason for hiding the Chain of Thought is to prevent others from fine-tuning on it, similar to how Vicuna (LLaMA fine-tuned on ChatGPT data) achieved good performance <a class="yt-timestamp" data-t="07:22:00">[07:22:00]</a>. OpenAI appears to use a "safety argument" to justify what is perceived as anti-competitive corporate secrecy <a class="yt-timestamp" data-t="08:09:00">[08:09:00]</a>. Another theory for hiding the Chain of Thought is to prevent users from seeing potentially "weird crazy shit" from an unaligned model, allowing for a post-processing filter <a class="yt-timestamp" data-t="01:21:10">[01:21:10]</a>.
*   **AGI Claims** When asked if O1 represented Artificial General Intelligence (AGI), Sam Altman stated "no" <a class="yt-timestamp" data-t="08:32:00">[08:32:00]</a>. This denial is linked to OpenAI's non-profit structure and revenue-sharing agreement with Microsoft, which stipulates that the profitable relationship changes upon AGI declaration <a class="yt-timestamp" data-t="08:39:00">[08:39:00]</a>.

## Chain of Thought and Test-Time Compute
Chain of Thought (CoT) prompting is a technique that enables complex multi-step reasoning by eliciting step-by-step answers from LLMs <a class="yt-timestamp" data-t="01:54:00">[01:54:00]</a>. It has been documented in research literature since at least January 2023 <a class="yt-timestamp" data-t="01:51:00">[01:51:00]</a>.
*   **Test-Time Computation** This refers to the compute used during model evaluation or inference <a class="yt-timestamp" data-t="01:24:00">[01:24:00]</a>. Unlike traditional evaluation, where only the direct output is considered, test-time compute allows for an allocated budget of computation for each question <a class="yt-timestamp" data-t="01:07:00">[01:07:00]</a>.
    *   LLMs can improve performance by utilizing more test-time computation <a class="yt-timestamp" data-t="01:14:00">[01:14:00]</a>.
    *   Techniques like "best of N," beam search, and look-ahead search (which involves "rollouts" or simulations through a tree) are examples of utilizing test-time compute <a class="yt-timestamp" data-t="01:26:00">[01:26:00]</a>. These methods involve more inference steps and intermediate evaluations, significantly increasing compute usage to find better answers <a class="yt-timestamp" data-t="01:53:00">[01:53:00]</a>.
    *   OpenAI's O1 series generates internal reasoning tokens that exceed the number of visible tokens, indicating that it uses additional compute for internal thought processes <a class="yt-timestamp" data-t="02:31:00">[02:31:00]</a>. This suggests that the model is performing these search processes in the background <a class="yt-timestamp" data-t="02:34:00">[02:34:00]</a>.
    *   It's speculated that O1 dynamically allocates its test-time budget based on query difficulty, similar to an "adaptive compute optimal strategy" <a class="yt-timestamp" data-t="02:40:00">[02:40:00]</a>.

## The Role of Reinforcement Learning (RL)
The true advancement in O1, beyond glorified Chain of Thought, is believed to be the integration of sophisticated reinforcement learning for training <a class="yt-timestamp" data-t="01:45:00">[01:45:00]</a>.

### RL vs. RLHF
*   **RLHF (Reinforcement Learning from Human Feedback)**: This approach, commonly used for post-training, involves human labelers comparing and preferring different model outputs, then training a reward model to imitate this human "vibe check" <a class="yt-timestamp" data-t="00:46:57">[00:46:57]</a>. RLHF is limited by the scalability of human labeling and may not truly enhance reasoning, sometimes even making models "stupider" <a class="yt-timestamp" data-t="00:47:49">[00:47:49]</a>.
*   **"Real RL" (AlphaGo Zero Style)**: This involves models playing simulated games against themselves, generating millions of "rollouts" or paths through a problem space <a class="yt-timestamp" data-t="00:36:00">[00:36:00]</a>. A "process-based reward model" or "verifier" evaluates intermediate steps, assigning credit back along successful paths <a class="yt-timestamp" data-t="01:17:17">[01:17:17]</a>. This method can lead to superhuman intelligence, as demonstrated by AlphaGo Zero in Go <a class="yt-timestamp" data-t="00:48:16">[00:48:16]</a>.
    *   O1 is likely "trained with RL to think," meaning gradients are pushed into the model to improve its reasoning capabilities, rather than just adding Chain of Thought on top <a class="yt-timestamp" data-t="01:05:17">[01:05:17]</a>. This contrasts with earlier Chain of Thought implementations that did not fundamentally improve the base model <a class="yt-timestamp" data-t="01:31:07">[01:31:07]</a>.

> [!NOTE] Monte Carlo Tree Search (MCTS)
> MCTS is a search algorithm commonly used in game AI (like AlphaGo) <a class="yt-timestamp" data-t="00:20:20">[00:20:20]</a>. It involves building a search tree by exploring possible actions and outcomes, then using simulations (rollouts) to evaluate paths and update node values <a class="yt-timestamp" data-t="00:19:01">[00:19:01]</a>. In the context of LLMs, MCTS would involve searching through the discrete space of language tokens <a class="yt-timestamp" data-t="00:21:51">[00:21:51]</a>. This requires a Markov Decision Process (MDP) framework, where states (text sequences) and actions (next tokens) are discrete and repeatable <a class="yt-timestamp" data-t="00:22:22">[00:22:22]</a>.

### Synthetic Data Generation for [[training_and_finetuning_of_language_models_for_code | Training and Finetuning of Language Models for Code]]
A key aspect of this "real RL" is the generation of high-quality synthetic data through self-play simulations <a class="yt-timestamp" data-t="01:10:00">[01:10:00]</a>. This data includes full reasoning traces (Chain of Thought) that lead to correct answers <a class="yt-timestamp" data-t="01:49:26">[01:49:26]</a>. This process is particularly effective for problems with clear, verifiable answers, such as math and coding, where a final "correctness" signal can be obtained automatically <a class="yt-timestamp" data-t="00:50:18">[00:50:18]</a>. This is distinct from human preference data, which is subjective <a class="yt-timestamp" data-t="01:08:09">[01:08:09]</a>.

## Financial and Hardware Implications
The "real RL" approach, exemplified by AlphaGo Zero's $35 million computing cost, represents a significant investment in compute for post-training <a class="yt-timestamp" data-t="00:35:42">[00:35:42]</a>.
*   The conventional view of [[large_language_models_in_machine_learning_research | LLM Large Language Models in Machine Learning Research]] development involves a large pre-training phase, a smaller RLHF phase, and then inference <a class="yt-timestamp" data-t="00:34:00">[00:34:00]</a>. This new paradigm suggests an "exploding" green bar of RL training that becomes as significant as pre-training <a class="yt-timestamp" data-t="00:35:26">[00:35:26]</a>.
*   This could lead to specialized hardware and data centers: some optimized for pre-training (GPU-heavy for gradients) and others for RL (CPU-heavy for simulations and rollouts) <a class="yt-timestamp" data-t="00:30:52">[00:30:52]</a>.
*   A potential future direction is "inference on device," where the raw foundation model is hosted on large data centers, but the iterative inference compute (search through possible answers) is performed locally on a user's device, like an iPhone <a class="yt-timestamp" data-t="00:31:51">[00:31:51]</a>.

## Path to Superhuman Intelligence (ASI)
The integration of "real RL" opens a path to Artificial Super Intelligence (ASI):
*   **Superhuman Reasoning**: Just as AlphaGo became superhuman at Go, applying RL to language models in verifiable domains like math and coding can lead to superhuman reasoning capabilities <a class="yt-timestamp" data-t="00:48:40">[00:48:40]</a>.
*   **Transfer of Intelligence**: Even if "real RL" is only directly applicable to specific domains (e.g., math, coding), there's evidence that intelligence gained in one task can transfer to others, suggesting superhuman math abilities could lead to improvements in other domains like writing or literature <a class="yt-timestamp" data-t="00:51:47">[00:51:47]</a>.
*   **Infinite Self-Play**: The ability to continuously generate synthetic data through self-play offers an "infinite amount of signal" for training, allowing for continuous improvement without relying on external data limits or human labeling <a class="yt-timestamp" data-t="01:37:19">[01:37:19]</a>. This allows for models to keep refining their reasoning, potentially leading to extremely small models that are superhuman at reasoning <a class="yt-timestamp" data-t="00:56:00">[00:56:00]</a>.

> [!CAUTION] The Bitter Lesson
> The "Bitter Lesson," a concept from Rich Sutton, suggests that over time, simpler, scalable methods (like larger models trained on more data) tend to outperform complex, human-engineered systems <a class="yt-timestamp" data-t="01:17:22">[01:17:22]</a>. While the current integration of RL adds complexity, some speculate that eventually, a simple, massively scaled pre-training alone might achieve similar or greater performance without the need for additional RL stages <a class="yt-timestamp" data-t="01:18:52">[01:18:52]</a>.

## Conclusion
OpenAI's O1 model is simultaneously overhyped and underhyped <a class="yt-timestamp" data-t="01:43:43">[01:43:43]</a>. While Chain of Thought and test-time compute are not new, the innovation lies in OpenAI's production-grade application of "real RL" methods to bake reasoning capabilities directly into the [[llm_large_language_models_development | LLM]]. This represents a significant investment in a new training paradigm, merging the large-scale pre-training of LLMs with DeepMind's self-play RL techniques <a class="yt-timestamp" data-t="01:49:00">[01:49:00]</a>. This combination redefines the game, providing a scalable path to achieving superhuman reasoning and potentially Artificial Super Intelligence <a class="yt-timestamp" data-t="01:43:45">[01:43:45]</a>.