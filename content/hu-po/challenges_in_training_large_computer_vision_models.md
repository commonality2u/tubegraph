---
title: Challenges in training large computer vision models
videoId: KSZiJ4k28b4
---

From: [[hu-po]] <br/> 

Training large-scale computer vision models presents numerous significant challenges, encompassing computational resources, data management, architectural design, and collaborative efforts. These challenges have led to a shift in how machine learning research is conducted and the accessibility of developing such models.

## Scale and Computational Resources

The primary hurdle in training large computer vision models is their sheer scale. These models, often referred to as [[scaling_of_language_models_and_vision_transformers | Vision Transformers]] (ViT), are "big giant multi-task computer vision models" designed to be task-agnostic and applicable to a wide variety of tasks like segmentation, classification, and bounding box detection <a class="yt-timestamp" data-t="00:01:38">[00:01:38]</a>, <a class="yt-timestamp" data-t="00:02:04">[00:02:04]</a>, <a class="yt-timestamp" data-t="00:02:11">[00:02:11]</a>.

Key computational challenges include:
*   **Massive Model Sizes** Models like Dino V2's ViT-G (Giant) backbone have 1.1 billion parameters <a class="yt-timestamp" data-t="00:43:54">[00:43:54]</a>, making them impossible to fit on consumer GPUs <a class="yt-timestamp" data-t="00:07:43">[00:07:43]</a>.
*   **Extreme Hardware Requirements** Training runs often require immense distributed systems. For example, Dino V2's training utilized 12 A100 GPUs for some runs <a class="yt-timestamp" data-t="00:03:04">[00:03:04]</a>, and their full processing pipeline was distributed across 20 nodes, each equipped with eight V100 32GB GPUs <a class="yt-timestamp" data-t="00:30:18">[00:30:18]</a>, <a class="yt-timestamp" data-t="00:30:20">[00:30:20]</a>, costing approximately half a million dollars <a class="yt-timestamp" data-t="00:31:35">[00:31:35]</a>, <a class="yt-timestamp" data-t="00:31:40">[00:31:40]</a>.
*   **Memory Management in Transformers** Transformers are "very memory hungry" because attention mechanisms involve multiplying every vector by every other vector, causing memory usage to grow quadratically with sequence length <a class="yt-timestamp" data-t="00:41:08">[00:41:08]</a>, <a class="yt-timestamp" data-t="00:41:17">[00:41:17]</a>, <a class="yt-timestamp" data-t="00:41:30">[00:41:30]</a>. Techniques like "flash attention" are implemented to improve memory usage and speed <a class="yt-timestamp" data-t="00:41:46">[00:41:46]</a>.
*   **Cross-GPU Communication** In distributed training, the GPU itself is often not the limiting factor, but rather the communication between GPUs <a class="yt-timestamp" data-t="00:51:00">[00:51:00]</a>. This has led to advancements in hardware interconnects and model parallelism to save on communication costs <a class="yt-timestamp" data-t="00:50:40">[00:50:40]</a>, <a class="yt-timestamp" data-t="00:51:12">[00:51:12]</a>.
*   **High-Resolution Training** Increasing image resolution is crucial for pixel-level tasks (like segmentation or detection) where small objects might disappear at low resolutions <a class="yt-timestamp" data-t="00:38:45">[00:38:45]</a>, <a class="yt-timestamp" data-t="00:38:49">[00:38:49]</a>. However, training at high resolutions (e.g., 416x416 pixels) takes three times more compute than at 224x224 <a class="yt-timestamp" data-t="01:06:44">[01:06:44]</a>. A common strategy is a curriculum approach, where models are initially trained at low resolution and then fine-tuned for a short period at higher resolution <a class="yt-timestamp" data-t="00:39:56">[00:39:56]</a>, <a class="yt-timestamp" data-t="01:06:36">[01:06:36]</a>, <a class="yt-timestamp" data-t="01:07:34">[01:07:34]</a>.

## Data Management and Quality

Data quality and diversity are paramount for training high-performing foundational models <a class="yt-timestamp" data-t="00:16:06">[00:16:06]</a>, <a class="yt-timestamp" data-t="00:37:31">[00:37:31]</a>.
*   **Curated Data** Large models require "curated data from diverse sources" <a class="yt-timestamp" data-t="00:05:20">[00:05:20]</a>. While natural language processing (NLP) has seen success with large quantities of "raw text," image data cleaning is even more important due to its broader and more varied distribution <a class="yt-timestamp" data-t="00:05:44">[00:05:44]</a>, <a class="yt-timestamp" data-t="00:05:56">[00:05:56]</a>, <a class="yt-timestamp" data-t="00:10:29">[00:10:29]</a>.
*   **Automated Curation Pipelines** Projects like Dino V2 use automatic pipelines to filter and rebalance datasets, including deduplication (e.g., using Faiss library for similarity search) <a class="yt-timestamp" data-t="00:18:56">[00:18:56]</a>, <a class="yt-timestamp" data-t="00:23:45">[00:23:45]</a>, <a class="yt-timestamp" data-t="00:26:54">[00:26:54]</a>. This process often involves NSFW filtering and blurring identifiable faces <a class="yt-timestamp" data-t="00:26:22">[00:26:22]</a>, <a class="yt-timestamp" data-t="00:26:26">[00:26:26]</a>.
*   **Overfitting on Dominant Modes** A major difficulty is rebalancing concepts and avoiding overfitting on a few dominant modes, which can lead to "mode collapse" <a class="yt-timestamp" data-t="00:19:34">[00:19:34]</a>, <a class="yt-timestamp" data-t="00:19:36">[00:19:36]</a>, <a class="yt-timestamp" data-t="00:19:43">[00:19:43]</a>.

## Model Design and Training Stability

[[Challenges and insights in Transformer architecture and training | Technical contributions]] are critical for accelerating and stabilizing training at scale <a class="yt-timestamp" data-t="00:07:07">[00:07:07]</a>.
*   **Balancing Underfitting and Overfitting** In very large models, it's possible to have underfitting at the patch level (lower layers) while overfitting at the image level (model head) <a class="yt-timestamp" data-t="00:34:30">[00:34:30]</a>, <a class="yt-timestamp" data-t="00:34:32">[00:34:32]</a>, <a class="yt-timestamp" data-t="00:34:48">[00:34:48]</a>, <a class="yt-timestamp" data-t="00:35:17">[00:35:17]</a>. Untying head weights between objectives can resolve this issue <a class="yt-timestamp" data-t="00:34:26">[00:34:26]</a>, <a class="yt-timestamp" data-t="00:35:35">[00:35:35]</a>.
*   **Regularization and Normalization** Various forms of regularization (like [[Challenges and approaches in adapting large language models for specific tasks | Coleo regularizer]]) <a class="yt-timestamp" data-t="00:36:32">[00:36:32]</a> and normalization (like Sinkhorn-Knopp batch normalization) are used to stabilize training and encourage uniform feature distribution <a class="yt-timestamp" data-t="00:31:55">[00:31:55]</a>, <a class="yt-timestamp" data-t="00:36:05">[00:36:05]</a>, <a class="yt-timestamp" data-t="00:36:43">[00:36:43]</a>.
*   **Large Batch Sizes** Larger batch sizes are inherently better as they lead to more stable gradients and a "straighter line" through the loss landscape, improving training stability and final solution quality <a class="yt-timestamp" data-t="00:18:11">[00:18:11]</a>, <a class="yt-timestamp" data-t="00:18:37">[00:18:37]</a>, <a class="yt-timestamp" data-t="00:55:00">[00:55:00]</a>. However, this necessitates distributed systems with hundreds of GPUs <a class="yt-timestamp" data-t="00:55:27">[00:55:27]</a>, <a class="yt-timestamp" data-t="00:55:31">[00:55:31]</a>.
*   **Hardware-Driven Architecture Design** Hyperparameters for model architecture are often chosen not just for performance, but to be specific to the underlying hardware (e.g., GPU memory alignment) for efficiency <a class="yt-timestamp" data-t="00:42:11">[00:42:11]</a>, <a class="yt-timestamp" data-t="00:42:27">[00:42:27]</a>, <a class="yt-timestamp" data-t="00:43:03">[00:43:03]</a>.
*   **Model Distillation** Rather than training multiple models of different sizes from scratch, a common and effective technique is to train one very large model and then [[Challenges and approaches in adapting large language models for specific tasks | distill]] smaller models from it <a class="yt-timestamp" data-t="00:07:45">[00:07:45]</a>, <a class="yt-timestamp" data-t="00:42:00">[00:42:00]</a>, <a class="yt-timestamp" data-t="00:53:41">[00:53:41]</a>. Surprisingly, a distilled model can sometimes outperform the larger teacher model it was derived from <a class="yt-timestamp" data-t="01:05:06">[01:05:06]</a>, <a class="yt-timestamp" data-t="01:05:50">[01:05:50]</a>.

## Research and Development Landscape

The monumental effort required for training these models has reshaped the machine learning research landscape.
*   **Large Teams** Papers now routinely feature 20 or more authors, reflecting the "whole squad of people" required to train these massive foundation models <a class="yt-timestamp" data-t="00:03:57">[00:03:57]</a>, <a class="yt-timestamp" data-t="00:04:02">[00:04:02]</a>, <a class="yt-timestamp" data-t="00:04:19">[00:04:19]</a>.
*   **Difficulty of Reproduction** For independent researchers, it's "basically impossible to re-implement" these papers due to the extensive hardware and team resources required <a class="yt-timestamp" data-t="00:46:09">[00:46:09]</a>, <a class="yt-timestamp" data-t="00:46:36">[00:46:36]</a>. The focus for individual contributors shifts to utilizing the pre-trained models rather than replicating their training <a class="yt-timestamp" data-t="00:46:17">[00:46:17]</a>, <a class="yt-timestamp" data-t="00:46:32">[00:46:32]</a>.
*   **Openness vs. Secrecy** Companies like Meta are praised for releasing their models and detailing their training techniques, allowing for transparency and broader understanding of how these foundational models are built <a class="yt-timestamp" data-t="00:06:27">[00:06:27]</a>, <a class="yt-timestamp" data-t="00:20:25">[00:20:25]</a>, <a class="yt-timestamp" data-t="00:53:03">[00:53:03]</a>, <a class="yt-timestamp" data-t="01:56:15">[01:56:15]</a>. In contrast, some other major AI companies maintain secrecy about their training data, methods, and specific tricks used <a class="yt-timestamp" data-t="00:06:14">[00:06:14]</a>, <a class="yt-timestamp" data-t="00:20:28">[00:20:28]</a>, <a class="yt-timestamp" data-t="01:56:43">[01:56:43]</a>.
*   **Benchmark Evolution** As models become more powerful, older benchmarks (e.g., ImageNet 1K, CIFAR-10) become less meaningful due to near-perfect scores <a class="yt-timestamp" data-t="01:29:19">[01:29:19]</a>, <a class="yt-timestamp" data-t="01:29:50">[01:29:50]</a>, <a class="yt-timestamp" data-t="01:30:19">[01:30:19]</a>. There's a need for harder, more difficult benchmarks where models still have room for significant improvement <a class="yt-timestamp" data-t="01:29:12">[01:29:12]</a>, <a class="yt-timestamp" data-t="01:29:42">[01:29:42]</a>, <a class="yt-timestamp" data-t="01:30:24">[01:30:24]</a>.

## Conclusion

The [[Challenges and methodologies in AI training | challenges and strategies in model training and performance | landscape of training]] large computer vision models is characterized by escalating computational demands, sophisticated data management, hardware-optimized architectural design, and the necessity of large collaborative teams. These factors underscore the difficulty and cost of creating state-of-the-art AI, highlighting the importance of open research and shared models for wider accessibility and future innovation.