---
title: Reinforcement Learning with Human Feedback RLHF
videoId: nTdJEQLday0
---

From: [[hu-po]] <br/> 

[[reinforcement_learning_in_ai | Reinforcement Learning with Human Feedback (RLHF)]] is a technique that has gained popularity due to OpenAI's stated use of it to fine-tune their ChatGPT models <a class="yt-timestamp" data-t="00:00:10">[00:00:10]</a>. This article explains RLHF based on a HuggingFace blog post <a class="yt-timestamp" data-t="00:00:25">[00:00:25]</a>.

## Core Concepts

### Fine-tuning
Fine-tuning is the process of taking a pre-trained machine learning or deep learning model that has been trained for a long time on one dataset, and then training it for a bit longer on a different dataset <a class="yt-timestamp" data-t="00:00:34">[00:00:34]</a>. This process occurs within the sphere of [[reinforcement_learning_in_ai | training]] machine learning models <a class="yt-timestamp" data-t="00:00:51">[00:00:51]</a>. Fine-tuning involves pushing gradients into the [[reinforcement_learning_in_ai | large language model]] (LLM), unlike in-context learning or prompt engineering, where model parameters are not modified <a class="yt-timestamp" data-t="00:08:58">[00:08:58]</a>.

### Large Language Models (LLMs)
LLMs are a type of machine learning model that powers tools like ChatGPT, Google's Palm, and Meta's Llama <a class="yt-timestamp" data-t="00:01:06">[00:01:06]</a>. The majority (99%) of an LLM's intelligence comes from its initial pre-training, often using simple tasks like next-token prediction <a class="yt-timestamp" data-t="00:04:42">[00:04:42]</a>. RLHF is primarily used for [[reinforcement_learning_concepts_applied_to_ai_agents | AI safety]] and ensuring the LLM's output aligns with specific task requirements <a class="yt-timestamp" data-t="00:05:02">[00:05:02]</a>.

### Reinforcement Learning (RL)
[[reinforcement_learning_in_ai | Reinforcement learning]] is a type of machine learning that predates deep learning <a class="yt-timestamp" data-t="00:02:31">[00:02:31]</a>. Its foundation is the [[reinforcement_learning_concepts_applied_to_ai_agents | Markov Decision Process (MDP)]], a mathematical framework that describes the world as a set of states where [[reinforcement_learning_concepts_applied_to_ai_agents | agents]] take actions, producing more states, and an environment rewards successful completions <a class="yt-timestamp" data-t="00:02:49">[00:02:49]</a>.

### Reward Model (Value Function)
In [[reinforcement_learning_in_ai | reinforcement learning]], a reward model (often a deep neural network) approximates the [[reinforcement_learning_concepts_applied_to_ai_agents | value function]] <a class="yt-timestamp" data-t="00:07:11">[00:07:11]</a>. This function predicts the reward for any given state or action within the environment <a class="yt-timestamp" data-t="00:07:31">[00:07:31]</a>. For LLMs, it can predict how appropriate an answer is, given a human prompt (state) and the LLM's answer (action) <a class="yt-timestamp" data-t="00:07:55">[00:07:55]</a>.

## RLHF Training Process

Training a language model with RLHF involves three key steps:

1.  **Fine-tune a pre-trained LLM:** This initial fine-tuning uses a smaller, clean dataset of instructions and human demonstrations (the human feedback) <a class="yt-timestamp" data-t="00:06:27">[00:06:27]</a>. The dataset is usually created in-house rather than from user data to ensure cleanliness <a class="yt-timestamp" data-t="00:10:57">[00:10:57]</a>.
2.  **Collect human-annotated data and train a reward model:** Human labelers rank various outputs generated by the LLM for a given prompt, providing the learning signal for the reward model <a class="yt-timestamp" data-t="00:11:10">[00:11:10]</a>.
3.  **Fine-tune the LLM using the reward model and dataset with RL:** Once the reward model (value function) is trained, it can be used to continuously push gradients into the LLM <a class="yt-timestamp" data-t="00:08:30">[00:08:30]</a>. This optimization step often uses [[reinforcement_learning_concepts_applied_to_ai_agents | Proximal Policy Optimization (PPO)]], a common [[challenges_and_techniques_in_reinforcement_learning | reinforcement learning algorithm]] <a class="yt-timestamp" data-t="00:12:07">[00:12:07]</a>. The reward model provides a score (e.g., 0-1) indicating the appropriateness of an LLM's answer to a prompt, which then guides the PPO model to select better answers <a class="yt-timestamp" data-t="00:13:05">[00:13:05]</a>.

The process is iterative: the language model generates responses (actions), the reward model evaluates them, and the policy is updated based on these evaluations <a class="yt-timestamp" data-t="00:24:57">[00:24:57]</a>. Human labelers shape the distribution of "good" answers, and the reward model estimates this distribution <a class="yt-timestamp" data-t="00:14:35">[00:14:35]</a>.

## Technical Considerations in RLHF

### GPU Memory and Precision
LLMs with billions of parameters require significant GPU memory <a class="yt-timestamp" data-t="00:01:40">[00:01:40]</a>. The amount of memory depends on the precision (data type) used to store parameters <a class="yt-timestamp" data-t="00:28:46">[00:28:46]</a>:
*   **Float32:** 4 gigabytes per billion parameters <a class="yt-timestamp" data-t="00:29:23">[00:29:23]</a>. A 20 billion parameter model would require 80 GB <a class="yt-timestamp" data-t="00:45:01">[00:45:01]</a>.
*   **Float16:** Halves the memory requirement (2 GB/billion parameters) <a class="yt-timestamp" data-t="00:29:29">[00:29:29]</a>. A 20 billion parameter model fits in 40 GB <a class="yt-timestamp" data-t="00:45:21">[00:45:21]</a>.
*   **Int8:** Reduces memory further to 1 GB/billion parameters <a class="yt-timestamp" data-t="00:29:16">[00:29:16]</a>. A 20 billion parameter model fits in 20 GB <a class="yt-timestamp" data-t="00:45:30">[00:45:30]</a>.
*   **4-bit:** Possible to store parameters in just 4 bits, requiring 0.5 GB/billion parameters <a class="yt-timestamp" data-t="00:17:26">[00:17:26]</a>. A 20 billion parameter model fits in 10 GB <a class="yt-timestamp" data-t="00:45:51">[00:45:51]</a>.

While lower precision saves memory, it can lead to performance degradation (quantization issues), similar to using a less precise value of Pi in calculations <a class="yt-timestamp" data-t="00:36:45">[00:36:45]</a>. Techniques like 8-bit matrix multiplication address this by performing outlier parts of calculations in higher precision (float16) and non-outlier parts in lower precision (int8) <a class="yt-timestamp" data-t="00:38:08">[00:38:08]</a>.

### Model Copies and Logits
[[agent_loops_and_reinforcement_learning_in_ai | Fine-tuning]] RL models often requires keeping two copies of the model: an "active" model being updated and a "reference" model <a class="yt-timestamp" data-t="00:20:26">[00:20:26]</a>. This prevents the active model from deviating too much from its original behavior <a class="yt-timestamp" data-t="00:20:32">[00:20:32]</a>.

Logits are the raw outputs from the final layer of a neural network, representing a distribution over actions or next tokens <a class="yt-timestamp" data-t="00:22:44">[00:22:44]</a>. The Kullbackâ€“Leibler (KL) Divergence is used to compare the probability distributions of the active and reference models' outputs <a class="yt-timestamp" data-t="00:27:03">[00:27:03]</a>.

### Parameter-Efficient Fine-Tuning (PEFT)
[[reinforcement_learning_and_stateoftheart_models | Low-Rank Adaptation (LoRA)]], a technique introduced in 2021, allows fine-tuning LLMs by freezing most pre-trained weights and creating low-rank versions of specific matrices (e.g., query and value layer attention matrices) <a class="yt-timestamp" data-t="00:39:39">[00:39:39]</a>. These low-rank matrices have significantly fewer parameters than the original model, reducing GPU memory requirements <a class="yt-timestamp" data-t="00:41:31">[00:41:31]</a>.

This approach saves computation time by avoiding gradient calculations for frozen weights <a class="yt-timestamp" data-t="00:47:57">[00:47:57]</a>. It also means that only the smaller "adapter" weights need to be shared, rather than the entire model, when deploying fine-tuned models <a class="yt-timestamp" data-t="00:48:14">[00:48:14]</a>.

### Distributed Training and Scaling
Scaling RLHF training can be challenging due to model size and optimizer memory requirements <a class="yt-timestamp" data-t="00:30:01">[00:30:01]</a>.
*   **Data Parallelism:** The same model is replicated across multiple machines, with each instance processing a different batch of data <a class="yt-timestamp" data-t="00:30:07">[00:30:07]</a>.
*   **Model Parallelism (Pipeline/Tensor Parallelism):** The model itself is distributed across multiple machines. Pipeline parallelism splits the model layer-wise, while tensor parallelism splits tensors or operations within layers across GPUs <a class="yt-timestamp" data-t="00:30:51">[00:30:51]</a>.

Implementing distributed training is complex, requiring a deep understanding of hardware, potential bottlenecks, and communication protocols <a class="yt-timestamp" data-t="00:33:36">[00:33:36]</a>.

### Tools and Libraries
*   **TRL (Transformer Reinforcement Learning):** A library that aims to make the [[reinforcement_learning_in_ai | RL]] step easier for LLM fine-tuning <a class="yt-timestamp" data-t="00:17:40">[00:17:40]</a>. It supports [[reinforcement_learning_concepts_applied_to_ai_agents | PPO]] and leverages Hugging Face's Accelerate library for distributed training <a class="yt-timestamp" data-t="00:18:01">[00:18:01]</a>.
*   **PEFT (Parameter-Efficient Fine-tuning):** A Hugging Face library designed for creating and fine-tuning adapter layers on LLMs <a class="yt-timestamp" data-t="00:39:39">[00:39:39]</a>.
*   **Weights & Biases:** A platform for [[reinforcement_learning_concepts_applied_to_ai_agents | experiment tracking]] and reporting in machine learning <a class="yt-timestamp" data-t="00:56:20">[00:56:20]</a>.

## Limitations and Future Outlook

While RLHF is powerful, some suspect it may eventually lead to "catastrophic forgetting" in LLMs, similar to transfer learning, potentially reducing their ability to generalize <a class="yt-timestamp" data-t="00:05:13">[00:05:13]</a>.

Despite efforts to make RLHF accessible, the complexity of multi-GPU scaling and optimizing training speed remains significant challenges <a class="yt-timestamp" data-t="00:57:35">[00:57:35]</a>. It is anticipated that major companies like Google and Microsoft will develop their own comprehensive libraries and tooling for RLHF, potentially superseding smaller, bespoke libraries <a class="yt-timestamp" data-t="00:59:37">[00:59:37]</a>.