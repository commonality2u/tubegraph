---
title: Use of multiple visual encoders
videoId: uYb38g-weEY
---

From: [[hu-po]] <br/> 

Recent advancements in vision language models (VLMs) have explored the use of multiple visual encoders, or "experts," to address challenges such as insufficient capabilities of a single visual component and excessively long visual tokens <a class="yt-timestamp" data-t="01:03:04">[01:03:04]</a>. This approach, exemplified by models like Musy Poly Visual Expert, aims to synergize the capabilities of individual visual encoders for improved performance <a class="yt-timestamp" data-t="01:05:21">[01:05:21]</a>.

## Motivation and Approach
Current large vision language models often face limitations due to a single visual component's capabilities, particularly when interpreting complex visual information <a class="yt-timestamp" data-t="01:03:06">[01:03:06]</a>. The Musy Poly Visual Expert model proposes an [[combining_multiple_vision_encoders_for_improved_performance | ensemble of experts]] technique where "expert" refers to different image encoders, distinct from how the term is used in Mixture of Experts models <a class="yt-timestamp" data-t="01:04:06">[01:04:06]</a>, <a class="yt-timestamp" data-t="01:04:28">[01:04:28]</a>. This technique introduces a Fusion Network to unify the processing of outputs from these diverse visual experts <a class="yt-timestamp" data-t="01:05:35">[01:05:35]</a>.

## Specialized Visual Encoders
The Musy model integrates several distinct visual encoders, each bringing a unique strength to the overall understanding of visual content <a class="yt-timestamp" data-t="01:09:47">[01:09:47]</a>:

*   **CLIP** (Contrastive Language-Image Pre-training): Excels in aligning images with textual descriptions, providing robust semantic understanding through its [[multimodal_learning_and_embeddings | contrastive learning]] approach <a class="yt-timestamp" data-t="01:09:51">[01:09:51]</a>, <a class="yt-timestamp" data-t="01:10:00">[01:10:00]</a>. It is trained on a large dataset of 400 million noisy image-text pairs <a class="yt-timestamp" data-t="01:11:30">[01:11:30]</a>. Its vision encoder is a [[pretrained_vision_transformers_and_koco | Vision Transformer]] with 300 million parameters, typically operating at a 336x336 resolution <a class="yt-timestamp" data-t="01:11:34">[01:11:34]</a>.
*   **DINOv2**: Offers advances in robust and stabilized feature extraction without relying on labeled data, thanks to its self-supervised learning paradigm at both image and patch levels <a class="yt-timestamp" data-t="01:10:26">[01:10:26]</a>. DINOv2 is a [[pretrained_vision_transformers_and_koco | Vision Transformer]] with 1.1 billion parameters <a class="yt-timestamp" data-t="01:11:54">[01:11:54]</a>.
*   **Segment Anything Model (SAM)**: A large-scale segmentation model trained on a billion masks <a class="yt-timestamp" data-t="01:12:30">[01:12:30]</a>. SAM is highly effective at capturing fine details and edges, making it useful for tasks requiring precise [[challenges_in_visual_segmentation_and_encoding | segmentation]] <a class="yt-timestamp" data-t="01:12:36">[01:12:36]</a>, <a class="yt-timestamp" data-t="01:13:06">[01:13:06]</a>. The SAM Vision Transformer has 637 million parameters <a class="yt-timestamp" data-t="01:13:40">[01:13:40]</a>.
*   **LayoutLMv3**: Good at Optical Character Recognition (OCR) <a class="yt-timestamp" data-t="01:15:15">[01:15:15]</a>.

## Combination and Fusion Network
Each visual encoder produces a different number of tokens with varying dimensionalities <a class="yt-timestamp" data-t="01:14:23">[01:14:23]</a>, e.g., CLIP tokens are 1024-dimensional, DINOv2 are 1536-dimensional, and SAM produces many more tokens <a class="yt-timestamp" data-t="01:14:12">[01:14:12]</a>, <a class="yt-timestamp" data-t="01:15:26">[01:15:26]</a>. To address this, the outputs from these frozen visual encoders are concatenated <a class="yt-timestamp" data-t="01:16:06">[01:16:06]</a> and then processed by a "Poly Expert Fusion Network" <a class="yt-timestamp" data-t="01:16:36">[01:16:36]</a>. This network, primarily implemented as a series of Multi-Layer Perceptrons (MLPs), adjusts the dimensionality of the tokens from each expert to a consistent size <a class="yt-timestamp" data-t="01:17:15">[01:17:15]</a>. This is analogous to how a single MLP adapter connects a visual encoder to a language model <a class="yt-timestamp" data-t="01:17:52">[01:17:52]</a>. The MLP fusion method consistently outperforms other approaches like Q-Former Fusion, despite being simpler and having fewer parameters <a class="yt-timestamp" data-t="01:23:42">[01:23:42]</a>.

## Positional Encoding and Token Order
The paper also explores different positional encoding schemes for the combined visual tokens <a class="yt-timestamp" data-t="01:04:44">[01:04:44]</a>. Traditional [[convolutional_neural_networks_and_visual_systems | Vision Transformers]] add positional embeddings to each patch, but this can lead to excessively long sequences and wasted potential <a class="yt-timestamp" data-t="01:06:01">[01:06:01]</a>. The research suggests that using a shared positional encoding (e.g., the same embedding for all image patches, or shared by row/column) can significantly reduce computational cost without notable performance degradation, implying that detailed positional information from the visual encoder might be redundant for VLMs <a class="yt-timestamp" data-t="01:22:01">[01:22:01]</a>, <a class="yt-timestamp" data-t="01:27:30">[01:27:30]</a>.

Another interesting finding is that the order in which tokens from different visual experts are concatenated matters due to the autoregressive and position-aware characteristics of the Large Language Model (LLM) backbone <a class="yt-timestamp" data-t="01:24:24">[01:24:24]</a>. The LLM perceives the combined visual tokens as a sequence, and their arrangement can influence the final output <a class="yt-timestamp" data-t="01:25:07">[01:25:07]</a>.

## Performance
Models leveraging multiple visual encoders consistently demonstrate superior performance over those using isolated visual encoders <a class="yt-timestamp" data-t="01:07:30">[01:07:30]</a>. Performance significantly boosts as more experts are integrated, although there can be diminishing returns <a class="yt-timestamp" data-t="01:07:37">[01:07:37]</a>, <a class="yt-timestamp" data-t="01:09:28">[01:09:28]</a>. For instance, models with three visual experts generally outperform those with two, which in turn outperform single-expert models <a class="yt-timestamp" data-t="01:09:38">[01:09:38]</a>. In terms of individual contributions, CLIP, DINOv2, and LayoutLMv3 are noted as the most impactful in descending order <a class="yt-timestamp" data-t="01:28:49">[01:28:49]</a>.

Despite the performance gains, using multiple visual encoders increases the computational cost, as the image must be fed into multiple encoders, and the language model then processes a longer sequence of tokens <a class="yt-timestamp" data-t="01:54:30">[01:54:30]</a>. This highlights a trend where state-of-the-art performance often correlates with increased computational expense and multi-step inference pipelines <a class="yt-timestamp" data-t="01:36:15">[01:36:15]</a>.