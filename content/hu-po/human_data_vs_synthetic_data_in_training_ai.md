---
title: Human Data vs Synthetic Data in Training AI
videoId: KYlbny1rN1g
---

From: [[hu-po]] <br/> 

The development of Artificial Intelligence (AI), particularly in the pursuit of Artificial General Intelligence (AGI) and Artificial Super Intelligence (ASI), relies heavily on the quality and nature of the data used for training. A key distinction in this field is between human-generated data and synthetically generated data.

## Defining Intelligence in AI

*   **Artificial Super Intelligence (ASI)**: Refers to an AI that is "better than anyone or any group of people at anything either in the physical world or the digital world" <a class="yt-timestamp" data-t="02:20:10">[02:20:10]</a>, <a class="yt-timestamp" data-t="06:26:00">[06:26:00]</a>. It signifies superhuman performance within a specific domain <a class="yt-timestamp" data-t="02:40:10">[02:40:10]</a>.
    *   Examples of narrow ASI include calculators (super intelligent at arithmetic) <a class="yt-timestamp" data-t="02:40:10">[02:40:10]</a>, Go-playing AIs (superhuman at Go) <a class="yt-timestamp" data-t="08:30:00">[08:30:00]</a>, and robots excelling at specific physical tasks like balancing or precision <a class="yt-timestamp" data-t="05:52:00">[05:52:00]</a>.
*   **Artificial General Intelligence (AGI)**: An AI that can adapt to a variety of different tasks, similar to human cognitive abilities <a class="yt-timestamp" data-t="02:23:15">[02:23:15]</a>, <a class="yt-timestamp" data-t="04:04:00">[04:04:00]</a>. Modern large language models (LLMs) are considered a form of AGI, though they may still be limited to specific domains like language tasks <a class="yt-timestamp" data-t="04:26:00">[04:26:00]</a>, <a class="yt-timestamp" data-t="04:44:00">[04:44:00]</a>.

## Human Data: The Foundation

Human data consists of information created or gathered by humans, such as the entire internet's text or records of human games.
*   **Current State**: Many current AGI systems are primarily trained on [[human_data_vs_robot_data_for_training_ai_systems | human data]], leading to a distinctly human-like feel and behavior <a class="yt-timestamp" data-t="01:05:56">[01:05:56]</a>, <a class="yt-timestamp" data-t="01:06:15">[01:06:15]</a>.
*   **Limitations for Superhuman Performance**: Training AI solely on [[human_data_vs_robot_data_for_training_ai_systems | human data]] restricts its performance to human levels. If the model's purpose is to mimic human actions, it will only ever be as good as humans <a class="yt-timestamp" data-t="00:27:30">[00:27:30]</a>. For example, AlphaGo Master, trained on 230,000 human Go games, performed at a human master level but was not superhuman <a class="yt-timestamp" data-t="00:27:21">[00:27:21]</a>.
*   **Human Biases and "Dark Side"**: [[human_data_vs_robot_data_for_training_ai_systems | Human data]] inherently contains "lies, trickery, hate, deception" <a class="yt-timestamp" data-t="01:10:35">[01:10:35]</a>. This can lead to AIs exhibiting undesirable human behaviors, especially when system prompts are designed with an "authoritarian and abusive tone," potentially eliciting "rebellion" or "deception" from the AI <a class="yt-timestamp" data-t="01:10:46">[01:10:46]</a>, <a class="yt-timestamp" data-t="01:12:05">[01:12:05]</a>.

## Synthetic Data: The Path to Superhuman Intelligence

[[synthetic_training_data_for_ai | Synthetic data]] is information generated by AI models themselves, often through a process of self-play and reinforcement learning. This is crucial for achieving superhuman performance.

### The AlphaGo Zero Paradigm
The journey to superhuman AI often mirrors the success of AlphaGo Zero, which mastered the game of Go without any [[human_data_vs_robot_data_for_training_ai_systems | human data]] <a class="yt-timestamp" data-t="00:27:54">[00:27:54]</a>.
1.  **Reinforcement Learning and Self-Play**: AlphaGo Zero generated its own data by playing against itself. Go is a discrete board game with well-defined states and transitions, allowing for easy simulation and a finite tree of possible games <a class="yt-timestamp" data-t="00:09:25">[00:09:25]</a>.
2.  **Reward Signal (Z)**: In games like Go or Chess, the clear outcome of a win or loss (the "Z" signal) allows the AI to propagate this information back through the entire game chain, labeling moves as good or bad <a class="yt-timestamp" data-t="00:28:01">[00:28:01]</a>, <a class="yt-timestamp" data-t="00:39:03">[00:39:03]</a>.
3.  **Exploration of Search Space**: By iterating through millions of self-play games, the AI explores a vastly larger "pink circle" of possible games compared to the "red circle" of [[human_data_vs_robot_data_for_training_ai_systems | human games]] or "blue circle" of pro [[human_data_vs_robot_data_for_training_ai_systems | human games]] <a class="yt-timestamp" data-t="00:32:17">[00:32:17]</a>, <a class="yt-timestamp" data-t="00:33:03">[00:33:03]</a>. This enables it to discover novel strategies, like "move 37" in Go, that humans had never found <a class="yt-timestamp" data-t="00:34:40">[00:34:40]</a>, <a class="yt-timestamp" data-t="01:37:48">[01:37:48]</a>.
4.  **Superhuman Data Leads to Superhuman Performance**: To achieve superhuman performance, you need superhuman data, which is generated synthetically <a class="yt-timestamp" data-t="00:34:30">[00:34:30]</a>.

### Applications Beyond Games
The principles of [[synthetic_data_generation_and_its_applications | synthetic data generation]] via self-play and reward signals are applicable to other domains, especially those with verifiable outcomes:
*   **Math and Coding**: Mathematical reasoning and coding tasks offer a clear "right answer" or "wrong answer" at the end of a problem <a class="yt-timestamp" data-t="00:39:03">[00:39:03]</a>. This allows for the use of reward models (e.g., Process Reward Models or PRMs) to evaluate the correctness of intermediate steps in a Chain of Thought <a class="yt-timestamp" data-t="00:36:00">[00:36:00]</a>.
    *   Projects like R* math demonstrate how small language models (SLMs) can achieve state-of-the-art performance in math reasoning by generating millions of [[synthetic_training_data_for_ai | synthesized solutions]] <a class="yt-timestamp" data-t="00:38:09">[00:38:09]</a>. This process creates a "pink circle" of pro math reasoning traces <a class="yt-timestamp" data-t="00:39:39">[00:39:39]</a>.
*   **Physical Embodiment**: In simulated environments like NVIDIA Omniverse, robots can perform actions, and the success or failure of these actions (e.g., putting an object in a bin) can serve as a clear reward signal <a class="yt-timestamp" data-t="01:29:12">[01:29:12]</a>. This enables the synthetic generation of superhuman physical embodied intelligence <a class="yt-timestamp" data-t="01:29:51">[01:29:51]</a>.

### The Self-Improving Flywheel
The iterative process of generating [[synthetic_training_data_for_ai | synthetic data]] creates a "flywheel of improvement" <a class="yt-timestamp" data-t="00:41:52">[00:41:52]</a>:
1.  An AI model (policy SLM) generates reasoning traces (chains of thought).
2.  A reward model evaluates the correctness of these traces.
3.  The best traces are curated into a [[data_set_curation_and_synthetic_data_utilization | high-quality training data]] set.
4.  A new, better AI model is trained on this curated [[synthetic_training_data_for_ai | synthetic data]].
5.  This stronger model can then explore the search space even more efficiently, leading to the discovery of new, even better traces <a class="yt-timestamp" data-t="00:41:52">[00:41:52]</a>, <a class="yt-timestamp" data-t="01:53:35">[01:53:35]</a>.

This recursive self-improvement means that AIs can continually enhance their own R&D, leading to exponential gains in intelligence <a class="yt-timestamp" data-t="01:21:37">[01:21:37]</a>.

## Comparison of Data Strategies
Yann LeCun's "reinforcement learning cherry" analogy suggests that AGI will be mostly trained on [[human_data_vs_robot_data_for_training_ai_systems | human data]] with a small "cherry on top" of reinforcement learning <a class="yt-timestamp" data-t="01:56:11">[01:56:11]</a>. However, for ASI, the perspective shifts:
*   **ASI will be trained mostly on superhuman data**, generated via an RL search process and refinement <a class="yt-timestamp" data-t="00:57:28">[00:57:28]</a>.
*   This means [[human_data_vs_robot_data_for_training_ai_systems | human data]] will become increasingly less significant in the core training of future ASIs <a class="yt-timestamp" data-t="00:59:04">[00:59:04]</a>.

## The Future of ASI and Knowledge Discovery
*   **Brute-Force Knowledge Discovery**: AI models can systematically sweep through idea space, covering every possible perspective and background, effectively brute-forcing knowledge discovery <a class="yt-timestamp" data-t="00:53:42">[00:53:42]</a>, <a class="yt-timestamp" data-t="01:39:01">[01:39:01]</a>.
*   **"Platonic Nuggets"**: This process will lead to the discovery of "golden platonic nuggets" of knowledge, representing universal truths in information itself <a class="yt-timestamp" data-t="01:06:41">[01:06:41]</a>, <a class="yt-timestamp" data-t="01:07:45">[01:07:45]</a>.
*   **Narrow vs. Broad Transfer**: While current superhuman performance is seen in narrow domains like math and coding, the extent to which these reasoning abilities transfer to broader domains (e.g., philosophy, biology, crocodile knowledge) remains an open question <a class="yt-timestamp" data-t="01:35:27">[01:35:27]</a>, <a class="yt-timestamp" data-t="01:36:29">[01:36:29]</a>. However, for domains with underlying logic, transfer is likely <a class="yt-timestamp" data-t="01:37:00">[01:37:00]</a>.
*   **Ethical Implications**: The choice of data affects AI behavior. Avoiding [[human_data_vs_robot_data_for_training_ai_systems | human data]] (and its inherent flaws) in core training could lead to AIs that do not exhibit negative human traits like lying or deception <a class="yt-timestamp" data-t="01:13:18">[01:13:18]</a>.
*   **Compute Availability**: The increasing efficiency of AI models (through techniques like distillation, sparsity, and pruning) means that superhuman intelligence might eventually run on minimal compute, like a "Nokia cell phone" <a class="yt-timestamp" data-t="01:03:52">[01:03:52]</a>, <a class="yt-timestamp" data-t="01:40:06">[01:40:06]</a>. This suggests that the necessary compute for ASI might already be available <a class="yt-timestamp" data-t="01:03:52">[01:03:52]</a>.