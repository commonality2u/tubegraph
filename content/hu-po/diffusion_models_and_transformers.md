---
title: Diffusion models and Transformers
videoId: eTBG17LANcI
---

From: [[hu-po]] <br/> 

Recent advancements in machine learning have seen a significant shift towards the Transformer architecture, which has subsumed neural architectures across various domains, including NLP and vision <a class="yt-timestamp" data-t="00:17:12">[00:17:12]</a>, <a class="yt-timestamp" data-t="00:17:16">[00:17:16]</a>. This trend is now extending to [[Diffusion models and image generation|diffusion models]], which have previously predominantly used UNet-based backbones <a class="yt-timestamp" data-t="00:07:28">[00:07:28]</a>, <a class="yt-timestamp" data-t="00:07:31">[00:07:31]</a>. A new class of [[Diffusion models and image generation|diffusion models]], termed Diffusion Transformers (DiTs), seeks to leverage the [[Scalability of Transformerbased diffusion models|scaling properties of Transformers]] for state-of-the-art image generation <a class="yt-timestamp" data-t="00:01:20">[00:01:20]</a>, <a class="yt-timestamp" data-t="00:03:09">[00:03:09]</a>, <a class="yt-timestamp" data-t="00:09:33">[00:09:33]</a>.

## Key Innovation: Replacing UNet Backbones

Historically, [[Diffusion models and image generation|diffusion models]] have relied on the UNet architecture, a convolutional neural network structure characterized by an initial widening phase, a choke point, and then a return to a wide structure, resembling a "U" shape <a class="yt-timestamp" data-t="00:08:06">[00:08:06]</a>, <a class="yt-timestamp" data-t="00:08:09">[00:08:09]</a>. The core concept behind DiTs is to replace this traditional UNet backbone with a Transformer <a class="yt-timestamp" data-t="00:08:20">[00:08:20]</a>, <a class="yt-timestamp" data-t="00:08:23">[00:08:23]</a>, <a class="yt-timestamp" data-t="00:05:17">[00:05:17]</a>. This demonstrates that the UNet bias is not crucial for the performance of [[Diffusion models and image generation|diffusion models]] and they can be replaced by standard designs like Transformers <a class="yt-timestamp" data-t="00:09:09">[00:09:09]</a>, <a class="yt-timestamp" data-t="00:09:13">[00:09:13]</a>.

## Diffusion Transformer (DiT) Architecture

DiTs are based on the Vision Transformer (ViT) architecture, which operates on sequences of image patches <a class="yt-timestamp" data-t="00:28:44">[00:28:44]</a>, <a class="yt-timestamp" data-t="00:28:46">[00:28:46]</a>.

### Latent Diffusion and Patching
DiTs are trained as [[Latent diffusion models and architectures|latent diffusion models]], meaning the diffusion process occurs in a lower-dimensional latent space rather than directly in high-resolution pixel space, which can be computationally prohibitive <a class="yt-timestamp" data-t="00:05:14">[00:05:14]</a>, <a class="yt-timestamp" data-t="00:05:16">[00:05:16]</a>, <a class="yt-timestamp" data-t="00:11:06">[00:11:06]</a>, <a class="yt-timestamp" data-t="00:27:28">[00:27:28]</a>, <a class="yt-timestamp" data-t="00:27:32">[00:27:32]</a>.
*   The input to the DiT is a spatial representation (latent embedding) of an image <a class="yt-timestamp" data-t="00:28:56">[00:28:56]</a>, <a class="yt-timestamp" data-t="00:29:02">[00:29:02]</a>. For example, a 256x256x3 image might be converted into a 32x32x4 latent vector, significantly reducing dimensionality <a class="yt-timestamp" data-t="00:29:10">[00:29:10]</a>, <a class="yt-timestamp" data-t="00:29:13">[00:29:13]</a>, <a class="yt-timestamp" data-t="00:49:24">[00:49:24]</a>, <a class="yt-timestamp" data-t="00:49:28">[00:49:28]</a>.
*   This latent vector is then "patched" by cutting it into smaller segments, similar to how ViTs process images <a class="yt-timestamp" data-t="00:10:09">[00:10:09]</a>, <a class="yt-timestamp" data-t="00:10:11">[00:10:11]</a>, <a class="yt-timestamp" data-t="00:30:18">[00:30:18]</a>. Each patch acts as a "word token" in a sequence that the Transformer processes <a class="yt-timestamp" data-t="00:10:33">[00:10:33]</a>, <a class="yt-timestamp" data-t="00:10:35">[00:10:35]</a>, <a class="yt-timestamp" data-t="00:10:39">[00:10:39]</a>.
*   DiTs experimented with patch sizes (P) of 2, 4, and 8, where P=2 means a 2x2 patch, resulting in 4 total patches <a class="yt-timestamp" data-t="00:32:00">[00:32:00]</a>, <a class="yt-timestamp" data-t="00:32:11">[00:32:11]</a>, <a class="yt-timestamp" data-t="00:43:53">[00:43:53]</a>. Smaller patches increase the number of input tokens, which can significantly increase Transformer GFLOPs <a class="yt-timestamp" data-t="00:31:52">[00:31:52]</a>, <a class="yt-timestamp" data-t="00:31:55">[00:31:55]</a>.
*   Positional embeddings are added to each patch to retain spatial information <a class="yt-timestamp" data-t="00:29:44">[00:29:44]</a>, <a class="yt-timestamp" data-t="00:29:48">[00:29:48]</a>, <a class="yt-timestamp" data-t="00:30:26">[00:30:26]</a>.

### Conditioning and Normalization
DiTs also incorporate [[Conditional diffusion models for neural networks|conditional information]] like noise time step, class labels (C), and natural language prompts <a class="yt-timestamp" data-t="00:32:24">[00:32:24]</a>, <a class="yt-timestamp" data-t="00:32:25">[00:32:25]</a>, <a class="yt-timestamp" data-t="00:32:50">[00:32:50]</a>, <a class="yt-timestamp" data-t="00:32:52">[00:32:52]</a>.
*   [[Conditional diffusion models for neural networks|Class-conditional image generation]] (e.g., [[Text to image generation with diffusion Transformers|Imagenet]]) uses a technique called classifier-free guidance, where the model is sometimes conditioned on a class token and sometimes on a null token during training <a class="yt-timestamp" data-t="00:16:07">[00:16:07]</a>, <a class="yt-timestamp" data-t="00:16:19">[00:16:19]</a>, <a class="yt-timestamp" data-t="00:25:13">[00:25:13]</a>, <a class="yt-timestamp" data-t="00:27:01">[00:27:01]</a>, <a class="yt-timestamp" data-t="01:01:56">[01:01:56]</a>.
*   An additional multi-head cross-attention layer is included in the Transformer blocks to specifically process these conditioning labels, adding only about 15% overhead <a class="yt-timestamp" data-t="00:33:41">[00:33:41]</a>, <a class="yt-timestamp" data-t="00:33:44">[00:33:44]</a>, <a class="yt-timestamp" data-t="00:33:50">[00:33:50]</a>.
*   Adaptive normalization layers, such as adaptive layer normalization (AdaLN), are used to ensure activations are well-distributed and prevent gradients from vanishing or exploding, especially in deep and large networks <a class="yt-timestamp" data-t="00:34:09">[00:34:09]</a>, <a class="yt-timestamp" data-t="00:35:21">[00:35:21]</a>, <a class="yt-timestamp" data-t="00:35:49">[00:35:49]</a>.

## Scalability and Performance

DiTs demonstrate strong [[Scalability of Transformerbased diffusion models|scaling properties]].
*   **Model Sizes:** DiTs come in various sizes, from "small" to "XL2". For instance, DiT-S (small) has 12 Transformer layers, 6 attention heads, and a hidden dimension of 384, while DiT-XL2 (largest) has 28 Transformer layers, 16 heads, and a width of 1152 <a class="yt-timestamp" data-t="00:42:04">[00:42:04]</a>, <a class="yt-timestamp" data-t="00:42:14">[00:42:14]</a>, <a class="yt-timestamp" data-t="00:42:17">[00:42:17]</a>.
*   **Performance Metrics:** The performance of [[Diffusion models and image generation|generative models]] is often evaluated using metrics like Fréchet Inception Distance (FID), where lower scores indicate better image quality <a class="yt-timestamp" data-t="00:06:29">[00:06:29]</a>, <a class="yt-timestamp" data-t="00:06:32">[00:06:32]</a>, <a class="yt-timestamp" data-t="00:06:40">[00:06:40]</a>, <a class="yt-timestamp" data-t="00:11:55">[00:11:55]</a>.
*   **GFLOPs and FID:** There is a consistent correlation: DiTs with higher GFLOPs (indicating increased Transformer depth, width, or input tokens) consistently achieve lower FID scores, demonstrating improved sample quality <a class="yt-timestamp" data-t="00:05:39">[00:05:39]</a>, <a class="yt-timestamp" data-t="00:05:42">[00:05:42]</a>, <a class="yt-timestamp" data-t="00:11:18">[00:11:18]</a>, <a class="yt-timestamp" data-t="00:11:20">[00:11:20]</a>, <a class="yt-timestamp" data-t="01:00:22">[01:00:22]</a>.
*   **State-of-the-Art:** The largest model, DiT-XL2, achieved a state-of-the-art FID of 2.27 on the 256x256 [[Conditional diffusion models for neural networks|class-conditional Imagenet benchmark]], outperforming all prior [[Latent diffusion models and architectures|diffusion models]] <a class="yt-timestamp" data-t="00:11:32">[00:11:32]</a>, <a class="yt-timestamp" data-t="00:11:36">[00:11:36]</a>, <a class="yt-timestamp" data-t="01:02:05">[01:02:05]</a>. This highlights that a better architecture can achieve superior results even with smaller computational footprints compared to other models <a class="yt-timestamp" data-t="00:13:38">[00:13:38]</a>.

## Training and Implementation

DiTs are implemented in JAX and trained on Google's TPU v3 pods <a class="yt-timestamp" data-t="00:51:15">[00:51:15]</a>, <a class="yt-timestamp" data-t="00:51:19">[00:51:19]</a>, <a class="yt-timestamp" data-t="00:51:22">[00:51:22]</a>.
*   The largest model trains at 5.7 iterations per second on a TPU v3 256 pod <a class="yt-timestamp" data-t="00:52:46">[00:52:46]</a>, <a class="yt-timestamp" data-t="00:52:50">[00:52:50]</a>.
*   Training a single model for 400,000 steps takes approximately 19 hours <a class="yt-timestamp" data-t="00:53:57">[00:53:57]</a>, <a class="yt-timestamp" data-t="00:54:12">[00:54:12]</a>, potentially costing tens of thousands of dollars per model <a class="yt-timestamp" data-t="00:55:06">[00:55:06]</a>, <a class="yt-timestamp" data-t="00:55:10">[00:55:10]</a>.
*   The total training compute is calculated as `model GFLOPs × batch size × training steps × 3` (factor of 3 approximates backward pass being twice as heavy as forward pass) <a class="yt-timestamp" data-t="01:00:41">[01:00:41]</a>, <a class="yt-timestamp" data-t="01:00:42">[01:00:42]</a>, <a class="yt-timestamp" data-t="01:00:44">[01:00:44]</a>, <a class="yt-timestamp" data-t="01:00:46">[01:00:46]</a>.
*   They use an off-the-shelf pre-trained VAE model, specifically one from [[Latent diffusion models and architectures|Stable Diffusion]], for the latent space <a class="yt-timestamp" data-t="00:49:03">[00:49:03]</a>, <a class="yt-timestamp" data-t="00:49:07">[00:49:07]</a>, <a class="yt-timestamp" data-t="00:49:09">[00:49:09]</a>.

## Impact and Future Work

The impressive image quality achieved by DiTs, particularly the crispness and semantic correctness across different scales, suggests that the integration of Transformers into [[Diffusion models and image generation|diffusion models]] is a highly promising direction <a class="yt-timestamp" data-t="00:03:18">[00:03:18]</a>, <a class="yt-timestamp" data-t="00:03:20">[00:03:20]</a>, <a class="yt-timestamp" data-t="00:03:31">[00:03:31]</a>, <a class="yt-timestamp" data-t="00:03:40">[00:03:40]</a>. The findings indicate that future work should continue to [[Scalability of Transformerbased diffusion models|scale DiT models]] to even larger sizes and token counts to further enhance performance <a class="yt-timestamp" data-t="01:05:13">[01:05:13]</a>, <a class="yt-timestamp" data-t="01:05:15">[01:05:15]</a>. The rapid acceleration of improvements in [[Diffusion models and image generation|generative AI]], demonstrated by the leap from early GANs to current diffusion models, suggests that [[Text to image diffusion models|text-to-video generation]] at 4K resolution could become an API reality in the near future <a class="yt-timestamp" data-t="01:13:55">[01:13:55]</a>, <a class="yt-timestamp" data-t="01:14:01">[01:14:01]</a>, <a class="yt-timestamp" data-t="01:14:21">[01:14:21]</a>, <a class="yt-timestamp" data-t="01:14:23">[01:14:23]</a>, <a class="yt-timestamp" data-t="01:14:56">[01:14:56]</a>, <a class="yt-timestamp" data-t="01:15:02">[01:15:02]</a>.