---
title: Application of LoRA in Transformer architectures
videoId: vjEPXSCbmDE
---

From: [[hu-po]] <br/> 

Low-Rank Adaptation (LoRA) is a technique for adapting large language models (LLMs) that has gained significant popularity <a class="yt-timestamp" data-t="00:00:52">[00:00:52]</a>. Developed by Microsoft, LoRA focuses on training additional parameters rather than fully fine-tuning existing models <a class="yt-timestamp" data-t="00:01:09">[00:01:09]</a>, making it widely used for LLMs like Alpaca and [[transformer_architecture_in_image_processing | image generation models]] such as Stable Diffusion's diffusion models <a class="yt-timestamp" data-t="00:01:27">[00:01:27]</a>. Despite being published in October 2021, which is considered "infinity ago" in machine learning years, the [[lora_technique_for_model_adaptation | LoRA approach]] has "stood the test of time" and continues to be used in various open-source fine-tuning repositories <a class="yt-timestamp" data-t="00:01:42">[00:01:42]</a>.

## Core Concept of LoRA

The fundamental idea behind LoRA is to freeze the pre-trained model's original weights and instead inject trainable rank decomposition matrices into each layer of the [[transformerbased_model_architectures | Transformer architecture]] <a class="yt-timestamp" data-t="00:03:40">[00:03:40]</a>. This means gradients are not pushed into the original model itself <a class="yt-timestamp" data-t="00:03:54">[00:03:54]</a>. Instead, new, smaller model weights, referred to as "trainable rank decomposition matrices," are added and trained <a class="yt-timestamp" data-t="00:04:07">[00:04:07]</a>. These "rank decomposition matrices" are essentially smaller weight matrices with a lower rank, meaning they have a lower intrinsic dimension <a class="yt-timestamp" data-t="00:04:21">[00:04:21]</a>.

The core hypothesis is that the "change in weights during model adaptation also has low intrinsic rank" <a class="yt-timestamp" data-t="00:36:50">[00:36:50]</a>. While original weight matrices in neural networks typically have full rank, meaning they cannot be further reduced <a class="yt-timestamp" data-t="00:35:57">[00:35:57]</a>, research has shown that pre-trained language models possess a low intrinsic dimension and can still learn effectively even with a random projection into a smaller subspace <a class="yt-timestamp" data-t="00:36:27">[00:36:27]</a>.

LoRA represents the update to a weight matrix `W` (denoted as `Î”W`) as a low-rank decomposition `BA`, where `B` is a `d x r` matrix and `A` is an `r x k` matrix <a class="yt-timestamp" data-t="00:38:31">[00:38:31]</a>. Here, `r` is the rank of the LoRA module, and `r` is chosen to be much smaller than `d` and `k`, the dimensions of the original weight matrix <a class="yt-timestamp" data-t="00:41:06">[00:41:06]</a>. During training, the original `W` is frozen, and only `A` and `B` are updated <a class="yt-timestamp" data-t="00:40:05">[00:40:05]</a>. `A` is typically initialized with a random Gaussian distribution, and `B` is initialized with zeros <a class="yt-timestamp" data-t="00:41:32">[00:41:32]</a>.

## Advantages of LoRA

LoRA offers several significant advantages over traditional fine-tuning:

*   **Reduced Trainable Parameters and GPU Memory**: Compared to full fine-tuning with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and GPU memory requirements by three times when adapting a model like GPT-3 <a class="yt-timestamp" data-t="00:05:32">[00:05:32]</a>. For GPT-3 175B, trainable parameters can be as small as 0.01% of the total <a class="yt-timestamp" data-t="01:16:10">[01:16:10]</a>.
*   **Comparable or Better Model Quality**: LoRA performs "on par or better" than full fine-tuning in terms of model quality, despite training significantly fewer parameters <a class="yt-timestamp" data-t="00:05:40">[00:05:40]</a>.
*   **No Additional Inference Latency**: LoRA's simple linear design allows the newly trained low-rank matrices to be merged directly with the frozen pre-trained weights during deployment <a class="yt-timestamp" data-t="01:19:37">[01:19:37]</a>. This means the number of weights remains the same, avoiding additional inference latency <a class="yt-timestamp" data-t="00:06:52">[00:06:52]</a>. This is a key difference from adapter-based methods <a class="yt-timestamp" data-t="00:05:50">[00:05:50]</a>.
*   **Efficient Task Switching**: A pre-trained model can be shared across multiple tasks by creating different small LoRA modules for each task <a class="yt-timestamp" data-t="01:12:11">[01:12:11]</a>. To switch tasks, the model simply recovers the original weights by subtracting the current LoRA `BA` and then adds the new `BA` for the desired task <a class="yt-timestamp" data-t="01:48:40">[01:48:40]</a>. This reduces storage requirements and task switching overhead <a class="yt-timestamp" data-t="00:12:21">[00:12:21]</a>.
*   **Faster Training Throughput**: LoRA offers a higher training throughput, showing a 25% speed-up during training compared to full fine-tuning, as gradients are not calculated for the vast majority of parameters <a class="yt-timestamp" data-t="00:52:03">[00:52:03]</a>.

## Application in Transformers

In the [[transformerbased_model_architectures | Transformer architecture]], LoRA can be applied to any subset of weight matrices <a class="yt-timestamp" data-t="00:46:53">[00:46:53]</a>. The Transformer's self-attention module contains four weight matrices (query, key, value, and output), and the MLP module has two <a class="yt-timestamp" data-t="00:47:03">[00:47:03]</a>. While the paper primarily focuses on adapting the query (`WQ`) and value (`WV`) projection matrices for simplicity <a class="yt-timestamp" data-t="01:03:46">[01:03:46]</a> and freezes the MLP modules <a class="yt-timestamp" data-t="00:49:36">[00:49:36]</a>, it acknowledges that the optimal configuration (where to apply LoRA) varies for different model architectures and tasks <a class="yt-timestamp" data-t="00:34:30">[00:34:30]</a>.

The rank `r` of the LoRA module is determined by a "parameter budget" <a class="yt-timestamp" data-t="01:25:44">[01:25:44]</a>. For instance, an 18 million parameter budget corresponds to an `r` of 8 for adapting one type of attention weight, or `r` of 4 for adapting two types across all 96 layers <a class="yt-timestamp" data-t="01:25:34">[01:25:34]</a>. Empirically, even a small rank like `r=1` can suffice for tasks like WikiSQL and MultiNLI, suggesting that the "update Matrix could have a very small intrinsic rank" <a class="yt-timestamp" data-t="01:30:28">[01:30:28]</a>. The research suggests that it's often preferable to adapt more weight matrices with a smaller rank than to adapt a single type of weights with a larger rank <a class="yt-timestamp" data-t="01:28:00">[01:28:00]</a>.

## Comparison with Other Adaptation Methods

The paper contrasts LoRA with other efficient adaptation strategies <a class="yt-timestamp" data-t="00:28:57">[00:28:57]</a>:

*   **Adapter Layers**: These methods involve inserting additional layers between existing layers of a neural network and training only these new layers <a class="yt-timestamp" data-t="01:19:19">[01:19:19]</a>. While adapters also reduce trainable parameters, they introduce "additional inference latency" because the new layers must be processed sequentially, particularly problematic in latency-sensitive production environments with small batch sizes <a class="yt-timestamp" data-t="00:29:27">[00:29:27]</a>. In contrast, LoRA avoids this by merging its learned weights with the pre-trained ones <a class="yt-timestamp" data-t="01:19:33">[01:19:33]</a>.
*   **Optimizing Input Layer Activations (e.g., Prefix/Infixed Tuning)**: These methods involve adding special trainable tokens to the input prompt (either at the beginning or end) <a class="yt-timestamp" data-t="01:01:18">[01:01:18]</a>. While parameter-efficient, they are "difficult to optimize" and their performance can change non-monotonically with increased trainable parameters <a class="yt-timestamp" data-t="01:31:30">[01:31:30]</a>. Critically, reserving part of the sequence length for adaptation "reduces the sequence length available to process a downstream task" <a class="yt-timestamp" data-t="01:41:41">[01:41:41]</a>. LoRA does not suffer from this limitation <a class="yt-timestamp" data-t="01:49:24">[01:49:24]</a>.

For GPT-3, LoRA matches or exceeds the fine-tuning baseline across various datasets <a class="yt-timestamp" data-t="01:11:49">[01:11:49]</a>. Larger models like GPT-2 and GPT-3 seem to benefit more from LoRA compared to smaller models like Roberta <a class="yt-timestamp" data-t="01:05:52">[01:05:52]</a>. This is hypothesized to be because larger models have higher "model capacity" and thus a greater likelihood of having lower intrinsic rank in their weight updates <a class="yt-timestamp" data-t="01:07:33">[01:07:33]</a>.

## Future Directions and Insights

The paper suggests several avenues for future research, including combining LoRA with other efficient adaptation methods <a class="yt-timestamp" data-t="01:50:00">[01:50:00]</a>. The exact mechanism behind fine-tuning and LoRA is still not entirely clear <a class="yt-timestamp" data-t="01:50:06">[01:50:06]</a>, and LoRA's parameter efficiency could make it easier to interpret how update weights correlate with pre-trained weights <a class="yt-timestamp" data-t="01:23:59">[01:23:59]</a>. The selection of which weight matrices to apply LoRA to is currently based on heuristics <a class="yt-timestamp" data-t="01:50:21">[01:50:21]</a>, implying potential for further optimization. Finally, the observed rank deficiency in LoRA updates suggests that the original weight matrices themselves might also be amenable to lower-rank representations, which could inspire future work on model compression and efficiency <a class="yt-timestamp" data-t="01:50:48">[01:50:48]</a>.