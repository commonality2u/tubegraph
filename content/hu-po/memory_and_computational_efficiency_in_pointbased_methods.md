---
title: Memory and Computational Efficiency in PointBased Methods
videoId: xgwvU7S0K-k
---

From: [[hu-po]] <br/> 

Point-based methods offer an alternative approach to representing 3D scenes compared to continuous scene representations like Neural Radiance Fields (Nerfs) <a class="yt-timestamp" data-t="01:48:50">[01:48:50]</a>. While Nerfs implicitly store scene information within a small neural network, point-based methods explicitly define scene geometry and appearance using discrete elements such as meshes, point clouds, or surfels <a class="yt-timestamp" data-t="01:53:50">[01:53:50]</a>. This distinction leads to significant differences in [[optimization_techniques_and_challenges_for_3D_Scene_Representation | memory usage]] and computational efficiency for training and rendering.

## 3D Gaussian Splatting: A Point-Based Approach

The paper "3D Gaussian Splatting for Real-Time Radiance Field Rendering" introduces a novel point-based method that leverages 3D gaussians to represent a scene <a class="yt-timestamp" data-t="01:04:07">[01:04:07]</a>. Each gaussian is defined by its 3D position, an anisotropic covariance matrix (describing its shape and orientation), an opacity value, and spherical harmonic coefficients for color <a class="yt-timestamp" data-t="00:59:44">[00:59:44]</a>.

> [!INFO] Anisotropic Covariance
> Anisotropic covariance allows the gaussians to stretch and rotate, enabling them to represent complex shapes and fine structures compactly, such as a thin bicycle spoke or a wall <a class="yt-timestamp" data-t="01:00:43">[01:00:43]</a>. This contrasts with isotropic gaussians, which would be forced to be simple spheres, leading to less efficient representation of detailed geometry <a class="yt-timestamp" data-t="02:20:01">[02:20:01]</a>.

### Training Efficiency

The 3D Gaussian Splatting method focuses on achieving competitive training times and real-time rendering <a class="yt-timestamp" data-t="01:17:15">[01:17:15]</a>.

*   **Training Time Comparison:**
    *   **3D Gaussian Splatting:** Achieves state-of-the-art quality in 35 to 45 minutes on an A6000 GPU <a class="yt-timestamp" data-t="02:10:50">[02:10:50]</a>. It can achieve comparable quality to other methods in 5 to 10 minutes <a class="yt-timestamp" data-t="02:11:00">[02:11:00]</a>.
    *   **Neural Radiance Fields (Nerfs):** A standard Nerf can take up to 48 hours to train <a class="yt-timestamp" data-t="02:52:00">[02:52:00]</a>.
    *   **Instant NGP:** A faster Nerf variant, takes approximately 7 minutes to train <a class="yt-timestamp" data-t="02:01:16">[02:01:16]</a>.

*   **Optimization Process:** The training involves an interleaved optimization and adaptive density control of the 3D gaussians <a class="yt-timestamp" data-t="00:07:43">[00:07:43]</a>. This includes:
    *   **Initialization:** Starting from a sparse point cloud generated by Structure from Motion (SfM) <a class="yt-timestamp" data-t="00:06:38">[00:06:38]</a>.
    *   **Adaptive Density Control:**
        *   Transparent gaussians are periodically removed (pruning) <a class="yt-timestamp" data-t="01:31:58">[01:31:58]</a>.
        *   New gaussians are added (densification) in under-reconstructed areas by cloning existing gaussians or splitting large ones <a class="yt-timestamp" data-t="01:32:54">[01:32:54]</a>. This process dynamically adjusts the number of gaussians, potentially increasing from an initial 100,000 to 500,000 per scene <a class="yt-timestamp" data-t="02:11:50">[02:11:50]</a>.
    *   **Explicit Gradients:** To avoid the overhead of automatic differentiation, gradients for gaussian parameters are derived explicitly <a class="yt-timestamp" data-t="01:16:07">[01:16:07]</a>.

### Rendering Efficiency

Real-time rendering is a key feature of 3D Gaussian Splatting, achieved through a fast, differentiable tile-based rasterizer <a class="yt-timestamp" data-t="01:39:58">[01:39:58]</a>.

*   **Tile-Based Rasterization:** The screen is divided into 16x16 pixel tiles <a class="yt-timestamp" data-t="01:40:05">[01:40:05]</a>.
*   **GPU Sorting:** Gaussians are sorted by depth and assigned a key that combines view space depth and tile ID <a class="yt-timestamp" data-t="01:43:03">[01:43:03]</a>. This sorting is performed once per frame using a fast GPU Radix sort, avoiding the slower per-pixel sorting <a class="yt-timestamp" data-t="01:43:18">[01:43:18]</a>.
*   **Parallel Processing:** The sorted list allows parallel processing of tiles, with each tile processed by a dedicated thread block <a class="yt-timestamp" data-t="01:46:09">[01:46:09]</a>. Alpha blending then accumulates colors and alpha values for each pixel <a class="yt-timestamp" data-t="01:46:15">[01:46:15]</a>.
*   **Early Termination:** Threads stop when a target saturation of opacity is reached, similar to how Nerfs avoid sampling points beyond an opaque surface <a class="yt-timestamp" data-t="01:46:50">[01:46:50]</a>.
*   **Backward Pass Efficiency:** The rasterizer supports a fast backward pass for gradient propagation by reusing sorted arrays and traversing them efficiently from back to front <a class="yt-timestamp" data-t="01:54:15">[01:54:15]</a>.

> [!NOTE] Comparison to Nerf Rendering
> Unlike Nerfs, which require thousands or millions of neural network inferences per pixel through ray marching <a class="yt-timestamp" data-t="01:51:30">[01:51:30]</a>, 3D Gaussian Splatting directly renders the gaussians, leveraging efficient GPU operations like sorting and parallel blending <a class="yt-timestamp" data-t="02:40:56">[02:40:56]</a>.

## Memory Usage

A key trade-off for point-based methods, including 3D Gaussian Splatting, is their [[impact_of_quantization_on_memory_usage_and_power_consumption | memory usage]].

*   **Model Size:**
    *   **3D Gaussian Splatting:** A scene representation can be as large as 734 MB due to storing millions of gaussians <a class="yt-timestamp" data-t="02:36:22">[02:36:22]</a>.
    *   **Plenoxels:** Another point-based method, can require approximately 2 GB of memory per scene <a class="yt-timestamp" data-t="02:03:31">[02:03:31]</a>.
    *   **Instant NGP (Nerf):** Significantly smaller, requiring only 8-13 MB to store its neural network weights <a class="yt-timestamp" data-t="02:03:21">[02:03:21]</a>.

*   **GPU Memory Consumption:** During training of large scenes, peak GPU [[memory optimization in neural networks | memory consumption]] for 3D Gaussian Splatting can exceed 20 GB <a class="yt-timestamp" data-t="02:33:45">[02:33:45]</a>. An additional 30 to 500 MB is needed for the rasterizer depending on scene size and image resolution <a class="yt-timestamp" data-t="02:24:18">[02:24:18]</a>. This highlights a challenge for widespread adoption on consumer-grade hardware.

> [!WARNING] Future Optimizations
> The authors acknowledge the significant memory footprint and suggest that [[advancements_in_model_compression | compression techniques]] for point clouds could be adapted to their representation to reduce memory consumption <a class="yt-timestamp" data-t="02:29:29">[02:29:29]</a>. They also note that porting more of their Python-based optimization logic to optimized Cuda kernels could further accelerate the process <a class="yt-timestamp" data-t="02:26:24">[02:26:24]</a>.

## Conclusion

[[Comparison between gaussian splats and traditional 3D representation methods | 3D Gaussian Splatting]] represents a significant step in [[technical_insights_balancing_complexity_and_efficiency_in_3D_Modeling | balancing complexity and efficiency in 3D modeling]]. It achieves superior visual quality and faster rendering speeds compared to many Nerf variants, primarily by adopting an explicit, point-based representation optimized for GPU rasterization <a class="yt-timestamp" data-t="02:41:19">[02:41:19]</a>. However, this comes at the cost of substantially higher memory consumption for storing the scene representation <a class="yt-timestamp" data-t="02:36:22">[02:36:22]</a>. Future work will likely focus on [[advancements_in_model_efficiency_through_quantization | memory optimization]] and extending the technique to dynamic scenes.