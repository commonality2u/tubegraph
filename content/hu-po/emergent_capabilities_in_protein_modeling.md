---
title: Emergent capabilities in protein modeling
videoId: I_ll4L9TpU4
---

From: [[hu-po]] <br/> 
This article discusses **Emergent capabilities in protein modeling**, drawing insights from a paper by Facebook Research titled "Evolutionary scale prediction of atomic protein structure with a language model" <a class="yt-timestamp" data-t="00:00:49">[00:00:49]</a>. This work positions Meta as a contender in the field of [[Protein structure prediction using language models | protein structure prediction]], competing with DeepMind's AlphaFold <a class="yt-timestamp" data-t="00:01:11">[00:01:11]</a>.

### Evolutionary Scale Modeling (ESM)

The paper introduces a new family of Transformer protein [[Large language models for protein sequences | language models]] called ESM2 <a class="yt-timestamp" data-t="00:30:51">[00:30:51]</a>. These models range in size from 8 million to 15 billion parameters, making them the largest [[Large language models for protein sequences | language models]] of proteins to date <a class="yt-timestamp" data-t="00:03:46">[00:03:46]</a>. The core idea is to infer protein structure directly from its primary sequence using a [[Large language models for protein sequences | large language model]] <a class="yt-timestamp" data-t="00:03:20">[00:03:20]</a>.

#### Training and Architecture
ESM models are trained using a masked language modeling objective <a class="yt-timestamp" data-t="00:31:22">[00:31:22]</a>, which is unsupervised and allows for the infinite creation of training data from existing protein databases <a class="yt-timestamp" data-t="00:31:40">[00:31:40]</a>. This involves masking out parts of a protein sequence and training the model to predict the missing parts <a class="yt-timestamp" data-t="00:31:29">[00:31:29]</a>. The training data utilized includes UniRef50, comprising 43 million clusters and 65 million unique sequences, and UniRef90 sequences, totaling 138 million <a class="yt-timestamp" data-t="00:32:23">[00:32:23]</a>. The largest model (15 billion parameters) was trained on 650 million unique sequences <a class="yt-timestamp" data-t="00:32:35">[00:32:35]</a>.

For generating 3D structures, ESM uses ESMFold, which incorporates a "folding head" that takes the encoded representation from ESM2 to predict the protein's fold <a class="yt-timestamp" data-t="00:50:50">[00:50:50]</a>. This process involves a series of "folding blocks" that iteratively update sequence and pairwise representations, followed by a structure module that outputs 3D coordinates and confidence scores <a class="yt-timestamp" data-t="00:51:30">[00:51:30]</a>.

#### Comparison with AlphaFold
A key distinction between ESMFold and AlphaFold is ESMFold's ability to eliminate the need for multiple sequence alignment (MSA) <a class="yt-timestamp" data-t="00:13:20">[00:13:20]</a>, which is deeply integrated into AlphaFold's architecture <a class="yt-timestamp" data-t="00:52:29">[00:52:29]</a>. This simplification significantly speeds up inference: AlphaFold can take over 10 minutes to predict a structure <a class="yt-timestamp" data-t="00:14:26">[00:14:26]</a>, while ESMFold can make a prediction for a protein with 384 residues in 14.2 seconds on a single Nvidia V100 GPU <a class="yt-timestamp" data-t="00:53:38">[00:53:38]</a>. This difference is particularly important for *de novo* protein design, which often relies on single sequence inputs <a class="yt-timestamp" data-t="01:05:47">[01:05:47]</a>.

Despite these architectural differences, models like AlphaFold and ESM models exhibit similar inaccuracies, suggesting they may be learning the same "low-hanging fruit" or fundamental understanding of proteins <a class="yt-timestamp" data-t="01:07:52">[01:07:52]</a>.

### Emergent Capabilities in Protein Modeling
The concept of [[Emergent behaviors and community dynamics in AI villages | emergent capabilities]] in [[Large language models for protein sequences | large language models]] (LLMs) refers to the phenomenon where capabilities not explicitly programmed or apparent in smaller models emerge as the model's computation, data, and parameter count increase <a class="yt-timestamp" data-t="00:08:43">[00:08:43]</a>.

For text-based LLMs, emergent intelligence is relatively easy for humans to understand and quantify because we think in words and sentences <a class="yt-timestamp" data-t="00:09:10">[00:09:10]</a>. However, the prospect of [[Emergent behaviors and community dynamics in AI villages | emergent capabilities]] in LLMs trained on protein sequences raises unique questions:
*   **Understanding Beyond Human Comprehension**: What does emergent intelligence mean for models predicting protein structures, a domain where human intuition is limited <a class="yt-timestamp" data-t="00:09:41">[00:09:41]</a>?
*   **Discovery of Novel Patterns**: These models might uncover patterns in protein sequences that humans do not understand <a class="yt-timestamp" data-t="00:09:57">[00:09:57]</a>. This also poses a challenge in verifying if the model has truly understood something beyond our current knowledge <a class="yt-timestamp" data-t="01:12:36">[01:12:36]</a>.
*   **Non-Linear Improvements**: The accuracy of these models shows non-linear improvements as their scale increases, akin to "reflection points" or "emergence points" seen in other deep learning domains <a class="yt-timestamp" data-t="00:36:41">[00:36:41]</a>. For example, the 150 million parameter ESM model shows a significant leap in perplexity reduction compared to smaller models, closing the gap with the 15 billion parameter model <a class="yt-timestamp" data-t="00:47:35">[00:47:35]</a>.
*   **Novel Structure Prediction**: ESMFold has made 225 million high-confidence predictions, with a vast majority being novel compared to experimentally determined structures <a class="yt-timestamp" data-t="00:04:41">[00:04:41]</a>. This suggests the model is characterizing regions of metagenomic space distant from existing knowledge <a class="yt-timestamp" data-t="01:15:43">[01:15:43]</a>. The confidence scores provided by the model help identify these reliable novel structures <a class="yt-timestamp" data-t="01:14:57">[01:14:57]</a>.

### Evaluation Metrics
Several metrics are used to quantify model performance and understand [[Emergent behaviors and community dynamics in AI villages | emergent properties]]:
*   **Perplexity**: A measurement of how well a probability distribution or model predicts a sample <a class="yt-timestamp" data-t="00:33:43">[00:33:43]</a>. Lower perplexity indicates better prediction <a class="yt-timestamp" data-t="00:33:50">[00:33:50]</a>. ESM2 models show significant improvements in perplexity with increasing parameters, with the 15 billion parameter model achieving a perplexity of 6.37 <a class="yt-timestamp" data-t="00:33:30">[00:33:30]</a>.
*   **Precision at L**: Measures the accuracy of contact map predictions by checking if a predicted contact is within the top L most likely contacts for a sequence of length L <a class="yt-timestamp" data-t="00:42:24">[00:42:24]</a>.
*   **pLDDT (Predicted Local Distance Difference Test)**: A well-calibrated estimate of prediction accuracy <a class="yt-timestamp" data-t="01:01:05">[01:01:05]</a>. It measures the predicted distance between the actual ground truth structure and the model's prediction <a class="yt-timestamp" data-t="01:01:14">[01:01:14]</a>.
*   **RMSD (Root Mean Squared Deviation)**: Measures the distance between the predicted atomic structures and the true 3D structures <a class="yt-timestamp" data-t="01:10:41">[01:10:41]</a>.
*   **TM score**: Used to compare the topological similarity of protein structures <a class="yt-timestamp" data-t="01:19:52">[01:19:52]</a>.

### Challenges and Future Directions
A significant challenge in protein modeling is the "ground truth" data. Experimental determination of protein structures can be complex, leading to questions about the accuracy of existing labeled datasets <a class="yt-timestamp" data-t="01:11:21">[01:11:21]</a>. If both AlphaFold and ESM models agree on a different structure than the "ground truth," it raises the possibility that the ground truth itself might be inaccurate, a problem difficult for humans to verify <a class="yt-timestamp" data-t="01:12:11">[01:12:11]</a>.

The ability of these models to characterize unknown regions of protein space suggests a future where the structure of virtually all proteins discovered through gene sequencing experiments can be understood <a class="yt-timestamp" data-t="01:25:14">[01:25:14]</a>. The widespread availability of these models and data, often funded by government research, implies a future where [[challenges_and_innovations_in_model_adaptation | protein structure prediction]] becomes a more accessible and collaborative field <a class="yt-timestamp" data-t="01:24:22">[01:24:22]</a>. The continuing advancement in this field holds immense potential for the discovery of new medicines and molecules <a class="yt-timestamp" data-t="02:00:24">[02:00:24]</a>.