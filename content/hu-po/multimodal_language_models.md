---
title: Multimodal Language Models
videoId: H5yd-uh9acY
---

From: [[hu-po]] <br/> 

[[overview_of_multimodal_models | Multimodal Large Language Models (MLLMs)]] represent a significant advancement in artificial intelligence, enabling systems to process and interpret information from multiple modalities, such as text, images, and potentially audio or video <a class="yt-timestamp" data-t="02:00:13">[02:00:13]</a>. Unlike traditional [[multimodal_learning_and_embeddings | large language models (LLMs)]] that primarily handle text, MLLMs are designed to bridge the gap between different data types, leading to more comprehensive and context-aware AI agents <a class="yt-timestamp" data-t="02:00:16">[02:00:16]</a>.

## Core Architecture and Functionality

MLLMs typically integrate different components to handle various data types. A common setup involves combining a [[multimodal_capabilities_in_large_language_models_using_clip | Vision Language Model (VLM)]] with a large language model <a class="yt-timestamp" data-t="00:04:21">[00:04:21]</a>. For instance, models like GPT-4 Vision and GPT-4 Turbo are used in tandem to allow the system to consume both visual observations (e.g., screenshots) and text <a class="yt-timestamp" data-t="00:04:27">[00:04:27]</a>, <a class="yt-timestamp" data-t="00:06:52">[00:06:52]</a>.

The process often follows a "sense-plan-act" loop <a class="yt-timestamp" data-t="01:54:43">[01:54:43]</a>:
1.  **Sensing**: The MLLM receives input such as user requests (text), screenshots (visual), and retrieves historical information from its memory <a class="yt-timestamp" data-t="01:54:59">[01:54:59]</a>. A visual encoder, often a pre-trained vision model, transforms visual data into green tokens, while a text tokenizer processes text inputs into blue tokens <a class="yt-timestamp" data-t="04:06:50">[04:06:50]</a>.
2.  **Planning**: An LLM component, often the primary decision-maker, processes these diverse tokens. It engages in "Chain of Thought" planning, essentially "thinking out loud" by generating text tokens that explain its reasoning and outline future steps <a class="yt-timestamp" data-t="01:55:18">[01:55:18]</a>.
3.  **Acting**: Based on the plan, the MLLM outputs "action tokens" which correspond to specific operations <a class="yt-timestamp" data-t="00:08:58">[00:08:58]</a>. These actions are then executed via programmatic interfaces, allowing the AI to interact with software or real-world environments <a class="yt-timestamp" data-t="01:55:35">[01:55:35]</a>.

## Capabilities and Applications

Recent advancements demonstrate MLLMs operating across a wide range of tasks and environments, exhibiting capabilities akin to Artificial General Intelligence (AGI) <a class="yt-timestamp" data-t="00:02:47">[00:02:47]</a>:

*   **Operating System Interaction**:
    *   **OS Co-pilot**: An MLLM agent capable of operating Mac OS computers. It can perform complex tasks like calculating and drawing charts in Excel, creating websites (adjusting layouts, themes), playing music, filling spreadsheets, and generating files for web pages <a class="yt-timestamp" data-t="00:05:00">[00:05:00]</a>, <a class="yt-timestamp" data-t="00:05:18">[00:05:18]</a>.
    *   **UFO (UI-focused agent for Windows OS interaction)**: Developed by Microsoft, this agent operates Windows OS. It can extract information from Word documents, scroll through photo apps to find specific images, create PowerPoints, and send them via email, spanning multiple applications to achieve a task <a class="yt-timestamp" data-t="00:07:39">[00:07:39]</a>, <a class="yt-timestamp" data-t="00:07:53">[00:07:53]</a>. It uses `pywinauto` for programmatic control over Windows elements like clicks, setting text, summarizing, and scrolling <a class="yt-timestamp" data-t="00:34:01">[00:34:01]</a>.
*   **Robotics and 3D Design**: MLLMs can control robotic manipulators and navigate environments. They can also perform 3D asset and art design, for example, by generating Python scripts for Blender's API to create objects from primitive shapes based on natural language commands <a class="yt-timestamp" data-t="00:49:14">[00:49:14]</a>, <a class="yt-timestamp" data-t="00:50:51">[00:50:51]</a>.
*   **Gaming**: MLLMs can operate within video games, taking actions based on visual inputs, often utilizing specific action tokens for game commands <a class="yt-timestamp" data-t="00:44:56">[00:44:56]</a>.
*   **Healthcare Diagnostics**: Surprisingly, models trained on robotics and gaming data have shown positive transfer to unseen domains like healthcare, where they can caption videos of healthcare situations <a class="yt-timestamp" data-t="00:44:21">[00:44:21]</a>, <a class="yt-timestamp" data-t="00:44:37">[00:44:37]</a>.

## Key Techniques and Concepts

*   **Action Grounding**: To help [[multimodal_capabilities_in_large_language_models_using_clip | VLMs]] understand and interact with user interfaces or real-world objects, techniques like action grounding are used. This involves annotating screenshots with red rectangles or numbers to highlight clickable elements, or drawing arrows to indicate directions <a class="yt-timestamp" data-t="00:18:10">[00:18:10]</a>, <a class="yt-timestamp" data-t="00:18:40">[00:18:40]</a>, <a class="yt-timestamp" data-t="00:20:27">[00:20:27]</a>.
*   **Context Window and Memory**: MLLMs rely on memory to store previous states, actions, user requests, and screenshots. With advancements like Gemini 1.5, which boasts significantly larger context windows (consuming hours of video and millions of tokens), the need for explicit "retrieval memory" patterns might diminish, as the model can fit more history directly into its context <a class="yt-timestamp" data-t="00:14:26">[00:14:26]</a>, <a class="yt-timestamp" data-t="00:15:10">[00:15:10]</a>.
*   **Tokenization**: The way information is broken down into tokens is crucial. Current LLMs often use Byte-Pair Encoding (BPE) for text, resulting in vocabularies of tens of thousands of tokens <a class="yt-timestamp" data-t="01:05:20">[01:05:20]</a>, <a class="yt-timestamp" data-t="01:07:40">[01:07:40]</a>. For MLLMs, different modalities (text, vision, action) are often handled by explicit start/end tokens to denote modality changes <a class="yt-timestamp" data-t="00:45:17">[00:45:17]</a>. The future might see a move towards a universal, larger vocabulary that can represent all modalities at a more fundamental level <a class="yt-timestamp" data-t="01:09:07">[01:09:07]</a>.
*   **Self-improvement and Self-play**: The concept of self-play, where AI models train by interacting with themselves (e.g., playing games or solving coding problems), is crucial for achieving superhuman performance <a class="yt-timestamp" data-t="01:34:00">[01:34:00]</a>. This positive transfer of learning across domains means MLLMs can improve in one area (like healthcare) by training in seemingly unrelated areas (like gaming), reducing reliance on specific, potentially regulated, datasets <a class="yt-timestamp" data-t="01:01:01">[01:01:01]</a>, <a class="yt-timestamp" data-t="01:01:09">[01:01:09]</a>.
*   [[ensemble_learning_for_language_models | Ensemble Learning]] and Chain of Thought Decoding:
    *   **Chain of Thought (CoT) Decoding**: Instead of merely picking the most probable next token (greedy decoding), MLLMs can explore alternative decoding paths <a class="yt-timestamp" data-t="01:14:09">[01:14:09]</a>. Longer reasoning paths often correlate with higher confidence in the final answer <a class="yt-timestamp" data-t="01:20:40">[01:20:40]</a>. This technique can reveal intrinsic reasoning within the model, reducing the need for explicit prompt engineering <a class="yt-timestamp" data-t="01:26:00">[01:26:00]</a>.
    *   **Ensemble of Agents**: Combining multiple MLLM agents to answer a question and using majority voting (based on similarity of responses) can significantly improve accuracy <a class="yt-timestamp" data-t="01:39:12">[01:39:12]</a>, <a class="yt-timestamp" data-t="01:40:36">[01:40:36]</a>. This approach allows smaller, less powerful models to collectively outperform larger, individual models <a class="yt-timestamp" data-t="01:44:58">[01:44:58]</a>. The ability to use heterogeneous ensembles (models from different providers) could foster competition among AI developers <a class="yt-timestamp" data-t="01:40:51">[01:40:51]</a>.

## Challenges and Future Outlook

While MLLMs are rapidly advancing, several aspects are evolving:

*   **Complexity of Frameworks**: Current MLLM frameworks often involve intricate "chains of thought" and memory management to achieve desired performance <a class="yt-timestamp" data-t="00:51:05">[00:51:05]</a>. It's anticipated that as underlying LLMs and VLMs become more capable (e.g., with larger context windows and more unified tokenization), much of this explicit complexity will become obsolete <a class="yt-timestamp" data-t="00:15:10">[00:15:10]</a>, <a class="yt-timestamp" data-t="00:51:25">[00:51:25]</a>.
*   **User Interface (UI) Design**: As AI agents become the primary users of applications, current human-centric UIs (with many buttons and complex layouts) may drastically change to optimize for "agent usability" and reduce inference costs <a class="yt-timestamp" data-t="00:37:30">[00:37:30]</a>, <a class="yt-timestamp" data-t="00:39:03">[00:39:03]</a>.
*   **On-Device vs. Cloud**: While current OS-operating agents rely on cloud-based models (like GPT-4), the increasing power of local processors (e.g., Apple's chips) may allow for more on-device MLLM inference, although companies like Microsoft might prefer cloud interaction for data collection and control <a class="yt-timestamp" data-t="00:10:02">[00:10:02]</a>, <a class="yt-timestamp" data-t="00:24:16">[00:24:16]</a>.
*   **Privacy and Control**: The continuous monitoring of user activity (e.g., screenshots sent to data centers) by MLLMs raises concerns about privacy and potential restrictions on user actions <a class="yt-timestamp" data-t="00:41:42">[00:41:42]</a>.
*   **The Singularity**: The rapid, continuous integration of AI capabilities into daily life suggests a gradual "singularity" where the world transforms without a single, dramatic moment. Human skills like typing may become obsolete, replaced by natural language communication with AI agents <a class="yt-timestamp" data-t="01:00:51">[01:00:51]</a>, <a class="yt-timestamp" data-t="01:02:20">[01:02:20]</a>.

The development of MLLMs is pushing towards [[stateoftheart_video_generation_and_multimodal_models | Artificial Super Intelligence (ASI)]], with self-play loops and advanced decoding/ensembling techniques enabling models to acquire intelligence that may surpass human capabilities in various domains <a class="yt-timestamp" data-t="01:59:52">[01:59:52]</a>.