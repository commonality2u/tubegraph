---
title: Meta AI research
videoId: eMFfMz9uYlc
---

From: [[hu-po]] <br/> 

Meta AI Research, formerly known as Facebook AI Research (Fair), has introduced the Segment Anything Project (SAM) <a class="yt-timestamp" data-t="00:00:45">[00:00:45]</a> <a class="yt-timestamp" data-t="00:00:48">[00:00:48]</a>. This initiative focuses on a large segmentation model designed to segment anything <a class="yt-timestamp" data-t="00:01:15">[00:01:15]</a> <a class="yt-timestamp" data-t="00:01:29">[00:01:29]</a>.

## The Segment Anything Model (SAM)

SAM is a large segmentation model that has been pre-trained on a vast amount of data over a very long period <a class="yt-timestamp" data-t="00:01:17">[00:01:17]</a> <a class="yt-timestamp" data-t="00:01:20">[00:01:20]</a>. This type of research is typically conducted by large AI companies due to the resources required <a class="yt-timestamp" data-t="00:01:24">[00:01:24]</a> <a class="yt-timestamp" data-t="00:01:26">[00:01:26]</a>. The model exhibits zero-shot capabilities, allowing it to segment novel images and tasks <a class="yt-timestamp" data-t="00:01:29">[00:01:29]</a> <a class="yt-timestamp" data-t="00:05:31">[00:05:31]</a> <a class="yt-timestamp" data-t="00:05:33">[00:05:33]</a>.

While generally effective, SAM can sometimes make mistakes, such as creating a double mask for closely related objects like two cat heads <a class="yt-timestamp" data-t="00:01:33">[00:01:33]</a> <a class="yt-timestamp" data-t="00:01:43">[00:01:43]</a>. However, it is largely quite good, capable of detecting separate parts of an object <a class="yt-timestamp" data-t="00:01:49">[00:01:49]</a> <a class="yt-timestamp" data-t="00:01:53">[00:01:53]</a>.

### Model Architecture

The SAM model is composed of three interconnected components <a class="yt-timestamp" data-t="00:04:27">[00:04:27]</a> <a class="yt-timestamp" data-t="00:13:52">[00:13:52]</a> <a class="yt-timestamp" data-t="00:37:38">[00:37:38]</a>:
1.  **Image Encoder:** A powerful component that generates image embeddings <a class="yt-timestamp" data-t="00:13:54">[00:13:54]</a> <a class="yt-timestamp" data-t="00:13:59">[00:13:59]</a>. This encoder runs once per image and is likely the largest and most memory-intensive part of the model <a class="yt-timestamp" data-t="00:38:24">[00:38:24]</a> <a class="yt-timestamp" data-t="00:38:32">[00:38:32]</a>. It is based on a minimally adapted Vision Transformer (ViT) <a class="yt-timestamp" data-t="00:38:03">[00:38:03]</a> <a class="yt-timestamp" data-t="00:38:15">[00:38:15]</a>.
2.  **Prompt Encoder:** Embeds user prompts, which can be sparse (points, boxes) or dense (masks, text) <a class="yt-timestamp" data-t="00:14:01">[00:14:01]</a> <a class="yt-timestamp" data-t="00:39:00">[00:39:00]</a>. Points and boxes are represented by positional encodings <a class="yt-timestamp" data-t="00:39:05">[00:39:05]</a> <a class="yt-timestamp" data-t="00:39:09">[00:39:09]</a>. Text prompts are processed through CLIP's text encoder <a class="yt-timestamp" data-t="01:11:50">[01:11:50]</a> <a class="yt-timestamp" data-t="01:39:13">[01:39:13]</a> <a class="yt-timestamp" data-t="01:39:23">[01:39:23]</a> <a class="yt-timestamp" data-t="01:41:10">[01:41:10]</a> <a class="yt-timestamp" data-t="01:41:12">[01:41:12]</a>.
3.  **Mask Decoder:** Combines the image and prompt information to predict segmentation masks <a class="yt-timestamp" data-t="00:14:28">[00:14:28]</a> <a class="yt-timestamp" data-t="00:14:33">[00:14:33]</a> <a class="yt-timestamp" data-t="00:40:48">[00:40:48]</a>. This component employs a modified Transformer decoder block and a dynamic mask prediction head <a class="yt-timestamp" data-t="00:40:56">[00:40:56]</a> <a class="yt-timestamp" data-t="00:40:59">[00:40:59]</a>. It can predict multiple masks from a single prompt, offering confidence scores for each <a class="yt-timestamp" data-t="00:15:08">[00:15:08]</a> <a class="yt-timestamp" data-t="00:15:10">[00:15:10]</a> <a class="yt-timestamp" data-t="00:37:17">[00:37:17]</a>. The mask decoder and prompt encoder are designed for real-time performance, predicting a mask in around 50 milliseconds in a web browser environment, suggesting they run on a backend <a class="yt-timestamp" data-t="00:14:55">[00:14:55]</a> <a class="yt-timestamp" data-t="00:14:59">[00:14:59]</a> <a class="yt-timestamp" data-t="00:43:02">[00:43:02]</a> <a class="yt-timestamp" data-t="00:43:07">[00:43:07]</a>.

The model uses a combination of focal loss and Dice loss for supervised mask prediction <a class="yt-timestamp" data-t="00:53:52">[00:53:52]</a> <a class="yt-timestamp" data-t="00:53:55">[00:53:55]</a>.

## [[Foundation models in AI]] and SAM

Meta AI aims to build a [[foundation_models_in_ai | foundation model]] for segmentation <a class="yt-timestamp" data-t="00:11:11">[00:11:11]</a> <a class="yt-timestamp" data-t="00:11:13">[00:11:13]</a>. [[Foundation models in AI | Foundation models]] are typically very large models trained on vast datasets with generic self-supervised tasks, enabling fine-tuning for downstream tasks <a class="yt-timestamp" data-t="00:03:11">[00:03:11]</a> <a class="yt-timestamp" data-t="00:03:23">[00:03:23]</a> <a class="yt-timestamp" data-t="00:03:25">[00:03:25]</a> <a class="yt-timestamp" data-t="00:03:27">[00:03:27]</a>. While the concept of a segmentation [[foundation_models_in_ai | foundation model]] might seem specific, the "Segment Anything" project aims to generalize the task <a class="yt-timestamp" data-t="00:04:00">[00:04:00]</a> <a class="yt-timestamp" data-t="00:04:10">[00:04:10]</a>.

The project drew inspiration from Natural Language Processing (NLP), where tasks like next-token prediction are used for [[foundation_models_in_ai | foundation model]] pre-training <a class="yt-timestamp" data-t="00:25:11">[00:25:11]</a> <a class="yt-timestamp" data-t="00:25:22">[00:25:22]</a>. The ability to generalize to tasks beyond initial training is a key characteristic of [[foundation_models_in_ai | foundation models]] <a class="yt-timestamp" data-t="00:06:58">[00:06:58]</a> <a class="yt-timestamp" data-t="00:07:01">[00:07:01]</a>. Scaling laws in machine learning suggest that larger datasets yield better results than smaller, fully supervised datasets <a class="yt-timestamp" data-t="00:05:50">[00:05:50]</a> <a class="yt-timestamp" data-t="00:05:56">[00:05:56]</a> <a class="yt-timestamp" data-t="00:06:06">[00:06:06]</a>.

## SA-1B Dataset and Data Engine

A significant contribution of the Segment Anything project is the creation of the SA-1B dataset, comprising over 1 billion masks on 11 million licensed, privacy-respecting images <a class="yt-timestamp" data-t="00:04:38">[00:04:38]</a> <a class="yt-timestamp" data-t="00:04:41">[00:04:41]</a> <a class="yt-timestamp" data-t="00:05:10">[00:05:10]</a> <a class="yt-timestamp" data-t="00:05:14">[00:05:14]</a>. This is considered the largest amount of data any segmentation model has been trained on <a class="yt-timestamp" data-t="00:05:19">[00:05:19]</a> <a class="yt-timestamp" data-t="00:05:24">[00:05:24]</a>. The dataset is 400 times larger than any existing segmentation dataset in terms of masks <a class="yt-timestamp" data-t="00:19:16">[00:19:16]</a> <a class="yt-timestamp" data-t="01:03:19">[01:03:19]</a>.

The data engine used to build SA-1B involved three stages <a class="yt-timestamp" data-t="00:10:54">[00:10:54]</a> <a class="yt-timestamp" data-t="00:16:37">[00:16:37]</a>:
1.  **Assisted Manual Annotation:** SAM assists human annotators in creating masks <a class="yt-timestamp" data-t="00:16:43">[00:16:43]</a> <a class="yt-timestamp" data-t="00:16:46">[00:16:46]</a>. Professional annotators, primarily based in Kenya <a class="yt-timestamp" data-t="02:04:07">[02:04:07]</a> <a class="yt-timestamp" data-t="02:04:09">[02:04:09]</a>, refine masks using pixel-precise brush and eraser tools <a class="yt-timestamp" data-t="01:13:00">[01:13:00]</a> <a class="yt-timestamp" data-t="01:18:12">[01:18:12]</a>. The average annotation time decreased from 34 to 14 seconds as the model improved <a class="yt-timestamp" data-t="00:53:17">[00:53:17]</a> <a class="yt-timestamp" data-t="00:53:21">[00:53:21]</a>.
2.  **Semi-Automatic:** Annotators focus on less prominent objects, with the model automatically detecting confident masks <a class="yt-timestamp" data-t="00:54:19">[00:54:19]</a> <a class="yt-timestamp" data-t="00:54:21">[00:54:21]</a>.
3.  **Fully Automatic:** The model automatically generates masks by prompting itself with a regular grid of foreground points, yielding hundreds of high-quality masks per image <a class="yt-timestamp" data-t="00:17:59">[00:17:59]</a> <a class="yt-timestamp" data-t="00:18:01">[00:18:01]</a> <a class="yt-timestamp" data-t="00:18:16">[00:18:16]</a> <a class="yt-timestamp" data-t="00:18:21">[00:18:21]</a>. Non-maximum suppression (NMS) is used to filter masks <a class="yt-timestamp" data-t="01:07:04">[01:07:04]</a> <a class="yt-timestamp" data-t="01:07:06">[01:07:06]</a>. 99% of the masks in SA-1B are generated automatically <a class="yt-timestamp" data-t="01:00:29">[01:00:29]</a> <a class="yt-timestamp" data-t="01:00:32">[01:00:32]</a>.

The model was retrained six times during the data collection process, improving its ability to generate masks for diverse objects <a class="yt-timestamp" data-t="00:54:01">[00:54:01]</a> <a class="yt-timestamp" data-t="00:54:04">[00:54:04]</a> <a class="yt-timestamp" data-t="00:54:10">[00:54:10]</a> <a class="yt-timestamp" data-t="00:54:12">[00:54:12]</a>. This cyclical process of using the model to generate its own training data resembles [[selfimprovement_in_ai_models | self-supervised learning]] or pseudo-labeling <a class="yt-timestamp" data-t="00:11:04">[00:11:04]</a> <a class="yt-timestamp" data-t="00:11:07">[00:11:07]</a> <a class="yt-timestamp" data-t="00:50:01">[00:50:01]</a> <a class="yt-timestamp" data-t="00:50:06">[00:50:06]</a> <a class="yt-timestamp" data-t="01:48:03">[01:48:03]</a> <a class="yt-timestamp" data-t="01:48:06">[01:48:06]</a>.

## Performance and Evaluation

SAM is designed to transfer zero-shot to new image tasks <a class="yt-timestamp" data-t="00:05:31">[00:05:31]</a> <a class="yt-timestamp" data-t="00:05:33">[00:05:33]</a>. Its zero-shot performance is often competitive with or superior to prior full-size results <a class="yt-timestamp" data-t="00:05:38">[00:05:38]</a> <a class="yt-timestamp" data-t="00:05:43">[00:05:43]</a>.

The model's capabilities were evaluated across a diverse suite of 23 segmentation datasets, including novel image distributions like underwater, egocentric, and X-ray images <a class="yt-timestamp" data-t="01:10:13">[01:10:13]</a> <a class="yt-timestamp" data-t="01:10:17">[01:10:17]</a> <a class="yt-timestamp" data-t="01:15:22">[01:15:22]</a> <a class="yt-timestamp" data-t="01:15:25">[01:15:25]</a>.
SAM was tested on various tasks:
*   **Promptable Segmentation:** Returning valid segmentation masks given flexible prompts (spatial or text) <a class="yt-timestamp" data-t="00:11:52">[00:11:52]</a> <a class="yt-timestamp" data-t="00:11:56">[00:11:56]</a>.
*   **Edge Detection:** SAM can produce reasonable edge maps even though it was not explicitly trained for this task <a class="yt-timestamp" data-t="01:27:07">[01:27:07]</a> <a class="yt-timestamp" data-t="01:28:01">[01:28:01]</a>.
*   **Object Proposal Generation:** Compared favorably to strong baselines <a class="yt-timestamp" data-t="01:29:22">[01:29:22]</a> <a class="yt-timestamp" data-t="01:31:43">[01:31:43]</a>.
*   **Instance Segmentation:** By running an object detector and prompting SAM with the output bounding boxes, it functions as an instance segmenter <a class="yt-timestamp" data-t="01:32:11">[01:32:11]</a> <a class="yt-timestamp" data-t="01:32:13">[01:32:13]</a> <a class="yt-timestamp" data-t="01:32:19">[01:32:19]</a>.
*   **Text-to-Mask Segmentation:** SAM can segment objects from freeform text prompts, utilizing CLIP's text encoder for embedding <a class="yt-timestamp" data-t="01:39:04">[01:39:04]</a> <a class="yt-timestamp" data-t="01:39:07">[01:39:07]</a> <a class="yt-timestamp" data-t="01:41:00">[01:41:00]</a>. This capability can be combined with spatial prompts (e.g., "wiper plus point") for powerful and nuanced segmentation <a class="yt-timestamp" data-t="01:43:05">[01:43:05]</a> <a class="yt-timestamp" data-t="01:43:09">[01:43:09]</a> <a class="yt-timestamp" data-t="01:43:16">[01:43:16]</a>.

Human annotators rated 94% of automatically generated masks as having greater than 90% Intersection over Union (IOU) with professionally corrected masks, indicating high quality <a class="yt-timestamp" data-t="01:01:10">[01:01:10]</a> <a class="yt-timestamp" data-t="01:01:13">[01:01:13]</a> <a class="yt-timestamp" data-t="01:01:18">[01:01:18]</a> <a class="yt-timestamp" data-t="01:01:20">[01:01:20]</a>.

## [[Open source AI models and accessibility]] and Future Impact

Meta AI is releasing the Segment Anything model and the SA-1B dataset under an Apache license to foster research <a class="yt-timestamp" data-t="00:20:58">[00:20:58]</a> <a class="yt-timestamp" data-t="00:21:00">[00:21:00]</a> <a class="yt-timestamp" data-t="00:59:38">[00:59:38]</a> <a class="yt-timestamp" data-t="00:59:44">[00:59:44]</a> <a class="yt-timestamp" data-t="00:59:50">[00:59:50]</a>. This makes the model and its massive dataset accessible for further development of [[foundation_models_in_ai | foundation models]] and applications <a class="yt-timestamp" data-t="00:59:44">[00:59:44]</a>.

SAM's capabilities, particularly its real-time performance and support for flexible prompts (spatial and text), position it for new applications such as augmented reality (AR) and virtual reality (VR) <a class="yt-timestamp" data-t="01:43:55">[01:43:55]</a> <a class="yt-timestamp" data-t="01:44:02">[01:44:02]</a> <a class="yt-timestamp" data-t="01:49:18">[01:49:18]</a>. For example, it could be prompted with gaze points from a wearable device <a class="yt-timestamp" data-t="01:49:20">[01:49:20]</a> <a class="yt-timestamp" data-t="01:49:23">[01:49:23]</a> <a class="yt-timestamp" data-t="01:49:25">[01:49:25]</a>. This suggests SAM could serve as a component in larger AI systems, similar to how CLIP is used in models like DALL-E <a class="yt-timestamp" data-t="01:48:46">[01:48:46]</a> <a class="yt-timestamp" data-t="01:48:54">[01:48:54]</a> <a class="yt-timestamp" data-t="01:48:56">[01:48:56]</a>.

## [[Challenges and Advancements in AI Research]] and Limitations

While powerful, SAM is not perfect <a class="yt-timestamp" data-t="01:49:35">[01:49:35]</a> <a class="yt-timestamp" data-t="01:49:36">[01:49:36]</a>. It can find structures, hallucinate small disconnected components, and its boundaries may not always be perfectly crisp <a class="yt-timestamp" data-t="01:49:38">[01:49:38]</a> <a class="yt-timestamp" data-t="01:49:39">[01:49:39]</a> <a class="yt-timestamp" data-t="01:49:41">[01:49:41]</a>. The text-to-mask task is still exploratory and not entirely robust, though it is believed to be improvable <a class="yt-timestamp" data-t="01:50:31">[01:50:31]</a> <a class="yt-timestamp" data-t="01:50:33">[01:50:33]</a>.

The question remains whether SAM achieves the status of a true [[foundation_models_in_ai | foundation model]] due to its specialization in segmentation, rather than being a general-purpose model adaptable to a wide range of downstream tasks outside of computer vision <a class="yt-timestamp" data-t="01:46:43">[01:46:43]</a> <a class="yt-timestamp" data-t="01:46:45">[01:46:45]</a> <a class="yt-timestamp" data-t="01:46:50">[01:46:50]</a> <a class="yt-timestamp" data-t="01:46:53">[01:46:53]</a>.

## Responsible AI

Meta AI has conducted a responsible AI analysis, studying potential fairness concerns and biases within the dataset <a class="yt-timestamp" data-t="01:08:38">[01:08:38]</a> <a class="yt-timestamp" data-t="01:08:42">[01:08:42]</a>. Efforts were made to ensure geographic diversity in the image sources, with a substantial percentage of images from Europe and Asia, although Antarctica and Greenland were not represented <a class="yt-timestamp" data-t="01:08:46">[01:08:46]</a> <a class="yt-timestamp" data-t="01:08:48">[01:08:48]</a> <a class="yt-timestamp" data-t="01:08:51">[01:08:51]</a> <a class="yt-timestamp" data-t="01:07:48">[01:07:48]</a> <a class="yt-timestamp" data-t="01:07:51">[01:07:51]</a>. The dataset also considers perceived gender representation, skin tone, and age group <a class="yt-timestamp" data-t="01:08:55">[01:08:55]</a> <a class="yt-timestamp" data-t="01:08:57">[01:08:57]</a>. Personal identifying information, like faces and vehicle license plates, were blurred to preserve privacy <a class="yt-timestamp" data-t="01:00:22">[01:00:22]</a> <a class="yt-timestamp" data-t="01:00:24">[01:00:24]</a>.