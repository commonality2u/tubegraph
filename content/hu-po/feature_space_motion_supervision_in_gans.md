---
title: feature space motion supervision in GANs
videoId: ExfMg4v5DMA
---

From: [[hu-po]] <br/> 

The paper "Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold" introduces a novel method for [[interactive_pointbased_manipulation_using_gans | interactive point-based manipulation]] of images generated by Generative Adversarial Networks (GANs) <a class="yt-timestamp" data-t="00:00:46">[00:00:46]</a>. This approach allows users to precisely control the pose, shape, expression, and layout of objects in generated images by simply dragging points <a class="yt-timestamp" data-t="00:02:26">[00:02:26]</a>. Developed by institutions including the Max Planck Institute, MIT CSAIL, and Google AR/VR, the method demonstrates impressive results <a class="yt-timestamp" data-t="00:01:12">[00:01:12]</a>.

## Core Components of DragGAN
DragGAN consists of two primary components to achieve its precise [[applications_of_gans_in_image_editing | image editing]] capabilities <a class="yt-timestamp" data-t="00:06:08">[00:06:08]</a>:
1.  **Feature-based motion supervisor**: This component drives a "handle point" (the point the user wants to move) towards a "target position" <a class="yt-timestamp" data-t="00:06:12">[00:06:12]</a>.
2.  **Point tracking approach**: This leverages the discriminative features of the generator to continuously localize the handle point's position throughout the editing process <a class="yt-timestamp" data-t="00:06:18">[00:06:18]</a>.

The entire manipulation is performed on the learned [[Generative Latent Spaces in AI | generative image manifold]] of the GAN <a class="yt-timestamp" data-t="00:06:50">[00:06:50]</a>. This means the edits maintain realism by adhering to the underlying structure learned by the GAN from its training data, even hallucinating occluded content like teeth inside a lion's mouth when it opens <a class="yt-timestamp" data-t="00:08:15">[00:08:15]</a>.

## Method Overview
The DragGAN approach operates as an iterative optimization process <a class="yt-timestamp" data-t="00:19:04">[00:19:04]</a>:
1.  **Input**: A GAN-generated image (I) from a latent code (W) <a class="yt-timestamp" data-t="00:45:01">[00:45:01]</a>. Users define handle points (P) and target points (T), and optionally a binary mask (M) to denote a movable region <a class="yt-timestamp" data-t="00:45:04">[00:45:04]</a>.
2.  **Motion Supervision Step**: A loss function is applied to enforce the handle points to move towards their target points by optimizing the latent code <a class="yt-timestamp" data-t="00:49:54">[00:49:54]</a>. This results in a new latent code (W') and a slightly updated image (I') <a class="yt-timestamp" data-t="00:50:00">[00:50:00]</a>.
3.  **Point Tracking Step**: The positions of the handle points are updated to accurately track the corresponding object points in the new image (I') <a class="yt-timestamp" data-t="00:50:53">[00:50:53]</a>. This is crucial to ensure subsequent motion supervision steps work on the correct points <a class="yt-timestamp" data-t="00:50:58">[00:50:58]</a>.
4.  **Iteration**: Steps 2 and 3 are repeated until the handle points reach their targets, typically requiring 20 to 200 iterations <a class="yt-timestamp" data-t="00:51:12">[00:51:12]</a>.

The key insight is that the intermediate features of the GAN generator are sufficiently discriminative to enable both motion supervision and precise point tracking without additional neural networks, enhancing efficiency <a class="yt-timestamp" data-t="00:17:11">[00:17:11]</a>.

### Motion Supervision Details
The motion supervision relies on a simple loss function <a class="yt-timestamp" data-t="00:52:45">[00:52:45]</a>. It uses the feature maps (F) generated after the sixth block of the StyleGAN2 architecture <a class="yt-timestamp" data-t="00:53:44">[00:53:44]</a>. This specific block is chosen due to its optimal balance between resolution and discriminativeness <a class="yt-timestamp" data-t="00:55:12">[00:55:12]</a>.

The loss aims to match the features of a small patch around the handle point (P) with the features of a patch that has been shifted towards the target point (T) <a class="yt-timestamp" data-t="00:56:16">[00:56:16]</a>. This is expressed as an L1 loss, summed over pixels within a defined radius (R1 = 3 pixels) around each handle point <a class="yt-timestamp" data-t="00:57:13">[00:57:13]</a> <a class="yt-timestamp" data-t="01:20:08">[01:20:08]</a>. Bilinear interpolation is used to sample features at non-integer coordinates <a class="yt-timestamp" data-t="01:00:03">[01:00:03]</a>.

The latent code `W` is optimized in the `W+` space, which is more expressive as it allows different `W` vectors to be fed into different layers of the generator <a class="yt-timestamp" data-t="01:05:33">[01:05:33]</a> <a class="yt-timestamp" data-t="01:55:46">[01:55:46]</a>. For spatial attributes like pose and shape, only the first six layers of `W` are updated, while later layers are fixed to preserve overall appearance <a class="yt-timestamp" data-t="01:05:57">[01:06:06]</a>. This selective optimization ensures slight, desired movements of the image content <a class="yt-timestamp" data-t="01:07:32">[01:07:32]</a>.

### Point Tracking Details
After motion supervision, the handle points must be re-localized accurately. Unlike traditional [[Motion modeling in AI | point tracking]] methods that rely on optical flow models or particle videos, DragGAN performs point tracking via a nearest-neighbor search in the feature space <a class="yt-timestamp" data-t="01:11:57">[01:12:21]</a>.

The process involves identifying the initial handle point's feature and then searching within a larger radius (R2 = 12 pixels) in the updated feature map (F') for the point whose features are most similar (nearest neighbor in L1 distance) <a class="yt-timestamp" data-t="01:12:21">[01:13:00]</a> <a class="yt-timestamp" data-t="01:13:59">[01:14:50]</a> <a class="yt-timestamp" data-t="01:20:08">[01:20:08]</a>. This is effective because the intermediate feature maps from StyleGAN2's early layers encode highly discriminative, texture-like information, making nearby pixels in the feature space highly correlated with textured regions in the image <a class="yt-timestamp" data-t="01:17:40">[01:17:40]</a>.

## Advantages and Comparisons
DragGAN offers several advantages over previous [[comparison of GANbased methods for image manipulation | GAN-based image manipulation methods]] like "User Controllable LT" <a class="yt-timestamp" data-t="01:07:40">[01:07:40]</a>:
*   **Precision and Naturalness**: It produces more natural and superior results, accurately moving handle points without undesirable semantic changes (e.g., changing a dress to pants or ocean to forest when moving an arm or the sun) <a class="yt-timestamp" data-t="01:08:07">[01:08:07]</a> <a class="yt-timestamp" data-t="01:30:01">[01:30:01]</a>.
*   **Generality**: The method is agnostic to object categories, working across diverse images like faces, cats, cars, houses, and even biological slides <a class="yt-timestamp" data-t="00:03:52">[00:03:52]</a> <a class="yt-timestamp" data-t="00:55:01">[01:54:50]</a>.
*   **Efficiency**: It achieves efficient manipulation, taking only a few seconds on a single RTX 3090 GPU, despite requiring many iterations (20-200 steps) <a class="yt-timestamp" data-t="01:18:20">[00:26:28]</a>. This is partly due to not relying on additional neural networks for motion supervision or point tracking <a class="yt-timestamp" data-t="01:10:04">[01:10:04]</a>.
*   **Out-of-Distribution Manipulation**: By optimizing in the W+ space, DragGAN can achieve manipulations that are outside the distribution of the training data (e.g., closing only one eye of a cat, which likely doesn't exist in the training dataset) <a class="yt-timestamp" data-t="01:55:30">[01:55:30]</a>.

## Limitations and Future Work
Despite its strengths, DragGAN's editing quality is still influenced by the diversity of the training data <a class="yt-timestamp" data-t="01:50:55">[01:50:55]</a>. Manipulating human poses that deviate significantly from the training distribution can lead to artifacts <a class="yt-timestamp" data-t="01:50:59">[01:50:59]</a>. Additionally, handling points in textureless regions sometimes suffers from more drift in tracking <a class="yt-timestamp" data-t="01:51:05">[01:51:05]</a>.

For future work, the researchers plan to extend this point-based image editing to [[Advancements in 3D generative models using neural networks | 3D generative models]] <a class="yt-timestamp" data-t="01:53:01">[01:53:01]</a>. Other potential extensions include applying the technique to [[Video diffusion models in generative 3D | video diffusion models]] or [[State space models in vision | audio spectrograms]] <a class="yt-timestamp" data-t="01:35:35">[01:35:00]</a> <a class="yt-timestamp" data-t="01:56:57">[01:57:00]</a>.

> [!NOTE] Implementation Details
> *   **Framework**: PyTorch <a class="yt-timestamp" data-t="01:21:05">[01:05:07]</a>
> *   **Optimizer**: Adam <a class="yt-timestamp" data-t="01:21:13">[01:13:00]</a>
> *   **Datasets for evaluation**: FFHQ, AFHQ Cat, LSUN Car, LSUN Church, Landscapes HQ, Microscope, Lion, Dog, Elephant <a class="yt-timestamp" data-t="01:23:51">[01:23:51]</a>.
> *   **Hyperparameters**:
>    *   Lambda (weight for reconstruction loss): 20 <a class="yt-timestamp" data-t="01:19:43">[01:19:43]</a>
>    *   R1 (radius for motion supervision): 3 pixels <a class="yt-timestamp" data-t="01:20:08">[01:20:08]</a>
>    *   R2 (radius for point tracking): 12 pixels <a class="yt-timestamp" data-t="01:20:12">[01:20:12]</a>
>    *   D (target distance for stopping optimization): 1-2 pixels <a class="yt-timestamp" data-t="01:21:36">[01:21:36]</a>