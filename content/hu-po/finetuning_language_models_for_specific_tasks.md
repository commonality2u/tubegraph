---
title: finetuning language models for specific tasks
videoId: yxAcTRp9EyQ
---

From: [[hu-po]] <br/> 

Fine-tuning is a critical step in adapting large language models (LLMs) to perform specific tasks, often following a broader pre-training phase <a class="yt-timestamp" data-t="00:06:40">[00:06:40]</a>. This process tailors the model's capabilities for particular applications, frequently utilizing smaller, highly curated datasets.

## The Phi-1 Model and Data Quality

Microsoft Research introduced Phi-1, a language model designed for code generation, which is significantly smaller than competing models with 1.3 billion parameters <a class="yt-timestamp" data-t="02:28:44">[02:28:44]</a>. The development of Phi-1 emphasizes the importance of data quality in achieving strong performance with less computational expense <a class="yt-timestamp" data-t="08:24:00">[08:24:00]</a>.

Traditional approaches often scale up the amount of compute or the size of the network to improve performance, a phenomenon known as [[efficiency_of_large_language_models | scaling laws]] <a class="yt-timestamp" data-t="07:43:00">[07:43:00]</a>. However, the Phi-1 paper suggests that high-quality data can dramatically change the shape of these scaling laws, potentially allowing smaller models with leaner training to match the performance of large-scale models <a class="yt-timestamp" data-t="09:08:00">[09:08:00]</a>.

Phi-1 was trained for four days on eight A100 GPUs, a relatively inexpensive setup compared to models requiring hundreds of thousands or millions of dollars for training <a class="yt-timestamp" data-t="03:44:00">[03:44:00]</a>. This efficiency is largely attributed to the use of "textbook quality" data <a class="yt-timestamp" data-t="04:06:00">[04:06:00]</a>.

## Data Curation for Fine-tuning

The data used for Phi-1's [[finetuning_machine_learning_models | finetuning]] process, called "code exercises," consists of less than 100 million tokens of Python exercises <a class="yt-timestamp" data-t="04:48:00">[04:48:00]</a> <a class="yt-timestamp" data-t="05:05:00">[05:05:00]</a> <a class="yt-timestamp" data-t="06:34:00">[06:34:00]</a> <a class="yt-timestamp" data-t="14:34:00">[14:34:00]</a> <a class="yt-timestamp" data-t="43:48:00">[43:48:00]</a>. Each exercise involves a doc string of a function that needs to be completed <a class="yt-timestamp" data-t="43:55:00">[43:55:00]</a>. The objective of this dataset is to align the model to perform function completion tasks based on natural language instructions <a class="yt-timestamp" data-t="43:58:00">[43:58:00]</a>.

The data for fine-tuning was also synthetically generated by GPT-3.5, with diversity elicited by constraining function names <a class="yt-timestamp" data-t="44:05:00">[44:05:00]</a> <a class="yt-timestamp" data-t="44:10:00">[44:10:00]</a>. This highlights a key aspect of [[challenges_and_approaches_in_adapting_large_language_models_for_specific_tasks | adapting LLMs for specific tasks]]: the quality and structure of the data are paramount <a class="yt-timestamp" data-t="08:30:00">[08:30:00]</a>.

Manually inspecting datasets and considering whether specific examples will help or hurt a model is crucial <a class="yt-timestamp" data-t="23:57:00">[23:57:00]</a>. For example, self-contained and well-documented code snippets are considered high educational value for learning, whereas code buried in complex, poorly documented functions is not <a class="yt-timestamp" data-t="35:48:00">[35:48:00]</a> <a class="yt-timestamp" data-t="36:16:00">[36:16:00]</a>.

## [[Finetuning and Training Curriculums in AI Models | Training and Fine-tuning Processes for AI Models]]

The overall [[training_and_finetuning_processes_for_ai_models | training and finetuning process for AI models]] involves two main phases:
1.  **Pre-training:** The base model (Phi-1 base) is pre-trained on a "code textbook" dataset, which combines filtered code from the web (like The Stack and Stack Overflow) with synthetically generated textbook data <a class="yt-timestamp" data-t="31:58:00">[31:58:00]</a> <a class="yt-timestamp" data-t="32:00:00">[32:00:00]</a> <a class="yt-timestamp" data-t="32:04:00">[32:04:00]</a>. This pre-training involves approximately eight epochs over 7 billion tokens <a class="yt-timestamp" data-t="14:05:00">[14:05:00]</a> <a class="yt-timestamp" data-t="59:09:00">[59:09:00]</a> <a class="yt-timestamp" data-t="59:12:00">[59:12:00]</a>.
2.  **Fine-tuning:** The pre-trained Phi-1 base model is then fine-tuned on the "code exercises" dataset to create the final Phi-1 model <a class="yt-timestamp" data-t="32:10:00">[32:10:00]</a> <a class="yt-timestamp" data-t="59:17:00">[59:17:00]</a>. Fine-tuning uses a smaller learning rate and weight decay compared to pre-training to avoid overwriting prior knowledge <a class="yt-timestamp" data-t="59:49:00">[59:49:00]</a>.

## Benefits and Emergent Capabilities

Despite its small size and minimal compute, Phi-1 achieved a pass@1 accuracy of 50% on HumanEval, a benchmark for code generation <a class="yt-timestamp" data-t="05:02:00">[05:02:00]</a>. This performance surpasses much larger and more data-intensive open-source models like StarCoder, which is 10 times bigger and trained on 100 times more data <a class="yt-timestamp" data-t="21:00:00">[21:00:00]</a> <a class="yt-timestamp" data-t="12:01:00">[12:01:00]</a> <a class="yt-timestamp" data-t="12:22:00">[12:22:00]</a>.

An interesting outcome of this [[finetuning_machine_learning_models | fine-tuning]] process is the emergence of unexpected capabilities <a class="yt-timestamp" data-t="21:40:00">[21:40:00]</a>. For example, fine-tuning on basic Python tasks unexpectedly improved the model's ability to use external libraries like Pygame and Tkinter, even though these libraries were not present in the fine-tuning dataset <a class="yt-timestamp" data-t="01:01:17">[01:01:17]</a> <a class="yt-timestamp" data-t="01:03:54">[01:03:54]</a> <a class="yt-timestamp" data-t="01:04:08">[01:04:08]</a>. This suggests that fine-tuning can help the model reorganize and consolidate knowledge acquired during pre-training, even enabling it to perform unrelated tasks <a class="yt-timestamp" data-t="01:01:36">[01:01:36]</a>. Furthermore, Phi-1 demonstrated better chat capabilities than its base model, despite chat being exclusive to pre-training and not fine-tuning <a class="yt-timestamp" data-t="01:12:58">[01:12:58]</a> <a class="yt-timestamp" data-t="01:13:03">[01:13:03]</a>.

## Addressing Contamination and Evaluation

A significant concern in [[evaluation_metrics_for_language_models | evaluating language models]] is data contamination, where benchmark datasets might inadvertently be included in the training data, leading to inflated scores <a class="yt-timestamp" data-t="01:10:06">[01:10:06]</a> <a class="yt-timestamp" data-t="01:14:13">[01:14:13]</a>. To address this, the researchers used a dedicated team to create new, unconventional evaluation problems that were unlikely to appear in any training dataset <a class="yt-timestamp" data-t="01:17:47">[01:17:47]</a>. They also employed methods like embedding and syntax-based distances to prune similar code snippets and ensure that the fine-tuning data was not contaminated by the human evaluation benchmark <a class="yt-timestamp" data-t="01:27:49">[01:27:49]</a> <a class="yt-timestamp" data-t="01:27:51">[01:27:51]</a>.

Even after aggressively pruning more than 40% of the code exercises dataset, the retained Phi-1 still outperformed StarCoder, reinforcing the notion that quality trumps quantity in data <a class="yt-timestamp" data-t="01:24:24">[01:24:24]</a> <a class="yt-timestamp" data-t="01:24:33">[01:24:33]</a>.

## [[Technical aspects of AI model training and finetuning | Technical Aspects of AI Model Training and Finetuning]]

The models use a decoder-only Transformer architecture <a class="yt-timestamp" data-t="00:46:27">[00:46:27]</a>, similar to the decoder part of the original "Attention Is All You Need" paper <a class="yt-timestamp" data-t="00:47:10">[00:47:10]</a> <a class="yt-timestamp" data-t="00:47:48">[00:47:48]</a>. Key [[technical_aspects_of_ai_model_training_and_finetuning | technical aspects]] include:
*   **Flash Attention:** A technique to reduce the memory and compute footprint of Transformers <a class="yt-timestamp" data-t="00:48:31">[00:48:31]</a>.
*   **Rotary Position Embeddings (RoPE):** Used for encoding positional information within the sequence <a class="yt-timestamp" data-t="00:50:40">[00:50:40]</a>.
*   **Quantized Models:** Training was performed using floating point 16 (FP16) precision, which is a common approach for [[finetuning_with_quantized_models | finetuning with quantized models]] to reduce memory usage and accelerate computation <a class="yt-timestamp" data-t="00:54:05">[00:54:05]</a>.

## Limitations and Future Directions

Despite its successes, Phi-1 has limitations:
*   It specializes in Python coding and may lack domain-specific knowledge for particular APIs <a class="yt-timestamp" data-t="01:32:37">[01:32:37]</a> <a class="yt-timestamp" data-t="01:32:48">[01:32:48]</a>.
*   It is less robust to stylistic variations or errors in prompts due to the structured nature of its training data <a class="yt-timestamp" data-t="01:32:55">[01:32:55]</a>.
*   Smaller models can be more sensitive to prompt variations, indicating that the local minima found during training might be more fragile <a class="yt-timestamp" data-t="01:40:46">[01:40:46]</a>.

The paper suggests that using GPT-4 to generate synthetic data instead of GPT-3.5 could lead to significant gains <a class="yt-timestamp" data-t="01:33:08">[01:33:08]</a>. The findings underscore that developing effective methodologies for creating high-quality, diverse, and non-repetitive datasets is central to advancing natural language processing <a class="yt-timestamp" data-t="01:33:30">[01:33:30]</a> <a class="yt-timestamp" data-t="01:34:44">[01:34:44]</a>. The idea of "domain randomization" for text data, similar to techniques used in computer vision, is a promising area for future research <a class="yt-timestamp" data-t="01:34:09">[01:34:09]</a>. This approach of [[finetuning_pretrained_models_with_minimal_additional_parameters | finetuning pretrained models with minimal additional parameters]] using highly curated data opens new avenues for creating more efficient and task-specific LLMs.