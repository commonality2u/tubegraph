---
title: Selfimproving AI through reinforcement learning and reasoning
videoId: 5qkjxDzEaaw
---

From: [[hu-po]] <br/> 

## Introduction to Visual Reasoning
Visual reasoning involves the ability of AI models to interpret and understand visual information, often in conjunction with textual prompts, to make decisions or answer questions. This field is rapidly advancing, with key trends indicating significant improvements in model capabilities and deployment strategies <a class="yt-timestamp" data-t="00:00:49">[00:00:49]</a> <a class="yt-timestamp" data-t="00:01:42">[00:01:42]</a>.

## Advancements in Vision Encoders
Recent developments in AI, particularly from companies like Apple, show that Vision Encoders—the initial neural network modules that process images—can be effectively pre-trained using auto-regressive objectives, similar to how large language models (LLMs) are trained <a class="yt-timestamp" data-t="00:03:55">[00:03:55]</a> <a class="yt-timestamp" data-t="00:04:09">[00:04:09]</a>.

Traditionally, vision encoders like CLIP and SigLIP rely on contrastive losses, which pull similar items together and push different items apart in a data landscape <a class="yt-timestamp" data-t="00:04:35">[00:04:35]</a>. However, the new approach allows for simple auto-regressive pre-training, where the model predicts raw image patches and text tokens sequentially <a class="yt-timestamp" data-t="00:05:08">[00:05:08]</a> <a class="yt-timestamp" data-t="00:06:48">[00:06:48]</a>. This method has achieved high accuracy, such as 89.5% on ImageNet 1K with a frozen trunk <a class="yt-timestamp" data-t="00:04:50">[00:04:50]</a> <a class="yt-timestamp" data-t="00:05:45">[00:05:45]</a>.

A significant finding is that [[SelfImprovement in AI Models | scaling properties]] observed in LLMs also apply to vision encoders. This means that increasing the number of parameters and pre-training data consistently leads to improved performance, suggesting a continuous path for making vision encoders "smarter" over time <a class="yt-timestamp" data-t="00:07:48">[00:07:48]</a> <a class="yt-timestamp" data-t="00:08:31">[00:08:31]</a>.

## Inference Optimization Challenges
While pre-training continues to scale, optimizing inference for vision language models (VLMs) presents complex challenges. Papers like "Inference optimal VLMs need only one visual token" and "BlueLM v3b" explore how to make VLMs run efficiently, particularly on resource-constrained devices like mobile phones <a class="yt-timestamp" data-t="00:10:04">[00:10:04]</a> <a class="yt-timestamp" data-t="00:10:16">[00:10:16]</a>.

VLMs typically process images by segmenting them into patches, encoding them into "vision tokens," and then concatenating these with "language tokens" (text embeddings) for the language model <a class="yt-timestamp" data-t="00:11:09">[00:11:09]</a> <a class="yt-timestamp" data-t="00:11:27">[00:11:27]</a>. High latency often arises from the large number of image input tokens <a class="yt-timestamp" data-t="00:14:57">[00:14:57]</a>.

To mitigate this, strategies include:
*   **Pipeline Parallelism**: Running the vision embedding module on the CPU and the vision transformer on a neural processing unit (NPU) simultaneously <a class="yt-timestamp" data-t="00:12:37">[00:12:37]</a> <a class="yt-timestamp" data-t="00:13:39">[00:13:39]</a>. NPUs are specialized chips aimed at deep learning, similar to GPUs but often rebranded by companies seeking market share <a class="yt-timestamp" data-t="00:12:51">[00:12:51]</a>.
*   **Batching Image Patches**: Processing image patches in batches on the NPU rather than sequentially to speed up inference <a class="yt-timestamp" data-t="00:14:07">[00:14:07]</a> <a class="yt-timestamp" data-t="01:29:02">[01:29:02]</a>.
*   **Reducing Visual Token Count**: Minimizing the number of vision tokens generated from an image, sometimes even to a single token, which can significantly speed up the language model's processing <a class="yt-timestamp" data-t="00:15:49">[00:15:49]</a> <a class="yt-timestamp" data-t="00:16:11">[00:16:11]</a>. This reduction can be achieved by filtering tokens with low similarity to a "class token" (representing the whole image) or using a "token packer" with cross-attention <a class="yt-timestamp" data-t="00:20:15">[00:20:15]</a>.

The optimal balance between LLM size and visual token count is task-specific <a class="yt-timestamp" data-t="00:24:17">[00:24:17]</a>. For visual reasoning, larger LLMs with fewer visual tokens (often a single token) perform optimally given an inference budget <a class="yt-timestamp" data-t="00:16:34">[00:16:34]</a> <a class="yt-timestamp" data-t="00:25:31">[00:25:31]</a>. However, for tasks like Optical Character Recognition (OCR), more visual tokens are detrimental to performance as fine-grained details are crucial <a class="yt-timestamp" data-t="00:24:37">[00:24:37]</a> <a class="yt-timestamp" data-t="00:25:06">[00:25:06]</a>.

Caching text input tokens can further improve efficiency for repetitive queries, as the encoding process only needs to occur once <a class="yt-timestamp" data-t="00:22:13">[00:22:13]</a>.

## Practical Application: GUI Agents
A significant future application for VLMs is "GUI agents"—AI models that interact with graphical user interfaces (GUIs) like human users do, using mouse and keyboard actions <a class="yt-timestamp" data-t="00:26:47">[00:26:47]</a> <a class="yt-timestamp" data-t="00:27:31">[00:27:31]</a>. This approach bypasses the need for companies to maintain separate internal APIs for AI agents, allowing agents to interact with the same GUI designed for humans <a class="yt-timestamp" data-t="00:28:19">[00:28:19]</a>.

The "Dawn of a GUI Agent" paper highlights that commercial software's closed-source nature makes GUI-based interaction a practical necessity for agents <a class="yt-timestamp" data-t="00:27:19">[00:27:19]</a>. This embodies the "Keep It Simple, Stupid" (KISS) principle: it's simpler to have one front-end for both humans and AI than two <a class="yt-timestamp" data-t="00:27:44">[00:27:44]</a>. Despite potential computational inefficiency (e.g., taking screenshots and processing them through vision encoders), this universal applicability is likely to win out, much like Python's popularity despite its inefficiency <a class="yt-timestamp" data-t="00:35:50">[00:35:50]</a> <a class="yt-timestamp" data-t="01:15:38">[01:15:38]</a>.

Current GUI agents like Claude Computer Use employ a reasoning-acting paradigm, observing the environment and deciding on actions, often maintaining an extensive context of history screenshots <a class="yt-timestamp" data-t="00:29:30">[00:29:30]</a> <a class="yt-timestamp" data-t="00:30:39">[00:30:39]</a>. These historical observations can be stored efficiently as vision tokens rather than raw image frames <a class="yt-timestamp" data-t="00:30:57">[00:30:57]</a>. Interestingly, these agents can sometimes fail on simple tasks (like updating a phone number in Word) but succeed on more complicated ones (like creating a deck in the video game Hearthstone) <a class="yt-timestamp" data-t="00:31:38">[00:31:38]</a> <a class="yt-timestamp" data-t="00:34:40">[00:34:40]</a>.

## Improving Reasoning in VLMs
To enhance VLM performance in complex tasks, especially visual reasoning, methods that promote step-by-step thinking are crucial <a class="yt-timestamp" data-t="00:39:09">[00:39:09]</a>. "LAVA-01: Let Vision Language Models Reason Step-by-Step" introduces a structured, hardcoded Chain of Thought approach where the VLM is prompted to perform a sequence of steps: question, summary, caption, reasoning, and then answer <a class="yt-timestamp" data-t="00:37:03">[00:37:03]</a> <a class="yt-timestamp" data-t="00:38:06">[00:38:06]</a>. This process provides a richer context for the final answer <a class="yt-timestamp" data-t="00:39:31">[00:39:31]</a>.

Additionally, LAVA-01 incorporates an inference-time stage-level beam search method. Unlike greedy search (which always picks the most likely next token), beam search looks ahead multiple steps, considering multiple plausible token sequences to find a more optimal path <a class="yt-timestamp" data-t="00:39:46">[00:39:46]</a> <a class="yt-timestamp" data-t="00:40:54">[00:40:54]</a>. This search is applied at each structured reasoning stage <a class="yt-timestamp" data-t="00:41:46">[00:41:46]</a>.

## [[SelfImprovement in AI Models | Self-Improvement]] and Consistency
A groundbreaking area is the ability of LLMs to [[SelfImprovement in AI Models | self-improve]]. The Seong paper, "Large language models can [[SelfImprovement in AI Models | self-improve]] in Long context Reasoning," demonstrates that models can enhance their performance by fine-tuning on their own outputs <a class="yt-timestamp" data-t="00:41:53">[00:41:53]</a> <a class="yt-timestamp" data-t="00:48:02">[00:48:02]</a>.

Seong samples multiple outputs for each question and scores them using Minimum Bayes Risk, which prioritizes outputs showing higher consistency with others based on sentence embedding similarity <a class="yt-timestamp" data-t="00:42:31">[00:42:31]</a> <a class="yt-timestamp" data-t="00:43:55">[00:43:55]</a>. This process automatically generates high-quality reasoning traces, reducing reliance on human experts <a class="yt-timestamp" data-t="00:47:15">[00:47:15]</a>. The selected consistent outputs are then used for supervised fine-tuning or preference optimization, leading to superior performance across various benchmarks <a class="yt-timestamp" data-t="00:43:51">[00:43:51]</a> <a class="yt-timestamp" data-t="00:48:09">[00:48:09]</a>. This effectively shows that an AI can get "smarter" by iterating and fine-tuning on its own refined outputs, a concept that challenges traditional views on fixed intelligence within models <a class="yt-timestamp" data-t="00:48:51">[00:48:51]</a> <a class="yt-timestamp" data-t="00:49:03">[00:49:03]</a>.

## Anticipatory Reasoning and Generative Worlds
Beyond current observations, AI agents are evolving to anticipate future states. "Generative World Explorer" (GenX) from Johns Hopkins University introduces an egocentric world exploration framework for autonomous vehicles <a class="yt-timestamp" data-t="00:52:12">[00:52:12]</a> <a class="yt-timestamp" data-t="00:52:21">[00:52:21]</a>. The core idea is to imagine potential future observations to update the agent's internal beliefs and improve its planning <a class="yt-timestamp" data-t="00:53:27">[00:53:27]</a>.

This means a sophisticated GUI agent could generate images of what a screen might look like after a specific action, evaluate if that's the desired outcome, and then decide whether to proceed with the action <a class="yt-timestamp" data-t="00:54:55">[00:54:55]</a> <a class="yt-timestamp" data-t="01:03:57">[01:03:57]</a>. GenX uses a generative video model to simulate future RGB observations, allowing the agent to incorporate "new" imagined knowledge into its reasoning <a class="yt-timestamp" data-t="00:54:28">[00:54:28]</a>. This represents a more advanced form of reasoning, where the model doesn't just organize existing knowledge but actively generates new data to inform its decisions <a class="yt-timestamp" data-t="01:24:43">[01:24:43]</a>.

## The Arms Race: Efficiency vs. Complexity
The field of visual reasoning is experiencing an "arms race" <a class="yt-timestamp" data-t="01:05:50">[01:05:50]</a>. On one side, hardware engineers are relentlessly pursuing optimizations to increase inference speed (tokens per second) <a class="yt-timestamp" data-t="01:05:54">[01:05:54]</a>. On the other, model developers are pushing to increase the complexity of reasoning traces, requiring more tokens and imagined outputs to improve performance <a class="yt-timestamp" data-t="01:06:05">[01:06:05]</a>. This dynamic interplay suggests a future where complex, multi-modal reasoning, including generating millions of tokens for Chain of Thought and imagined future scenarios, could happen almost instantaneously, becoming an invisible background process for users <a class="yt-timestamp" data-t="01:06:42">[01:06:42]</a>.

## Discussion on Intelligence and Privacy
The concept of [[SelfImprovement in AI Models | self-improving AI]] sparks a debate about the nature of intelligence itself <a class="yt-timestamp" data-t="01:29:52">[01:29:52]</a>. While some argue that models merely improve on benchmarks without gaining true intelligence, others believe that continuous self-restructuring and comparison of internal knowledge can lead to genuine intelligence growth, even in a closed system without external reward signals <a class="yt-timestamp" data-t="01:30:44">[01:30:44]</a>.

Privacy is another significant concern. The increasing reliance on visual agents, like Microsoft's Windows Co-pilot sending screenshots to the cloud, raises questions about constant data collection and its implications <a class="yt-timestamp" data-t="01:13:06">[01:13:06]</a>. While privacy might decrease due to pervasive cameras and sensors, the potential for transparent AI governance could mitigate some fears <a class="yt-timestamp" data-t="01:21:32">[01:21:32]</a>. The future could lean towards either centralized corporate data collection or a decentralized, open-source AI ecosystem where models run locally, ensuring user data privacy <a class="yt-timestamp" data-t="01:11:41">[01:11:41]</a> <a class="yt-timestamp" data-t="01:17:19">[01:17:19]</a>.

Ultimately, the inherent simplicity and universality of pixel-space interaction for AI agents (taking screenshots to interact with any GUI) are likely to drive widespread adoption, even if more "efficient" text-based or HTML-based methods exist for specific applications like web pages <a class="yt-timestamp" data-t="01:15:01">[01:15:01]</a>.