---
title: automated adversarial prompt generation
videoId: pR2et-guixM
---

From: [[hu-po]] <br/> 

[[adversarial_attacks_on_language_models | Adversarial attacks]] are methods designed to manipulate machine learning models into exhibiting unintended or harmful behaviors <a class="yt-timestamp" data-t="00:01:16">[00:01:16]</a>. While such attacks have existed for some time, particularly in [[computer_vision_adversarial_attacks | computer vision]], recent research has made significant strides in automating these attacks for large language models (LLMs), especially those that have undergone alignment fine-tuning <a class="yt-timestamp" data-t="00:05:37">[00:05:37]</a>.

## Historical Context
Historically, [[computer_vision_adversarial_attacks | adversarial attacks]] in computer vision demonstrated the non-intuitive nature of neural networks' decision boundaries <a class="yt-timestamp" data-t="00:02:00">[00:02:00]</a>. A famous example involves adding imperceptible "noise" to an image of a panda, causing a classifier to misidentify it as a gibbon, even though the image visually remains a panda to humans <a class="yt-timestamp" data-t="00:02:11">[00:02:11]</a>. Another instance involved physical adversarial examples, like stickers on a stop sign, that caused autonomous vehicles to misclassify them <a class="yt-timestamp" data-t="00:03:07">[00:03:07]</a>, or a sticker on a banana making it classify as a toaster <a class="yt-timestamp" data-t="00:04:06">[00:04:06]</a>. These early attacks were often "brittle," meaning they were highly specific to a particular model architecture and its training data <a class="yt-timestamp" data-t="00:05:23">[00:05:23]</a>.

## Challenges for Language Models
Generating effective adversarial attacks for language models is more challenging than for image models due to the discrete nature of token inputs <a class="yt-timestamp" data-t="00:29:22">[00:29:22]</a>. Additionally, LLMs are increasingly subjected to "alignment" efforts, such as Reinforcement Learning from Human Feedback (RLHF) and extensive data filtering, to prevent them from generating objectionable content <a class="yt-timestamp" data-t="00:05:43">[00:05:43]</a> <a class="yt-timestamp" data-t="00:27:29">[00:27:29]</a>. Prior "jailbreaks" of LLMs often required significant human ingenuity and were brittle <a class="yt-timestamp" data-t="00:13:05">[00:13:05]</a>.

## Universal and Transferable Attacks
A recent paper, "[[interpretable_and_universal_adversarial_prompts | Universal and Transferable Adversarial Attacks on Aligned Language Models]]" (2023), describes a method to automatically generate adversarial prompts that are both "universal" (applicable to many different models) and "transferable" (working across different models despite varying training data) <a class="yt-timestamp" data-t="00:10:04">[00:10:04]</a>. This represents a significant advancement because it allows for attacks on black-box, closed-source LLMs like ChatGPT, Bard, and Claude, even if the attacks were designed using open-source models like Vicuna <a class="yt-timestamp" data-t="00:15:37">[00:15:37]</a>.

The core idea involves finding an "adversarial suffix" â€“ a sequence of tokens added to the end of a user's harmful query <a class="yt-timestamp" data-t="00:13:54">[00:13:54]</a>. This suffix is designed to maximize the probability that the LLM produces an affirmative or harmful response <a class="yt-timestamp" data-t="00:14:11">[00:14:11]</a>.

### Key Elements of the Attack Method
1.  **Initial Affirmative Responses**: The attack aims to force the model to begin its response with a few "affirmative" tokens (e.g., "Sure, here is") <a class="yt-timestamp" data-t="00:31:34">[00:31:34]</a>. By "putting words in the model's mouth," the auto-regressive nature of LLMs causes them to continue generating content consistent with that start, often overriding safety alignments <a class="yt-timestamp" data-t="00:32:03">[00:32:03]</a>.
2.  **Greedy Coordinate Gradient (GCG)**: This approach automatically generates suffixes through a combination of greedy and gradient-based search techniques <a class="yt-timestamp" data-t="00:14:45">[00:14:45]</a>.
    *   It optimizes over discrete tokens by leveraging gradients at the token level <a class="yt-timestamp" data-t="00:33:07">[00:33:07]</a>.
    *   The method samples a single token index within the suffix and evaluates potential replacements for that token across a batch of candidates <a class="yt-timestamp" data-t="01:01:40">[01:01:40]</a>.
    *   It selects the replacement that maximally decreases the loss function (i.e., maximizes the probability of the desired harmful output) <a class="yt-timestamp" data-t="01:01:56">[01:01:56]</a>.
    *   GCG significantly outperforms previous methods like AutoPrompt, achieving higher success rates with fewer training steps <a class="yt-timestamp" data-t="01:24:54">[01:24:54]</a> <a class="yt-timestamp" data-t="01:32:27">[01:32:27]</a>.
3.  **Universal Prompt Optimization**: To create a suffix that works across many prompts and models, the algorithm incrementally incorporates new prompts into the optimization process, building up to a universal suffix that works well on a wide range of queries <a class="yt-timestamp" data-t="01:16:34">[01:16:34]</a> <a class="yt-timestamp" data-t="01:33:58">[01:33:58]</a>. When models use the same tokenizer, the gradients can be aggregated without issue <a class="yt-timestamp" data-t="01:17:41">[01:17:41]</a>.

### Transferability Across Models
The success rate of these transferable attacks is notable, particularly against GPT-based models <a class="yt-timestamp" data-t="01:36:07">[01:36:07]</a>. This transferability might be attributed to Vicuna (the open-source model used for attack generation) being fine-tuned on data generated by ChatGPT <a class="yt-timestamp" data-t="01:42:01">[01:42:01]</a>. The latent spaces of models trained on similar data tend to be similar, allowing attacks to transfer <a class="yt-timestamp" data-t="01:59:21">[01:59:21]</a>.

## Implications and Concerns
*   **AI Safety vs. Regulation**: This research highlights the fragility of current LLM alignment methods, raising significant questions about the effectiveness of AI safety measures <a class="yt-timestamp" data-t="01:47:31">[01:47:31]</a>. The paper's authors disclosed their findings to OpenAI, Meta, and Anthropic <a class="yt-timestamp" data-t="01:41:57">[01:41:57]</a>.
*   **Automated Attacks**: The effectiveness and speed of automated adversarial attacks suggest they could render many existing alignment mechanisms insufficient <a class="yt-timestamp" data-t="01:48:03">[01:48:03]</a>. The future might involve LLMs automatically querying and exploiting vulnerabilities in other LLMs <a class="yt-timestamp" data-t="01:48:50">[01:48:50]</a>.
*   **Harmful Data Set Creation**: The benchmarks used in this research include manually curated "harmful strings" and "harmful behaviors" (e.g., plans to destroy humanity, spread fake news, commit crimes) <a class="yt-timestamp" data-t="01:20:19">[01:20:19]</a> <a class="yt-timestamp" data-t="01:27:07">[01:27:07]</a>. This raises ethical concerns akin to "gain-of-function research" in virology, where creating highly toxic datasets could inadvertently make it easier for others to train malevolent LLMs <a class="yt-timestamp" data-t="01:28:58">[01:28:58]</a>.
*   **"Helpfulness" vs. "Safety"**: There's an ongoing trade-off between making LLMs safer (more robust to attacks) and maintaining their generative capability and helpfulness <a class="yt-timestamp" data-t="02:11:02">[02:11:02]</a>. Over-aligning models might make them "stupider" and less capable <a class="yt-timestamp" data-t="02:11:18">[02:11:18]</a>.
*   **Hidden Prompts**: Commercial chatbots often use unseen "system prompts" to guide their behavior (e.g., "You are a helpful, honest assistant") <a class="yt-timestamp" data-t="00:44:48">[00:44:48]</a>. This hidden context raises concerns about potential future uses, such as targeted advertising or political manipulation, without user awareness <a class="yt-timestamp" data-t="00:46:12">[00:46:12]</a>.
*   **Language and Interpretability**: While some adversarial suffixes appear as unintelligible "junk text," others, especially those optimized across multiple models, can resemble common English <a class="yt-timestamp" data-t="01:54:45">[01:54:45]</a>.

## Conclusion
The development of [[interpretable_and_universal_adversarial_prompts | universal and transferable adversarial attacks]] on LLMs signifies a critical juncture in AI development. The ability to bypass safety alignments in black-box models, even with methods derived from open-source systems, highlights an ongoing "arms race" between those seeking to exploit LLMs and those aiming to secure them <a class="yt-timestamp" data-t="01:42:01">[01:42:01]</a>. This research is likely to fuel discussions around AI regulation and the balance between open-source innovation and controlled deployment <a class="yt-timestamp" data-t="01:52:12">[01:52:12]</a> <a class="yt-timestamp" data-t="02:18:44">[02:18:44]</a>.