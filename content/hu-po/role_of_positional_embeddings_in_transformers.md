---
title: Role of positional embeddings in Transformers
videoId: PFxi6SmozZ4
---

From: [[hu-po]] <br/> 

Positional embeddings are a crucial component in [[Transformers]] that provides information about the order of input elements to the model <a class="yt-timestamp" data-t="02:29:51">[02:29:51]</a>. They are particularly important because [[Transformers]] are sequence-to-sequence models that inherently lack an understanding of the position or order of tokens within a sequence <a class="yt-timestamp" data-t="02:09:09">[02:09:09]</a>, <a class="yt-timestamp" data-t="02:29:51">[02:29:51]</a>.

## Why Positional Information is Needed
Without explicit positional information, a [[Transformers]] model would process all tokens simultaneously through its self-attention mechanism, leading to a loss of sequence order <a class="yt-timestamp" data-t="02:03:01">[02:03:01]</a>. For instance, when processing an image converted into a sequence of visual patches or tokens for a [[Vision Transformers and their applications | Vision Transformer]] (ViT), positional embeddings (PE) are added to tell the Transformer where each patch belongs in the original image sequence <a class="yt-timestamp" data-t="06:01:00">[06:01:00]</a>, <a class="yt-timestamp" data-t="06:36:00">[06:36:00]</a>, <a class="yt-timestamp" data-t="06:51:00">[06:51:00]</a>. Similarly, in language models, they indicate the position of words or tokens within a sentence <a class="yt-timestamp" data-t="06:51:00">[06:51:00]</a>. This ensures that the model can understand relative distances between tokens, such as "king" and "queen" being next to each other versus separated by paragraphs <a class="yt-timestamp" data-t="02:00:00">[02:00:00]</a>, <a class="yt-timestamp" data-t="02:00:00">[02:00:00]</a>.

Each position in a sequence is represented by a unique positional embedding vector <a class="yt-timestamp" data-t="10:20:00">[10:20:00]</a>, which is then typically combined (e.g., added) with the token embedding to form the input to the next layer of the Transformer <a class="yt-timestamp" data-t="10:48:00">[10:48:00]</a>.

## Evolution of Positional Embeddings

### Rotary Position Embeddings (RoPE)
RoPE, short for Rotary Position Embeddings, originated from the 2021 RoFormer paper by Jo Ye Technology Co <a class="yt-timestamp" data-t="01:47:00">[01:47:00]</a>, <a class="yt-timestamp" data-t="01:54:00">[01:54:00]</a>. RoPE was designed to encode absolute position within a rotation matrix while incorporating relative position dependency in the self-attention formulation <a class="yt-timestamp" data-t="01:14:00">[01:14:00]</a>, <a class="yt-timestamp" data-t="01:16:00">[01:16:00]</a>.

Key properties of RoPE include:
*   **Flexibility with sequence length**: RoPE is a continuous function that can be discretized to different sequence lengths <a class="yt-timestamp" data-t="03:29:00">[03:29:00]</a>, <a class="yt-timestamp" data-t="03:30:00">[03:30:00]</a>.
*   **Decaying inter-token dependency**: The similarity (dot product) between token embeddings decreases as their relative distance in the sequence increases <a class="yt-timestamp" data-t="01:30:00">[01:30:00]</a>, <a class="yt-timestamp" data-t="02:01:00">[02:01:00]</a>. This means tokens closer together have a stronger connection <a class="yt-timestamp" data-t="01:28:00">[01:28:00]</a>.

The mathematical intuition behind RoPE involves rotating token vectors by an angle that depends on their position <a class="yt-timestamp" data-t="01:38:00">[01:38:00]</a>, <a class="yt-timestamp" data-t="01:40:00">[01:40:00]</a>. This rotation affects the dot product computation in the self-attention mechanism <a class="yt-timestamp" data-t="02:04:00">[02:04:00]</a>. Using Euler's identity, these rotations can be expressed using cosine and sine terms <a class="yt-timestamp" data-t="02:40:00">[02:40:00]</a>, <a class="yt-timestamp" data-t="03:00:00">[03:00:00]</a>. The `Theta J` term, which controls the rotation frequency, varies across the dimensions of the positional embedding, leading to different frequencies for lower vs. higher dimensions <a class="yt-timestamp" data-t="03:19:00">[03:19:00]</a>, <a class="yt-timestamp" data-t="03:22:00">[03:22:00]</a>, <a class="yt-timestamp" data-t="04:09:00">[04:09:00]</a>.

RoPE is [[comparison_of_designed_versus_learned_positional_embeddings | hand-designed]], meaning a human conceived and formulated its mathematical structure, rather than being learned by the model through training <a class="yt-timestamp" data-t="01:58:00">[01:58:00]</a>, <a class="yt-timestamp" data-t="03:05:00">[03:05:00]</a>, <a class="yt-timestamp" data-t="03:50:00">[03:50:00]</a>.

### Remixes of RoPE
Later papers introduced "remixes" of RoPE to improve its performance for longer contexts:
*   **Position Interpolation (PI)**: Introduced by Meta Platforms, this method involves scaling the original position indices by an "extension ratio" (s) to fit a longer context <a class="yt-timestamp" data-t="02:59:00">[02:59:00]</a>, <a class="yt-timestamp" data-t="04:47:00">[04:47:00]</a>, <a class="yt-timestamp" data-t="04:54:00">[04:54:00]</a>. However, this can lead to "crowded" position information, hindering the model's ability to distinguish closely positioned tokens because rotations become too small <a class="yt-timestamp" data-t="05:01:00">[05:01:00]</a>, <a class="yt-timestamp" data-t="05:22:00">[05:22:00]</a>.
*   **NTK-based Interpolation**: This approach involved rescaling based on where the dimension is within the positional embedding vector, splitting up low and high frequency dimensions <a class="yt-timestamp" data-t="05:25:00">[05:25:00]</a>, <a class="yt-timestamp" data-t="05:32:00">[05:32:00]</a>.
*   **Yarn**: Yarn further divided RoPE dimensions into three frequency-based groups, each with a different interpolation strategy <a class="yt-timestamp" data-t="05:09:00">[05:09:00]</a>, <a class="yt-timestamp" data-t="05:40:00">[05:40:00]</a>. These groupings relied on human-led empirical experiments, which may lead to suboptimal performance for new models <a class="yt-timestamp" data-t="05:11:00">[05:11:00]</a>, <a class="yt-timestamp" data-t="05:40:00">[05:40:00]</a>.

### Long RoPE: An Evolutionary Approach
Long RoPE, a February 2024 paper by Microsoft Research, is presented as a remix of these prior innovations <a class="yt-timestamp" data-t="03:56:00">[03:56:00]</a>, <a class="yt-timestamp" data-t="03:59:00">[03:59:00]</a>. It addresses the non-uniformities in positional interpolation methods by introducing an efficient evolutionary search to discover better non-uniform positional interpolations, guided by perplexity <a class="yt-timestamp" data-t="05:50:00">[05:50:00]</a>, <a class="yt-timestamp" data-t="05:58:00">[05:58:00]</a>.

Long RoPE's key innovations:
1.  **Evolutionary Search for Rescale Factors**: Instead of relying on human-designed rules for frequency bins or arbitrary thresholds, Long RoPE uses an evolutionary search algorithm to find optimal rescale factors (Lambda I) for RoPE's rotation angles and an optimal 'n hat' value <a class="yt-timestamp" data-t="04:25:00">[04:25:00]</a>, <a class="yt-timestamp" data-t="04:50:00">[04:50:00]</a>, <a class="yt-timestamp" data-t="05:28:00">[05:28:00]</a>, <a class="yt-timestamp" data-t="05:58:00">[05:58:00]</a>. This search optimizes for low perplexity <a class="yt-timestamp" data-t="05:58:00">[05:58:00]</a>.
2.  **Progressive Extension Strategy**: Long RoPE uses a fine-tuning strategy where a model is initially fine-tuned on a smaller context length, and then progressively increased with a series of consecutive fine-tunes <a class="yt-timestamp" data-t="03:04:00">[03:04:00]</a>, <a class="yt-timestamp" data-t="03:06:00">[03:06:00]</a>, <a class="yt-timestamp" data-t="03:08:00">[03:08:00]</a>, <a class="yt-timestamp" data-t="04:06:00">[04:06:00]</a>, <a class="yt-timestamp" data-t="04:08:00">[04:08:00]</a>.
3.  **Non-interpolation for Initial Tokens**: For an initial 'n hat' number of tokens (e.g., 256), Long RoPE applies no interpolation to their positional embeddings <a class="yt-timestamp" data-t="04:57:00">[04:57:00]</a>, <a class="yt-timestamp" data-t="04:59:00">[04:59:00]</a>, <a class="yt-timestamp" data-t="05:01:00">[05:01:00]</a>, <a class="yt-timestamp" data-t="05:06:00">[05:06:00]</a>. This is hypothesized to leverage the "attention sink" situation where initial tokens often receive large attention scores and are critical to performance <a class="yt-timestamp" data-t="05:01:00">[05:01:00]</a>, <a class="yt-timestamp" data-t="05:02:00">[05:02:00]</a>.

Long RoPE can extend pre-trained Large Language Models (LLMs) context windows significantly, up to an impressive 2048K tokens <a class="yt-timestamp" data-t="03:28:00">[03:28:00]</a>, <a class="yt-timestamp" data-t="03:29:00">[03:29:00]</a>. It has shown to significantly outperform previous methods like PI and Yarn in maintaining low perplexity and high pass key retrieval accuracy (needle-in-the-haystack tasks) at extended context lengths <a class="yt-timestamp" data-t="04:00:00">[04:00:00]</a>, <a class="yt-timestamp" data-t="04:10:00">[04:10:00]</a>, <a class="yt-timestamp" data-t="04:12:00">[04:12:00]</a>. Notably, it can achieve an 8x context window extension without fine-tuning, and much longer contexts with the progressive fine-tuning strategy <a class="yt-timestamp" data-t="05:00:00">[05:00:00]</a>, <a class="yt-timestamp" data-t="05:01:00">[05:01:00]</a>, <a class="yt-timestamp" data-t="05:03:00">[05:03:00]</a>. While some slight performance loss on standard benchmarks (like MMLU, H-SWAG) is observed, the ability to handle vast contexts may outweigh this for certain tasks <a class="yt-timestamp" data-t="04:13:00">[04:13:00]</a>, <a class="yt-timestamp" data-t="04:15:00">[04:15:00]</a>, <a class="yt-timestamp" data-t="04:29:00">[04:29:00]</a>.

## Comparison of Designed vs. Learned Positional Embeddings
A fundamental debate in deep learning, reflected in Rich Sutton's "Bitter Lesson", questions the reliance on human-designed heuristics versus the power of scalable computation and learning <a class="yt-timestamp" data-t="01:47:00">[01:47:00]</a>.

> "General methods that leverage computation are ultimately the most effective." <a class="yt-timestamp" data-t="01:30:00">[01:30:00]</a>
> "AI researchers have often tried to build knowledge into their agents. This always helps in the short term and is ply satisfying, but in the long term it plateaus and even inhibits further progress. A breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning." <a class="yt-timestamp" data-t="01:39:00">[01:39:00]</a>

Historically, in [[Vision Transformers vs Convolutional Networks | computer vision]], hand-designed Gabor filters for edge detection were replaced by learned convolutional filters in Convolutional Neural Networks (CNNs) <a class="yt-timestamp" data-t="01:42:00">[01:42:00]</a>, <a class="yt-timestamp" data-t="01:46:00">[01:46:00]</a>, <a class="yt-timestamp" data-t="01:50:00">[01:50:00]</a>. While human intuition (like Gabor's) sometimes aligns with what models learn, the learned approaches ultimately scale better <a class="yt-timestamp" data-t="01:54:00">[01:54:00]</a>, <a class="yt-timestamp" data-t="01:58:00">[01:58:00]</a>.

While current [[comparison_of_designed_versus_learned_positional_embeddings | hand-designed positional embeddings]] like RoPE and Long RoPE have proven effective, there is an ongoing effort to develop [[comparison_of_designed_versus_learned_positional_embeddings | learned positional embeddings]] <a class="yt-timestamp" data-t="01:29:00">[01:29:00]</a>, <a class="yt-timestamp" data-t="01:38:00">[01:38:00]</a>. Early attempts cited in the RoFormer paper itself showed that "trainable absolute position encoding" was tried but did not outperform generated (hand-designed) ones <a class="yt-timestamp" data-t="01:30:00">[01:30:00]</a>, <a class="yt-timestamp" data-t="01:38:00">[01:38:00]</a>. However, proponents of the Bitter Lesson suggest that with increased scale in data and training, learned positional embeddings will eventually surpass all heuristically hand-designed methods <a class="yt-timestamp" data-t="01:30:00">[01:30:00]</a>, <a class="yt-timestamp" data-t="01:31:00">[01:31:00]</a>.

The development of Long RoPE, which uses evolutionary search to *optimize* human-designed parameters, represents the current pinnacle of this "hand-design and remix" approach <a class="yt-timestamp" data-t="01:47:00">[01:47:00]</a>, <a class="yt-timestamp" data-t="01:48:00">[01:48:00]</a>. However, it still falls within the paradigm of engineering explicit biases rather than letting the model learn them entirely <a class="yt-timestamp" data-t="01:52:00">[01:52:00]</a>. The future may see [[Transformers]] learning positional embeddings implicitly, similar to how they learn token embeddings <a class="yt-timestamp" data-t="01:55:00">[01:55:00]</a>, <a class="yt-timestamp" data-t="01:57:00">[01:57:00]</a>.