---
title: Applications and limitations of 3D Gaussians
videoId: hDuy1TgD8I4
---

From: [[hu-po]] <br/> 

[[dynamic_3d_gaussian_technique | Dynamic 3D Gaussians]] are a new 3D format that extends [[gaussian_splatting_and_its_advantages | 3D Gaussian Splatting]] to include the dimension of time, allowing for dynamic scenes rather than just static ones <a class="yt-timestamp" data-t="00:01:29">[00:01:29]</a>. This approach models a scene as a collection of 3D Gaussians that can move and rotate over time <a class="yt-timestamp" data-t="00:11:35">[00:11:35]</a>.

## Applications

[[dynamic_3d_gaussian_technique | Dynamic 3D Gaussians]] simultaneously address tasks such as dynamic scene novel view synthesis and 6 degree of freedom (6DoF) tracking of dense scene elements <a class="yt-timestamp" data-t="00:06:06">[00:06:06]</a>. This allows for creating images from views not seen before <a class="yt-timestamp" data-t="00:06:10">[00:06:10]</a> and tracking the position and rotation (6DoF) of every element <a class="yt-timestamp" data-t="00:13:31">[00:13:31]</a>.

Key applications include:
*   **Novel View Synthesis** These models can generate novel unseen renders of both color images and depth maps <a class="yt-timestamp" data-t="00:09:17">[00:09:17]</a>, <a class="yt-timestamp" data-t="00:22:58">[00:22:58]</a>. This means a virtual camera can move around a dynamic scene and render new perspectives <a class="yt-timestamp" data-t="00:06:18">[00:06:18]</a>.
*   **Dense 6DoF Tracking** The method implicitly achieves dense 6DoF tracking of all 3D points without requiring explicit correspondence or optical flow as input <a class="yt-timestamp" data-t="00:13:31">[00:13:31]</a>, <a class="yt-timestamp" data-t="00:13:59">[00:13:59]</a>, <a class="yt-timestamp" data-t="00:41:00">[00:41:00]</a>.
*   **First-Person View Synthesis** Cameras can be attached to moving scene elements, enabling first-person views that follow the action <a class="yt-timestamp" data-t="00:15:55">[00:15:55]</a>, <a class="yt-timestamp" data-t="02:21:00">[02:21:00]</a>.
*   **Dynamic Compositional Scene Synthesis** The explicit nature of 3D Gaussians allows for easily combining multiple dynamic elements from different scenes <a class="yt-timestamp" data-t="00:15:57">[00:15:57]</a>, <a class="yt-timestamp" data-t="02:16:54">[02:16:54]</a>. Objects can be removed, duplicated, or added to dynamic scenes <a class="yt-timestamp" data-t="02:16:38">[02:16:38]</a>, <a class="yt-timestamp" data-t="02:19:09">[02:19:09]</a>.
*   **4D Video Editing** The representation enables editing of dynamic scenes over time, such as propagating edits across all time steps <a class="yt-timestamp" data-t="00:16:00">[00:16:00]</a>, <a class="yt-timestamp" data-t="02:21:00">[02:21:00]</a>.
*   **Metric Space Reconstruction** It enables a metric space reconstruction of every part of the scene over time, allowing for distance measurements between elements <a class="yt-timestamp" data-t="00:20:31">[00:20:31]</a>.
*   **Robotics, Augmented Reality (AR), and Self-Driving** Understanding where everything is, has been, and is moving is crucial for these fields <a class="yt-timestamp" data-t="00:20:40">[00:20:40]</a>, <a class="yt-timestamp" data-t="02:20:00">[02:20:00]</a>.
*   **Physics-Based Interactions** Unlike Neural Radiance Fields ([[comparison_of_3d_gaussian_splatting_to_neural_radiance_fields | NeRFs]]), 3D Gaussians, being explicit representations, are more amenable to physics-based interactions and collision geometry for applications in robotics and games <a class="yt-timestamp" data-t="02:11:00">[02:11:00]</a>, <a class="yt-timestamp" data-t="02:16:18">[02:16:18]</a>.
*   **Content Creation** Such models could enable controllable and editable high-resolution dynamic 3D assets for movies, video games, or the metaverse <a class="yt-timestamp" data-t="02:08:06">[02:08:06]</a>.

## Limitations

Despite its capabilities, the [[dynamic_3d_gaussian_technique | Dynamic 3D Gaussian]] method has several limitations:
*   **Limited to Initially Visible Scene Parts** The method is only able to track parts of the scene that are visible in the initial frame <a class="yt-timestamp" data-t="01:05:57">[01:05:57]</a>, <a class="yt-timestamp" data-t="02:11:11">[02:11:11]</a>. It would completely fail to reconstruct new objects entering the scene <a class="yt-timestamp" data-t="02:12:23">[02:12:23]</a>.
*   **Assumptions about Object Properties** The core assumption is that the number, color, opacity, and size of the Gaussians remain constant over time, with only their position and orientation varying <a class="yt-timestamp" data-t="00:30:41">[00:30:41]</a>, <a class="yt-timestamp" data-t="01:17:47">[01:17:47]</a>. This works well for rigid objects like humans or balls <a class="yt-timestamp" data-t="00:31:07">[00:31:07]</a> but would not be suitable for deformable substances like smoke or turbulent water, where opacity and size change <a class="yt-timestamp" data-t="00:12:47">[00:12:47]</a>, <a class="yt-timestamp" data-t="00:31:16">[00:31:16]</a>.
*   **Reliance on Multi-Camera Setup** The method heavily relies on a multi-camera setup, specifically a geodesic camera dome with perfectly calibrated cameras <a class="yt-timestamp" data-t="01:17:19">[01:17:19]</a>, <a class="yt-timestamp" data-t="02:11:27">[02:11:27]</a>. This implies knowing the exact intrinsic and extrinsic matrices for all cameras <a class="yt-timestamp" data-t="00:58:03">[00:58:03]</a>, which is difficult to replicate with off-the-shelf monocular video (e.g., from a cell phone) <a class="yt-timestamp" data-t="00:59:25">[00:59:25]</a>, <a class="yt-timestamp" data-t="02:12:28">[02:12:28]</a>. The uniform distribution of cameras around the object is also a strong assumption <a class="yt-timestamp" data-t="02:02:30">[02:02:30]</a>.
*   **Lack of Lighting Model** The current approach does not inherently model lighting <a class="yt-timestamp" data-t="00:16:40">[00:16:40]</a>. Objects inserted into a scene captured with [[dynamic_3d_gaussian_technique | Dynamic 3D Gaussians]] may appear out of place due to lighting inconsistencies <a class="yt-timestamp" data-t="00:17:39">[00:17:39]</a>. While the capability for view-dependent color via spherical harmonics exists, it was turned off for simplicity in this work <a class="yt-timestamp" data-t="01:10:47">[01:10:47]</a>.
*   **Sensitivity to Uniform Color Areas** In areas with large, near-uniform colors, Gaussians can move freely without penalty, leading to inaccurate tracking without additional constraints <a class="yt-timestamp" data-t="01:33:50">[01:33:50]</a>.
*   **Potential for Density Artifacts** The reliance on local rigidity constraints could lead to issues if objects have varying densities of Gaussians, potentially causing an object with fewer Gaussians to be "pulled" by a denser one <a class="yt-timestamp" data-t="01:42:28">[01:42:28]</a>.
*   **Hyperparameter Tuning** The method involves several "sketchy" hyperparameters, such as the weighting factor (Lambda W = 2000) and the number of nearest neighbors (K=20), which may require specific tuning for different scenes or datasets <a class="yt-timestamp" data-t="01:47:40">[01:47:40]</a>.
*   **Error Propagation Over Time** While short-term constraints are applied between successive frames, relying solely on these can lead to drift over longer time horizons <a class="yt-timestamp" data-t="01:48:11">[01:48:11]</a>. Although a long-term isometry loss is introduced to mitigate this <a class="yt-timestamp" data-t="01:49:12">[01:49:12]</a>, it could lead to issues if objects deform or separate over time <a class="yt-timestamp" data-t="01:50:48">[01:50:48]</a>.
*   **Background/Foreground Dependence** The method benefits from pre-defined foreground/background masks to improve tracking and efficiency, implying a dependency on static backgrounds for optimal performance <a class="yt-timestamp" data-t="01:57:17">[01:57:17]</a>.
*   **High Memory Usage for Explicit Representation** While rendering is fast, storing the explicit 3D Gaussian representation requires significantly more memory compared to implicit representations like [[comparison_of_3d_gaussian_splatting_to_neural_radiance_fields | NeRFs]] <a class="yt-timestamp" data-t="02:10:50">[02:10:50]</a>.
*   **Subjective Metric Evaluation** Some metrics like PSNR, LPIPS, and SSIM are considered subjective and hard to interpret, making visual coherence a more reliable indicator of quality <a class="yt-timestamp" data-t="02:18:37">[02:18:37]</a>.