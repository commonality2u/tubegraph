---
title: KL Divergence and Bayesian Inference
videoId: VLrqFH1Xtrs
---

From: [[hu-po]] <br/> 

[[Bayesian Statistics and Machine Learning | Bayesian inference]] is a statistical method used to update the probability for a hypothesis as more evidence or information becomes available. This approach plays a crucial role in the development and understanding of modern generative models, including [[Bayesian Flow Networks | Bayesian Flow Networks]] (BFNs) and [[Conditional diffusion models for neural networks | diffusion models]] <a class="yt-timestamp" data-t="02:51">00:02:51</a>.

## Core Concepts of Bayesian Inference

### Bayes' Theorem
At its foundation, [[Bayesian Statistics and Machine Learning | Bayesian statistics]] is built upon [[Bayesian Statistics and Machine Learning | Bayes' Theorem]] <a class="yt-timestamp" data-t="02:05:07">02:05:07</a>. The theorem is expressed as:

$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$ <a class="yt-timestamp" data-t="02:48:48">02:48:48</a>

Here's what each component represents:
*   **$P(A|B)$ (Posterior)**: The conditional probability of hypothesis $A$ given evidence $B$. This is "Bob's distribution" â€“ what Bob thinks the distribution is after observing data <a class="yt-timestamp" data-t="02:25:56">02:25:56</a>, <a class="yt-timestamp" data-t="02:54:57">02:54:57</a>.
*   **$P(B|A)$ (Likelihood)**: The conditional probability of observing evidence $B$ given that hypothesis $A$ is true. This represents "How likely is this data $X$ if we have some model $\theta$?" <a class="yt-timestamp" data-t="01:05:01">01:05:01</a>, <a class="yt-timestamp" data-t="02:05:06">02:05:06</a>.
*   **$P(A)$ (Prior)**: The initial probability of hypothesis $A$ before any evidence is considered. This is Bob's "initial guess" or belief about the distribution <a class="yt-timestamp" data-t="03:18">00:03:18</a>, <a class="yt-timestamp" data-t="02:20:00">02:20:00</a>. A common simple prior is a [[Concepts of probability distributions in ML | Gaussian distribution]] <a class="yt-timestamp" data-t="03:31">00:03:31</a>.
*   **$P(B)$ (Marginal Likelihood or Evidence)**: The probability of observing the evidence $B$. This term is often challenging to compute because it involves an intractable integral over all possible hypotheses <a class="yt-timestamp" data-t="02:43:00">02:43:00</a>, <a class="yt-timestamp" data-t="02:45:00">02:45:00</a>.

### The Alice and Bob Metaphor
The concept of [[Bayesian Statistics and Machine Learning | Bayesian inference]] and information exchange is often illustrated using the Alice and Bob metaphor <a class="yt-timestamp" data-t="01:05:05">01:05:05</a>.
*   **Alice**: Represents nature or the "Oracle" who knows the true data distribution but can only provide samples from it <a class="yt-timestamp" data-t="02:04:00">02:04:00</a>, <a class="yt-timestamp" data-t="02:53:00">02:53:00</a>.
*   **Bob**: Represents the learning model that tries to understand or "guess" the true distribution based on messages (data samples) received from Alice <a class="yt-timestamp" data-t="01:05:54">01:05:54</a>, <a class="yt-timestamp" data-t="02:25:56">02:25:56</a>.

Each message Alice sends reveals something about the data, and Bob uses this information to improve his guess, or update his prior. The better Bob's guess, the fewer "bits" (information) are needed to transmit the message <a class="yt-timestamp" data-t="01:05:54">01:05:54</a>. This leads to the concept of [[Bayesian Statistics and Machine Learning | data compression]] as a measure of model quality, where the loss function is the total number of bits required for all messages <a class="yt-timestamp" data-t="01:21:00">01:21:00</a>, <a class="yt-timestamp" data-t="01:26:27">01:26:27</a>.

### Variational Inference and ELBO
Because the marginal likelihood ($P(B)$) in [[Bayesian Statistics and Machine Learning | Bayes' Theorem]] is often intractable to compute, [[Bayesian Statistics and Machine Learning | variational inference]] offers a solution <a class="yt-timestamp" data-t="02:57:01">02:57:01</a>. It reframes the problem of computing the posterior distribution into an [[Scaling and optimization in diffusion models | optimization]] problem <a class="yt-timestamp" data-t="02:57:00">02:57:00</a>.

Instead of directly computing the true posterior $P$, [[Bayesian Statistics and Machine Learning | variational inference]] proposes a simpler, tractable distribution $Q$ that approximates $P$ <a class="yt-timestamp" data-t="02:57:25">02:57:25</a>. The "closeness" between $Q$ and $P$ is measured using [[Relative entropy and KL divergence | KL Divergence]] <a class="yt-timestamp" data-t="02:57:37">02:57:37</a>.

Since directly minimizing [[Relative entropy and KL divergence | KL Divergence]] can still be difficult, the approach maximizes the [[Bayesian Statistics and Machine Learning | evidence lower bound]] (ELBO) <a class="yt-timestamp" data-t="02:59:58">02:59:58</a>. The [[Bayesian Statistics and Machine Learning | evidence lower bound]] is related to the [[Relative entropy and KL divergence | KL Divergence]] and allows for [[Scaling and optimization in diffusion models | gradient-based optimization]] methods to iteratively refine $Q$, making it a very good approximation of the true posterior <a class="yt-timestamp" data-t="03:02:23">03:02:23</a>.

## KL Divergence in Machine Learning
[[Relative entropy and KL divergence | KL Divergence]] (Kullback-Leibler Divergence) is a measure of how one [[Concepts of probability distributions in ML | probability distribution]] is different from a second, reference [[Concepts of probability distributions in ML | probability distribution]] <a class="yt-timestamp" data-t="02:36:00">02:36:00</a>. It quantifies the "distance" between two distributions.
*   **Measurement**: If two distributions are identical, their [[Relative entropy and KL divergence | KL Divergence]] is zero <a class="yt-timestamp" data-t="02:36:00">02:36:00</a>, <a class="yt-timestamp" data-t="02:24:56">02:24:56</a>. The goal in many models is to minimize this divergence to make the learned distribution as close as possible to the target (true) distribution <a class="yt-timestamp" data-t="02:37:00">02:37:00</a>.
*   **Asymmetry**: Notably, [[Relative entropy and KL divergence | KL Divergence]] is not symmetrical; the divergence from distribution A to B is generally not the same as from B to A <a class="yt-timestamp" data-t="00:44:53">00:44:53</a>.
*   **Loss Function**: In models like [[Conditional diffusion models for neural networks | diffusion models]] and [[Bayesian Flow Networks | Bayesian Flow Networks]], [[Relative entropy and KL divergence | KL Divergence]] serves as a key component of the loss function <a class="yt-timestamp" data-t="00:46:49">00:46:49</a>, <a class="yt-timestamp" data-t="01:39:27">01:39:27</a>. The sum of [[Relative entropy and KL divergence | KL divergences]] over multiple steps is equivalent to the [[Bayesian Statistics and Machine Learning | evidence lower bound]] <a class="yt-timestamp" data-t="02:43:00">02:43:00</a>.

### KL Divergence and Data Representation
The way data is represented can impact [[Relative entropy and KL divergence | KL Divergence]]. For discretized data, such as categories or bins, [[Relative entropy and KL divergence | KL Divergence]] can behave differently:
*   **Discretization Impact**: When a continuous distribution is discretized into bins, the [[Relative entropy and KL divergence | KL Divergence]] for the discretized version can be lower than for the continuous one <a class="yt-timestamp" data-t="02:32:00">02:32:00</a>, <a class="yt-timestamp" data-t="02:27:37">02:27:37</a>. This is because probability mass might accumulate on "density bumps" at bin centers, making the distributions appear more similar in discrete space <a class="yt-timestamp" data-t="02:28:51">02:28:51</a>.

## Application in Bayesian Flow Networks
[[Bayesian Flow Networks | Bayesian Flow Networks]] (BFNs) leverage both [[Bayesian Statistics and Machine Learning | Bayesian inference]] and [[Relative entropy and KL divergence | KL Divergence]] as core mechanisms for generating new data <a class="yt-timestamp" data-t="00:02:19">00:02:19</a>.
*   **Parameters, Not Data**: Unlike some [[Conditional diffusion models for neural networks | diffusion models]] that operate on noisy versions of the data itself, [[Bayesian Flow Networks | BFNs]] operate on the parameters of data distributions (e.g., mean and variance for [[Concepts of probability distributions in ML | normal distributions]], or probabilities for [[Concepts of probability distributions in ML | categorical distributions]]) <a class="yt-timestamp" data-t="01:04:46">01:04:46</a>, <a class="yt-timestamp" data-t="01:07:09">01:07:09</a>, <a class="yt-timestamp" data-t="01:46:01">01:46:01</a>.
*   **Continuous and Differentiable**: This approach ensures that the generative process in [[Bayesian Flow Networks | BFNs]] is fully continuous and differentiable, even when dealing with discrete data <a class="yt-timestamp" data-t="01:47:41">01:47:41</a>.
*   **Iterative Refinement**: [[Bayesian Flow Networks | BFNs]] start with a simple prior distribution (e.g., a [[Concepts of probability distributions in ML | standard normal distribution]] or a [[Concepts of probability distributions in ML | uniform categorical distribution]]) <a class="yt-timestamp" data-t="01:50:00">01:50:00</a>. Through iterative steps, they feed the parameters of an input distribution to a neural network. This network then outputs parameters for a second, interdependent distribution <a class="yt-timestamp" data-t="03:48:00">03:48:00</a>.
*   **Loss Calculation**: The "transmission cost" in [[Bayesian Flow Networks | BFNs]] is defined by the [[Relative entropy and KL divergence | KL Divergence]] between a "sender distribution" (Alice's data + noise) and a "receiver distribution" (Bob's output + noise) <a class="yt-timestamp" data-t="00:46:49">00:46:49</a>, <a class="yt-timestamp" data-t="04:01:00">04:01:00</a>. Bob then uses Bayesian inference rules to update his input distribution based on samples from the sender <a class="yt-timestamp" data-t="04:10:00">04:10:00</a>. This continuous updating process is what gives the "flow" in [[Bayesian Flow Networks | Bayesian Flow Networks]] <a class="yt-timestamp" data-t="01:00:00">01:00:00</a>.
*   **Efficiency and [[Scaling and optimization in diffusion models | Optimization]]**: [[Bayesian Flow Networks | BFNs]] are hypothesized to lead to faster learning on large datasets due to less noisy network inputs, as they begin with a fixed prior instead of pure noise <a class="yt-timestamp" data-t="01:02:03">01:02:03</a>. They also simplify the process by not requiring the definition and inversion of a forward process, which is often necessary in [[Conditional diffusion models for neural networks | diffusion models]] <a class="yt-timestamp" data-t="01:02:44">01:02:44</a>.

In essence, [[Bayesian Flow Networks | BFNs]] provide a theoretically elegant and computationally efficient framework for generative modeling by directly optimizing data compression through [[Relative entropy and KL divergence | KL Divergence]] and iterative [[Bayesian Statistics and Machine Learning | Bayesian inference]] <a class="yt-timestamp" data-t="00:08:05">00:08:05</a>.