---
title: Large Language Models as optimizers
videoId: lR9isPmwZ3s
---

From: [[hu-po]] <br/> 

[[Large Language Models and their applications | Large Language Models]] (LLMs) are being explored for their capacity to function as generic optimizers for various tasks. This concept is introduced in a recent Google DeepMind paper titled "Large Language Models as Optimizers," published on September 7, 2023 <a class="yt-timestamp" data-t="00:03:04">[00:03:04]</a>. The paper proposes a method called "Optimization by Prompting" (OpPRO), which leverages the natural language understanding capabilities of LLMs to improve processes <a class="yt-timestamp" data-t="00:05:54">[00:05:54]</a>.

## Traditional Optimization Challenges
Traditional optimization, particularly in [[Large Language Models and their applications | machine learning]], often relies on derivative-based algorithms like gradient descent <a class="yt-timestamp" data-t="00:03:43">[00:03:43]</a>. These methods require the ability to calculate a gradient (or slope) for every part of the system, meaning the system must be differentiable <a class="yt-timestamp" data-t="00:05:02">[00:05:02]</a>. However, many real-world applications face challenges due to the absence of a gradient, especially when dealing with processes described in natural language or discrete spaces <a class="yt-timestamp" data-t="00:06:04">[00:06:04]</a>. For instance, optimizing in a discrete space, like text, prevents the use of gradient-based methods because derivatives cannot be taken <a class="yt-timestamp" data-t="00:30:19">[00:30:19]</a>.

## OpPRO: Optimization by Prompting
OpPRO enables LLMs to optimize without explicitly calculating the slope <a class="yt-timestamp" data-t="00:05:27">[00:05:27]</a>. The core idea is to describe the optimization task in natural language, instructing the LLM to iteratively generate new solutions based on previously found solutions and their values <a class="yt-timestamp" data-t="00:06:16">[00:06:16]</a>. This means the LLM acts as the optimizer, handling the entire process <a class="yt-timestamp" data-t="00:33:05">[00:33:05]</a>.

The process is iterative: the LLM receives a prompt containing prior solutions and their performance scores, then generates improved solutions <a class="yt-timestamp" data-t="00:06:16">[00:06:16]</a>. This makes [[Large Language Models and their applications | LLMs]] adaptable to different tasks simply by changing the problem description in the prompt <a class="yt-timestamp" data-t="00:24:08">[00:24:08]</a>.

### Metaprompt Design
The "metaprompt" is the key input for the optimizer LLM <a class="yt-timestamp" data-t="00:42:42">[00:42:42]</a>. It consists of two main parts:
1.  **Previously Generated Prompts with Scores**: A history of solutions (e.g., text additions) and their corresponding performance values (e.g., accuracy), typically sorted in ascending order of scores <a class="yt-timestamp" data-t="00:34:55">[00:34:55]</a>. This guides the optimizer to build upon successful attempts <a class="yt-timestamp" data-t="00:45:28">[00:45:28]</a>.
2.  **Optimization Problem Description**: This includes the objective function and desired solution properties, often exemplified by a few randomly selected samples from a training set <a class="yt-timestamp" data-t="00:32:57">[00:32:57]</a>.

## Demonstrations of OpPRO

### Mathematical Optimization
OpPRO was showcased on two classic optimization problems:
*   **Linear Regression**: A continuous optimization problem where the goal is to find the best-fit line (parameters W and B) for a set of points <a class="yt-timestamp" data-t="00:08:12">[00:08:12]</a> <a class="yt-timestamp" data-t="00:50:40">[00:50:40]</a>. [[Large Language Models and their applications | LLMs]] like GPT-4 and Text Bison (a [[Large Language Models and their applications | Palm 2]] model from Google) successfully captured optimization directions <a class="yt-timestamp" data-t="00:54:59">[00:54:59]</a>. Performance was sensitive to initial conditions and suffered when optimal values were negative or large <a class="yt-timestamp" data-t="00:55:54">[00:55:54]</a>.
*   **Traveling Salesman Problem (TSP)**: A discrete combinatorial optimization problem where the goal is to find the shortest route connecting a given set of nodes <a class="yt-timestamp" data-t="00:08:57">[00:08:57]</a> <a class="yt-timestamp" data-t="01:00:06">[01:00:06]</a>. For TSP, [[Large Language Models and their applications | LLMs]] generally had a harder time, especially with larger numbers of nodes <a class="yt-timestamp" data-t="01:06:41">[01:06:41]</a>. While GPT-4 achieved better results, Text Bison struggled to reach the global optimum, suggesting a potential limitation in handling non-gradient-based discrete problems <a class="yt-timestamp" data-t="01:07:21">[01:07:21]</a>.

### Prompt Optimization
This is the paper's main and most practical application <a class="yt-timestamp" data-t="00:40:09">[00:40:09]</a>. OpPRO uses an LLM to optimize prompts for other LLMs, aiming to maximize task accuracy <a class="yt-timestamp" data-t="00:09:42">[00:09:42]</a>.
*   **Benchmarks**: Experiments were conducted on GSM8K, a dataset of grade school math word problems <a class="yt-timestamp" data-t="01:00:03">[01:00:03]</a>, and Big Bench Hard (BBH), a Google-created benchmark with about 200 diverse tasks <a class="yt-timestamp" data-t="01:16:52">[01:16:52]</a> <a class="yt-timestamp" data-t="00:16:42">[00:16:42]</a>.
*   **Performance**: The optimized prompts generated by OpPRO consistently outperformed human-designed prompts <a class="yt-timestamp" data-t="00:25:04">[00:25:04]</a> <a class="yt-timestamp" data-t="01:35:45">[01:35:45]</a>. For example, simply adding "take a deep breath and work on this problem step by step" improved accuracy on GSM8K by 10% compared to "let's think step by step," and significantly over an empty prompt <a class="yt-timestamp" data-t="00:10:59">[00:10:59]</a> <a class="yt-timestamp" data-t="00:14:01">[00:14:01]</a>. This highlights the "psychology for [[Large Language Models and their applications | LLMs]]" aspect, where subtle phrasing can have a large impact <a class="yt-timestamp" data-t="00:13:36">[00:13:36]</a> <a class="yt-timestamp" data-t="00:14:45">[00:14:45]</a>.
*   **Score LLM vs. Optimizer LLM**: The evaluation involves two LLMs: a "score LLM" (e.g., Palm 2L-IT, Text Bison), which answers the questions with the optimized prompt appended, and an "optimizer LLM" (e.g., Palm 2L-IT), which generates the new prompts <a class="yt-timestamp" data-t="01:14:04">[01:14:04]</a>. The score LLM operates with temperature set to zero (deterministic), while the optimizer LLM uses temperature 1 (more exploratory) <a class="yt-timestamp" data-t="01:21:20">[01:21:20]</a>.
*   **Prompt Placement**: The optimal placement of the additional prompt was found to be at the end of the question (Q_end), as it is closer to the answer and thus has more impact due to "recency bias" <a class="yt-timestamp" data-t="01:39:24">[01:39:24]</a>.
*   **Conciseness vs. Detail**: Interestingly, concise prompts generated by some LLMs (like Palm 2L) performed as well as longer, more detailed ones generated by others (like GPT models), suggesting that optimal prompts don't necessarily require more text <a class="yt-timestamp" data-t="01:41:21">[01:41:21]</a>.

## Factors Influencing OpPRO Performance (Ablation Studies)

*   **Exploration vs. Exploitation**: A fundamental challenge in optimization <a class="yt-timestamp" data-t="00:40:16">[00:40:16]</a>. [[Large Language Models and their applications | LLMs]] can balance these by generating multiple solutions at each step and adjusting the sampling temperature <a class="yt-timestamp" data-t="00:48:08">[00:48:08]</a>. A higher temperature promotes more aggressive exploration <a class="yt-timestamp" data-t="00:50:24">[00:50:24]</a>.
*   **Order of Instructions**: Sorting previous solutions by score (ascending order) was found to be more effective than descending or random order <a class="yt-timestamp" data-t="01:25:21">[01:25:21]</a>. This might be due to a recency bias, where LLMs are more influenced by examples closer to the end of the metaprompt <a class="yt-timestamp" data-t="01:24:24">[01:24:24]</a>.
*   **Instruction Scores**: Presenting accuracy scores to the optimizer LLM improves performance compared to not showing them. Rounding scores into fewer "buckets" (e.g., 20 buckets) also generally performed better than more precise scores (e.g., 100 buckets) <a class="yt-timestamp" data-t="01:26:40">[01:26:40]</a>.
*   **Number of Exemplars**: Providing a few examples from the task (e.g., three exemplars) is critical for helping the optimizer LLM understand the task and phrase new instructions <a class="yt-timestamp" data-t="01:29:46">[01:29:46]</a>. However, too many exemplars can sometimes hinder performance by distracting the LLM from other important components like the optimization trajectory <a class="yt-timestamp" data-t="01:30:35">[01:30:35]</a>.
*   **Starting Point**: The initial conditions (e.g., starting with an empty string or a common phrase like "let's solve the problem") significantly influence the direction of the optimization, potentially leading to specific phrasing biases (e.g., many optimized prompts starting with "let's") <a class="yt-timestamp" data-t="01:33:02">[01:33:02]</a>.

## Limitations and Future Directions

The paper acknowledges several limitations:
*   **Context Window Length**: The [[efficiency_of_large_language_models | length limit]] of LLM context windows restricts the scale of optimization problems that can be described in the prompt <a class="yt-timestamp" data-t="01:39:40">[01:39:40]</a>. Larger context windows could enable more complex optimizations <a class="yt-timestamp" data-t="01:10:05">[01:10:05]</a>.
*   **Bumpy Landscapes**: For problems with very complex or "bumpy" optimization landscapes, LLMs might struggle to propose the correct descending direction, leading to the optimization getting stuck <a class="yt-timestamp" data-t="01:50:00">[01:50:00]</a>.
*   **Error Case Utilization**: Current OpPRO models do not effectively use information from error cases to infer promising directions, instead relying on aggregated accuracy <a class="yt-timestamp" data-t="01:46:15">[01:46:15]</a>.
*   **Training Data Dependency**: Prompt optimization still requires a training set to compute accuracy for guidance <a class="yt-timestamp" data-t="01:46:46">[01:46:46]</a>.

The broader implication is that LLMs demonstrate a general ability to recognize and extrapolate patterns, making them capable of solving various optimization problems <a class="yt-timestamp" data-t="00:23:01">[00:23:01]</a>. This paper is considered a proof of concept, opening the door for future research into using LLMs to optimize even more complex systems, potentially including the direct optimization of neural network weights or even [[Gradientbased optimization vs language modelbased optimization | replacing traditional gradient-based optimization]] altogether <a class="yt-timestamp" data-t="01:11:10">[01:11:10]</a> <a class="yt-timestamp" data-t="01:09:02">[01:09:02]</a>. This could lead to a future where LLMs generate "meta-instructions" for other LLMs, creating a multi-layered optimization process <a class="yt-timestamp" data-t="01:18:00">[01:18:00]</a>.