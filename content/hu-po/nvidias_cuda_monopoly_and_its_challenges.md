---
title: Nvidias CUDA monopoly and its challenges
videoId: t21REMsFJ_4
---

From: [[hu-po]] <br/> 

The landscape of machine learning software development has undergone significant changes over the last decade, with many frameworks relying heavily on Nvidia's CUDA and performing best on Nvidia GPUs <a class="yt-timestamp" data-t="00:06:06">[00:06:06]</a>. This has led to Nvidia's dominant position in the field, primarily due to its software moat <a class="yt-timestamp" data-t="00:03:36">[00:03:36]</a>. However, with the arrival of PyTorch 2.0 and OpenAI's Triton, this dominance is being challenged <a class="yt-timestamp" data-t="00:02:32">[00:02:32]</a>.

The article, written by Dylan Patel on January 16th, discusses how Nvidia's CUDA monopoly is breaking and potentially [[developments_in_deep_learning_hardware | heralding a new age of deep learning hardware]] <a class="yt-timestamp" data-t="00:01:43">[00:01:43]</a>. The core message is that the default software stack for machine learning models will no longer be closed-source CUDA, as OpenAI and Meta have taken control of the software stack by building their own tools <a class="yt-timestamp" data-t="00:05:33">[00:05:33]</a>.

## Evolution of Machine Learning Frameworks

A few years ago, the framework ecosystem was fragmented, with TensorFlow being a frontrunner, often on par with or larger than PyTorch <a class="yt-timestamp" data-t="00:05:57">[00:05:57]</a>. Google appeared poised to control the machine learning industry, possessing a first-mover advantage, the most commonly used framework (TensorFlow), and the only successful AI application-specific accelerator (the TPU) <a class="yt-timestamp" data-t="00:06:10">[00:06:10]</a>.

However, PyTorch ultimately won the framework race <a class="yt-timestamp" data-t="00:07:05">[00:07:05]</a>. This shift is evident in major machine learning conferences like ICLR, CVPR, and NeurIPS (formerly NIPS), where PyTorch's unique mentions grew from roughly 10% in 2017 to almost 70-80% by 2020 <a class="yt-timestamp" data-t="00:06:44">[00:06:44]</a>. Google is now somewhat isolated due to its preference for its own software stack and hardware (TPUs) over PyTorch and Nvidia GPUs <a class="yt-timestamp" data-t="00:07:13">[00:07:13]</a>. PyTorch's victory was primarily due to its increased flexibility and usability compared to TensorFlow <a class="yt-timestamp" data-t="00:08:33">[00:08:33]</a>. TensorFlow's compiled, graph-based approach made it challenging to debug and understand, akin to C++, whereas PyTorch's Python-like, eager execution workflow offered greater ease of use <a class="yt-timestamp" data-t="00:08:44">[00:08:44]</a>. Nearly every generative AI model that has made headlines is based on PyTorch <a class="yt-timestamp" data-t="00:10:08">[00:10:08]</a>.

## Memory Wall and Hardware Constraints

Machine learning model training time is primarily composed of two factors: compute and memory <a class="yt-timestamp" data-t="00:10:43">[00:10:43]</a>. Historically, compute time (waiting for matrix multiplies) was the dominant factor <a class="yt-timestamp" data-t="00:12:03">[00:12:03]</a>. However, with the rapid development of Nvidia GPUs, leveraging Moore's Law and architectural changes like tensor cores and lower-precision floating-point formats (e.g., FP16, FP8, and emerging FP4), compute capabilities have increased significantly <a class="yt-timestamp" data-t="00:12:09">[00:12:09]</a>.

Today, the primary bottleneck in [[inference_and_training_considerations_for_modern_GPUs | modern GPU inference and training]] is memory bandwidth <a class="yt-timestamp" data-t="00:14:46">[00:14:46]</a>. GPU flops (Floating Point Operations Per Second) have grown at a much faster rate than memory bandwidth (gigabytes per second) <a class="yt-timestamp" data-t="00:14:17">[00:14:17]</a>. This means tensor cores are often idle, waiting for data to be shuffled between memory caches <a class="yt-timestamp" data-t="00:11:23">[00:11:23]</a>. In 2018, compute-bound workloads made up 99.8% of flops but only 61% of runtime, indicating that memory-heavy operations like normalization and pointwise operations consume a significant portion of runtime despite requiring fewer flops <a class="yt-timestamp" data-t="00:16:13">[00:16:13]</a>.

### Challenges of Large Models
As models continue to grow, with large language models (LLMs) requiring hundreds of gigabytes or even terabytes for model weights alone, the issue of memory capacity becomes critical <a class="yt-timestamp" data-t="00:17:01">[00:17:01]</a>. It's increasingly difficult to fit deep learning models, especially large foundational models like GPT-3 or Stable Diffusion, onto a consumer GPU <a class="yt-timestamp" data-t="00:17:11">[00:17:11]</a>. Production recommendation networks, such as those deployed by Baidu and Meta, require dozens of terabytes of memory <a class="yt-timestamp" data-t="00:18:03">[00:18:03]</a>. This highlights [[challenges_in_training_large_computer_vision_models | challenges in training large computer vision models]] (and other large models), as most training time is spent waiting for data to reach compute resources <a class="yt-timestamp" data-t="00:18:32">[00:18:32]</a>.

The solution of putting more memory closer to the compute units is primarily limited by cost <a class="yt-timestamp" data-t="00:18:55">[00:18:55]</a>. SRAM (Static Random-Access Memory), which is fast and expensive, is used for on-chip memory, but it's not feasible to store entire models there due to cost and silicon area requirements <a class="yt-timestamp" data-t="00:25:00">[00:25:00]</a>. DRAM (Dynamic Random-Access Memory), while cheaper, has significantly higher latency <a class="yt-timestamp" data-t="00:28:40">[00:28:40]</a>. The cost reduction of DRAM has plateaued since 2012, and it now comprises 50% of total server costs <a class="yt-timestamp" data-t="00:29:14">[00:29:14]</a>. To achieve the necessary bandwidth, Nvidia uses HBM (High Bandwidth Memory), a more expensive 3D-stacked DRAM <a class="yt-timestamp" data-t="00:30:42">[00:30:42]</a>.

Even with these advancements, Nvidia's A100 GPUs often have low flops utilization without heavy optimization, with tensor cores being used only about 60% of the time, the rest being idle overhead <a class="yt-timestamp" data-t="00:31:20">[00:31:20]</a>. The problem is expected to worsen as flops grow faster than memory bandwidth <a class="yt-timestamp" data-t="00:32:17">[00:32:17]</a>.

## PyTorch 2.0 and OpenAI Triton

PyTorch 2.0, released with full availability in March, introduces a compiled solution that supports graph execution <a class="yt-timestamp" data-t="00:48:24">[00:48:24]</a>. This dramatically reduces compute time and cost, offering an 86% performance improvement for training on Nvidia's A100 <a class="yt-timestamp" data-t="00:48:48">[00:48:48]</a>. These benefits can extend to other GPUs and accelerators from AMD, Intel, Tesla, Google, and more, as it makes utilizing various hardware resources much easier <a class="yt-timestamp" data-t="00:49:04">[00:49:04]</a>.

Key components of PyTorch 2.0 include:
*   **PrimTorch**: Reduces the 2000+ PyTorch operators down to a core set of 250 primitive operators, simplifying the implementation of non-Nvidia backends <a class="yt-timestamp" data-t="00:54:50">[00:54:50]</a>.
*   **Torch Dynamo**: Ingests PyTorch user scripts, including those calling third-party libraries, and generates a computational graph <a class="yt-timestamp" data-t="00:55:41">[00:55:41]</a>. It leverages partial graph capture, guarded graph capture, and just-in-time recapture to enable seamless compilation while maintaining flexibility <a class="yt-timestamp" data-t="00:59:19">[00:59:19]</a>.
*   **Torch Inductor**: A Python-native deep learning compiler that takes the FX graph (with fused operators) and generates optimized code for multiple accelerator backends, including CPUs, GPUs, or other AI accelerators <a class="yt-timestamp" data-t="01:02:14">[01:02:14]</a>. This drastically reduces the work required for compiler teams when making an AI accelerator <a class="yt-timestamp" data-t="01:04:07">[01:04:07]</a>.

### OpenAI Triton's Disruptive Role
OpenAI's Triton is highly disruptive to Nvidia's closed-source hardware moat <a class="yt-timestamp" data-t="01:04:49">[01:04:49]</a>. Triton takes Python input, often fed through the PyTorch Inductor stack, converts it to an LLVM intermediate representation, and then directly generates PTX code for Nvidia GPUs, **bypassing Nvidia's closed-source CUDA libraries** such as cuBLAS (in favor of open-source alternatives like Cutlass) <a class="yt-timestamp" data-t="01:05:04">[01:05:04]</a>.

CUDA is typically used by specialists in accelerated computing and requires a deep understanding of hardware architecture <a class="yt-timestamp" data-t="01:06:09">[01:06:09]</a>. This leads machine learning experts to rely on CUDA experts to optimize their code, as seen in the re-implementation of [[compatibility_of_nerf_studio_with_pytorch_and_cuda | Nerf (Neural Radiance Fields)]] that was 100 times faster due to CUDA optimization <a class="yt-timestamp" data-t="01:07:01">[01:07:01]</a>. Triton bridges this gap by enabling higher-level languages to achieve comparable performance to lower-level languages, making kernels more legible to typical ML researchers <a class="yt-timestamp" data-t="01:07:52">[01:07:52]</a>.

## [[Future trends in machine learning software and hardware | Future Outlook]]

The ability for other hardware accelerators to integrate directly into Triton dramatically reduces the time needed to build an AI compiler stack for new hardware, thereby opening up the market for AI hardware and custom ASICs <a class="yt-timestamp" data-t="01:08:39">[01:08:39]</a>. Nvidia's "colossal software organization" lacked the foresight to become the default compiler for machine learning, and its lack of focus on usability enabled outsiders like OpenAI and Meta to create a portable software stack <a class="yt-timestamp" data-t="01:08:54">[01:08:54]</a>.

While large companies like Google (with TPUs) and Tesla (with Dojo chips) are developing their own in-house hardware for deep learning <a class="yt-timestamp" data-t="00:04:19">[00:04:19]</a>, the emergence of PyTorch 2.0 and Triton suggests a more open market where hardware can be swapped out while keeping the same framework <a class="yt-timestamp" data-t="00:47:58">[00:47:58]</a>. This shift towards a combined eager-and-graph-based compilation approach, aware of underlying hardware, represents a significant change, leading to more diverse winners in the machine learning hardware and software ecosystem <a class="yt-timestamp" data-t="00:58:47">[00:58:47]</a>.