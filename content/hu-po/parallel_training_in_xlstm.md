---
title: Parallel training in xLSTM
videoId: udIEwt0xM6A
---

From: [[hu-po]] <br/> 

[[introduction_to_xlstm_architecture | xLSTM]], an extended Long Short-Term Memory ([[Transition from Transformers to recurrent neural networks RNNs | LSTM]]) architecture, introduces features specifically designed to enable parallel training, a critical aspect for scaling deep learning models <a class="yt-timestamp" data-t="00:06:11">[00:06:11]</a>.

## Importance of Parallel Training
[[parallelism_and_scalability_in_machine_learning | Parallel training]] is crucial because it allows machine learning models to be scaled effectively <a class="yt-timestamp" data-t="00:06:13">[00:06:13]</a>. The ability to train in parallel enables the leveraging of "Big Data, Big Training, Big Model" scaling laws, which leads to improved performance <a class="yt-timestamp" data-t="00:06:16">[00:06:16]</a>. This contrasts with the sequential nature of traditional recurrent networks <a class="yt-timestamp" data-t="00:13:42">[00:13:42]</a>.

## Challenges with Traditional LSTMs
Traditional [[Transition from Transformers to recurrent neural networks RNNs | LSTMs]] suffer from a lack of parallelizability due to memory mixing <a class="yt-timestamp" data-t="00:11:41">[00:11:41]</a>. To calculate a current block in a traditional [[Transition from Transformers to recurrent neural networks RNNs | LSTM]], information from the previous block's hidden state and cell state is required <a class="yt-timestamp" data-t="00:13:27">[00:13:27]</a>. This dependency creates a sequential chain where each step must be completed before the next can begin, preventing parallel computation across time steps <a class="yt-timestamp" data-t="00:13:42">[00:13:42]</a>. This inherent sequential nature makes training traditional [[Transition from Transformers to recurrent neural networks RNNs | LSTMs]] difficult at scale <a class="yt-timestamp" data-t="00:27:24">[00:27:24]</a>.

## M-LSTM: Enabling Parallelism
To address the limitations of traditional [[Transition from Transformers to recurrent neural networks RNNs | LSTMs]] regarding parallelization, [[introduction_to_xlstm_architecture | xLSTM]] introduces two variants: S-LSTM and M-LSTM <a class="yt-timestamp" data-t="00:37:30">[00:37:30]</a>. The M-LSTM (Matrix [[Transition from Transformers to recurrent neural networks RNNs | LSTM]]) specifically tackles the challenge of parallel training <a class="yt-timestamp" data-t="00:43:51">[00:43:51]</a>.

### Key Changes in M-LSTM
1.  **Matrix Memory Cell**: Unlike the traditional [[Transition from Transformers to recurrent neural networks RNNs | LSTM]] which uses a scalar or vector for its cell state (C), M-LSTM increases the memory cell from a scalar to a matrix C <a class="yt-timestamp" data-t="00:44:13">[00:44:13]</a>. This "brick of information" <a class="yt-timestamp" data-t="00:44:47">[00:44:47]</a> allows for individual parts of the matrix to be calculated separately <a class="yt-timestamp" data-t="00:51:16">[00:51:16]</a>.
2.  **Externalized Output Gate**: A crucial modification in M-LSTM is that its output gate is no longer dependent on the hidden state from the previous time step <a class="yt-timestamp" data-t="01:16:25">[01:16:25]</a>. In traditional [[Transition from Transformers to recurrent neural networks RNNs | LSTMs]], the output gate depends on the hidden state, forcing sequential processing <a class="yt-timestamp" data-t="01:15:59">[01:15:59]</a>. By externalizing this dependency, M-LSTM removes the "memory mixing" <a class="yt-timestamp" data-t="01:11:45">[01:11:45]</a>, allowing its recurrence to be formulated in a parallel version <a class="yt-timestamp" data-t="01:43:50">[01:43:50]</a>.
3.  **Exponential Gating and Stabilization**: Both S-LSTM and M-LSTM incorporate [[Exponential gating and memory stabilization in xLSTM | exponential gating]] instead of sigmoid activation functions for their input and forget gates <a class="yt-timestamp" data-t="00:28:10">[00:28:10]</a>. To prevent numerical issues like overflow or underflow from these exponents, an additional stabilization state (M of T) is introduced <a class="yt-timestamp" data-t="00:29:53">[00:29:53]</a>.

## Comparison with Other Architectures
*   **Transformers**: Transformers, with their [[parallelism and scalability in machine learning | parallelizable self-attention]] mechanism, marked a new era in [[AI model architecture and parallelism strategies | AI model architecture]] by enabling large-scale training <a class="yt-timestamp" data-t="00:05:09">[00:05:09]</a>. The reason [[comparison_of_xlstm_with_transformers_and_other_models | Transformers]] can be parallelized is that all elements (words in a sequence) can compute their attention scores independently <a class="yt-timestamp" data-t="00:48:50">[00:48:50]</a>.
*   **Mamba**: Similar to [[Transition from Transformers to recurrent neural networks RNNs | LSTMs]], Mamba models are recurrent architectures. However, they can be formulated in a way that allows for parallel training <a class="yt-timestamp" data-t="00:06:05">[00:06:05]</a>. M-LSTM's approach to parallel training is noted as similar to that of Mamba <a class="yt-timestamp" data-t="00:55:00">[00:55:00]</a>.

## Implementation and Future Outlook
While M-LSTM is theoretically parallelizable, the current [[AI model architecture and parallelism strategies | CUDA kernels]] for M-LSTM and S-LSTM are not yet optimized <a class="yt-timestamp" data-t="01:05:41">[01:05:41]</a>. As a result, their current implementation is about four times slower than highly optimized methods like flash attention used in [[comparison_of_xlstm_with_transformers_and_other_models | Transformers]] or the scan operation in Mamba <a class="yt-timestamp" data-t="01:07:03">[01:07:03]</a>. This highlights a gap between theoretical architectural design and practical, efficient implementation <a class="yt-timestamp" data-t="01:06:01">[01:06:01]</a>.

Despite this, [[introduction_to_xlstm_architecture | xLSTM]] shows promising performance at smaller scales compared to older versions of [[comparison_of_xlstm_with_transformers_and_other_models | Transformers]] (Llama 1) and State Space Models <a class="yt-timestamp" data-t="01:27:51">[01:27:51]</a>. The paper suggests that larger [[introduction_to_xlstm_architecture | xLSTM]] models could become serious competitors to current LLMs built with [[comparison_of_xlstm_with_transformers_and_other_models | Transformer]] technology <a class="yt-timestamp" data-t="01:36:31">[01:36:31]</a>. The authors express hope that with further optimization, potentially from programmers skilled in [[AI model architecture and parallelism strategies | CUDA kernel]] development, [[introduction_to_xlstm_architecture | xLSTM]] can achieve comparable speeds to [[comparison_of_xlstm_with_transformers_and_other_models | Transformers]] <a class="yt-timestamp" data-t="01:38:08">[01:38:08]</a>.

The ability to mix different block types (S-LSTM and M-LSTM) within a single [[AI model architecture and parallelism strategies | xLSTM architecture]] is also noted <a class="yt-timestamp" data-t="01:19:20">[01:19:20]</a>, similar to recent "Jamba" architectures that combine [[comparison_of_xlstm_with_transformers_and_other_models | Transformer]] and Mamba blocks <a class="yt-timestamp" data-t="01:20:05">[01:20:05]</a>. This suggests a future where optimal [[AI model architecture and parallelism strategies | AI model architecture]] might involve combining different block types to leverage their respective strengths <a class="yt-timestamp" data-t="01:21:04">[01:21:04]</a>.