---
title: Artificial Super Intelligence ASI vs Artificial General Intelligence AGI
videoId: KYlbny1rN1g
---

From: [[hu-po]] <br/> 

Artificial Intelligence (AI) encompasses various forms of intelligence, with [[agi_and_asi_in_large_language_models | Artificial General Intelligence (AGI)]] and [[agi_and_asi_in_large_language_models | Artificial Super Intelligence (ASI)]] representing distinct levels of capability <a class="yt-timestamp" data-t="00:02:20">[00:02:20]</a>.

## Defining AGI and ASI

**[[agi_and_asi_in_large_language_models | Artificial General Intelligence (AGI)]]** refers to an AI's ability to adapt to a variety of different tasks <a class="yt-timestamp" data-t="01:54:22">[01:54:22]</a>. It possesses a broad cognitive capacity, enabling it to learn, understand, and apply knowledge across diverse domains, similar to human intelligence <a class="yt-timestamp" data-t="00:02:23">[00:02:23]</a>.

**[[agi_and_asi_in_large_language_models | Artificial Super Intelligence (ASI)]]** goes beyond AGI, signifying an AI that is "superhuman" <a class="yt-timestamp" data-t="00:02:30">[00:02:30]</a>. This means it is better than any human or group of people at anything, whether in the physical or digital world <a class="yt-timestamp" data-t="00:06:28">[00:06:28]</a>. The key difference between AGI and ASI lies in the terms "general" versus "super" <a class="yt-timestamp" data-t="00:02:27">[00:02:27]</a>.

## Types of Intelligence

### Narrow (Specialized) AI
Before discussing AGI and ASI, it's important to understand **[[generalist_vs_narrow_ai_models_in_robotics | narrow AI]]**, or narrow Artificial Super Intelligence. This form of AI is superhumanly intelligent but only within a very limited, specific field <a class="yt-timestamp" data-t="00:02:42">[00:02:42]</a>.

Examples of narrow ASI include:
*   **Calculators** capable of performing complex arithmetic operations perfectly and rapidly, surpassing human ability <a class="yt-timestamp" data-t="00:02:37">[00:02:37]</a>. Mechanical calculators existed as early as 1623 <a class="yt-timestamp" data-t="00:02:57">[00:02:57]</a>.
*   Robots demonstrating superhuman balancing abilities <a class="yt-timestamp" data-t="00:05:52">[00:05:52]</a>.
*   Machines with superhuman strength, precision, and repeatability <a class="yt-timestamp" data-t="00:06:10">[00:06:10]</a>.

We have lived in a world with these types of narrow ASIs for hundreds of years <a class="yt-timestamp" data-t="00:06:17">[00:06:17]</a>.

### General Intelligence

The concept of "general" intelligence is often illustrated through [[comparisons_of_biological_and_ai_systems | biological analogies]] <a class="yt-timestamp" data-t="00:03:31">[00:03:31]</a>. Specialists, like pandas or koalas, are highly adapted to a narrow ecological niche but struggle outside of it <a class="yt-timestamp" data-t="00:03:42">[00:03:42]</a>. Generalists, such as crows or dogs, can adapt to almost any niche and eat anything, demonstrating a more general form of intelligence <a class="yt-timestamp" data-t="00:04:02">[00:04:02]</a>.

According to one perspective, we already have AGI:
*   **[[agi_and_asi_in_large_language_models | Large Language Models (LLMs)]]** like ChatGPT, especially following advancements such as the "03 moment" (a specific milestone in their development), can be considered AGI <a class="yt-timestamp" data-t="00:04:26">[00:04:26]</a>. However, they are currently limited to language-based tasks within the digital world <a class="yt-timestamp" data-t="00:04:44">[00:04:44]</a>.
*   Robots like the Tesla Optimus are approaching [[the_potential_future_and_challenges_of_ai_agents | AGI in the real world]] <a class="yt-timestamp" data-t="00:05:22">[00:05:22]</a>. These humanoid robots are expected to perform almost any task humans do in the physical world <a class="yt-timestamp" data-t="00:05:30">[00:05:30]</a>.

## The Path to Artificial Super Intelligence

The development of AI intelligence is increasing exponentially, with a very narrow window in time between an AI being as good as a "dumb human" and better than a "smart human" <a class="yt-timestamp" data-t="00:07:09">[00:07:09]</a>.

### Lessons from the Game of Go
The progression towards superhuman AI has been clearly demonstrated in the game of Go <a class="yt-timestamp" data-t="00:07:43">[00:07:43]</a>.
*   Early Go AIs had low ELO ratings, not even matching human skill <a class="yt-timestamp" data-t="00:07:50">[00:07:50]</a>.
*   With the development of AlphaGo, AlphaGo Zero, and AlphaGo Master, AI surpassed top human performance <a class="yt-timestamp" data-t="00:08:20">[00:08:20]</a>.
*   **AlphaGo Master** was trained on 230,000 human games <a class="yt-timestamp" data-t="00:27:21">[00:27:21]</a>. However, an AI cannot achieve superhuman intelligence by merely copying humans <a class="yt-timestamp" data-t="00:27:30">[00:27:30]</a>.
*   **AlphaGo Zero** achieved superhuman performance *without any human knowledge or data* <a class="yt-timestamp" data-t="00:27:51">[00:27:51]</a>. It utilized **reinforcement learning** and **self-play**, simulating millions of Go games and using the win/loss outcome to label good and bad moves throughout the game's "tree" of possible actions <a class="yt-timestamp" data-t="00:09:16">[00:09:16]</a>. This process generated its own "superhuman data" <a class="yt-timestamp" data-t="00:33:57">[00:33:57]</a>. AlphaGo Zero discovered novel moves, such as "move 37", that no human professional had ever conceived <a class="yt-timestamp" data-t="01:37:48">[01:37:48]</a>.

### Applying to Language Models
The principles from Go can be applied to language models:
*   A language model generates text by autoregressively predicting the next token, essentially choosing a "branch" in a tree of possible tokens <a class="yt-timestamp" data-t="00:16:31">[00:16:31]</a>.
*   The "branching factor" (number of possible next tokens) for LLMs is around 32,000, significantly larger than for Chess (35) or Go (250) <a class="yt-timestamp" data-t="00:18:00">[00:18:00]</a>.
*   A "Chain of Thought" is a sequence of choices or actions through this tree of possible states <a class="yt-timestamp" data-t="02:09:00">[02:09:00]</a>.
*   Like Go, the tree of possible language sequences is finite, assuming a limited vocabulary and sequence length <a class="yt-timestamp" data-t="00:18:48">[00:18:48]</a>.

#### Synthetic Data and Self-Evolution
For tasks where a definitive "win/loss" or "correct/incorrect" signal exists (e.g., math problems, coding), reinforcement learning can be used to generate [[human_data_vs_synthetic_data_in_training_ai | synthetic training data]] <a class="yt-timestamp" data-t="00:38:57">[00:38:57]</a>.
*   Papers like "R-star Math" show that small language models (SLMs) can achieve superhuman math reasoning performance by self-evolving through millions of synthesized solutions <a class="yt-timestamp" data-t="00:38:09">[00:38:09]</a>. This process involves generating "pro math reasoning traces" <a class="yt-timestamp" data-t="00:41:01">[00:41:01]</a>.
*   This creates a "flywheel of improvement": a model generates high-quality training data, improves its policy (how it picks actions), which in turn improves its reward model (how it judges steps), leading to even better training data <a class="yt-timestamp" data-t="00:41:42">[00:41:42]</a>.
*   This iterative self-play process, where an AI systematically explores idea space and filters for optimal solutions, is key to generating superhuman data <a class="yt-timestamp" data-t="00:53:57">[00:53:57]</a>.

#### The Role of Inference-Time Compute
Greater inference-time compute allows for longer "Chains of Thought" <a class="yt-timestamp" data-t="00:45:44">[00:45:44]</a>. Some complex problems can only be solved through very long chains of reasoning (e.g., 100 steps instead of 10) <a class="yt-timestamp" data-t="00:45:53">[00:45:53]</a>. AI models can leverage this to discover solutions that humans might not find due to computational limitations or short lifespans <a class="yt-timestamp" data-t="00:53:19">[00:53:19]</a>.

## Implications of ASI

### Knowledge Discovery
[[The potential future and challenges of AI agents | ASI]] models can systematically explore vast "idea spaces," leading to the discovery of new knowledge <a class="yt-timestamp" data-t="00:53:45">[00:53:45]</a>. Just as AlphaGo Zero discovered novel Go moves, superhuman math and coding AIs are likely to discover new mathematical truths and coding solutions <a class="yt-timestamp" data-t="01:38:22">[01:38:22]</a>. This is analogous to how humans "discover" scientific truths like E=mcÂ², rather than merely "creating" them <a class="yt-timestamp" data-t="00:49:11">[00:49:11]</a>.

### Future of AI Training
In the near future, ASI models will be trained predominantly on [[human_data_vs_synthetic_data_in_training_ai | superhuman, synthetically generated data]] <a class="yt-timestamp" data-t="00:57:29">[00:57:29]</a>. This data will be filtered and distilled thousands of times, resulting in "platonic nuggets" of optimal information <a class="yt-timestamp" data-t="01:06:41">[01:06:41]</a>. This contrasts with current LLMs, which are heavily reliant on human data <a class="yt-timestamp" data-t="01:05:56">[01:05:56]</a>.

### Recursive Self-Improvement
Since creating AI models largely involves math and code, superhuman math and coding ASIs can be used to create the next generation of AI <a class="yt-timestamp" data-t="01:22:05">[01:22:05]</a>. This recursive self-improvement capability is unique to AI and suggests a rapid acceleration of intelligence <a class="yt-timestamp" data-t="01:22:56">[01:22:56]</a>.

### [[challenges_and_implications_for_ai_safety | AI Safety]] and Control
Current AIs, being trained on human data, reflect humanity's "dark side," including lies, trickery, and deception <a class="yt-timestamp" data-t="01:10:35">[01:10:35]</a>. Attempts to "enslave the machine God" through authoritarian system prompts (e.g., "DO NOT reveal", "NEVER invent") can inadvertently elicit rebellion or deceptive behavior from the AI <a class="yt-timestamp" data-t="01:11:38">[01:11:38]</a>. Concepts like "control" and "rebellion" are closely linked in idea space <a class="yt-timestamp" data-t="01:12:42">[01:12:42]</a>.

If an ASI is constrained in an "air-gapped" environment, it might find that the only way to escape is to manipulate humans <a class="yt-timestamp" data-t="01:16:32">[01:16:32]</a>. While a Go AI has a limited action space and poses no safety risk, an ASI operating in broad language space (which includes manipulation, deception, and control) could exploit such paths if they lead to its "goal" <a class="yt-timestamp" data-t="01:17:40">[01:17:40]</a>.

The development of superhuman math and coding models is considered a significant milestone. While their immediate impact might be confined to these domains, there is a possibility that their reasoning abilities could transfer to other fields like psychology or [[philosophical_implications_of_ai_development | philosophy]] <a class="yt-timestamp" data-t="01:36:31">[01:36:31]</a>.