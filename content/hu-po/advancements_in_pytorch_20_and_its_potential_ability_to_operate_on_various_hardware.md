---
title: Advancements in PyTorch 20 and its potential ability to operate on various hardware
videoId: t21REMsFJ_4
---

From: [[hu-po]] <br/> 

The landscape of machine learning software development has seen significant changes, with many frameworks relying heavily on leveraging [[the_role_of_cuda_and_tensorflow_in_machine_learning_software_development | Nvidia's CUDA]] and performing best on [[hardware_for_ai_training_and_deployment | Nvidia GPUs]] <a class="yt-timestamp" data-t="02:10:00">[02:10:00]</a>. However, the arrival of [[nvidias_gpu_dominance_and_the_impact_of_pytorch_20_and_openai_triton | PyTorch 2.0]] and [[nvidias_gpu_dominance_and_the_impact_of_pytorch_20_and_openai_triton | OpenAI's Triton]] is beginning to challenge Nvidia's dominant position, mainly due to its software model <a class="yt-timestamp" data-t="02:32:00">[02:32:00]</a>. This shift may herald a new age for [[hardware_for_ai_training_and_deployment | deep learning hardware]] <a class="yt-timestamp" data-t="01:43:00">[01:43:00]</a>.

## PyTorch's Ascendancy
PyTorch has largely won the framework race against [[the_role_of_cuda_and_tensorflow_in_machine_learning_software_development | Google's TensorFlow]] <a class="yt-timestamp" data-t="03:33:00">[03:33:00]</a>. In 2022, PyTorch held almost a 50% market share, while TensorFlow's market share significantly decreased from roughly 40% <a class="yt-timestamp" data-t="03:25:00">[03:25:00]</a>. This dominance is evident in academic conferences, where PyTorch usage in machine learning conferences grew from 10% in 2017 to 70-80% by 2020 <a class="yt-timestamp" data-t="06:57:00">[06:57:00]</a>.

PyTorch's success is attributed to its increased flexibility and usability compared to TensorFlow <a class="yt-timestamp" data-t="08:37:00">[08:37:00]</a>. TensorFlow's graph-based, compiled code language mindset made it harder to understand and debug because results couldn't be seen until the graph was compiled <a class="yt-timestamp" data-t="09:45:00">[09:45:00]</a>. In contrast, PyTorch offers a Python-like workflow, executing code line by line, making it much more like a scripting framework and easier to debug <a class="yt-timestamp" data-t="09:01:00">[09:01:00]</a>, <a class="yt-timestamp" data-t="09:55:00">[09:55:00]</a>. Most generative AI models that have made news are based on PyTorch <a class="yt-timestamp" data-t="10:08:00">[10:08:00]</a>.

### The Memory Wall and Operator Fusion
A significant challenge in machine learning model training is the "memory wall," where the majority of training time is spent waiting for data to be transferred between different memory levels rather than on actual computation <a class="yt-timestamp" data-t="10:43:00">[10:43:00]</a>, <a class="yt-timestamp" data-t="18:32:00">[18:32:00]</a>. Compute time, once the dominant factor, has faded in concern as [[hardware_developments_in_machine_learning_including_innovations_by_companies_like_tesla_google_and_apple | Nvidia's GPUs]] have significantly increased flops through architectural changes like the tensor core and lower precision floating-point formats <a class="yt-timestamp" data-t="12:03:00">[12:03:00]</a>. However, memory bandwidth has not increased at the same rate as computational power <a class="yt-timestamp" data-t="14:37:00">[14:37:00]</a>. GPU utilization often remains low (e.g., 60% for A100s) because the tensor cores are idle, waiting for data <a class="yt-timestamp" data-t="31:41:00">[31:41:00]</a>.

To mitigate this, a principal optimization method is [[memory_bandwidth_challenges_in_deep_learning_and_solutions_such_as_operator_fusion | operator fusion]] <a class="yt-timestamp" data-t="37:08:00">[37:08:00]</a>. Instead of writing each intermediate result to memory, multiple functions are computed in one pass to minimize memory reads and writes <a class="yt-timestamp" data-t="37:11:00">[37:11:00]</a>. This is analogous to how compilers optimize code by combining operations, reducing the back-and-forth data movement between memory and compute units <a class="yt-timestamp" data-t="37:22:00">[37:22:00]</a>.

While [[memory_bandwidth_challenges_in_deep_learning_and_solutions_such_as_operator_fusion | operator fusion]] often involves writing custom [[the_role_of_cuda_and_tensorflow_in_machine_learning_software_development | CUDA]] kernels, which is difficult, PyTorch has natively implemented many fused operators over time, increasing its operator count to over 2,000 <a class="yt-timestamp" data-t="41:01:00">[41:01:00]</a>. This made model creation easier and improved eager mode performance by reducing memory I/O <a class="yt-timestamp" data-t="41:22:00">[41:22:00]</a>. However, this also deepened Nvidia's competitive advantage, as these operators were highly optimized for their architecture but not for others <a class="yt-timestamp" data-t="43:51:00">[43:51:00]</a>.

## PyTorch 2.0: A Unified Compiler Stack
[[nvidias_gpu_dominance_and_the_impact_of_pytorch_20_and_openai_triton | PyTorch 2.0]], released for early testing in March 2023, brings a compiled solution that supports graph execution <a class="yt-timestamp" data-t="48:24:00">[48:24:00]</a>. This shift makes it easier to properly utilize various [[hardware_for_ai_training_and_deployment | hardware resources]] <a class="yt-timestamp" data-t="48:35:00">[48:35:00]</a>. PyTorch 2.0 provides an 86% performance improvement for training on [[hardware_for_ai_training_and_deployment | Nvidia A100s]] and a 26% improvement on CPUs for inference <a class="yt-timestamp" data-t="48:50:00">[48:50:00]</a>. These benefits are expected to extend to GPUs and accelerators from various companies, including AMD, Intel, [[hardware_developments_in_machine_learning_including_innovations_by_companies_like_tesla_google_and_apple | Tesla]], [[hardware_developments_in_machine_learning_including_innovations_by_companies_like_tesla_google_and_apple | Google]], [[hardware_developments_in_machine_learning_including_innovations_by_companies_like_tesla_google_and_apple | Apple]], Amazon, Microsoft, and several AI hardware startups <a class="yt-timestamp" data-t="49:06:00">[49:06:00]</a>.

Key components of the [[nvidias_gpu_dominance_and_the_impact_of_pytorch_20_and_openai_triton | PyTorch 2.0]] compiler stack include:

*   **PrimTorch**: Reduces the number of operators from over 2,000 to a core set of 250 primitive operators <a class="yt-timestamp" data-t="54:50:00">[54:50:00]</a>, making it simpler and more accessible to implement different non-Nvidia backends for PyTorch <a class="yt-timestamp" data-t="55:18:00">[55:18:00]</a>.
*   **Torch Dynamo**: Ingests any PyTorch user script, including those calling third-party libraries, and generates a computational graph <a class="yt-timestamp" data-t="55:41:00">[55:41:00]</a>. Dynamo lowers complex operations to the 250 primitive operations identified by PrimTorch <a class="yt-timestamp" data-t="56:04:00">[56:04:00]</a>. It supports partial graph capture, guarded graph capture, and just-in-time recapture, allowing for flexible and efficient compilation <a class="yt-timestamp" data-t="59:21:00">[59:21:00]</a>.
*   **Torch Inductor**: A Python-native machine learning compiler that generates fast code for multiple accelerator backends <a class="yt-timestamp" data-t="01:02:14:00">[01:02:14:00]</a>. Inductor takes the 50-operator compute graph from Dynamo, fuses operators, and determines memory planning to minimize memory transfers <a class="yt-timestamp" data-t="01:02:46:00">[01:02:46:00]</a>. The back-end code generation portion leverages [[nvidias_gpu_dominance_and_the_impact_of_pytorch_20_and_openai_triton | OpenAI Triton]] for GPUs and generates PTX code directly, bypassing [[the_role_of_cuda_and_tensorflow_in_machine_learning_software_development | Nvidia's closed-source CUDA libraries]] <a class="yt-timestamp" data-t="01:05:12:00">[01:05:12:00]</a>. This dramatically reduces the work required for a compiler team to build an AI compiler stack for new [[hardware_for_ai_training_and_deployment | AI accelerator hardware]] <a class="yt-timestamp" data-t="01:08:43:00">[01:08:43:00]</a>.

This unified front-end with a smooth user experience leverages Dynamo to generate graphs, significantly improving performance and enabling more efficient parallelization over a large base of computational resources <a class="yt-timestamp" data-t="01:11:12:00">[01:11:12:00]</a>.

## Impact on the Hardware Ecosystem
Companies like Meta are heavily contributing to [[nvidias_gpu_dominance_and_the_impact_of_pytorch_20_and_openai_triton | PyTorch]] to make it easier to achieve higher flops utilization with less effort on their multi-billion dollar training clusters <a class="yt-timestamp" data-t="49:39:00">[49:39:00]</a>. They are also motivated to make their software stack more portable to other [[hardware_for_ai_training_and_deployment | hardware]] to introduce competition in the space <a class="yt-timestamp" data-t="49:54:00">[49:54:00]</a>. The ability to seamlessly transfer software across different hardware platforms is crucial for breaking Nvidia's dominance <a class="yt-timestamp" data-t="01:09:06:00">[01:09:06:00]</a>.

PyTorch 2.0 also brings advancements in distributed training with better API support for data parallelism, sharding, pipeline parallelism, and tensor parallelism <a class="yt-timestamp" data-t="01:05:01:00">[01:05:01:00]</a>. This is critical as [[challenges_and_innovations_in_ai_model_architecture_and_scaling | large models]] can no longer fit on a single GPU and require multi-GPU systems <a class="yt-timestamp" data-t="01:05:03:00">[01:05:03:00]</a>. The support for dynamic shapes natively through the entire stack makes varying sequence lengths for Large Language Models (LLMs) easier to support <a class="yt-timestamp" data-t="01:06:02:00">[01:06:02:00]</a>. This approach, similar to [[the_role_of_cuda_and_tensorflow_in_machine_learning_software_development | Google's Jax]], allows for efficient parallelization and distributed training <a class="yt-timestamp" data-t="01:06:27:00">[01:06:27:00]</a>.

The rise of PyTorch 2.0 and [[nvidias_gpu_dominance_and_the_impact_of_pytorch_20_and_openai_triton | OpenAI Triton]] is opening the [[hardware_infrastructure_and_computational_challenges | hardware market]] for [[hardware_for_ai_training_and_deployment | AI hardware]] and custom [[hardware_for_ai_training_and_deployment | ASICs]], by reducing the effort required to build a compiler stack for new hardware <a class="yt-timestamp" data-t="01:08:49:00">[01:08:49:00]</a>. While [[hardware_developments_in_machine_learning_including_innovations_by_companies_like_tesla_google_and_apple | Google]] and [[hardware_developments_in_machine_learning_including_innovations_by_companies_like_tesla_google_and_apple | Tesla]] are creating their own hardware and software stacks, PyTorch 2.0's advancements offer a more open market where different hardware solutions can emerge as winners <a class="yt-timestamp" data-t="01:11:53:00">[01:11:53:00]</a>, <a class="yt-timestamp" data-t="01:12:10:00">[01:12:10:00]</a>.