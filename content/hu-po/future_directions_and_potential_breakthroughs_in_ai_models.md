---
title: Future directions and potential breakthroughs in AI models
videoId: _TghtP0ZyoM
---

From: [[hu-po]] <br/> 

The release of Llama 3.1 is considered a very strong paradigm shift in AI <a class="yt-timestamp" data-t="00:04:06">[00:04:06]</a>, <a class="yt-timestamp" data-t="00:04:11">[00:04:11]</a>, despite language models potentially reaching the top of their current performance S-curve <a class="yt-timestamp" data-t="02:21:47">[02:21:47]</a>. Future advancements will likely require new approaches, moving beyond simple scaling of existing architectures.

## Redefining AI Products

The current focus on developing AI for traditional applications like search engines (e.g., GPT search) is viewed as "uncreative" and primarily aimed at short-term stock price increases <a class="yt-timestamp" data-t="00:07:23">[00:07:23]</a>, <a class="yt-timestamp" data-t="00:07:33">[00:07:33]</a>. The future leader in the AI market is expected to emerge from "some other kind of weird product that we don't actually even know about yet" <a class="yt-timestamp" data-t="00:08:03">[00:08:03]</a>, <a class="yt-timestamp" data-t="00:08:07">[00:08:07]</a>. Frontier AI companies should prioritize creative ideas and focus on discovering these next-generation products <a class="yt-timestamp" data-t="00:08:11">[00:08:11]</a>. An example of a more promising direction is a locally running AI assistant on a cell phone, as seen in the partnership between OpenAI and Apple <a class="yt-timestamp" data-t="00:09:35">[00:09:35]</a>, <a class="yt-timestamp" data-t="00:09:44">[00:09:44]</a>, <a class="yt-timestamp" data-t="00:09:46">[00:09:46]</a>.

## [[Challenges and Advancements in AI Research | Scaling Laws as Hypotheses]]

The term "scaling laws" in AI is reconsidered as "scaling hypotheses" <a class="yt-timestamp" data-t="00:17:30">[00:17:30]</a>, <a class="yt-timestamp" data-t="00:17:39">[00:17:39]</a>, due to their potential to be "noisy and unreliable" <a class="yt-timestamp" data-t="00:18:04">[00:18:04]</a> and possibly not maintaining their trend indefinitely <a class="yt-timestamp" data-t="00:18:21">[00:18:21]</a>. Instead, a key observation is that as AI models increase in size, the need for numerous specialized "tricks and hacks" actually diminishes <a class="yt-timestamp" data-t="00:37:15">[00:37:15]</a>, <a class="yt-timestamp" data-t="00:37:50">[00:37:50]</a>. This simplification as models scale up could potentially ease the path to [[Potential future impacts and predictions of AI agents | AGI]] <a class="yt-timestamp" data-t="00:38:08">[00:38:08]</a>, <a class="yt-timestamp" data-t="00:38:11">[00:38:11]</a>.

## Next S-Curves in AI

Technology often progresses in S-curves, reaching a limit for one approach before a new S-curve is discovered <a class="yt-timestamp" data-t="02:22:13">[02:22:13]</a>, <a class="yt-timestamp" data-t="02:22:20">[02:22:20]</a>, <a class="yt-timestamp" data-t="02:22:23">[02:22:23]</a>. Current large language models are believed to be nearing the peak of their S-curve <a class="yt-timestamp" data-t="00:45:26">[00:45:26]</a>, <a class="yt-timestamp" data-t="02:21:52">[02:21:52]</a>, <a class="yt-timestamp" data-t="02:21:56">[02:21:56]</a>. Future breakthroughs are anticipated to arise from:

*   **Hybrid Architectures:** Incorporating linear attention models like [[Limitations and potential of Mamba models in AI | Mambas]] and LSTMs into more complex architectures <a class="yt-timestamp" data-t="00:45:58">[00:45:58]</a>, <a class="yt-timestamp" data-t="00:46:07">[00:46:07]</a>, <a class="yt-timestamp" data-t="02:28:07">[02:28:07]</a>.
*   **Multimodality:** Integrating modalities such as image, video, and speech more natively and deeply into AI models <a class="yt-timestamp" data-t="00:49:17">[00:49:17]</a>, <a class="yt-timestamp" data-t="02:22:00">[02:22:00]</a>. This would unlock the next 10x in data for training <a class="yt-timestamp" data-t="02:28:22">[02:28:22]</a>, <a class="yt-timestamp" data-t="02:28:24">[02:28:24]</a>. The Chameleon paper from Meta is highlighted as a more representative example of future multimodal approaches, which can interleave and generate both image and text in their output <a class="yt-timestamp" data-t="02:09:51">[02:09:51]</a>, <a class="yt-timestamp" data-t="02:10:12">[02:10:12]</a>.
*   **Energy Solutions:** Addressing energy consumption as a bottleneck in large-scale AI training <a class="yt-timestamp" data-t="00:58:39">[00:58:39]</a>, <a class="yt-timestamp" data-t="00:58:41">[00:58:41]</a>. This might involve designing specialized power plants (e.g., fusion or nuclear) tied directly to data centers to handle variable power demands <a class="yt-timestamp" data-t="00:58:48">[00:58:48]</a>, <a class="yt-timestamp" data-t="00:59:08">[00:59:08]</a>, <a class="yt-timestamp" data-t="01:00:20">[01:00:20]</a>, <a class="yt-timestamp" data-t="02:28:36">[02:28:36]</a>.
*   **Hardware Specialization:** A bifurcation of hardware development for training (requiring high precision) and inference (allowing low precision like FP8 for higher throughput) is anticipated <a class="yt-timestamp" data-t="02:05:04">[02:05:04]</a>, <a class="yt-timestamp" data-t="02:05:09">[02:05:09]</a>, <a class="yt-timestamp" data-t="02:05:26">[02:05:26]</a>.

## Advancements in Training and Data

### Data Curation and Synthesis

[[Challenges and Advancements in AI Research | Data quality]] and diversity are paramount for performance gains <a class="yt-timestamp" data-t="00:38:34">[00:38:34]</a>. Techniques like de-duplication, quality classification using models, and careful determination of data mix are crucial <a class="yt-timestamp" data-t="00:29:31">[00:29:31]</a>, <a class="yt-timestamp" data-t="00:32:35">[00:32:35]</a>, <a class="yt-timestamp" data-t="00:33:50">[00:33:50]</a>. A key trend is the use of [[SelfImprovement in AI Models | synthetic data generation]], particularly where outputs can be programmatically verified (e.g., code, math) <a class="yt-timestamp" data-t="01:31:03">[01:31:03]</a>, <a class="yt-timestamp" data-t="01:31:13">[01:31:13]</a>. This self-improvement flywheel, where models generate and filter their own training data, is seen as crucial for reaching superhuman performance in these domains <a class="yt-timestamp" data-t="01:33:48">[01:33:48]</a>, <a class="yt-timestamp" data-t="01:34:00">[01:34:00]</a>. Human annotators are also becoming more involved and specialized, performing multi-turn dialogues and editing responses for finer control <a class="yt-timestamp" data-t="01:20:49">[01:20:49]</a>, <a class="yt-timestamp" data-t="01:21:20">[01:21:20]</a>, <a class="yt-timestamp" data-t="01:23:40">[01:23:40]</a>.

### Dynamic Training Schedules

Training schedules are becoming increasingly dynamic, adjusting batch sizes, context lengths, and data mixtures over time <a class="yt-timestamp" data-t="01:17:29">[01:17:29]</a>, <a class="yt-timestamp" data-t="01:18:14">[01:18:14]</a>. This contrasts with traditional fixed-hyperparameter training.

### Expert Training

While the long-term goal for [[Potential future impacts and predictions of AI agents | AGI]] might be a single model capable of everything <a class="yt-timestamp" data-t="01:29:51">[01:29:51]</a>, current trends include training "expert" models through continued pre-training on domain-specific data (e.g., a code expert, a multilingual expert) to achieve superior performance on benchmarks <a class="yt-timestamp" data-t="01:28:12">[01:28:12]</a>, <a class="yt-timestamp" data-t="01:35:58">[01:35:58]</a>.

## Model Robustness and Evaluation

The current benchmarks used to evaluate AI models are increasingly saturated and can be manipulated, with performance often influenced by subtle changes like the format of multiple-choice answers <a class="yt-timestamp" data-t="01:50:00">[01:50:00]</a>, <a class="yt-timestamp" data-t="01:50:31">[01:50:31]</a>, <a class="yt-timestamp" data-t="01:52:25">[01:52:25]</a>. Many benchmarks are also contaminated by being present in the pre-training corpus, making them tests of memorization rather than reasoning <a class="yt-timestamp" data-t="01:53:46">[01:53:46]</a>. This highlights the need for new, more robust benchmarks for evaluating AI capabilities <a class="yt-timestamp" data-t="01:50:16">[01:50:16]</a>.

## Integration of External Tools

A significant direction is enhancing models to utilize external tools like search engines, code interpreters, and sophisticated calculators (e.g., Wolfram Alpha API) <a class="yt-timestamp" data-t="01:37:30">[01:37:30]</a>, <a class="yt-timestamp" data-t="01:38:18">[01:38:18]</a>, <a class="yt-timestamp" data-t="01:46:19">[01:46:19]</a>. This approach helps address the "hallucination problem" by offloading factual knowledge and complex calculations to reliable external sources, allowing the language model to focus on reasoning and tool invocation <a class="yt-timestamp" data-t="01:38:17">[01:38:17]</a>, <a class="yt-timestamp" data-t="01:46:38">[01:46:38]</a>. Python is emerging as a de-facto standard for defining these tools, due to its close resemblance to natural English <a class="yt-timestamp" data-t="01:47:12">[01:47:12]</a>, <a class="yt-timestamp" data-t="01:47:50">[01:47:50]</a>.

## Organizational Changes

The complexity of training massive AI models leads to new engineering specializations, particularly in data center maintenance and optimization, including understanding the unique power consumption behaviors of GPU training workloads <a class="yt-timestamp" data-t="01:13:52">[01:13:52]</a>, <a class="yt-timestamp" data-t="01:15:08">[01:15:08]</a>, <a class="yt-timestamp" data-t="01:15:30">[01:15:30]</a>. Organizational structures within AI companies are also adapting, with data procurement teams potentially separated and subject to NDAs to manage legal risks associated with training data sources <a class="yt-timestamp" data-t="02:23:31">[02:23:31]</a>, <a class="yt-timestamp" data-t="02:24:03">[02:24:03]</a>.

## Open Source vs. Closed Source

The increasing performance of open-source models like Llama 3, which is on par with closed-source counterparts like GPT-4 <a class="yt-timestamp" data-t="01:57:05">[01:57:05]</a>, changes the AI market significantly <a class="yt-timestamp" data-t="02:00:20">[02:00:20]</a>, <a class="yt-timestamp" data-t="02:00:20">[02:00:20]</a>. This trend challenges the "secret sauce" advantage of closed-source companies and emphasizes the role of open source in responsible AI development <a class="yt-timestamp" data-t="02:25:32">[02:25:32]</a>, <a class="yt-timestamp" data-t="02:25:39">[02:25:39]</a>, <a class="yt-timestamp" data-t="02:25:42">[02:25:42]</a>.