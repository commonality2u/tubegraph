---
title: Future potential of 3D diffusion models
videoId: Z6dB1zIfwr4
---

From: [[hu-po]] <br/> 

[[future_potential_and_direction_for_generative_3d_technology | 3D diffusion models]], particularly those like [[dreamfusion_and_its_relation_to_diffusion_models | DreamFusion]] which convert text prompts into 3D models, are seen as being in their early stages but possessing immense future potential <a class="yt-timestamp" data-t="00:01:53">[00:01:53]</a>. This area of research is considered a "really, really interesting kind of thread" <a class="yt-timestamp" data-t="01:20:15">[01:20:15]</a>.

## Disruptive Impact

While [[diffusion_models_and_image_generation | 2D image generation]] has already proven disruptive to the art community, [[techniques_for_text_to_3d_conversion_involving_diffusion_models | 3D generation]] is anticipated to be "absolutely insane" <a class="yt-timestamp" data-t="00:01:31">[00:01:31]</a>. The ability to create 3D models with just text prompts will be incredibly impactful <a class="yt-timestamp" data-t="00:01:49">[00:01:49]</a>, especially given that 3D modeling is a difficult and time-consuming profession for many <a class="yt-timestamp" data-t="00:01:41">[00:01:41]</a>.

Key areas of disruption include:
*   **Video Games** <a class="yt-timestamp" data-t="00:01:36">[00:01:36]</a>
*   **3D Modeling Industry** <a class="yt-timestamp" data-t="00:01:43">[00:01:43]</a>

## Long-Term Vision

The speaker believes [[future_potential_and_direction_for_generative_3d_technology | 3D diffusion models]] are "absolutely massive" <a class="yt-timestamp" data-t="01:20:21">[01:20:21]</a> and represent "our future" <a class="yt-timestamp" data-t="01:20:27">[01:20:27]</a>. This technology could lead to a future where users interact with virtual reality (VR) headsets by simply speaking commands to generate desired objects and scenes, ushering in the next iteration of the metaverse <a class="yt-timestamp" data-t="01:20:29">[01:20:29]</a>.

## Areas for Growth and Improvement

Despite the impressive initial results, there is "so much more room to grow" in [[techniques_for_text_to_3d_conversion_involving_diffusion_models | 3D generation]] and Nerf rendering <a class="yt-timestamp" data-t="01:08:18">[01:08:18]</a>. Potential improvements include:
*   **Larger Nerf MLPs**: Making the Neural Radiance Field (Nerf) Multi-Layer Perceptron (MLP) significantly larger, possibly integrating a [[scalability_of_transformerbased_diffusion_models | Transformer]] architecture <a class="yt-timestamp" data-t="01:07:48">[01:07:48]</a>.
*   **Pre-training on Extensive Datasets**: Pre-training Nerf MLPs on millions of objects could lead to much more powerful models <a class="yt-timestamp" data-t="01:07:56">[01:07:56]</a>.
*   **Hardware and Algorithm Advancements**: Realizing these advancements will require more [[scaling_and_optimization_in_diffusion_models | powerful GPUs]] and further improvements in the Nerf algorithm itself <a class="yt-timestamp" data-t="01:08:08">[01:08:08]</a>.