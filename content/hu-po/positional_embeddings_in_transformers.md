---
title: Positional embeddings in Transformers
videoId: PFxi6SmozZ4
---

From: [[hu-po]] <br/> 

Positional embeddings (PE) are a crucial component in [[Challenges and insights in Transformer architecture and training|Transformer]] architectures, providing essential information about the order of input tokens in a sequence <a class="yt-timestamp" data-t="02:30:30">[02:30:30]</a>, <a class="yt-timestamp" data-t="02:51:52">[02:51:52]</a>. Unlike recurrent neural networks (RNNs), which inherently process data sequentially and maintain state, Transformers compute attention between all tokens simultaneously, meaning they lack an inherent understanding of position <a class="yt-timestamp" data-t="02:07:07">[02:07:07]</a>, <a class="yt-timestamp" data-t="02:29:55">[02:29:55]</a>. Without explicit positional information, the model would treat a sequence as a "bag of words," losing critical semantic meaning derived from word order <a class="yt-timestamp" data-t="02:12:10">[02:12:10]</a>.

The concept of positional embeddings has evolved significantly, leading to sophisticated methods like "Long Rope," a recent innovation that aims to drastically extend the context length of large language models (LLMs) <a class="yt-timestamp" data-t="02:02:02">[02:02:02]</a>, <a class="yt-timestamp" data-t="03:36:38">[03:36:38]</a>. This development is particularly relevant in light of models like Google's Gemini 1.5, which boasts state-of-the-art context length (10 million tokens) through undisclosed "secret sauce" innovations <a class="yt-timestamp" data-t="02:24:26">[02:24:26]</a>, <a class="yt-timestamp" data-t="02:53:05">[02:53:05]</a>.

## How Positional Embeddings Work <a class="yt-timestamp" data-t="05:54:50">[05:54:50]</a>

When an image is fed into a [[Applications in Vision Transformers|Vision Transformer]] (ViT), it's first turned into a sequence of visual patches or tokens <a class="yt-timestamp" data-t="06:09:09">[06:09:09]</a>. Similarly, for language models, text is tokenized into word pieces <a class="yt-timestamp" data-t="08:05:00">[08:05:00]</a>, <a class="yt-timestamp" data-t="08:58:00">[08:58:00]</a>. These tokens are then represented by "token embeddings," which are high-dimensional vectors capturing their semantic meaning <a class="yt-timestamp" data-t="08:48:00">[08:48:00]</a>, <a class="yt-timestamp" data-t="09:51:00">[09:51:00]</a>.

Positional embeddings are distinct vectors, typically learned or hand-designed, that correspond to each position in the input sequence <a class="yt-timestamp" data-t="10:05:00">[10:05:00]</a>, <a class="yt-timestamp" data-t="10:27:00">[10:27:00]</a>. These PE vectors are combined with the token embeddings (often by simple addition) before being fed into the Transformer's self-attention mechanism <a class="yt-timestamp" data-t="10:48:00">[10:48:00]</a>, <a class="yt-timestamp" data-t="11:09:00">[11:09:00]</a>. This combined embedding allows the Transformer to understand the relative position of tokens and how they relate to each other in the sequence <a class="yt-timestamp" data-t="13:51:00">[13:51:00]</a>.

### Rotary Position Embeddings (RoPE) <a class="yt-timestamp" data-t="12:43:00">[12:43:00]</a>
[[Rotary Position Embeddings and Long Contexts|Rotary Position Embeddings (RoPE)]] were introduced in the 2021 "RoFormer" paper <a class="yt-timestamp" data-t="12:47:00">[12:47:00]</a>. RoPE aims to encode absolute position within a rotation matrix while incorporating relative position dependency into the self-attention mechanism <a class="yt-timestamp" data-t="16:14:00">[16:14:00]</a>. Key properties of RoPE include:
*   **Flexibility with Sequence Length**: Allows for easy extension of context <a class="yt-timestamp" data-t="16:28:00">[16:28:00]</a>.
*   **Decaying Inter-Token Dependency**: The similarity (dot product) between tokens decreases as their relative distance increases <a class="yt-timestamp" data-t="16:30:00">[16:30:00]</a>, <a class="yt-timestamp" data-t="21:01:00">[21:01:00]</a>. This aligns with the intuition that closer tokens are more strongly related <a class="yt-timestamp" data-t="17:05:00">[17:05:00]</a>.

The mathematical intuition behind RoPE involves rotating token vectors in a high-dimensional space <a class="yt-timestamp" data-t="18:21:00">[18:21:00]</a>, <a class="yt-timestamp" data-t="19:18:00">[19:18:00]</a>. This rotation is achieved by applying a function based on cosine and sine terms <a class="yt-timestamp" data-t="30:40:00">[30:40:00]</a>. Each dimension of the PE vector can be thought of as having a different "frequency" of oscillation, influencing the rotation angles <a class="yt-timestamp" data-t="32:12:00">[32:12:00]</a>. These frequencies are determined by a hand-designed formula (e.g., involving a constant like 10,000) <a class="yt-timestamp" data-t="31:44:00">[31:44:00]</a>, <a class="yt-timestamp" data-t="45:18:00">[45:18:00]</a>. The ultimate goal is for the dot product between query (Q) and key (K) vectors in the self-attention mechanism to become dependent only on the *relative* position (`m - n`) of the tokens <a class="yt-timestamp" data-t="30:57:00">[30:57:00]</a>.

## Extending Context Lengths: The Road to Long Rope <a class="yt-timestamp" data-t="03:36:38">[03:36:38]</a>

The push for longer context windows in LLMs is driven by the desire to feed more information into the model at once, enabling more complex tasks <a class="yt-timestamp" data-t="03:16:00">[03:16:00]</a>, <a class="yt-timestamp" data-t="03:35:00">[03:35:00]</a>. However, simply extrapolating existing positional embeddings to longer sequences can lead to "crowding" of positional information, hindering the model's ability to distinguish closely positioned tokens <a class="yt-timestamp" data-t="05:50:00">[05:50:00]</a>.

Several approaches built upon RoPE to address context extension:
*   **Position Interpolation (PI)**: Introduced by Meta in 2023, PI interpolates existing RoPE to extend context. It naively scales the `m` (position index) by a ratio `s` (extended length / original length) <a class="yt-timestamp" data-t="04:47:00">[04:47:00]</a>, <a class="yt-timestamp" data-t="05:00:00">[05:00:00]</a>.
*   **NTK-based Interpolation**: This method, and subsequent ones like Yarn, introduced "non-uniformities" by rescaling based on different frequency dimensions within the RoPE vector <a class="yt-timestamp" data-t="05:30:00">[05:30:00]</a>, <a class="yt-timestamp" data-t="05:53:00">[05:53:00]</a>. Yarn, for example, divides RoPE dimensions into three frequency-based groups, each with a different interpolation strategy <a class="yt-timestamp" data-t="05:10:00">[05:10:00]</a>. These methods often rely on "human-led empirical experiments" to define rules and boundaries <a class="yt-timestamp" data-t="05:11:00">[05:11:00]</a>, <a class="yt-timestamp" data-t="05:59:00">[05:59:00]</a>.

## Long Rope: An Evolutionary Approach <a class="yt-timestamp" data-t="03:55:00">[03:55:00]</a>

Long Rope, a paper by Microsoft Research (February 2024), represents a further refinement in [[Rotary Position Embeddings and Long Contexts|RoPE]]-based context extension <a class="yt-timestamp" data-t="03:56:00">[03:56:00]</a>. It extends pre-trained LLMs to an impressive 2048K tokens while maintaining the original architecture with minor modifications to positional embeddings <a class="yt-timestamp" data-t="03:50:00">[03:50:00]</a>, <a class="yt-timestamp" data-t="03:57:00">[03:57:00]</a>.

Long Rope introduces three key innovations:
1.  **Exploiting Non-Uniformities via Evolutionary Search**: Instead of relying on human-designed rules for positional interpolation (like in Yarn), Long Rope uses an evolutionary search algorithm to discover optimal "rescale factors" (`Lambda_I`) for RoPE's rotation angles based on token positions <a class="yt-timestamp" data-t="04:29:00">[04:29:00]</a>, <a class="yt-timestamp" data-t="04:40:00">[04:40:00]</a>, <a class="yt-timestamp" data-t="05:57:00">[05:57:00]</a>, <a class="yt-timestamp" data-t="05:59:00">[05:59:00]</a>. This search also identifies an optimal `n_hat` value, representing the initial `n_hat` tokens whose position embeddings are *not* interpolated, as they tend to receive large attention scores <a class="yt-timestamp" data-t="06:06:00">[06:06:00]</a>, <a class="yt-timestamp" data-t="06:29:00">[06:29:00]</a>, <a class="yt-timestamp" data-t="07:44:00">[07:44:00]</a>.
    *   **Evolutionary Search Process**: A population of rescale factor configurations is initialized <a class="yt-timestamp" data-t="05:51:00">[05:51:00]</a>. These are mutated, and the LLM's perplexity is computed for each <a class="yt-timestamp" data-t="05:53:00">[05:53:00]</a>. The top-performing individuals (low perplexity) become "parents," generating variants, while poor performers are discarded <a class="yt-timestamp" data-t="05:56:00">[05:56:00]</a>. This iterative process, leveraging significant compute (e.g., 8 to 16 A100 GPUs) <a class="yt-timestamp" data-t="06:09:00">[06:09:00]</a>, finds empirically optimal values <a class="yt-timestamp" data-t="06:14:00">[06:14:00]</a>.
2.  **Progressive Extension Strategy**: The model is fine-tuned iteratively, starting with a smaller context length and gradually increasing it <a class="yt-timestamp" data-t="06:04:00">[06:04:00]</a>. This strategy provides better initialization for fine-tuning <a class="yt-timestamp" data-t="06:20:00">[06:20:00]</a>.

### Performance <a class="yt-timestamp" data-t="06:09:00">[06:09:00]</a>

Long Rope demonstrates significant improvements over previous methods like PI and Yarn <a class="yt-timestamp" data-t="06:03:00">[06:03:00]</a>:
*   **Perplexity**: Long Rope (Long Rope Mistral and Long Rope Llama) maintains low perplexity even at 2048K token context lengths, unlike other models that see an "explosion" in perplexity <a class="yt-timestamp" data-t="06:00:00">[06:00:00]</a>.
*   **Pass Key Retrieval Accuracy**: In "needle in the haystack" tasks (retrieving a hidden phrase in a long document), Long Rope achieves nearly 100% accuracy even at very long context lengths, while other models fail <a class="yt-timestamp" data-t="06:11:00">[06:11:00]</a>, <a class="yt-timestamp" data-t="06:11:00">[06:11:00]</a>.
*   **Benchmark Performance**: While extending context via Long Rope causes a slight decrease in performance on standard benchmarks like MMLU or H-Swag (e.g., Llama 2 7B's MMLU score drops from 46 to 39) <a class="yt-timestamp" data-t="06:14:00">[06:14:00]</a>, this trade-off is often acceptable given the immense increase in usable context <a class="yt-timestamp" data-t="06:14:00">[06:14:00]</a>.

## The Future of Positional Embeddings: The Bitter Lesson <a class="yt-timestamp" data-t="06:40:00">[06:40:00]</a>

Despite the advancements seen in Long Rope, the method still represents a highly engineered, human-designed heuristic <a class="yt-timestamp" data-t="05:54:00">[05:54:00]</a>, <a class="yt-timestamp" data-t="06:45:00">[06:45:00]</a>. This approach contrasts with the principle of "The Bitter Lesson" by Rich Sutton, which posits that general methods leveraging computation and learning ultimately prove more effective than specific human-designed knowledge <a class="yt-timestamp" data-t="07:30:00">[07:30:00]</a>.

Historically, in computer vision, hand-engineered features like Gabor filters (designed to detect edges) were eventually superseded by learned convolutional filters in convolutional neural networks (ConvNets) <a class="yt-timestamp" data-t="07:21:00">[07:21:00]</a>, <a class="yt-timestamp" data-t="07:51:00">[07:51:00]</a>. Similarly, while token embeddings are learned, positional embeddings are still largely hand-designed <a class="yt-timestamp" data-t="07:38:00">[07:38:00]</a>, <a class="yt-timestamp" data-t="07:51:00">[07:51:00]</a>.

Researchers have attempted to create [[Future of learnable position embeddings|learnable positional encodings]], but these have historically underperformed hand-designed versions <a class="yt-timestamp" data-t="07:40:00">[07:40:00]</a>, <a class="yt-timestamp" data-t="07:41:00">[07:41:00]</a>. However, following the Bitter Lesson, it's hypothesized that with increased scale and compute, [[Future of learnable position embeddings|learnable positional embeddings]] will eventually outperform all heuristically designed methods <a class="yt-timestamp" data-t="07:48:00">[07:48:00]</a>. This shift would align the development of positional embeddings with the general trend towards less inductive bias and more reliance on brute-force learning in [[Scaling of language models and vision transformers|scaling of language models and vision transformers]] <a class="yt-timestamp" data-t="07:55:00">[07:55:00]</a>, <a class="yt-timestamp" data-t="08:18:00">[08:18:00]</a>.

The increasing context lengths also raise questions about the future of Retrieval Augmented Generation (RAG). As models can ingest larger amounts of information directly into their context window, the overhead of querying external databases for relevant text might become unnecessary, potentially leading to a decline in RAG's utility for performance improvement <a class="yt-timestamp" data-t="07:09:00">[07:09:00]</a>, <a class="yt-timestamp" data-t="07:55:00">[07:55:00]</a>.

Finally, it's worth noting the distinction between Transformers and architectures like Mamba (a state-space model). Mamba inherently possesses a sequential nature, reducing the explicit need for positional embeddings, unlike the Transformer's attention mechanism that lacks this implicit bias <a class="yt-timestamp" data-t="07:41:00">[07:41:00]</a>, <a class="yt-timestamp" data-t="07:48:00">[07:48:00]</a>.