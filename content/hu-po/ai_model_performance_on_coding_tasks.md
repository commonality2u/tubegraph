---
title: AI model performance on coding tasks
videoId: yxAcTRp9EyQ
---

From: [[hu-po]] <br/> 

A recent paper titled "Textbooks Are All You Need" from Microsoft Research introduces Phi-1, a new language model specifically designed for code generation <a class="yt-timestamp" data-t="00:02:02">[00:02:02]</a>. This paper explores the significant improvements in [[Performance and efficiency in machine learning models | model performance]] that can be achieved by focusing on the quality of training data rather than solely on the quantity of data or model size <a class="yt-timestamp" data-t="00:08:26">[00:08:26]</a>.

## Introducing Phi-1

Phi-1 is a Transformer-based language model with 1.3 billion parameters, making it significantly smaller than many competing models like Falcon 40b (40 billion parameters) or LLaMA (65 billion parameters) <a class="yt-timestamp" data-t="00:02:28">[00:02:28]</a>. The model was trained for four days on eight A100 GPUs, which is considered a relatively low-cost training setup <a class="yt-timestamp" data-t="00:03:44">[00:03:44]</a>. The choice of eight A100s is significant as it represents the capacity of a single server rack <a class="yt-timestamp" data-t="00:56:06">[00:56:06]</a>.

Despite its smaller scale, Phi-1 achieves a "pass@1 accuracy" of 50% on HumanEval and 55% on the Mostly Basic Python Programs (MBPP) benchmark <a class="yt-timestamp" data-t="00:05:02">[00:05:02]</a>. The "pass@1 accuracy" means the model correctly answers a problem on its first attempt <a class="yt-timestamp" data-t="00:05:11">[00:05:11]</a>.

## The "Textbook Quality" Data Concept

The core hypothesis of this research is that higher quality data leads to better results, extending beyond traditional data cleaning to fundamentally change scaling laws <a class="yt-timestamp" data-t="00:09:05">[00:09:05]</a>. The paper argues that standard web-based data sources are not optimal for teaching models how to plan and reason <a class="yt-timestamp" data-t="01:0:07">[01:1:07]</a>. Instead, models would benefit from a training set that mimics a "good textbook": clear, self-contained, instructive, and balanced <a class="yt-timestamp" data-t="01:0:05">[01:1:05]</a>. This approach aims to match the [[Performance and efficiency in machine learning models | performance]] of large-scale models with much leaner [[Training and finetuning processes for AI models | training]] and smaller models <a class="yt-timestamp" data-t="00:09:13">[00:09:13]</a>.

The data used for Phi-1 consists of less than 7 billion tokens in total <a class="yt-timestamp" data-t="00:04:59">[00:04:59]</a>. This is significantly less than the hundreds of billions or even trillions of tokens used for larger models.

### Data Curation and Generation

The data strategy for Phi-1 involves three main components:

1.  **Filtered Code Language Data**: A subset of deduplicated Python files from "The Stack" (a 6-terabyte dataset of open-source code) and Stack Overflow <a class="yt-timestamp" data-t="00:44:27">[00:44:27]</a>. Manual inspection revealed that many snippets from these standard datasets were not instructive for learning basic coding, often lacking self-containment or being poorly documented <a class="yt-timestamp" data-t="00:23:12">[00:23:12]</a>. To filter this, the researchers used a [[Probing classifiers for understanding AI models | Transformer-based classifier]] trained on a small, human-annotated (or GPT-4 annotated) subset of data to predict the educational value of code snippets <a class="yt-timestamp" data-t="00:32:30">[00:32:30]</a>.
2.  **Synthetic Textbook Data**: Less than 1 billion tokens generated by GPT-3.5 <a class="yt-timestamp" data-t="00:41:29">[00:41:29]</a>. This data is designed to provide high-quality, natural language-heavy text interleaved with relevant code snippets, promoting reasoning and basic algorithmic skills <a class="yt-timestamp" data-t="00:41:36">[00:41:36]</a>. Diversity in this synthetic data was achieved by constraining topics and target audiences in the prompts given to GPT-3.5 <a class="yt-timestamp" data-t="00:42:12">[00:42:12]</a>.
3.  **Small Synthetic Exercises Data Set**: Less than 100 million tokens of Python exercises, also generated by GPT-3.5 <a class="yt-timestamp" data-t="00:43:51">[00:43:51]</a>. This dataset consists of function docstrings that need to be completed with code, aligning the model to perform function completion tasks based on natural language instructions <a class="yt-timestamp" data-t="00:43:58">[00:43:58]</a>. Diversity for this set was elicited by constraining function names <a class="yt-timestamp" data-t="00:44:10">[00:44:10]</a>.

The combination of filtered code language data and synthetic textbook data was used for the pre-training phase of Phi-1 (called "Phi-1 base"), which was then [[finetuning language models for specific tasks | fine-tuned]] on the synthetic exercises data to obtain the final Phi-1 model <a class="yt-timestamp" data-t="00:31:58">[00:31:58]</a>.

## [[AI model architecture and parallelism strategies | Model Architecture]] and [[Training and finetuning processes for AI models | Training]]

Phi-1 utilizes a decoder-only Transformer model, an [[AI model architecture and parallelism strategies | architecture]] popular for language models <a class="yt-timestamp" data-t="00:47:55">[00:47:55]</a>. It incorporates techniques like Flash Attention to reduce memory and compute footprint, and Rotary Position Embeddings (RoPE) <a class="yt-timestamp" data-t="00:48:31">[00:48:31]</a>.

The larger Phi-1 model (1.3 billion parameters) consists of 24 layers, a hidden dimension of 2048, and 32 attention heads <a class="yt-timestamp" data-t="00:50:22">[00:50:22]</a>. The smaller Phi-1 small model (350 million parameters) has 20 layers and smaller dimensions <a class="yt-timestamp" data-t="00:50:26">[00:50:26]</a>. Both models were trained using floating-point 16 precision <a class="yt-timestamp" data-t="00:54:03">[00:54:03]</a>.

For pre-training, Phi-1 base was trained for 36,000 steps with a batch size of 1024, equivalent to eight epochs over the code textbook data set (approximately 50 billion tokens) <a class="yt-timestamp" data-t="00:58:04">[00:58:04]</a>. Phi-1 was then obtained by [[finetuning language models for specific tasks | fine-tuning]] Phi-1 base on the code exercises data set for 6,000 steps <a class="yt-timestamp" data-t="01:00:09">[01:00:09]</a>. Notably, the learning rate for fine-tuning was smaller than for pre-training to preserve the acquired knowledge <a class="yt-timestamp" data-t="00:59:49">[00:59:49]</a>.

## [[Performance and efficiency in machine learning models | Performance]] and Results

Phi-1's [[Performance and efficiency in machine learning models | performance]] demonstrates the power of high-quality data in breaking existing scaling laws <a class="yt-timestamp" data-t="01:00:00">[01:00:00]</a>. Even without [[finetuning language models for specific tasks | fine-tuning]], Phi-1 base achieves 29% accuracy on HumanEval <a class="yt-timestamp" data-t="01:00:00">[01:00:00]</a>. After [[finetuning language models for specific tasks | fine-tuning]], Phi-1 achieves 50.6% accuracy on HumanEval and 55% on MBPP <a class="yt-timestamp" data-t="01:00:00">[01:00:00]</a>.

A significant finding is the emergence of unexpected coding capabilities after [[finetuning language models for specific tasks | fine-tuning]] <a class="yt-timestamp" data-t="01:00:00">[01:00:00]</a>. Despite the fine-tuning data consisting exclusively of short Python tasks, the model showed substantial improvement in tasks not featured in the fine-tuning set, such as managing intricate algorithmic tasks and using external libraries like Pygame and Tkinter <a class="yt-timestamp" data-t="01:03:10">[01:03:10]</a>. This suggests that fine-tuning helps reorganize and consolidate knowledge acquired during pre-training <a class="yt-timestamp" data-t="01:03:55">[01:03:55]</a>.

The paper highlights that Phi-1, despite being 10 times smaller and trained on 100x less data, outperforms almost all open-source models on coding benchmarks, including StarCoder and CodeGen <a class="yt-timestamp" data-t="01:21:00">[01:21:00]</a>.

## [[Challenges and strategies in model training and performance | Challenges]] and Limitations

### Benchmark Contamination
A major concern in evaluating AI models is "contamination," where a model's training data might include parts of the evaluation benchmarks, leading to inflated scores <a class="yt-timestamp" data-t="01:14:13">[01:14:13]</a>. The researchers addressed this by:
*   **Data Pruning**: Using a combination of embedding and syntax-based distances (Abstract Syntax Trees or ASTs) to measure similarity between code snippets and filter out highly similar examples from the training data <a class="yt-timestamp" data-t="01:27:51">[01:27:51]</a>. Even after aggressively pruning over 40% of the code exercises dataset, Phi-1 still outperformed StarCoder <a class="yt-timestamp" data-t="01:24:24">[01:24:24]</a>.
*   **Unconventional Evaluation**: Creating new, unconventional evaluation problems by a dedicated team that did not have access to the training data or the final model <a class="yt-timestamp" data-t="01:17:47">[01:17:47]</a>.

### API Usage and Context Sensitivity
The model's ability to correctly use external APIs can be a [[Challenges and strategies in model training and performance | challenge]], especially with APIs that change over time <a class="yt-timestamp" data-t="01:12:21">[01:12:21]</a>. The transcript suggests that providing explicit type hints in function definitions could help the model pick the correct functions <a class="yt-timestamp" data-t="01:39:51">[01:39:51]</a>.

### Prompt Sensitivity
Phi-1 demonstrates sensitivity to prompt variations, meaning small changes in the prompt can significantly affect the output <a class="yt-timestamp" data-t="01:40:33">[01:40:33]</a>. This is theorized to be a property of smaller models where their learned "compression" of the data distribution is more fragile compared to larger models <a class="yt-timestamp" data-t="01:40:46">[01:40:46]</a>. This issue might also arise because the training exercises predominantly consist of short prompts, hindering generalization to varied input lengths <a class="yt-timestamp" data-t="01:42:26">[01:42:26]</a>.

## Broader Implications and Future Directions

This work provides strong evidence that developing good methodologies for creating high-quality data sets is a central direction for advancing [[SelfImprovement in AI Models | AI models]] <a class="yt-timestamp" data-t="01:33:30">[01:33:30]</a>. The success of Phi-1 suggests that [[SelfImprovement in AI Models | self-improvement]] in AI models can be achieved through strategic data curation, even without massive scale.

Key takeaways for future research include:
*   **Curriculum Learning**: The idea of slowly building up complexity in training data, similar to how humans are taught, could be crucial for [[Training and finetuning processes for AI models | training]] neural networks <a class="yt-timestamp" data-t="00:09:48">[00:09:48]</a>. This could involve multiple pre-training stages with slightly different data sets <a class="yt-timestamp" data-t="00:22:43">[00:22:43]</a>.
*   **Synthetic Data Generation**: The ability to control the training distribution through synthetically generated data gives control over the behavior of an LLM <a class="yt-timestamp" data-t="01:17:37">[01:17:37]</a>. The paper suggests that using GPT-4 for synthetic data generation, instead of GPT-3.5, could yield even more significant gains <a class="yt-timestamp" data-t="01:33:06">[01:33:06]</a>.
*   **Interactive Learning Environments**: Integrating an interpreter with [[SelfImprovement in AI Models | reinforcement learning]] could allow models to generate and test their own examples, leading to more robust learning <a class="yt-timestamp" data-t="01:09:00">[01:09:00]</a>.
*   **Domain Randomization for Text**: Applying concepts from computer vision's domain randomization (varying non-semantic aspects of data) to text could enhance diversity and generalization <a class="yt-timestamp" data-t="01:34:09">[01:34:09]</a>.

While Phi-1 specializes in Python coding, the paper's authors believe these [[Challenges and strategies in model training and performance | limitations]] are not fundamental and their approach could be extended to other programming languages and broader domains like medical data or general chatbots <a class="yt-timestamp" data-t="01:32:37">[01:32:37]</a>. The findings underscore that data quality, particularly through carefully curated "textbook-like" datasets, is a critical factor in the advancement of [[SelfImprovement in AI Models | AI models]].