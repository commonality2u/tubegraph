---
title: Evaluation of software design and development benchmarks
videoId: tFwYD1UPIfM
---

From: [[hu-po]] <br/> 

Recent advancements in automated task solving have been driven by multi-agent systems using large language models (LLMs) <a class="yt-timestamp" data-t="00:04:14">[00:04:14]</a>. However, existing LLM-based multi-agent systems often focus on simpler dialogue tasks, with complex tasks rarely studied due to challenges like LLM hallucination and cascading errors <a class="yt-timestamp" data-t="00:05:16">[00:05:16]</a>. This has led to the development of frameworks like MetaGPT, which aims to address these complexities by incorporating human workflows as a metaprogramming approach <a class="yt-timestamp" data-t="00:06:26">[00:06:26]</a>.

## MetaGPT's Approach to Software Engineering <a class="yt-timestamp" data-t="00:08:03">[00:08:03]</a>
MetaGPT encodes standard operating procedures (SOPs) into prompts to enhance structured coordination <a class="yt-timestamp" data-t="00:08:03">[00:08:03]</a>. It mandates modular outputs, assigning diverse roles to various agents, similar to an assembly line paradigm <a class="yt-timestamp" data-t="00:08:22">[00:08:22]</a>. This approach mirrors human software development teams, where distinct roles like [[role_of_product_managers_architects_and_engineers_in_software_development | product manager]], [[role_of_product_managers_architects_and_engineers_in_software_development | architect]], [[role_of_product_managers_architects_and_engineers_in_software_development | project manager]], [[role_of_product_managers_architects_and_engineers_in_software_development | engineer]], and QA engineer work in a "waterfall method" to decompose high-level tasks into actionable components <a class="yt-timestamp" data-t="00:13:06">[00:13:06]</a>. Each agent operates with specific capabilities like thinking, reflection, and knowledge accumulation, interacting with a shared environment through publication and subscription methods <a class="yt-timestamp" data-t="00:33:59">[00:33:59]</a>. This system aims to produce more coherent and correct software compared to existing chat-based multi-agent systems <a class="yt-timestamp" data-t="00:09:54">[00:09:54]</a>.

### The Role of Context in MetaGPT
The MetaGPT framework generates increasing amounts of context as tasks progress through the different agents <a class="yt-timestamp" data-t="00:47:06">[00:47:06]</a>. For example, the architect receives all the information written by the [[role_of_product_managers_architects_and_engineers_in_software_development | product manager]], and subsequent roles accumulate even more context <a class="yt-timestamp" data-t="00:47:15">[00:47:15]</a>. This process can lead to significant token usage <a class="yt-timestamp" data-t="00:47:31">[00:47:31]</a>.

## Benchmarks for Software Engineering Evaluation
The MetaGPT paper uses two main benchmarks to evaluate its performance:

### HumanEval <a class="yt-timestamp" data-t="01:34:44">[01:34:44]</a>
HumanEval is a problem-solving dataset used to measure functional correction for synthesizing programs from docstrings <a class="yt-timestamp" data-t="01:34:56">[01:34:56]</a>. It essentially tests the ability to write a function given its documentation.

### MBPP (Mostly Basic Programming Problems) <a class="yt-timestamp" data-t="01:35:26">[01:35:26]</a>
MBPP is a dataset of 1,000 crowdsourced Python programming problems designed to be solvable by entry-level programmers <a class="yt-timestamp" data-t="01:35:36">[01:35:36]</a>. These are simple coding benchmarks, often involving tasks like finding shared elements in lists <a class="yt-timestamp" data-t="01:36:13">[01:36:13]</a>.

### Critique of Current Benchmarks <a class="yt-timestamp" data-t="01:36:26">[01:36:26]</a>
These benchmarks are criticized for evaluating simple coding tasks (writing individual functions) rather than complex system design <a class="yt-timestamp" data-t="01:36:26">[01:36:26]</a>. Creating a working software product, which involves designing a whole system with interconnected components, is significantly more difficult than just writing a single function <a class="yt-timestamp" data-t="01:36:38">[01:36:38]</a>. The absence of "system design benchmarks" means that MetaGPT is evaluated on a metric that doesn't fully capture its advertised capabilities <a class="yt-timestamp" data-t="01:36:55">[01:36:55]</a>.

## Practical Evaluation and Critique
When given a task, such as creating a Gradio front-end for a robotic AI cat toy <a class="yt-timestamp" data-t="00:39:58">[00:39:58]</a>, MetaGPT produced:
*   **Product Requirement Document (PRD):** This included user stories and a competitive analysis, even generating fake competitors <a class="yt-timestamp" data-t="00:40:53">[00:40:53]</a>. While the user stories were generally agreeable, the competitive quadrant chart was deemed "meaningless" for strategic decision-making <a class="yt-timestamp" data-t="00:43:01">[00:43:01]</a>.
*   **System Design:** The architect's output, including data structures and API definitions, was largely deemed arbitrary and basic, using generic "control params" or "schedule params" that lacked specific meaning <a class="yt-timestamp" data-t="00:49:55">[00:49:55]</a>. The sequence flow diagram was criticized as stating the obvious without providing useful insights <a class="yt-timestamp" data-t="00:51:16">[00:51:16]</a>.
*   **Code Generation:** The generated code for the Gradio interface was found to be "mostly nonsense" <a class="yt-timestamp" data-t="00:58:01">[00:58:01]</a>. It used arbitrary object structures, inconsistent time formats, and questionable design patterns (e.g., a class with only static methods that merely wrapped existing functions) <a class="yt-timestamp" data-t="01:06:55">[01:06:55]</a>. The code was deemed "verbose and incorrect" <a class="yt-timestamp" data-t="01:24:10">[01:24:10]</a>.
*   **QA Engineer Output:** No test file was produced by the QA engineer, indicating an incomplete process <a class="yt-timestamp" data-t="01:11:18">[01:11:18]</a>.

In [[performance_analysis_and_benchmarks | comparison]] to MetaGPT's complex multi-agent process, a direct query to ChatGPT 4 for the same task yielded a working, albeit simple, Gradio interface with more meaningful concepts like direction and speed <a class="yt-timestamp" data-t="01:00:59">[01:00:59]</a>. This suggests that the extensive context generated by MetaGPT's multi-agent system might confuse the engineer rather than help them <a class="yt-timestamp" data-t="01:31:31">[01:31:31]</a>. The cost of running MetaGPT's demo (approximately $0.87 for one task) also highlighted the high token usage for potentially inferior results <a class="yt-timestamp" data-t="00:47:47">[00:47:47]</a>.

### Benchmarking Claims vs. Reality
MetaGPT claims to significantly outperform GPT-4 on HumanEval and MBPP <a class="yt-timestamp" data-t="01:38:59">[01:38:59]</a>. However, this is questioned given the observed poor quality of its system-level code generation <a class="yt-timestamp" data-t="01:38:11">[01:38:11]</a>. It's speculated that MetaGPT's performance on these specific function-level benchmarks might stem from the sheer volume of context provided (tens of thousands of tokens) <a class="yt-timestamp" data-t="01:38:00">[01:38:00]</a>, making it more likely to get the answer right on the first try (pass@1) <a class="yt-timestamp" data-t="01:39:15">[01:39:15]</a>. However, this doesn't translate to its ability to produce functional, well-designed software systems.

## Broader Implications for Software Engineering Organizations
The perceived inefficiencies of MetaGPT's multi-agent system mirror critiques of traditional, large-scale human software engineering organizations <a class="yt-timestamp" data-t="01:58:49">[01:58:49]</a>. The roles of [[role_of_product_managers_architects_and_engineers_in_software_development | product managers]], [[role_of_product_managers_architects_and_engineers_in_software_development | architects]], and [[role_of_product_managers_architects_and_engineers_in_software_development | project managers]] are often seen as creating "overly verbose, non-specific things" that lead to "overly verbose and non-specific" code, even when applied to LLMs <a class="yt-timestamp" data-t="00:59:30">[00:59:30]</a>. The argument is raised that directly tasking an [[role_of_product_managers_architects_and_engineers_in_software_development | engineer]] (or an LLM acting as one) might yield better results with less overhead <a class="yt-timestamp" data-t="01:19:02">[01:19:02]</a>. This highlights [[considerations_in_optimizing_software_engineering_processes | considerations in optimizing software engineering processes]] and questions whether current bureaucratic structures in software development are truly optimal or merely relics of historical legacy and compensation models <a class="yt-timestamp" data-t="01:59:50">[01:59:50]</a>.

## Conclusion
While MetaGPT presents an interesting concept for multi-agent collaboration in software engineering, its practical application, as demonstrated, struggles to produce robust and well-designed code compared to direct LLM prompting <a class="yt-timestamp" data-t="01:56:57">[01:56:57]</a>. The benchmarks used for evaluation, HumanEval and MBPP, are criticized for not adequately assessing complex system design capabilities <a class="yt-timestamp" data-t="01:58:08">[01:58:08]</a>. The findings suggest that the added complexity and overhead of mimicking traditional software roles with LLMs may not currently translate into a superior product, raising broader questions about the efficiency of established software development methodologies.