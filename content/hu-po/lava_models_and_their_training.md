---
title: Lava models and their training
videoId: uYb38g-weEY
---

From: [[hu-po]] <br/> 

Lava models are a prominent series of [[large language models and their applications | Vision Language Models (VLMs)]] designed to process both visual and textual information. They have seen continuous improvement, with new versions addressing [[Challenges and strategies in model training and performance | training]] complexities and performance.

## Core Architecture and Functionality
A typical Lava model integrates a frozen [[large language models and their applications | language model]] backbone (LLM) with a frozen [[Challenges in training large computer vision models | vision encoder]], such as Clip. The core of its functionality relies on a "connector" (also referred to as an MLP or adapter) [01:43:53](<a class="yt-timestamp" data-t="01:43:53">[01:43:53]</a>. This connector takes the visual tokens produced by the vision encoder and transforms them into "pseudo text tokens" that the [[large language models and their applications | language model]] can consume, effectively treating image patches as text [00:45:51](<a class="yt-timestamp" data-t="00:45:51">[00:45:51]</a>, [01:17:50](<a class="yt-timestamp" data-t="01:17:50">[01:17:50]</a>, [01:43:30](<a class="yt-timestamp" data-t="01:43:30">[01:43:30]</a>.

### Training Process
Lava models typically follow a multi-stage [[Training and finetuning processes for AI models | training process]]:
*   **Stage 1: Connector Training** <a class="yt-timestamp" data-t="01:46:49">[01:46:49]</a>
    *   The [[large language models and their applications | language model]] and [[Challenges in training large computer vision models | vision encoder]] are frozen [00:39:42](<a class="yt-timestamp" data-t="00:39:42">[00:39:42]</a>, [01:47:05](<a class="yt-timestamp" data-t="01:47:05">[01:47:05]</a>.
    *   Only the connector (MLP/adapter) is trained [00:40:11](<a class="yt-timestamp" data-t="00:40:11">[00:40:11]</a>, [01:47:14](<a class="yt-timestamp" data-t="01:47:14">[01:47:14]</a>.
    *   This stage uses data sets primarily composed of images paired with text captions (e.g., Lava v1.5 data set with 558,000 images) [00:39:49](<a class="yt-timestamp" data-t="00:39:49">[00:39:49]</a>, [01:47:40](<a class="yt-timestamp" data-t="01:47:40">[01:47:40]</a>. The goal is to adapt visual tokens to be consumable by the [[large language models and their applications | language model]] [00:40:20](<a class="yt-timestamp" data-t="00:40:20">[00:40:20]</a>.
*   **Stage 2: Full Model Training** <a class="yt-timestamp" data-t="01:47:46">[01:47:46]</a>
    *   The [[Challenges in training large computer vision models | vision encoder]] remains frozen, but gradients are pushed into the [[large language models and their applications | language model]] and the connector [00:41:27](<a class="yt-timestamp" data-t="00:41:27">[00:41:27]</a>, [01:48:01](<a class="yt-timestamp" data-t="01:48:01">[01:48:01]</a>.
    *   More complex data sets are used, consisting of images paired with questions and responses (e.g., ~964,000 examples from combined data sets like SVIT, LVIS, LRV) [00:40:39](<a class="yt-timestamp" data-t="00:40:39">[00:40:39]</a>, [01:47:47](<a class="yt-timestamp" data-t="01:47:47">[01:47:47]</a>.

### Lava v1.6 Enhancements
Lava v1.6 introduced several improvements over v1.5:
*   **Increased Input Image Resolution**: From 336x336 to four times that size (though this is achieved by cutting the image into four parts and feeding them, along with a resized image) [01:30:29](<a class="yt-timestamp" data-t="01:30:29">[01:30:29]</a>, [01:34:55](<a class="yt-timestamp" data-t="01:34:55">[01:34:55]</a>, [01:38:11](<a class="yt-timestamp" data-t="01:38:11">[01:38:11]</a>. This aims to preserve intricate details [01:34:43](<a class="yt-timestamp" data-t="01:34:43">[01:34:43]</a>.
*   **Better Visual Reasoning and OCR Capability**: Achieved through an improved [[Training and finetuning processes for AI models | tuning data]] mixture [01:31:01](<a class="yt-timestamp" data-t="01:31:01">[01:31:01]</a>.
*   **Better World Knowledge and Logical Reasoning** [01:31:32](<a class="yt-timestamp" data-t="01:31:32">[01:31:32]</a>.
*   **Efficient Deployment and Inference**: Utilizing SG Lang [01:31:37](<a class="yt-timestamp" data-t="01:31:37">[01:31:37]</a>.
*   **Open Source Commitment**: Lava v1.6 is considered highly open-source, releasing code, data, and model weights [01:31:48](<a class="yt-timestamp" data-t="01:31:48">[01:31:48]</a>.
*   **Training Resources**: The 34B variant can be trained in one day with 32 A100 GPUs [01:32:26](<a class="yt-timestamp" data-t="01:32:26">[01:32:26]</a>.

## MoE-Lava: Incorporating Mixture of Experts
MoE-Lava applies the [[LoRA technique for model adaptation | Mixture of Experts]] (MoE) paradigm to [[large language models and their applications | Vision Language Models]], a technique gaining popularity in [[large language models and their applications | language models]] (e.g., Mistral 8x7B) [00:31:54](<a class="yt-timestamp" data-t="00:31:54">[00:31:54]</a>, [01:44:56](<a class="yt-timestamp" data-t="01:44:56">[01:44:56]</a>.

### What is a Mixture of Experts?
In an MoE model, the standard Feed Forward Network (FFN) or Multi-Layer Perceptron (MLP) within a Transformer block is replaced by multiple "experts" [00:12:52](<a class="yt-timestamp" data-t="00:12:52">[00:12:52]</a>, [01:45:42](<a class="yt-timestamp" data-t="01:45:42">[01:45:42]</a>. Each expert is typically a small MLP [00:25:55](<a class="yt-timestamp" data-t="00:25:55">[00:25:55]</a>. A "router" mechanism then determines which subset of these experts (e.g., Top-K experts, where K is typically 2 for MoE-Lava) will process the input tokens [00:13:08](<a class="yt-timestamp" data-t="00:13:08">[00:13:08]</a>, [00:32:23](<a class="yt-timestamp" data-t="00:32:23">[00:32:23]</a>, [01:46:01](<a class="yt-timestamp" data-t="01:46:01">[01:46:01]</a>.

*   **Sparse MoE (Hard Assignment)**: Only selected experts process the information, with others being zeroed out [00:18:49](<a class="yt-timestamp" data-t="00:18:49">[00:18:49]</a>, [01:46:12](<a class="yt-timestamp" data-t="01:46:12">[01:46:12]</a>. This can be non-differentiable [00:22:47](<a class="yt-timestamp" data-t="00:22:47">[00:22:47]</a>.
*   **Soft MoE (Weighted Combination)**: Multiple experts process the information, and their outputs are combined with weights [00:19:21](<a class="yt-timestamp" data-t="00:19:21">[00:19:21]</a>, [01:46:21](<a class="yt-timestamp" data-t="01:46:21">[01:46:21]</a>. This is generally preferred as it is differentiable [00:22:47](<a class="yt-timestamp" data-t="00:22:47">[00:22:47]</a>. MoE-Lava uses a sparse selection of Top-K experts but applies a weighted sum within those selected experts [00:44:51](<a class="yt-timestamp" data-t="00:44:51">[00:44:51]</a>.

### MoE-Lava Training (Three Stages)
MoE-Lava adds a third stage to the standard VLM [[Training and finetuning processes for AI models | training process]]:
*   **Stage 1 & 2**: Same as the Lava VLM [[Training and finetuning processes for AI models | training process]] described above [00:38:33](<a class="yt-timestamp" data-t="00:38:33">[00:38:33]</a>.
*   **Stage 3**: The MLPs from the previously trained model are copied to create multiple experts [00:42:23](<a class="yt-timestamp" data-t="00:42:23">[00:42:23]</a>, [01:48:32](<a class="yt-timestamp" data-t="01:48:32">[01:48:32]</a>. A router is added, and only the MoE layers (experts and router) are trained [00:37:58](<a class="yt-timestamp" data-t="00:37:58">[00:37:58]</a>, [00:42:24](<a class="yt-timestamp" data-t="00:42:24">[00:42:24]</a>, [01:48:42](<a class="yt-timestamp" data-t="01:48:42">[01:48:42]</a>.
    *   This stage uses an instruction-tuning dataset (e.g., Lava v1.5 Mix 665k images) [00:42:44](<a class="yt-timestamp" data-t="00:42:44">[00:42:44]</a>, [01:48:52](<a class="yt-timestamp" data-t="01:48:52">[01:48:52]</a>.
    *   An auxiliary loss called the "differentiable load balancing loss" is used to encourage the router to distribute requests equally among experts, preventing collapse to a single expert [00:47:27](<a class="yt-timestamp" data-t="00:47:27">[00:47:27]</a>, [01:49:00](<a class="yt-timestamp" data-t="01:49:00">[01:49:00]</a>.
    *   Over [[Training and finetuning processes for AI models | training]], experts specialize, receiving slightly different gradients and becoming adept at different aspects of the input [00:44:13](<a class="yt-timestamp" data-t="00:44:13">[00:44:13]</a>.

### Parameter Counting and Performance
MoE models are often marketed based on "activated parameters," implying lower computational cost. However, this is misleading for single-GPU inference at home because all parameters must still be loaded into memory [00:48:52](<a class="yt-timestamp" data-t="00:48:52">[00:48:52]</a>, [01:49:57](<a class="yt-timestamp" data-t="01:49:57">[01:49:57]</a>. They are more efficient for batched inference on multi-GPU server setups where experts can be sharded across different GPUs [00:13:41](<a class="yt-timestamp" data-t="00:13:41">[00:13:41]</a>, [00:49:57](<a class="yt-timestamp" data-t="00:49:57">[00:49:57]</a>.

## Benchmarking and Challenges
Benchmarking [[large language models and their applications | Vision Language Models]] presents several [[Challenges and strategies in model training and performance | challenges]]:
*   **Inconsistent Benchmarks**: Different papers often use varying benchmarks or different calculation methods for scores, making direct [[Comparison and Evaluation of Code Llama and Other Language Models | comparison and evaluation of models]] difficult [01:33:17](<a class="yt-timestamp" data-t="01:33:17">[01:33:17]</a>, [01:40:51](<a class="yt-timestamp" data-t="01:40:51">[01:40:51]</a>.
*   **Misleading Reporting**: Some papers may present results in a way that exaggerates performance, such as highlighting performance on specific, less common benchmarks or using "activated parameters" for comparison [00:51:17](<a class="yt-timestamp" data-t="00:51:17">[00:51:17]</a>, [00:53:01](<a class="yt-timestamp" data-t="00:53:01">[00:53:01]</a>.
*   **Censorship and Safety Layers**: Proprietary models like GPT-4V and Bard may have AI safety layers that prevent them from performing certain tasks, such as identifying individuals in images, even if their underlying visual reasoning capability is high [00:57:00](<a class="yt-timestamp" data-t="00:57:00">[00:57:00]</a>, [00:57:07](<a class="yt-timestamp" data-t="00:57:07">[00:57:07]</a>.
*   **Obama Benchmark**: A custom benchmark involving an image of Obama with The Rock's face superimposed, designed to test subtle visual reasoning, demonstrated that most VLMs incorrectly identified the individual as Barack Obama [00:07:42](<a class="yt-timestamp" data-t="00:07:42">[00:07:42]</a>, [01:56:58](<a class="yt-timestamp" data-t="01:56:58">[01:56:58]</a>. Only Perplexity (using GPT-4V without its typical safety layers) correctly identified it as a digitally manipulated photo [01:02:45](<a class="yt-timestamp" data-t="01:02:45">[01:02:45]</a>.

## Related Techniques and Future Directions
Other methods for improving VLMs include:
*   **[[LoRA technique for model adaptation | LoRA technique for model adaptation]]**: Used in models like Intern LMX Composer 2, where [[LoRA technique for model adaptation | LoRA]] modules fine-tune the [[Challenges in training large computer vision models | vision encoder]] directly, rather than using a separate connector MLP [01:01:13](<a class="yt-timestamp" data-t="01:01:13">[01:01:13]</a>, [01:52:18](<a class="yt-timestamp" data-t="01:52:18">[01:52:18]</a>.
*   **Ensemble of Visual Experts**: The Musy Poly Visual Expert model uses multiple different [[Challenges in training large computer vision models | vision encoders]] (e.g., Clip, DinoV2, Sam) as "experts" [01:04:30](<a class="yt-timestamp" data-t="01:04:30">[01:04:30]</a>, [01:52:50](<a class="yt-timestamp" data-t="01:52:50">[01:52:50]</a>. Each encoder excels at different types of visual processing (semantic understanding, robust feature extraction, segmentation) [01:05:04](<a class="yt-timestamp" data-t="01:05:04">[01:05:04]</a>. A "Poly Expert Fusion Network" (essentially more MLPs) is used to combine their diverse outputs [01:14:36](<a class="yt-timestamp" data-t="01:14:36">[01:14:36]</a>, [01:53:42](<a class="yt-timestamp" data-t="01:53:42">[01:53:42]</a>.
    *   This approach consistently yields superior performance, though it increases computational cost due to processing through multiple encoders and longer input sequences for the LLM [01:07:30](<a class="yt-timestamp" data-t="01:07:30">[01:07:30]</a>, [01:08:50](<a class="yt-timestamp" data-t="01:08:50">[01:08:50]</a>, [01:54:30](<a class="yt-timestamp" data-t="01:54:30">[01:54:30]</a>.
    *   The order in which outputs from different visual experts are presented to the LLM can affect the final output due to the autoregressive and position-aware nature of [[large language models and their applications | language models]] [01:24:24](<a class="yt-timestamp" data-t="01:24:24">[01:24:24]</a>, [01:55:08](<a class="yt-timestamp" data-t="01:55:08">[01:55:08]</a>.
*   **Revisiting Positional Encodings**: Some research questions the necessity of traditional positional encodings in [[Challenges in training large computer vision models | Vision Transformers]] when their output is fed into a [[large language models and their applications | language model]]. Simpler positional encoding schemes (e.g., using the same encoding for all patches) yield comparable performance, suggesting potential for compute savings [01:06:50](<a class="yt-timestamp" data-t="01:06:50">[01:06:50]</a>, [01:21:51](<a class="yt-timestamp" data-t="01:21:51">[01:21:51]</a>, [01:55:47](<a class="yt-timestamp" data-t="01:55:47">[01:55:47]</a>.

These trends suggest that future [[large language models and their applications | Vision Language Models]] may increasingly leverage multiple visual encoders and MoE layers to achieve state-of-the-art performance, even if it comes with higher computational demands. The focus seems to be shifting from novel architecture tricks to orchestrating complex pipelines of existing models and techniques [01:36:15](<a class="yt-timestamp" data-t="01:36:15">[01:36:15]</a>.