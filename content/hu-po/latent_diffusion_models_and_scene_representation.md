---
title: Latent diffusion models and scene representation
videoId: 3updXylOFvY
---

From: [[hu-po]] <br/> 

The paper "Beyond Surface Statistics: Scene Representation in a Latent Diffusion Model" investigates the internal workings of [[latent_diffusion_models_and_architectures | latent diffusion models]] (LDMs) to understand how they represent 3D scenes <a class="yt-timestamp" data-t="00:00:38">[00:00:38]</a>, <a class="yt-timestamp" data-t="00:02:37">[00:02:37]</a>. The research aims to determine if LDMs create and utilize an internal representation of simple scene geometry, or if they merely memorize superficial correlations between pixel values and words <a class="yt-timestamp" data-t="00:09:42">[00:09:42]</a>, <a class="yt-timestamp" data-t="00:10:00">[00:10:00]</a>.

The paper is a pre-print, meaning it has been posted directly to platforms like arXiv without going through a formal peer-review process or being published in a journal <a class="yt-timestamp" data-t="00:01:00">[00:01:00]</a>, <a class="yt-timestamp" data-t="00:01:12">[00:01:12]</a>. While this would normally be a concern, it is not unusual in machine learning due to the rapid pace of development <a class="yt-timestamp" data-t="00:01:35">[00:01:35]</a>, <a class="yt-timestamp" data-t="00:01:42">[00:01:42]</a>.

## Interpretability Research
This work falls under the umbrella of interpretability research, which focuses on dissecting models to understand what their different internal pieces are doing <a class="yt-timestamp" data-t="00:02:24">[00:02:24]</a>, <a class="yt-timestamp" data-t="00:02:27">[00:02:27]</a>. This type of research is highly valued for providing intuition about model behavior, unlike papers that only propose new algorithms and benchmark their performance <a class="yt-timestamp" data-t="00:02:47">[00:02:47]</a>, <a class="yt-timestamp" data-t="00:02:55">[00:02:55]</a>.

A core question in this field is whether [[diffusion_models_and_image_generation | generative neural networks]] merely aggregate surface statistics or learn deeper "World models" <a class="yt-timestamp" data-t="00:11:08">[00:11:08]</a>, <a class="yt-timestamp" data-t="01:42:29">[01:42:29]</a>.

## Latent Diffusion Models (LDMs)
[[latent_diffusion_models_and_architectures | Latent diffusion models]] are a type of [[diffusion_models_and_image_generation | diffusion model]] that operate in a compressed "latent space" rather than directly in the image space <a class="yt-timestamp" data-t="00:03:27">[00:03:27]</a>, <a class="yt-timestamp" data-t="00:03:33">[00:03:33]</a>. This latent space is created by a Variational Autoencoder (VAE) <a class="yt-timestamp" data-t="00:03:38">[00:03:38]</a>. The primary motivation for using latent space is to save computational resources and memory, as the latent variables are smaller than full images <a class="yt-timestamp" data-t="02:00:27">[02:00:27]</a>, <a class="yt-timestamp" data-t="02:00:58">[02:00:58]</a>.

The chosen model for investigation was Stable Diffusion V1, an open-source [[text_to_image_diffusion_models | text-to-image diffusion model]] that was trained without explicit depth information <a class="yt-timestamp" data-t="02:44:55">[02:44:55]</a>, <a class="yt-timestamp" data-t="02:45:50">[02:45:50]</a>.

### LDM Architecture
A Stable Diffusion model consists of two main components <a class="yt-timestamp" data-t="02:46:25">[02:46:25]</a>:
1.  **Latent Diffusion Model (LDM):** This component predicts and removes noise within the latent space <a class="yt-timestamp" data-t="02:47:33">[02:47:33]</a>. It's trained to predict noise added during a forward diffusion process <a class="yt-timestamp" data-t="02:48:47">[02:48:47]</a>.
2.  **Variational Autoencoder (VAE):** The VAE has an encoder that compresses an RGB image into a smaller latent variable (Z), and a decoder that converts the denoised latent variable back into image space <a class="yt-timestamp" data-t="02:49:10">[02:49:10]</a>, <a class="yt-timestamp" data-t="02:51:24">[02:51:24]</a>, <a class="yt-timestamp" data-t="02:53:07">[02:53:07]</a>.

## Probing Methodology
To understand the internal representations, the researchers applied linear probing <a class="yt-timestamp" data-t="00:10:20">[00:10:20]</a>. This involves taking intermediate activations (the values of neurons at a specific layer) from the LDM and training a simple linear classifier or regressor on them <a class="yt-timestamp" data-t="02:58:52">[02:58:52]</a>, <a class="yt-timestamp" data-t="02:59:18">[02:59:18]</a>. A high prediction accuracy indicates a strong correlation between the learned representation and the property being probed <a class="yt-timestamp" data-t="02:59:54">[02:59:54]</a>.

### Types of Depth Probing
The study investigated two types of scene geometry representation <a class="yt-timestamp" data-t="03:52:13">[03:52:13]</a>:
1.  **Discrete Binary Depth:** This is akin to foreground-background distinction or salient object detection, where each pixel is classified as either belonging to a salient object or the background <a class="yt-timestamp" data-t="03:53:48">[03:53:48]</a>, <a class="yt-timestamp" data-t="03:59:57">[03:59:57]</a>. A cross-entropy loss was used for training the classifier <a class="yt-timestamp" data-t="03:00:27">[03:00:27]</a>, <a class="yt-timestamp" data-t="04:28:28">[04:28:28]</a>.
2.  **Continuous Depth:** This represents the distance of objects from the camera, with values ranging continuously (or across many discrete bins like 256 for a uint8 depth map) <a class="yt-timestamp" data-t="03:54:19">[03:54:19]</a>, <a class="yt-timestamp" data-t="03:55:07">[03:55:07]</a>, <a class="yt-timestamp" data-t="04:25:29">[04:25:29]</a>. A Huber loss was used for training the regression model <a class="yt-timestamp" data-t="05:25:24">[05:25:24]</a>, <a class="yt-timestamp" data-t="05:26:24">[05:26:24]</a>.

### Synthetic Dataset
A synthetic dataset of 1,000 images was generated using a pre-trained Stable Diffusion model, with prompts sampled from the LAION-Aesthetics V2 dataset <a class="yt-timestamp" data-t="04:02:01">[04:02:01]</a>, <a class="yt-timestamp" data-t="04:05:07">[04:05:07]</a>. Ground truth labels for salient objects were synthesized using an off-the-shelf Salient Object Tracing (Tracer) model, and depth maps were estimated using a pre-trained Midas model (for monocular depth estimation) <a class="yt-timestamp" data-t="04:22:20">[04:22:20]</a>, <a class="yt-timestamp" data-t="04:33:53">[04:33:53]</a>, <a class="yt-timestamp" data-t="04:39:57">[04:39:57]</a>. Problematic images (e.g., offensive content, corrupted objects) were manually removed <a class="yt-timestamp" data-t="04:51:50">[04:51:50]</a>.

## Key Findings
### Early Emergence of Scene Geometry
A significant finding is that [[using_diffusion_models_for_visual_world_understanding | internal representations]] of 3D depth data and salient object/background distinction appear surprisingly early in the denoising process of the LDM <a class="yt-timestamp" data-t="00:08:29">[00:08:29]</a>, <a class="yt-timestamp" data-t="00:14:10">[00:14:10]</a>, <a class="yt-timestamp" data-t="00:16:29">[00:16:29]</a>. Even when the decoded images appear as extreme noise to a human observer, the LDM's internal representations already encode meaningful scene geometry, including fine-grained details like limbs or parts of objects <a class="yt-timestamp" data-t="01:10:34">[01:10:34]</a>, <a class="yt-timestamp" data-t="01:10:46">[01:10:46]</a>, <a class="yt-timestamp" data-t="01:59:25">[01:59:25]</a>. This suggests an inductive bias in the model to establish fundamental scene structure early to allow more steps for texture generation <a class="yt-timestamp" data-t="01:03:32">[01:03:32]</a>.

### Comparison with VAE Representations
Probing studies on the VAE's self-attention layers showed weaker depth information <a class="yt-timestamp" data-t="01:13:13">[01:13:13]</a>. The VAE could not decode salient objects or depth from corrupted latents at early steps; its performance only improved once the latent vectors became more perceptible (i.e., less noisy, later in the diffusion process) <a class="yt-timestamp" data-t="01:12:07">[01:12:07]</a>, <a class="yt-timestamp" data-t="01:12:47">[01:12:47]</a>. This indicates that the LDM, not the VAE, is primarily responsible for learning and encoding these early depth representations <a class="yt-timestamp" data-t="01:14:06">[01:14:06]</a>, <a class="yt-timestamp" data-t="01:15:01">[01:15:01]</a>.

### Causal Role of Representations
Intervention experiments further indicated a causal role for these representations in image synthesis <a class="yt-timestamp" data-t="00:08:36">[00:08:36]</a>. By modifying the LDM's internal representations (e.g., shifting the foreground object's mask or depth map), the final output image was correspondingly changed, demonstrating that these internal representations directly influence the generated image <a class="yt-timestamp" data-t="01:20:20">[01:20:20]</a>, <a class="yt-timestamp" data-t="01:26:38">[01:26:38]</a>. This technique is similar to how ControlNet conditions [[diffusion_models_and_image_generation | diffusion models]] by injecting information at multiple layers <a class="yt-timestamp" data-t="01:17:18">[01:17:18]</a>, <a class="yt-timestamp" data-t="01:27:18">[01:27:18]</a>, <a class="yt-timestamp" data-t="01:33:40">[01:33:40]</a>.

### Vision Transformers (ViTs) vs. Convolutional Neural Networks (CNNs)
The study also explored the depth representation in Vision Transformers (ViTs) compared to Convolutional Neural Networks (CNNs). They found stronger depth and salient object representations in the self-attention layers of ViTs than in convolutional layers <a class="yt-timestamp" data-t="03:56:55">[03:56:55]</a>, <a class="yt-timestamp" data-t="04:40:40">[04:40:40]</a>. Notably, the final layers of the decoder in a ViT-based model showed a much stronger internal representation for depth than CNNs <a class="yt-timestamp" data-t="01:39:46">[01:39:46]</a>, <a class="yt-timestamp" data-t="01:42:04">[01:42:04]</a>.

## Implications and Future Work
The findings add nuance to the ongoing debate about whether [[diffusion_models_and_image_generation | generative models]] learn more than just surface statistics, showing that LDMs do indeed learn complex geometric properties like depth <a class="yt-timestamp" data-t="01:42:29">[01:42:29]</a>, <a class="yt-timestamp" data-t="01:43:00">[01:43:00]</a>.

Future avenues for research include looking for representations of other scene attributes like lighting or texture within LDMs <a class="yt-timestamp" data-t="01:42:40">[01:42:40]</a>. The paper suggests that, just as language models may "re-discover" the NLP pipeline, LDMs might recapitulate standard steps in computer graphics, learning models of semantic aspects of a scene, such as sentiment <a class="yt-timestamp" data-t="01:42:51">[01:42:51]</a>. These [[implications_of_models_reasoning_in_latent_space | implications of models reasoning in latent space]] highlight the potential for deeper understanding and control over complex generative processes.