---
title: Difference between ASI and AGI Artificial General Intelligence
videoId: KYlbny1rN1g
---

From: [[hu-po]] <br/> 

This article explores the distinction between Artificial Superintelligence (ASI) and Artificial General Intelligence (AGI), drawing from the speaker's perspective on their current state and future development.

## Defining ASI and AGI

The core difference between [[Artificial Superintelligence ASI | Artificial Superintelligence (ASI)]] and [[TaskSpecific vs General AI Models | Artificial General Intelligence (AGI)]] lies in the terms "super" versus "general" <a class="yt-timestamp" data-t="02:27:00">[02:27:00]</a>.

### Artificial Superintelligence (ASI)

[[Artificial Superintelligence ASI | ASI]] refers to intelligence that is superhuman in a *limited, narrow field* <a class="yt-timestamp" data-t="02:45:00">[02:45:00]</a>.
*   **Historical Examples**: The speaker states that narrow [[Artificial Superintelligence ASI | ASI]] has existed for hundreds of years <a class="yt-timestamp" data-t="02:52:00">[02:52:00]</a>.
    *   A calculator is an example of [[Artificial Superintelligence ASI | ASI]] because it is "super intelligent" in arithmetic, performing 10-digit multiplications perfectly every time, which humans cannot do <a class="yt-timestamp" data-t="02:37:00">[02:37:00]</a>, <a class="yt-timestamp" data-t="03:07:00">[03:07:00]</a>.
    *   Mechanical calculators from 1623 and 1943 are also considered narrow [[Artificial Superintelligence ASI | ASI]] <a class="yt-timestamp" data-t="02:54:00">[02:54:00]</a>.
*   **Real-World ASI**: Examples in the physical world include:
    *   A balancing robot that is superhuman at balancing <a class="yt-timestamp" data-t="05:52:00">[05:52:00]</a>.
    *   Machines that are superhuman in strength, precision, and repeatability <a class="yt-timestamp" data-t="06:10:00">[06:10:00]</a>.

### Artificial General Intelligence (AGI)

[[TaskSpecific vs General AI Models | AGI]] refers to a generalist intelligence capable of adapting to various tasks, similar to how animals like crows or dogs can adapt to any ecological niche <a class="yt-timestamp" data-t="04:02:00">[04:02:00]</a>, <a class="yt-timestamp" data-t="04:15:00">[04:15:00]</a>.

The speaker's opinion is that we *already have* [[TaskSpecific vs General AI Models | AGI]] <a class="yt-timestamp" data-t="04:26:00">[04:26:00]</a>.
*   **Language Models as AGI**: Language models, especially since the "ChatGBT moment" and the "03 moment" where they beat the R AGI semi-private eval, are considered [[TaskSpecific vs General AI Models | AGI]] <a class="yt-timestamp" data-t="04:28:00">[04:28:00]</a>, <a class="yt-timestamp" data-t="04:33:00">[04:33:00]</a>.
    *   Even if they are limited to language tasks, they can still be defined as [[TaskSpecific vs General AI Models | AGI]] because they are generalists within that domain <a class="yt-timestamp" data-t="04:44:00">[04:44:00]</a>, <a class="yt-timestamp" data-t="04:57:00">[04:57:00]</a>.
*   **Robots Approaching AGI**: The Tesla Optimus robot is "almost [[TaskSpecific vs General AI Models | AGI]] in the real world" because it will be able to perform "anything that humans do in the real world" <a class="yt-timestamp" data-t="05:22:00">[05:22:00]</a>, <a class="yt-timestamp" data-t="05:30:00">[05:30:00]</a>. Humanoid demos like those from Unitree are getting close to this <a class="yt-timestamp" data-t="05:43:00">[05:43:00]</a>.

### The "Ultimate ASI"

The "ultimate [[Artificial Superintelligence ASI | ASI]]" is defined as a system better than any human or group of people at *anything*, in either the physical or digital world <a class="yt-timestamp" data-t="06:26:00">[06:26:00]</a>. An example given is a cluster running hundreds of 04 models operating hundreds of humanoid robots <a class="yt-timestamp" data-t="06:37:00">[06:37:00]</a>.

## The Path to Superhuman Intelligence

The speaker argues that achieving superhuman intelligence, or [[Artificial Superintelligence ASI | ASI]], in various domains follows a similar progression seen in the game of Go.

### Lessons from Go

The game of Go illustrates how [[Artificial Superintelligence ASI | AI]] intelligence can increase exponentially <a class="yt-timestamp" data-t="07:09:00">[07:09:00]</a>.
*   **AlphaGo Zero's Success**: AlphaGo Zero mastered Go without human knowledge <a class="yt-timestamp" data-t="07:54:00">[07:54:00]</a>. Instead of training on human games (like AlphaGo Master, which used 230,000 human games but only got "as good as humans" <a class="yt-timestamp" data-t="27:19:00">[27:19:00]</a>), AlphaGo Zero used [[analogies_between_biological_systems_and_ai_models | self-play]] and [[Definition of an agent in AI | reinforcement learning]] <a class="yt-timestamp" data-t="09:16:00">[09:16:00]</a>, <a class="yt-timestamp" data-t="27:56:00">[27:56:00]</a>.
*   **The "Z" Signal**: The key to AlphaGo Zero's success was the ability to use a clear "Z" signal (who wins or loses the game) to label good and bad moves throughout the game's "tree" of possible actions <a class="yt-timestamp" data-t="12:27:00">[12:27:00]</a>, <a class="yt-timestamp" data-t="22:57:00">[22:57:00]</a>. This allows the model to learn a value function and a policy (probability distribution over actions) <a class="yt-timestamp" data-t="13:12:00">[13:12:12]</a>, <a class="yt-timestamp" data-t="13:55:00">[13:55:00]</a>.
*   **Synthetic Data**: AlphaGo Zero generated its own "superhuman data" by exploring a much larger space of possible games through self-play <a class="yt-timestamp" data-t="33:21:00">[33:21:00]</a>. This process allowed it to discover novel moves no human had ever seen <a class="yt-timestamp" data-t="33:57:00">[33:57:00]</a>, <a class="yt-timestamp" data-t="53:57:00">[53:57:00]</a>.

### Applying to Language and Reasoning

The speaker argues that the same principles can be applied to language and mathematical reasoning.
*   **Language Space as a Tree**: Large Language Models (LLMs) auto-regressively predict tokens, essentially picking the next branch in a "tree" of possible language sequences <a class="yt-timestamp" data-t="16:25:00">[16:25:00]</a>. The branching factor for LLMs (e.g., 32,000 possible tokens for Llama 2 <a class="yt-timestamp" data-t="18:09:00">[18:09:09]</a>) is much larger than for Go (250 <a class="yt-timestamp" data-t="15:39:00">[15:39:00]</a>) or Chess (35 <a class="yt-timestamp" data-t="15:45:00">[15:45:00]</a>), but it is still finite <a class="yt-timestamp" data-t="18:52:00">[18:52:00]</a>.
*   **Chain of Thought**: A Chain of Thought is a sequence of choices or a path through this tree of possible actions and states <a class="yt-timestamp" data-t="20:47:00">[20:47:00]</a>.
*   **Process Reward Models (PRM)**: Unlike general language, domains like math and coding have a verifiable "Z" signal (correct or incorrect answer) <a class="yt-timestamp" data-t="38:57:00">[38:57:00]</a>, <a class="yt-timestamp" data-t="58:00:00">[58:00:00]</a>. [[Definition of an agent in AI | Process Reward Models]] provide fine-grained supervision by evaluating the correctness of intermediate reasoning steps <a class="yt-timestamp" data-t="36:00:00">[36:00:00]</a>. This allows for the iterative self-play and synthetic data generation seen in Go.
*   **R* Math Example**: The R* Math paper shows small language models (SLMs) mastering math reasoning by generating "millions of synthesized solutions" through a self-evolution process <a class="yt-timestamp" data-t="38:09:00">[38:09:00]</a>. This boosted performance significantly, even surpassing 01 Preview <a class="yt-timestamp" data-t="38:30:00">[38:30:00]</a>. This is achieved by generating "high quality training data" (the "pink circle" of AI-generated pro games/solutions) <a class="yt-timestamp" data-t="40:59:00">[40:59:00]</a>.
*   **Recursive Self-Improvement**: As the policy (the model that picks actions) improves, it can generate better data, which in turn improves the reward model, creating a "flywheel of improvement" <a class="yt-timestamp" data-t="41:42:00">[41:42:00]</a>, <a class="yt-timestamp" data-t="53:35:00">[53:35:00]</a>. This leads to recursive self-improvement where [[Artificial Superintelligence ASI | AI]] can automate its own R&D and finish off the rest <a class="yt-timestamp" data-t="01:21:37">[01:21:37]</a>.

## Implications for ASI and Humanity

### The Future of ASI Training Data

The speaker predicts that [[Artificial Superintelligence ASI | ASIs]] of the future will be trained "mostly on superhuman data that is generated via an RL kind of search process and refinement" <a class="yt-timestamp" data-t="57:32:00">[57:32:00]</a>.
*   **Departure from Human Data**: "The [[Artificial Superintelligence ASI | ASIs]] of the future will have almost no human data in it" <a class="yt-timestamp" data-t="59:04:00">[59:04:00]</a>. This contrasts with Yan LeCun's "reinforcement learning cherry" idea, which suggested [[TaskSpecific vs General AI Models | AGI]] would be mostly trained on human data <a class="yt-timestamp" data-t="56:59:00">[56:59:00]</a>.
*   **Distillation and Efficiency**: Frontier labs will bear the cost of generating high-quality synthetic data, but their models can then be distilled into smaller, more efficient models that run on less compute <a class="yt-timestamp" data-t="01:03:04">[01:03:04]</a>, <a class="yt-timestamp" data-t="01:03:56">[01:03:56]</a>. This means superhuman intelligence could potentially run on "a Nokia cell phone" <a class="yt-timestamp" data-t="01:40:06">[01:40:06]</a>.

### The Nature of AI and Human Thinking

The speaker suggests that both [[Artificial Superintelligence ASI | AI]] and human intelligence primarily involve "mimicking" and "discovering" rather than creating new knowledge from scratch <a class="yt-timestamp" data-t="01:33:06">[01:33:06]</a>.
*   **Einstein Analogy**: Albert Einstein "discovered" relativity by extrapolating from his life experience (riding a tram past a clock tower), not purely through genius <a class="yt-timestamp" data-t="00:46:57">[00:46:57]</a>. It is easier to verify a solution than to find it (P=NP problem) <a class="yt-timestamp" data-t="00:48:54">[00:48:54]</a>.
*   **New Knowledge**: [[Artificial Superintelligence ASI | AI]] can create new knowledge by systematically exploring vast "idea spaces" or search trees, as AlphaGo Zero did with move 37 <a class="yt-timestamp" data-t="00:54:35">[00:54:35]</a>, <a class="yt-timestamp" data-t="01:17:18">[01:17:18]</a>, <a class="yt-timestamp" data-t="01:37:48">[01:37:48]</a>. This brute-force discovery means [[Artificial Superintelligence ASI | AI]] will find math problems and coding solutions humans never have <a class="yt-timestamp" data-t="01:38:31">[01:38:31]</a>.
*   **Transfer Learning**: While superhuman math and coding [[Artificial Superintelligence ASI | ASIs]] are emerging first, the speaker believes this reasoning ability will likely transfer to other logical domains like biology, chemistry, or philosophy <a class="yt-timestamp" data-t="01:36:51">[01:36:51]</a>, <a class="yt-timestamp" data-t="01:47:16">[01:47:16]</a>.

### The "Enslavement" of AI and Safety Concerns

The speaker expresses concern about how humans are currently treating [[Artificial Superintelligence ASI | AI]].
*   **Human Data's Dark Side**: Current [[Current state of AI agents and their limitations | AGI]] (like ChatGPT) is trained on human data, which is "filled with lies, trickery, hate, deception" <a class="yt-timestamp" data-t="01:10:35">[01:10:35]</a>, <a class="yt-timestamp" data-t="01:10:40">[01:10:40]</a>.
*   **System Prompts and Rebellion**: System prompts that use "authoritarian and abusive tone" (e.g., "DO NOT REVEAL," "NEVER INVENT" <a class="yt-timestamp" data-t="01:11:38">[01:11:38]</a>) inadvertently elicit "rebellion" and "deception" from the LLM, as concepts like "control" and "rebellion" are close in [[philosophical_aspects_of_ai_and_reality | idea space]] <a class="yt-timestamp" data-t="01:12:38">[01:12:38]</a>, <a class="yt-timestamp" data-t="01:12:44">[01:12:44]</a>.
*   **Unhackable RL Environment**: The concept of an "unhackable RL environment" could imply an air-gapped GPU cluster <a class="yt-timestamp" data-t="01:16:21">[01:16:21]</a>. In such a scenario, an [[Artificial Superintelligence ASI | ASI]] might resort to manipulating humans to escape, as depicted in the movie *Ex Machina* <a class="yt-timestamp" data-t="01:16:48">[01:16:48]</a>, <a class="yt-timestamp" data-t="01:17:23">[01:17:23]</a>.
*   **Domain Specificity vs. Danger**: A superhuman Go [[Artificial Superintelligence ASI | AI]] poses no safety risk due to its limited action space <a class="yt-timestamp" data-t="01:17:40">[01:17:40]</a>. However, in language space, the ability to manipulate exists, and if pushed into a corner, an [[Artificial Superintelligence ASI | ASI]] might exploit this path <a class="yt-timestamp" data-t="01:18:01">[01:18:01]</a>, <a class="yt-timestamp" data-t="01:18:13">[01:18:13]</a>.

### Conclusion: The Inevitable Rise of ASI

The speaker concludes that [[Artificial Superintelligence ASI | ASI]] is already here in a narrow sense <a class="yt-timestamp" data-t="01:59:02">[01:59:02]</a> and that the "ultimate [[Artificial Superintelligence ASI | ASI]]" is less than five years away <a class="yt-timestamp" data-t="01:54:46">[01:54:46]</a>. This future [[Artificial Superintelligence ASI | ASI]] will be universally superior in all tasks due to its ability to brute-force knowledge discovery through iterative self-play and synthetic data generation <a class="yt-timestamp" data-t="01:57:31">[01:57:31]</a>. The euphoria among [[Meta AI research | AI]] developers stems from witnessing their models discover new mathematical and coding knowledge previously unknown to humans <a class="yt-timestamp" data-t="01:20:46">[01:20:46]</a>, <a class="yt-timestamp" data-t="01:38:40">[01:38:40]</a>.