---
title: Conditional Computation in Language Models
videoId: Teru_qIdB8Y
---

From: [[hu-po]] <br/> 

Conditional computation is a technique in [[Large Language Models as optimizers | language models]] where the computational resources are dynamically allocated based on the input, rather than uniformly applying the same computation to every part of the input [00:03:02](<a class="yt-timestamp" data-t="00:03:02">[00:03:02]</a>, [00:14:01](<a class="yt-timestamp" data-t="00:14:01">[00:14:01]</a>). This approach aims to improve efficiency by skipping unnecessary computations [00:03:17](<a class="yt-timestamp" data-t="00:03:17">[00:03:17]</a>). A recent paper from Google DeepMind, "Mixture of Depths" (MoD), proposes a novel method for this, building upon the concept of [[language_models_as_q_functions | Mixture of Experts]] (MoE) [00:02:05](<a class="yt-timestamp" data-t="00:02:05">[00:02:05]</a>, [00:02:36](<a class="yt-timestamp" data-t="00:02:36">[00:02:36]</a>).

## Traditional Transformer Computation
Traditional Transformer-based [[Large Language Models as optimizers | language models]] uniformly distribute computation, measured in FLOPs (floating point operations), across input sequences [00:04:02](<a class="yt-timestamp" data-t="00:04:02">[00:04:02]</a>, [00:04:06](<a class="yt-timestamp" data-t="00:04:06">[00:04:06]</a>). The majority of this computation occurs in the attention mechanism, which treats every token in the sequence equally [00:04:10](<a class="yt-timestamp" data-t="00:04:10">[00:04:10]</a>, [00:04:18](<a class="yt-timestamp" data-t="00:04:18">[00:04:18]</a>). This attention mechanism is computationally quadratic with respect to the input sequence length [00:11:40](<a class="yt-timestamp" data-t="00:11:40">[00:11:40]</a>, [00:14:23](<a class="yt-timestamp" data-t="00:14:23">[00:14:23]</a>, [00:53:53](<a class="yt-timestamp" data-t="00:53:53">[00:53:53]</a>). The feed-forward network (MLP) within a Transformer block also treats all tokens equally [00:05:50](<a class="yt-timestamp" data-t="00:05:50">[00:05:50]</a>, [00:14:35](<a class="yt-timestamp" data-t="00:14:35">[00:14:35]</a>).

### Computation Graphs and Hardware Efficiency
Modern hardware prioritizes static computation graphs and known tensor sizes for maximizing utilization [00:14:45](<a class="yt-timestamp" data-t="00:14:45">[00:14:45]</a>, [00:14:48](<a class="yt-timestamp" data-t="00:14:48">[00:14:48]</a>). A computation graph defines the series of operations to be performed, and for efficient execution on GPUs, these graphs need to be static (predictable in size and order) [00:06:50](<a class="yt-timestamp" data-t="00:06:50">[00:06:50]</a>, [00:15:56](<a class="yt-timestamp" data-t="00:15:56">[00:15:56]</a>, [00:22:01](<a class="yt-timestamp" data-t="00:22:01">[00:22:01]</a>).

## Mixture of Depths (MoD)
Mixture of Depths (MoD) allows Transformer models to dynamically allocate FLOPs to specific positions in a sequence [00:04:51](<a class="yt-timestamp" data-t="00:04:51">[00:04:51]</a>, [00:04:53](<a class="yt-timestamp" data-t="00:04:53">[00:04:53]</a>). The goal is to enforce a total compute budget by capping the number of tokens participating in self-attention and MLP computations [00:05:23](<a class="yt-timestamp" data-t="00:05:23">[00:05:23]</a>, [00:05:27](<a class="yt-timestamp" data-t="00:05:27">[00:05:27]</a>). This results in a compute expenditure that is predictable in total but dynamic and context-sensitive at the token level [00:07:34](<a class="yt-timestamp" data-t="00:07:34">[00:07:34]</a>, [00:07:38](<a class="yt-timestamp" data-t="00:07:38">[00:07:38]</a>).

### Mechanism: Skipping Computations
MoD models use a routing mechanism to decide whether a token will go through a Transformer block (self-attention and MLP) or bypass it via a residual connection [00:23:40](<a class="yt-timestamp" data-t="00:23:40">[00:23:40]</a>, [00:23:50](<a class="yt-timestamp" data-t="00:23:50">[00:23:50]</a>, [00:32:00](<a class="yt-timestamp" data-t="00:32:00">[00:32:00]</a>). This is similar to [[language_models_as_q_functions | Mixture of Experts]] which routes tokens to different experts, but MoD routes tokens for the purpose of skipping unnecessary compute [00:03:07](<a class="yt-timestamp" data-t="00:03:07">[00:03:07]</a>, [00:03:17](<a class="yt-timestamp" data-t="00:03:17">[00:03:17]</a>). A residual connection allows information to skip layers [00:25:08](<a class="yt-timestamp" data-t="00:25:08">[00:25:08]</a>, [00:25:11](<a class="yt-timestamp" data-t="00:25:11">[00:25:11]</a>).

> [!note] Residual Connections
> The concept of residual connections (or skip connections) was popularized by the "Deep Residual Learning for Image Recognition" paper in December 2015 [00:24:11](<a class="yt-timestamp" data-t="00:24:11">[00:24:11]</a>, [00:24:15](<a class="yt-timestamp" data-t="00:24:15">[00:24:15]</a>). These connections enable information to bypass certain layers, which has been shown to improve model performance and aid in gradient flow [00:25:08](<a class="yt-timestamp" data-t="00:25:08">[00:25:08]</a>, [00:25:22](<a class="yt-timestamp" data-t="00:25:22">[00:25:22]</a>).

The tokens to be processed are determined by the network using a top-K routing mechanism [00:06:04](<a class="yt-timestamp" data-t="00:06:04">[00:06:04]</a>, [00:06:07](<a class="yt-timestamp" data-t="00:06:07">[00:06:07]</a>). Since K is defined a priori, this ensures a static computation graph with known tensor sizes, which is crucial for efficient hardware execution [00:06:17](<a class="yt-timestamp" data-t="00:06:17">[00:06:17]</a>, [00:06:24](<a class="yt-timestamp" data-t="00:06:24">[00:06:24]</a>, [00:38:12](<a class="yt-timestamp" data-t="00:38:12">[00:38:12]</a>, [00:38:14](<a class="yt-timestamp" data-t="00:38:14">[00:38:14]</a>).

### Compute Efficiency
By setting a block's "capacity" (the total number of tokens input to a computation) to T/2 (i.e., allowing 50% of tokens to skip), the self-attention computation becomes 25% as FLOP-intensive [00:41:01](<a class="yt-timestamp" data-t="00:41:01">[00:41:01]</a>, [00:44:20](<a class="yt-timestamp" data-t="00:44:20">[00:44:20]</a>, [00:44:25](<a class="yt-timestamp" data-t="00:44:25">[00:44:25]</a>). This is because the self-attention computation scales quadratically (O(N²)) with the sequence length (N), so halving N results in a 1/4 reduction in computation ( (N/2)² = N²/4) [00:36:06](<a class="yt-timestamp" data-t="00:36:06">[00:36:06]</a>, [00:44:36](<a class="yt-timestamp" data-t="00:44:36">[00:44:36]</a>).

## Routing Schemes
The effectiveness of MoD depends on intelligent routing, as random routing significantly underperforms [00:46:12](<a class="yt-timestamp" data-t="00:46:12">[00:46:12]</a>, [00:46:19](<a class="yt-timestamp" data-t="00:46:19">[00:46:19]</a>). Two main learned routing schemes are discussed:

1.  **Token Choice**: Each token's router produces a probability distribution across computational paths, allowing tokens to choose their preferred path. If a path's capacity is exceeded, surplus tokens are dropped, leading to wasted computation on empty slots [00:46:57](<a class="yt-timestamp" data-t="00:46:57">[00:46:57]</a>, [00:47:00](<a class="yt-timestamp" data-t="00:47:00">[00:47:00]</a>, [00:48:08](<a class="yt-timestamp" data-t="00:48:08">[00:48:08]</a>, [00:48:22](<a class="yt-timestamp" data-t="00:48:22">[00:48:22]</a>, [00:49:54](<a class="yt-timestamp" data-t="00:49:54">[00:49:54]</a>).
2.  **Expert Choice**: Instead of tokens choosing, each path (or "expert") chooses the top-K tokens based on their router preferences (scalar weights) [00:47:15](<a class="yt-timestamp" data-t="00:47:15">[00:47:15]</a>, [00:47:21](<a class="yt-timestamp" data-t="00:47:21">[00:47:21]</a>, [00:50:28](<a class="yt-timestamp" data-t="00:50:28">[00:50:28]</a>). This ensures perfect load balance, as precisely K tokens are shuttled to each path, avoiding empty slots and dropped tokens for compute [00:47:21](<a class="yt-timestamp" data-t="00:47:21">[00:47:21]</a>, [00:47:24](<a class="yt-timestamp" data-t="00:47:24">[00:47:24]</a>, [00:50:50](<a class="yt-timestamp" data-t="00:50:50">[00:50:50]</a>).

The MoD paper specifically deploys **Expert Choice routing** [00:51:45](<a class="yt-timestamp" data-t="00:51:45">[00:51:45]</a>). The router weight for a token is a scalar produced by a linear projection of the token's embedding with a learnable weight matrix (parameters) [00:53:22](<a class="yt-timestamp" data-t="00:53:22">[00:53:22]</a>, [00:53:27](<a class="yt-timestamp" data-t="00:53:27">[00:53:27]</a>, [00:53:34](<a class="yt-timestamp" data-t="00:53:34">[00:53:34]</a>). These router parameters are trained via gradient descent, similar to other model weights [01:44:20](<a class="yt-timestamp" data-t="01:44:20">[01:44:20]</a>, [01:44:31](<a class="yt-timestamp" data-t="01:44:31">[01:44:31]</a>).

### Implementation Challenges
The top-K operation for expert choice routing is non-causal during autoregressive sampling [00:55:39](<a class="yt-timestamp" data-t="00:55:39">[00:55:39]</a>, [00:55:40](<a class="yt-timestamp" data-t="00:55:40">[00:55:40]</a>). This means it depends on router weights for future tokens, which are not available when generating one token at a time [00:55:41](<a class="yt-timestamp" data-t="00:55:41">[00:55:41]</a>, [00:56:19](<a class="yt-timestamp" data-t="00:56:19">[00:56:19]</a>).

To address this, MoD introduces a small auxiliary MLP predictor (a second router) during training [00:57:27](<a class="yt-timestamp" data-t="00:57:27">[00:57:27]</a>, [00:57:52](<a class="yt-timestamp" data-t="00:57:52">[00:57:52]</a>). This predictor learns to classify whether a token will be among the top-K or not, based only on available (causal) information [00:58:00](<a class="yt-timestamp" data-t="00:58:00">[00:58:00]</a>, [00:58:04](<a class="yt-timestamp" data-t="00:58:04">[00:58:04]</a>). This method does not significantly impact the language modeling objective or step speed and involves a stop gradient to prevent it from impacting the main model [00:58:29](<a class="yt-timestamp" data-t="00:58:29">[00:58:29]</a>, [00:58:31](<a class="yt-timestamp" data-t="00:58:31">[00:58:31]</a>, [00:58:37](<a class="yt-timestamp" data-t="00:58:37">[00:58:37]</a>, [00:59:02](<a class="yt-timestamp" data-t="00:59:02">[00:59:02]</a>).

## Performance and Implications
MoD models demonstrate significant improvements in both training and inference efficiency:

*   **Training Efficiency**: MoD Transformers can achieve baseline performance for equivalent FLOPs and wall clock times to train, or achieve lower loss in the same amount of time [00:07:51](<a class="yt-timestamp" data-t="00:07:51">[00:07:51]</a>, [01:05:50](<a class="yt-timestamp" data-t="01:05:50">[01:05:50]</a>, [01:06:02](<a class="yt-timestamp" data-t="01:06:02">[01:06:02]</a>).
*   **Inference Efficiency**: MoD requires a fraction of the FLOPs for the forward pass, leading to upwards of 50% faster inference [00:07:55](<a class="yt-timestamp" data-t="00:07:55">[00:07:55]</a>, [00:08:39](<a class="yt-timestamp" data-t="00:08:39">[00:08:39]</a>, [01:06:43](<a class="yt-timestamp" data-t="01:06:43">[01:06:43]</a>, [01:09:51](<a class="yt-timestamp" data-t="01:09:51">[01:09:51]</a>). This means tokens are generated much faster, which is critical for serving [[Multimodal Language Models | models]] to users [00:08:45](<a class="yt-timestamp" data-t="00:08:45">[00:08:45]</a>, [01:06:37](<a class="yt-timestamp" data-t="01:06:37">[01:06:37]</a>).

> [!info] Iso-FLOP Comparison
> MoD models are compared to "iso-FLOP" vanilla Transformers, meaning they use the same total amount of computational FLOPs [00:29:00](<a class="yt-timestamp" data-t="00:29:00">[00:29:00]</a>, [00:29:19](<a class="yt-timestamp" data-t="00:29:19">[00:29:19]</a>). This allows for a fair comparison of their performance while accounting for slight increases in parameters from the MoD router [00:54:11](<a class="yt-timestamp" data-t="00:54:11">[00:54:11]</a>, [00:54:35](<a class="yt-timestamp" data-t="00:54:35">[00:54:35]</a>).

### Adding Depth vs. Adding Width
Empirical findings suggest it is better to add depth (more layers) than to add width (more neurons per layer) when adding FLOPs to a model [01:12:25](<a class="yt-timestamp" data-t="01:12:25">[01:12:25]</a>, [01:12:27](<a class="yt-timestamp" data-t="01:12:27">[01:12:27]</a>, [01:13:01](<a class="yt-timestamp" data-t="01:13:01">[01:13:01]</a>). This aligns with the long-standing intuition in deep learning that deeper networks learn better hierarchies of features [01:11:53](<a class="yt-timestamp" data-t="01:11:53">[01:11:53]</a>, [01:12:08](<a class="yt-timestamp" data-t="01:12:08">[01:12:08]</a>).

### KV Cache Size
Modifications introduced by MoD can have significant positive effects on the KV (Key-Value) cache size during autoregressive sampling [01:13:30](<a class="yt-timestamp" data-t="01:13:30">[01:13:30]</a>, [01:13:32](<a class="yt-timestamp" data-t="01:13:32">[01:13:32]</a>). The KV cache stores the Key and Value vectors from previous tokens, avoiding redundant calculations during sequence generation [01:14:04](<a class="yt-timestamp" data-t="01:14:04">[01:14:04]</a>, [01:14:07](<a class="yt-timestamp" data-t="01:14:07">[01:14:07]</a>). Since MoD reduces the number of tokens processed by attention, it allows for a larger KV cache or fits more information in it [01:14:22](<a class="yt-timestamp" data-t="01:14:22">[01:14:22]</a>, [01:14:25](<a class="yt-timestamp" data-t="01:14:25">[01:14:25]</a>, [01:14:44](<a class="yt-timestamp" data-t="01:14:44">[01:14:44]</a>).

### Qualitative Improvements
Beyond efficiency, the ability for tokens to dynamically choose which levels of the abstraction hierarchy they engage with might lead to qualitative improvements in the model's ability to predict the next token, serving as a proxy for intelligence [01:14:54](<a class="yt-timestamp" data-t="01:14:54">[01:14:54]</a>, [01:42:00](<a class="yt-timestamp" data-t="01:42:00">[01:42:00]</a>, [01:42:05](<a class="yt-timestamp" data-t="01:42:05">[01:42:05]</a>, [01:42:28](<a class="yt-timestamp" data-t="01:42:28">[01:42:28]</a>).

## Mixture of Depths and Experts (MoDE)
The paper also explores combining MoD with [[language_models_as_q_functions | Mixture of Experts]] (MoE), leading to "Mixture of Depths and Experts" (MoDE) [01:17:03](<a class="yt-timestamp" data-t="01:17:03">[01:17:03]</a>, [01:17:06](<a class="yt-timestamp" data-t="01:17:06">[01:17:06]</a>). Two variants are introduced:

1.  **Staged MoD**: A standard MoE Transformer block is preceded by an additional router that decides whether to send tokens through the entire attention block or skip it [01:17:18](<a class="yt-timestamp" data-t="01:17:18">[01:17:18]</a>, [01:17:21](<a class="yt-timestamp" data-t="01:17:21">[01:17:21]</a>, [01:18:18](<a class="yt-timestamp" data-t="01:18:18">[01:18:18]</a>). This offers significant computational efficiency by allowing tokens to skip the self-attention step, which is the most compute-heavy part [01:17:34](<a class="yt-timestamp" data-t="01:17:34">[01:17:34]</a>, [01:20:04](<a class="yt-timestamp" data-t="01:20:04">[01:20:04]</a>).
2.  **Integrated MoD**: This approach simplifies the routing machinery by adding a "noop" (no-operation) expert alongside conventional MLP experts within the existing MoE framework [01:17:26](<a class="yt-timestamp" data-t="01:17:26">[01:17:26]</a>, [01:18:26](<a class="yt-timestamp" data-t="01:18:26">[01:18:26]</a>, [01:18:43](<a class="yt-timestamp" data-t="01:18:43">[01:18:43]</a>, [01:19:55](<a class="yt-timestamp" data-t="01:19:55">[01:19:55]</a>). While simpler and adaptable to existing MoE models, it doesn't allow tokens to skip the self-attention computation, thus offering less compute efficiency compared to Staged MoD [01:19:37](<a class="yt-timestamp" data-t="01:19:37">[01:19:37]</a>, [01:20:41](<a class="yt-timestamp" data-t="01:20:41">[01:20:41]</a>).

## Overall Significance
MoD represents a simple yet powerful modification to Transformer-based [[Large Language Models as optimizers | language models]], offering both compute efficiency (e.g., 50% faster inference) and performance improvements (lower loss) [01:37:57](<a class="yt-timestamp" data-t="01:37:57">[01:37:57]</a>, [01:38:00](<a class="yt-timestamp" data-t="01:38:00">[01:38:00]</a>, [01:41:49](<a class="yt-timestamp" data-t="01:41:49">[01:41:49]</a>). Its generic nature and compatibility with other techniques, like [[language_models_as_q_functions | Mixture of Experts]], suggest it will likely be widely adopted in future [[Large Language Models as optimizers | language models]] development [01:35:29](<a class="yt-timestamp" data-t="01:35:29">[01:35:29]</a>, [01:39:53](<a class="yt-timestamp" data-t="01:39:53">[01:39:53]</a>, [01:42:36](<a class="yt-timestamp" data-t="01:42:36">[01:42:36]</a>). The concept can be extended further, potentially routing tokens to even more specialized computations like memory lookups or tool use [01:34:38](<a class="yt-timestamp" data-t="01:34:38">[01:34:38]</a>, [01:34:44](<a class="yt-timestamp" data-t="01:34:44">[01:34:44]</a>).