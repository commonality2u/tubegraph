---
title: LLM Large Language Models development
videoId: 1ZwXkw9_Xq8
---

From: [[hu-po]] <br/> 

Llama 2 is a collection of pre-trained and fine-tuned [[large_language_models_llms_and_scaling | large language models]] developed and released by Meta <a class="yt-timestamp" data-t="00:03:41">[00:03:41]</a>. It is described as the first open-source, big competitive [[large_language_models_llms_and_scaling | LLM]] <a class="yt-timestamp" data-t="00:00:55">[00:00:55]</a>. Unlike secretive models from companies like OpenAI and Google, the Llama 2 paper provides extensive detail on its development <a class="yt-timestamp" data-t="00:02:45">[00:02:45]</a>.

## Llama 2 Model Variants
Llama 2 models range in scale from 7 billion to 70 billion parameters <a class="yt-timestamp" data-t="00:03:55">[00:03:55]</a>. There are "normal models" (pre-trained) and "chat models" (fine-tuned) <a class="yt-timestamp" data-t="00:01:54">[00:01:54]</a>. The fine-tuned chat models, called Llama 2 Chat, are optimized for dialogue use cases <a class="yt-timestamp" data-t="00:03:59">[00:03:59]</a>. While the chat models are certainly filtered, even the pre-trained models are suspected to have undergone some filtering <a class="yt-timestamp" data-t="00:02:03">[00:02:03]</a>. Notably, a 34B parameter model was mentioned in results but not released due to lack of time for sufficient "Red Teaming" <a class="yt-timestamp" data-t="00:09:24">[00:09:24]</a>, <a class="yt-timestamp" data-t="00:25:38">[00:25:38]</a>.

## Development Process

### Pre-training
The Llama 2 models were pre-trained using an optimized auto-regressive Transformer architecture <a class="yt-timestamp" data-t="00:21:52">[00:21:52]</a>.
Key changes from Llama 1 include:
*   **Data Corpus:** The pre-training corpus size was increased by 40% <a class="yt-timestamp" data-t="00:18:18">[00:18:18]</a>. A new mix of publicly available data was used, explicitly excluding data from Meta's own products or services <a class="yt-timestamp" data-t="00:22:53">[00:22:53]</a>, <a class="yt-timestamp" data-t="02:23:00">[02:23:00]</a>. Efforts were made to remove data from sites known to contain high volumes of personal information <a class="yt-timestamp" data-t="00:23:35">[00:23:35]</a>. The models were trained on two trillion tokens of data <a class="yt-timestamp" data-t="00:23:45">[00:23:45]</a>.
*   **Data Mix:** Factual resources were "up-sampled" (sampled more frequently) in the data mix to increase knowledge and dampen hallucinations, optimizing performance-cost trade-off <a class="yt-timestamp" data-t="00:23:57">[00:23:57]</a>.
*   **Context Length:** The context length was doubled from 2048 tokens to 4096 tokens <a class="yt-timestamp" data-t="00:18:20">[00:18:20]</a>, <a class="yt-timestamp" data-t="00:29:16">[00:29:16]</a>. This significantly improved performance on long-context benchmarks like Scrolls <a class="yt-timestamp" data-t="00:31:10">[00:31:10]</a>.
*   **Attention Mechanism:** [[large_language_models_and_inference_efficiency | Grouped Query Attention]] (GQA) was adopted <a class="yt-timestamp" data-t="00:18:22">[00:18:22]</a>. This modification reduces memory cost associated with KV (Key-Value) cache size in multi-head attention during auto-regressive decoding <a class="yt-timestamp" data-t="00:33:17">[00:33:17]</a>. GQA involves sharing key and value projections across multiple attention heads, saving memory without significant performance degradation compared to Multi-Head Attention (MHA) or Multi-Query Attention (MQA) <a class="yt-timestamp" data-t="00:33:35">[00:33:35]</a>, <a class="yt-timestamp" data-t="00:35:50">[00:35:50]</a>. GQA was chosen over MQA for better inference performance, particularly with tensor parallelism across multiple GPUs <a class="yt-timestamp" data-t="00:39:36">[00:39:36]</a>.
*   **Tokenizer:** The same byte pair encoding tokenizer as Llama 1 was used, with a vocabulary size of 32,000 tokens <a class="yt-timestamp" data-t="00:50:01">[00:50:01]</a>, <a class="yt-timestamp" data-t="00:52:17">[00:52:17]</a>. Numbers are split into individual digits, and unknown UTF-8 characters are decomposed into bytes <a class="yt-timestamp" data-t="00:51:32">[00:51:32]</a>.

### Fine-tuning and Alignment
[[finetuning_large_language_models | Llama 2 Chat]] is the result of iterative applications of alignment techniques, including instruction tuning and [[selfimprovement_and_planning_for_large_language_models | Reinforcement Learning from Human Feedback]] (RLHF) <a class="yt-timestamp" data-t="01:03:18">[01:03:18]</a>.

*   **Supervised Fine-Tuning (SFT):** The SFT stage started with publicly available instruction tuning data, but Meta found it lacked diversity and quality <a class="yt-timestamp" data-t="01:05:20">[01:05:20]</a>. They focused on collecting several thousand examples of high-quality SFT data through their own vendor-based annotation efforts <a class="yt-timestamp" data-t="01:05:43">[01:05:43]</a>. This data did not include Meta user data <a class="yt-timestamp" data-t="01:07:22">[01:07:22]</a>. They found that a limited set of clean instruction tuning data could achieve a high level of quality (27,540 annotations) <a class="yt-timestamp" data-t="01:06:33">[01:06:33]</a>.
*   **Reinforcement Learning from Human Feedback (RLHF):** RLHF further aligns the fine-tuned model with human preferences <a class="yt-timestamp" data-t="01:10:36">[01:10:36]</a>.
    *   **Reward Model:** Human annotators select preferred responses between two model outputs, and this feedback is used to train a separate reward model <a class="yt-timestamp" data-t="01:10:51">[01:10:51]</a>, <a class="yt-timestamp" data-t="01:11:02">[01:11:02]</a>.
    *   **Helpfulness vs. Safety:** To address the trade-off between helpfulness and safety, two separate reward models were trained: one optimized for helpfulness and another for safety <a class="yt-timestamp" data-t="01:17:27">[01:17:27]</a>, <a class="yt-timestamp" data-t="01:17:35">[01:17:35]</a>.
    *   **Data Collection:** Human preference data for reward modeling was collected weekly in batches, using a binary comparison protocol <a class="yt-timestamp" data-t="01:11:17">[01:11:17]</a>. The process involves annotators writing a prompt and choosing between two sampled model responses (from different model variants) <a class="yt-timestamp" data-t="01:11:35">[01:11:35]</a>. Meta's reward modeling data set is significantly larger than others, with over 1.5 million comparisons <a class="yt-timestamp" data-t="01:15:52">[01:15:52]</a>. This data is designed for multi-turn conversations <a class="yt-timestamp" data-t="01:16:43">[01:16:43]</a>.
    *   **Training Objective:** A binary ranking loss is used to train the reward model, enforcing the chosen response to have a higher score than its counterpart <a class="yt-timestamp" data-t="01:18:05">[01:18:05]</a>. A margin component was added to the loss function to encourage more discrepant scores for more separable responses, improving helpfulness <a class="yt-timestamp" data-t="01:19:37">[01:19:37]</a>.
    *   **Iterative Fine-tuning (RLHF V1-V5):** As more human preference data was collected, successive versions of the RLHF models (V1 to V5) were trained <a class="yt-timestamp" data-t="01:29:03">[01:29:03]</a>. This process leverages the model to generate more fine-tuning data, similar to the self-labeling strategy in the Segment Anything Model <a class="yt-timestamp" data-t="01:29:21">[01:29:21]</a>.
    *   **Ghost Attention (GAt):** Llama 2 Chat initially struggled to maintain adherence to initial instructions over multiple turns <a class="yt-timestamp" data-t="01:43:15">[01:43:15]</a>. Ghost Attention (GAt) was proposed to address this by synthetically concatenating the original system instruction to all user messages during training <a class="yt-timestamp" data-t="01:43:30">[01:43:30]</a>, <a class="yt-timestamp" data-t="01:44:09">[01:44:09]</a>. This ensures the model continuously "pays attention" to the original prompt, even in long dialogues <a class="yt-timestamp" data-t="01:49:35">[01:49:35]</a>.

## Evaluation and Benchmarks

Llama 2 models were evaluated using both human evaluations and benchmark tests.
*   **Human Evaluation:** Human evaluators compared model generations for helpfulness and safety across thousands of prompts <a class="yt-timestamp" data-t="00:08:09">[00:08:09]</a>, <a class="yt-timestamp" data-t="00:09:09">[00:09:09]</a>. Llama 2 outperformed open-source chat models like Falcon 40B on most helpfulness benchmarks <a class="yt-timestamp" data-t="00:04:04">[00:04:04]</a>, <a class="yt-timestamp" data-t="00:09:09">[00:09:09]</a>. It performed comparably to Google's PaLM Bison and OpenAI's ChatGPT 3.5 in helpfulness <a class="yt-timestamp" data-t="00:09:38">[00:09:38]</a>, <a class="yt-timestamp" data-t="00:10:41">[00:10:41]</a>. In terms of safety, Llama 2 Chat showed lower violation rates than other open and closed-source models <a class="yt-timestamp" data-t="00:16:46">[00:16:46]</a>.
*   **Model-based Evaluation (GPT-4 as Judge):** GPT-4 was also used as a judge for helpfulness and safety win rates against commercial baselines like GPT-3, PaLM Bison, and Falcon 40B, where Llama 2 Chat showed better performance in most cases <a class="yt-timestamp" data-t="00:11:43">[00:11:43]</a>, <a class="yt-timestamp" data-t="00:12:22">[00:12:22]</a>.
*   **Academic Benchmarks:** Llama 2 was compared against other open-source and closed-source models on various academic benchmarks including MMLU, TriviaQA, and GSM8K (grade school math word problems) <a class="yt-timestamp" data-t="00:59:06">[00:59:06]</a>, <a class="yt-timestamp" data-t="01:00:49">[01:00:49]</a>. Llama 2 generally showed competitive performance, although GPT-4 still significantly outperformed it in some areas, particularly GSM8K <a class="yt-timestamp" data-t="01:00:21">[01:00:21]</a>.
*   **Safety & Helpfulness Trade-off:** The development showed that helpfulness and safety scores can increase together, indicating a balance can be achieved <a class="yt-timestamp" data-t="02:07:45">[02:07:45]</a>, <a class="yt-timestamp" data-t="02:20:05">[02:20:05]</a>. However, models with more safety mitigation tend to answer certain questions in a more conservative manner, leading to "false refusals" when a prompt is safe but contains words frequently associated with unsafe generations (e.g., "Christmas crack" or "bomb") <a class="yt-timestamp" data-t="02:08:40">[02:08:40]</a>, <a class="yt-timestamp" data-t="02:09:15">[02:09:15]</a>.

## Technical Details

*   **Architecture:** Llama 2 uses a standard Transformer architecture with pre-normalization using RMS Norm <a class="yt-timestamp" data-t="02:26:23">[02:26:23]</a>, SwiGLU activation functions <a class="yt-timestamp" data-t="02:27:10">[02:27:10]</a>, and Rotary Positional Embeddings (RoPE) <a class="yt-timestamp" data-t="02:28:46">[02:28:46]</a>.
*   **Hyperparameters:** The AdamW optimizer was used for training <a class="yt-timestamp" data-t="00:42:51">[00:42:51]</a>. A cosine learning rate schedule with a warm-up period (2000 steps for pre-training, 3% for reward model) and a decay down to 10% of the maximum learning rate was employed <a class="yt-timestamp" data-t="00:43:05">[00:43:05]</a>, <a class="yt-timestamp" data-t="01:23:51">[01:23:51]</a>. Weight decay (0.1) and gradient clipping (1.0) were also applied <a class="yt-timestamp" data-t="00:44:29">[00:44:29]</a>, <a class="yt-timestamp" data-t="00:44:32">[00:44:32]</a>.
*   **Hardware:** Models were pre-trained on Meta's Research Supercluster (RSC) and internal production clusters, both using Nvidia A100 GPUs <a class="yt-timestamp" data-t="00:52:28">[00:52:28]</a>. RSC uses Nvidia Quantum InfiniBand interconnects and 400W per GPU, while the production cluster uses RoCE (RDMA over Converged Ethernet) and 350W per GPU <a class="yt-timestamp" data-t="00:52:47">[00:52:47]</a>, <a class="yt-timestamp" data-t="00:53:30">[00:53:30]</a>. RoCE is a more affordable network that can scale almost as well as InfiniBand up to 2000 GPUs <a class="yt-timestamp" data-t="00:55:24">[00:55:24]</a>.

## Ethical Considerations & Release Strategy
Meta emphasizes responsible AI development, helpfulness, and safety <a class="yt-timestamp" data-t="00:04:36">[00:04:36]</a>. The Llama 2 paper includes extensive sections on safety, red teaming, limitations, and ethical considerations <a class="yt-timestamp" data-t="00:05:14">[00:05:14]</a>.
*   **Red Teaming:** Internal employees, contract workers, and external vendors (totaling 350 people) were employed to proactively identify risks and perform "evil things" with the model <a class="yt-timestamp" data-t="00:05:35">[00:05:35]</a>, <a class="yt-timestamp" data-t="02:12:10">[02:12:10]</a>, <a class="yt-timestamp" data-t="02:12:40">[02:12:40]</a>.
*   **Bias Mitigation:** The paper addresses pronoun bias in the pre-training data, acknowledging that models reflect real-world biases <a class="yt-timestamp" data-t="01:59:45">[01:59:45]</a>.
*   **Open Release:** Llama 2 is made available for both research and commercial use under an acceptable use policy <a class="yt-timestamp" data-t="02:34:04">[02:34:04]</a>. Meta believes that open releases promote transparency, decentralize AI expertise, stimulate innovation, accelerate progress, and consolidate costs by avoiding duplicated training efforts across companies <a class="yt-timestamp" data-t="02:34:24">[02:34:24]</a>.

## Key Learnings and Opportunities
*   **Data Quality:** Robust data cleaning and strategic data mixing (e.g., up-sampling factual sources) are crucial <a class="yt-timestamp" data-t="00:22:16">[00:22:16]</a>, <a class="yt-timestamp" data-t="00:24:04">[00:24:04]</a>.
*   **Scaling Potential:** The models did not show signs of saturation, indicating they could have been trained for longer <a class="yt-timestamp" data-t="00:47:32">[00:47:32]</a>.
*   **RLHF Effectiveness:** [[selfimprovement_and_planning_for_large_language_models | Reinforcement learning]] proved highly effective for alignment, balancing cost and time efficiency <a class="yt-timestamp" data-t="02:22:02">[02:22:02]</a>.
*   **Emergent Capabilities:** Llama 2 demonstrates emergent capabilities such as time awareness and tool use, without explicit annotation for these functions <a class="yt-timestamp" data-t="02:26:19">[02:26:19]</a>, <a class="yt-timestamp" data-t="02:27:07">[02:27:07]</a>.
*   **Future Improvements:** Opportunities for Llama 3 include using Meta's internal proprietary data, adopting a more modern tokenizer, and training for longer durations <a class="yt-timestamp" data-t="02:46:16">[02:46:16]</a>. More advanced RL algorithms could also be explored beyond PPO <a class="yt-timestamp" data-t="01:30:50">[01:30:50]</a>. Fine-tuning methods could potentially move towards more parameter-efficient techniques like LoRA, rather than full model fine-tuning <a class="yt-timestamp" data-t="01:10:00">[01:10:00]</a>.

Llama 2 represents a significant step for open-source [[large_language_models_llms_and_scaling | AI]] <a class="yt-timestamp" data-t="02:56:52">[02:56:52]</a>, offering a competitive alternative to proprietary models and fostering transparency in [[advancements_in_language_models | LLM development]] <a class="yt-timestamp" data-t="02:36:33">[02:36:33]</a>.