---
title: Differentiable Rendering for RealTime Image Synthesis
videoId: xgwvU7S0K-k
---

From: [[hu-po]] <br/> 

Recent advancements in computer vision have revolutionized [[novel_view_synthesis_and_rendering_techniques | novel view synthesis]] of scenes captured with multiple photos or videos <a class="yt-timestamp" data-t="00:03:09">[00:03:09]</a>. Historically, achieving high visual quality and real-time display rates for high-resolution rendering (e.g., 1080p) has been a significant challenge <a class="yt-timestamp" data-t="00:05:50">[00:05:50]</a>.

## 3D Gaussian Splatting: A Novel Approach <a class="yt-timestamp" data-t="00:01:03">[00:01:03]</a>
The paper "3D Gaussian Splatting for Real-Time Radiance Field Rendering" introduces a new method to achieve state-of-the-art visual quality, competitive training times, and high-quality real-time [[novel_view_synthesis_and_rendering_techniques | novel view synthesis]] <a class="yt-timestamp" data-t="00:16:14">[00:16:14]</a> <a class="yt-timestamp" data-t="00:17:01">[00:17:01]</a>. This approach departs from traditional neural [[rendering_technology_and_algorithms | radiance field]] methods (like NeRFs) by using a different scene representation and [[rendering_technology_and_algorithms | rendering algorithm]] <a class="yt-timestamp" data-t="00:01:16">[00:01:16]</a> <a class="yt-timestamp" data-t="00:01:24">[00:01:24]</a>.

### Core Elements
The 3D Gaussian Splatting method is built upon three key elements:
1.  **3D Gaussian Scene Representation** <a class="yt-timestamp" data-t="00:16:48">[00:16:48]</a>: The scene is represented as a set of 3D Gaussians, initialized from sparse point clouds obtained through camera calibration processes like Structure from Motion (SfM) <a class="yt-timestamp" data-t="00:06:35">[00:06:35]</a> <a class="yt-timestamp" data-t="00:20:02">[00:20:02]</a>. Each 3D Gaussian is defined by its 3D position (mean), an anisotropic covariance matrix (describing its shape and orientation), an opacity value, and spherical harmonic coefficients for color <a class="yt-timestamp" data-t="00:59:43">[00:59:43]</a> <a class="yt-timestamp" data-t="01:00:58">[01:00:58]</a>. This representation is differentiable and can be efficiently projected into 2D "splats" for rendering <a class="yt-timestamp" data-t="02:22:25">[02:22:25]</a> <a class="yt-timestamp" data-t="01:04:07">[01:04:07]</a>.
2.  **Interleaved Optimization and Adaptive Density Control** <a class="yt-timestamp" data-t="00:07:43">[00:07:43]</a>: The properties of the 3D Gaussians (position, opacity, covariance, and color coefficients) are optimized using stochastic gradient descent (SGD) <a class="yt-timestamp" data-t="02:00:00">[02:00:00]</a> <a class="yt-timestamp" data-t="01:23:38">[01:23:38]</a>. This process is interleaved with steps that adaptively control the density of Gaussians, allowing for creation, destruction, and movement of geometry to accurately represent the scene <a class="yt-timestamp" data-t="01:22:15">[01:22:15]</a> <a class="yt-timestamp" data-t="01:28:46">[01:28:46]</a>.
    *   Gaussians that are essentially transparent (low opacity) are removed <a class="yt-timestamp" data-t="01:31:58">[01:31:58]</a>.
    *   In under-reconstructed regions (identified by large positional gradients), Gaussians are cloned and moved <a class="yt-timestamp" data-t="01:34:01">[01:34:01]</a>.
    *   In over-reconstructed regions (large Gaussians with high variance), Gaussians are split into smaller ones <a class="yt-timestamp" data-t="01:35:03">[01:35:03]</a>.
3.  **Fast Visibility-Aware [[Rendering_Technology_and_Algorithms | Rendering Algorithm]]** <a class="yt-timestamp" data-t="00:08:28">[00:08:28]</a>: A tile-based rasterizer, inspired by previous software [[rendering_technology_and_algorithms | rasterization]] approaches, supports anisotropic splatting and enables real-time [[rendering_technology_and_algorithms | rendering]] <a class="yt-timestamp" data-t="00:39:58">[00:39:58]</a> <a class="yt-timestamp" data-t="01:17:15">[01:17:15]</a>.
    *   The screen is split into 16x16 tiles <a class="yt-timestamp" data-t="01:41:04">[01:41:04]</a>.
    *   Gaussians are culled against the view frustum, and only those intersecting with a 99% confidence interval are kept <a class="yt-timestamp" data-t="01:41:31">[01:41:31]</a>.
    *   Gaussians are sorted based on a key combining view space depth and tile ID using a fast GPU Radix sort <a class="yt-timestamp" data-t="01:43:09">[01:43:09]</a>.
    *   Alpha blending is performed per tile, accumulating colors and alpha values, stopping when a target saturation is reached <a class="yt-timestamp" data-t="01:46:08">[01:46:08]</a>.
    *   The rasterizer is fully differentiable, allowing efficient gradient propagation for all blended Gaussians without hard limits <a class="yt-timestamp" data-t="01:40:46">[01:40:46]</a> <a class="yt-timestamp" data-t="01:52:07">[01:52:07]</a>.

## Comparison with NeRFs and Other Methods
Radiance field methods, particularly Neural Radiance Fields (NeRFs), have [[novel_view_synthesis_and_rendering_techniques | revolutionized novel view synthesis]] <a class="yt-timestamp" data-t="00:03:09">[00:03:09]</a>. However, NeRFs require neural networks that are costly to train and render <a class="yt-timestamp" data-t="00:04:16">[00:04:16]</a>. A major drawback of NeRFs is the need to train a new neural network for every single scene or object, and changes in lighting or object positions require retraining <a class="yt-timestamp" data-t="00:43:56">[00:43:56]</a>. They also struggle to achieve real-time display rates at 1080p resolution <a class="yt-timestamp" data-t="00:51:24">[00:51:24]</a>.

In contrast, 3D Gaussian Splatting offers significant performance improvements:
*   **Training Time**: Compared to a standard NeRF (48 hours), 3D Gaussian Splatting trains in 35-45 minutes <a class="yt-timestamp" data-t="02:10:48">[02:10:48]</a> <a class="yt-timestamp" data-t="02:10:50">[02:10:50]</a>. Even against faster methods like Instant NGP (7 minutes), 3D Gaussian Splatting achieves state-of-the-art quality in 5 to 10 minutes <a class="yt-timestamp" data-t="02:10:59">[02:10:59]</a>.
*   **[[Rendering_Technology_and_Algorithms | Rendering]] Speed**: It provides the first real-time [[rendering_technology_and_algorithms | rendering]] with high quality for [[novel_view_synthesis_and_rendering_techniques | novel view synthesis]] <a class="yt-timestamp" data-t="00:27:51">[00:27:51]</a> <a class="yt-timestamp" data-t="01:17:15">[01:17:15]</a>.
*   **Visual Quality**: The method achieves quality comparable to, or in some cases surpassing, state-of-the-art NeRF methods like MIP-NeRF 360 <a class="yt-timestamp" data-t="00:16:14">[00:16:14]</a> <a class="yt-timestamp" data-t="01:00:20">[01:00:20]</a>. It excels at preserving fine details (e.g., bicycle spokes) and avoiding the "fuzz" or "blurriness" often seen in NeRF outputs <a class="yt-timestamp" data-t="01:59:09">[01:59:09]</a>.
*   **Memory Usage**: While Nerfs are very compact (e.g., 8-13 megabytes for a scene representation) <a class="yt-timestamp" data-t="02:03:19">[02:03:19]</a>, 3D Gaussian Splatting models can be significantly larger due to storing explicit Gaussian parameters (e.g., 734 megabytes, or 20 GB peak GPU memory during training) <a class="yt-timestamp" data-t="02:13:16">[02:13:16]</a> <a class="yt-timestamp" data-t="02:23:44">[02:23:44]</a>. However, it is still more compact than other explicit point-based models like Plenoxels (around 2 GB) <a class="yt-timestamp" data-t="02:13:14">[02:13:14]</a>.

## Key Innovations and Design Choices
*   **Anisotropic Covariance**: Modeling Gaussians with full 3D covariance matrices allows them to adapt to the geometry of different shapes, representing fine structures compactly <a class="yt-timestamp" data-t="00:59:55">[00:59:55]</a> <a class="yt-timestamp" data-t="01:00:39">[01:00:39]</a>. Ablation studies show that this anisotropic property significantly improves quality compared to using simple spheres <a class="yt-timestamp" data-t="02:19:49">[02:19:49]</a>.
*   **Explicit Gradient Calculation**: To avoid overhead from automatic differentiation, gradients for all Gaussian parameters (position, scaling, rotation) are explicitly derived <a class="yt-timestamp" data-t="01:16:05">[01:16:05]</a>.
*   **GPU-Optimized [[Rendering_Technology_and_Algorithms | Rasterization]]**: The use of custom CUDA kernels and leveraging fast GPU Radix sort for depth ordering are critical for efficiency <a class="yt-timestamp" data-t="01:23:44">[01:23:44]</a> <a class="yt-timestamp" data-t="01:42:04">[01:42:04]</a>.
*   **Unlimited Gradient Propagation**: Unlike some methods that limit the number of points receiving gradients, 3D Gaussian Splatting propagates gradients to all blended Gaussians, which is crucial for handling complex scenes with varying depth <a class="yt-timestamp" data-t="01:52:12">[01:52:12]</a> <a class="yt-timestamp" data-t="02:17:41">[02:17:41]</a>.

## Limitations and Future Work
*   **Static Scenes**: The current method is designed for static scenes, similar to most existing radiance field approaches <a class="yt-timestamp" data-t="00:58:48">[00:58:48]</a>. Extending to dynamic, time-varying scenes remains a challenge for future work <a class="yt-timestamp" data-t="02:44:26">[02:44:26]</a>.
*   **Reliance on SfM**: The initial sparse point cloud depends on Structure from Motion (SfM) for camera calibration, which introduces noise and can lead to artifacts in unobserved regions <a class="yt-timestamp" data-t="00:20:06">[00:20:06]</a> <a class="yt-timestamp" data-t="02:20:56">[02:20:56]</a>.
*   **Hard-Coded Hyperparameters**: The optimization process involves several hard-coded thresholds for pruning and densification <a class="yt-timestamp" data-t="01:35:12">[01:35:12]</a>, which may require scene-specific tuning <a class="yt-timestamp" data-t="02:22:56">[02:22:56]</a>.
*   **Artifacts**: The method can occasionally produce elongated artifacts or "splotchy" Gaussians, and "popping" artifacts when large Gaussians appear or disappear rapidly due to view-dependent effects or guard-band culling <a class="yt-timestamp" data-t="02:21:08">[02:21:08]</a>.
*   **Memory Consumption**: While efficient, the memory footprint for storing the entire Gaussian model can be substantial, especially for large scenes <a class="yt-timestamp" data-t="02:23:34">[02:23:34]</a>. Future work could explore compression techniques for point clouds to reduce this <a class="yt-timestamp" data-t="02:24:25">[02:24:25]</a>.
*   **Mesh Reconstruction**: It would be interesting to explore if the 3D Gaussian representation could be used to perform mesh reconstructions of captured scenes, bridging the gap between volumetric and surface representations <a class="yt-timestamp" data-t="02:27:21">[02:27:21]</a>.

## Conclusion
3D Gaussian Splatting demonstrates that a continuous representation is not strictly necessary for fast and high-quality [[rendering_technology_and_algorithms | radiance field]] training and [[rendering_technology_and_algorithms | rendering]] <a class="yt-timestamp" data-t="02:25:54">[02:25:54]</a>. By combining a novel discrete scene representation with an efficient, GPU-optimized [[rendering_technology_and_algorithms | rasterization]] pipeline, it achieves real-time, high-quality [[novel_view_synthesis_and_rendering_techniques | novel view synthesis]] with competitive training times <a class="yt-timestamp" data-t="02:25:20">[02:25:20]</a>. The approach has opened new avenues for research in explicit 3D scene representations and [[rendering_technology_and_algorithms | rendering techniques]] <a class="yt-timestamp" data-t="02:44:40">[02:44:40]</a>.