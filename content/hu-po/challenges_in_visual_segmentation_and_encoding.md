---
title: Challenges in visual segmentation and encoding
videoId: uYb38g-weEY
---

From: [[hu-po]] <br/> 

The development of Vision Language Models (VLMs) faces ongoing challenges, particularly concerning the efficiency and accuracy of visual encoding and the interpretation of complex visual information <a class="yt-timestamp" data-t="01:03:04">[01:03:04]</a>.

## Issues with Visual Tokenization and Information Loss

One significant challenge is how VLMs process images <a class="yt-timestamp" data-t="01:03:07">[01:03:07]</a>. Images are tokenized into "patches," which are then converted into visual tokens <a class="yt-timestamp" data-t="00:07:58">[00:07:58]</a>. This process can lead to information loss, making it difficult for the model to interpret subtle visual cues <a class="yt-timestamp" data-t="01:20:54">[01:20:54]</a>.

For instance, in a composite image of Barack Obama's body with Dwayne "The Rock" Johnson's face, most VLMs identify the person as Barack Obama <a class="yt-timestamp" data-t="00:08:50">[00:08:50]</a>. This occurs because the majority of the image patches "scream Obama," and the visual encoders tend to "gloss over" the crucial detail of the mismatched face <a class="yt-timestamp" data-t="00:08:05">[00:08:05]</a>, <a class="yt-timestamp" data-t="01:20:54">[01:20:54]</a>. Only more sophisticated systems, like GPT-4 Vision accessed via third-party providers such as Perplexity, can identify the image as digitally manipulated <a class="yt-timestamp" data-t="01:09:52">[01:09:52]</a>, <a class="yt-timestamp" data-t="01:57:38">[01:57:38]</a>.

## Managing Lengthy Visual Token Sequences

Another problem is the excessive length of visual token sequences generated by visual encoders <a class="yt-timestamp" data-t="01:03:13">[01:03:13]</a>. An image might generate thousands of tokens, compared to a question that might only be a few tokens long <a class="yt-timestamp" data-t="01:03:31">[01:03:31]</a>. This imbalance can "limit the model's effectiveness" and make it "inaccurately interpreting complex visual information" <a class="yt-timestamp" data-t="01:03:47">[01:03:47]</a>.

Furthermore, different visual encoders output tokens with varying dimensionalities and numbers <a class="yt-timestamp" data-t="01:14:23">[01:14:23]</a>, complicating their integration into a unified language model <a class="yt-timestamp" data-t="01:14:31">[01:14:31]</a>.

## Redundancy in Positional Encoding

Many vision models, particularly those based on the [[Convolutional Neural Networks and Visual Systems | Vision Transformer]] architecture, add positional encoding to each visual token <a class="yt-timestamp" data-t="01:06:04">[01:06:04]</a>. This helps the transformer understand the spatial relationship of patches <a class="yt-timestamp" data-t="01:06:35">[01:06:35]</a>. However, this practice is questioned, as it might be redundant for VLMs where visual experts already contain positional information <a class="yt-timestamp" data-t="01:21:51">[01:21:51]</a>. Simplified positional encoding schemes, such as assigning the same embedding to all image patches ("share all"), can significantly reduce computational cost without a substantial performance degradation <a class="yt-timestamp" data-t="01:22:20">[01:22:20]</a>, <a class="yt-timestamp" data-t="01:27:19">[01:27:19]</a>. This suggests that positional embeddings, while crucial for text, might not be as necessary for [[Challenges and Advances in Image Tokenization | image tokenization]] within a VLM context <a class="yt-timestamp" data-t="01:27:50">[01:27:50]</a>.

## Strategies to Address Challenges

### [[Use of multiple visual encoders | Multiple Visual Encoders]] (Ensemble of Experts)

A promising approach is to use an [[Use of multiple visual encoders | ensemble of visual encoders]] <a class="yt-timestamp" data-t="01:04:02">[01:04:02]</a>. Different visual encoders excel at different tasks:
*   **CLIP**: Known as a "semantic expert," excelling in image-text alignment through contrastive learning <a class="yt-timestamp" data-t="01:05:09">[01:05:09]</a>, <a class="yt-timestamp" data-t="01:09:52">[01:09:52]</a>.
*   **DINOv2**: Provides robust feature extraction through self-supervised learning at both image and patch levels <a class="yt-timestamp" data-t="01:10:26">[01:10:26]</a>.
*   **SAM (Segment Anything Model)**: A "segmentation expert" highly skilled in [[Image segmentation techniques | image segmentation]], capturing fine details and edges <a class="yt-timestamp" data-t="01:05:14">[01:05:14]</a>, <a class="yt-timestamp" data-t="01:12:30">[01:12:30]</a>.
*   **LayoutLMv3**: Good at [[Optical Character Recognition and Visual Systems | OCR]] <a class="yt-timestamp" data-t="01:15:15">[01:15:15]</a>.

By concatenating outputs from these diverse encoders, a VLM can leverage their combined strengths, leading to "consistently superior performance" <a class="yt-timestamp" data-t="01:07:31">[01:07:31]</a>. This approach requires a "Fusion Network" (often simple Multi-Layer Perceptrons or MLPs) to standardize the dimensionality of the varied visual tokens <a class="yt-timestamp" data-t="01:15:36">[01:15:36]</a>, <a class="yt-timestamp" data-t="01:17:47">[01:17:47]</a>.

### Order of Experts

The order in which the visual encoder outputs are fed into the Language Model (LLM) matters due to the autoregressive and position-aware nature of LLMs <a class="yt-timestamp" data-t="01:24:24">[01:24:24]</a>. The LLM processes these visual tokens as a sequence, and "the order of the experts affects the final output" <a class="yt-timestamp" data-t="01:24:31">[01:24:31]</a>, <a class="yt-timestamp" data-t="01:55:04">[01:55:04]</a>. This highlights a nuanced challenge in designing optimal VLM architectures.

## Trade-offs and Future Directions

While these strategies offer improved performance, they often come at the cost of increased computational expense <a class="yt-timestamp" data-t="01:08:49">[01:08:49]</a>. Running multiple visual encoders means processing the image through each, and the concatenated visual tokens result in a much longer prompt for the language model, increasing inference time <a class="yt-timestamp" data-t="01:54:30">[01:54:30]</a>.

This indicates an emerging trend where state-of-the-art performance in VLMs is increasingly tied to the willingness to expend more compute for inference <a class="yt-timestamp" data-t="01:35:55">[01:35:55]</a>. Future [[Discussion and implications of emerging visual features | research and development]] in VLMs will likely focus on optimizing these multi-expert approaches and refining tokenization and positional encoding to achieve better performance-to-compute ratios, akin to the discussions around [[Challenges in training large computer vision models | "state-of-the-art per compute budget"]] <a class="yt-timestamp" data-t="01:36:40">[01:36:40]</a>.