---
title: 3D reconstruction using multiview images
videoId: SkvyrgSzigo
---

From: [[hu-po]] <br/> 

[[3d_representations_in_computer_vision | 3D representations in computer vision]] have evolved significantly, with recent advancements focusing on generating detailed and usable 3D models from various inputs. This article explores different approaches to 3D reconstruction, particularly focusing on methods that utilize multiview images and their implications for the future of 3D content creation.

## MeshFormer: High-Quality Textured Mesh Generation
MeshFormer is a model that performs sparse-view reconstruction, taking a limited number of inputs to reconstruct 3D objects <a class="yt-timestamp" data-t="00:07:52">[00:07:52]</a>.

### Input and Output
*   **Input**: MeshFormer requires a sparse set of six multiview RGB images and their corresponding normal maps <a class="yt-timestamp" data-t="00:06:15">[00:06:15]</a>.
    *   A normal map is an image where each pixel's color represents the direction of the normal vector (perpendicular) to a surface at that point <a class="yt-timestamp" data-t="00:09:15">[00:09:15]</a>.
    *   Normal maps provide a 3D effect in rendering pipelines by aiding in the calculation of light interaction with surfaces <a class="yt-timestamp" data-t="00:09:56">[00:09:56]</a>.
*   **Output**: The model reconstructs a high-quality 3D textured mesh <a class="yt-timestamp" data-t="00:06:20">[00:06:20]</a>. A textured mesh includes vertices and an image (texture) wrapped around it <a class="yt-timestamp" data-t="00:06:26">[00:06:26]</a>.

### Key Innovation: Using Diffusion Models for Normal Maps
The crucial insight of MeshFormer is the use of [[monocular_depth_estimation_using_diffusion_models | diffusion models]] to generate these normal maps <a class="yt-timestamp" data-t="00:11:19">[00:11:19]</a>.
*   Pre-trained 2D diffusion models, trained on billions of natural images, can be fine-tuned to predict high-quality normal maps for arbitrary RGB images <a class="yt-timestamp" data-t="00:35:19">[00:35:19]</a>. This provides additional geometric signal compared to using only RGB data <a class="yt-timestamp" data-t="00:34:07">[00:34:07]</a>.
*   Similarly, depth images (showing distance to the camera) can also be generated by [[monocular_depth_estimation_using_diffusion_models | diffusion models]] trained on large datasets <a class="yt-timestamp" data-t="00:11:51">[00:11:51]</a>.

### Architecture and Training
MeshFormer utilizes a 3D U-Net architecture with a Transformer at the bottleneck <a class="yt-timestamp" data-t="00:20:44">[00:20:44]</a>.
*   It combines 3D sparse convolutions and projection-aware cross-attention, which leverages explicit 3D structure and scene-specific biases <a class="yt-timestamp" data-t="00:14:37">[00:14:37]</a>.
*   The model processes input RGB and normal images through a 2D feature extractor (like DINOv2) to generate patch features <a class="yt-timestamp" data-t="00:26:17">[00:26:17]</a>.
*   It predicts normals, color, and an SDF (Signed Distance Function) <a class="yt-timestamp" data-t="00:24:40">[00:24:40]</a>.
    *   An SDF defines the distance from any point in space to the surface of an object, with positive values outside, negative inside, and zero on the surface <a class="yt-timestamp" data-t="00:24:24">[00:24:24]</a>.
*   The final 3D mesh is extracted from the SDF using a marching cubes algorithm <a class="yt-timestamp" data-t="00:25:14">[00:25:14]</a>.
*   MeshFormer employs a unified single-stage training strategy, pushing gradients through the entire pipeline <a class="yt-timestamp" data-t="00:31:39">[00:31:39]</a>, though it relies on pre-trained 2D diffusion models and encoders <a class="yt-timestamp" data-t="00:15:47">[00:15:47]</a>.

### Limitations of 3D Data
One challenge in 3D reconstruction is the limited scale of 3D datasets <a class="yt-timestamp" data-t="00:33:08">[00:33:08]</a>. Models often train on the same datasets (e.g., Objaverse, ShapeNet) <a class="yt-timestamp" data-t="00:33:23">[00:33:23]</a>, which can limit the diversity and generalization of learned priors. The incorporation of dense normal map information, derived from rich 2D image datasets, helps to overcome this data scarcity <a class="yt-timestamp" data-t="00:34:54">[00:34:54]</a>.

## Mesh Anything: Artist-Created Mesh Generation with Autoregressive Transformers
Mesh Anything focuses on generating meshes with "good topology," similar to those created by human artists <a class="yt-timestamp" data-t="00:38:00">[00:38:00]</a>.

### The Importance of Topology
*   **Good Topology**: Refers to the clean, organized structure of a mesh's vertices and faces, often seen in artist-created models <a class="yt-timestamp" data-t="00:38:06">[00:38:06]</a>. This is crucial for smooth animations, efficient texturing, and compatibility with existing 3D industry tools like Unreal Engine and Unity <a class="yt-timestamp" data-t="00:39:00">[00:39:00]</a>.
*   **Traditional Methods vs. Topology**: Many [[generative_3d_models_and_techniques | generative 3D models]] output alternative [[3d_representations_in_computer_vision | 3D representations]] like Nerfs, [[3d_gaussian_modeling | Gaussian Splats]], or SDFs <a class="yt-timestamp" data-t="00:39:20">[00:39:20]</a>. Converting these to meshes (e.g., via marching cubes) often results in dense meshes with inefficient or undesirable topology, making them difficult to use in production pipelines <a class="yt-timestamp" data-t="00:52:51">[00:52:51]</a>.

### Generative Approach
Mesh Anything uses an auto-regressive Transformer to directly generate mesh tokens, building the mesh piece by piece <a class="yt-timestamp" data-t="00:50:46">[00:50:46]</a>.
*   **VQ-VAE for Mesh Vocabulary**: The paper trains a VQ-VAE (Vector Quantized Variational Autoencoder) to learn a "mesh vocabulary" or "shape tokens" <a class="yt-timestamp" data-t="00:48:27">[00:48:27]</a>.
    *   A VQ-VAE encodes continuous data (like images or 3D shapes) into a discrete set of tokens from a learned codebook <a class="yt-timestamp" data-t="00:44:56">[00:44:56]</a>. This process discretizes the continuous input into a limited vocabulary, similar to how language models use a finite set of tokens <a class="yt-timestamp" data-t="00:46:00">[00:46:00]</a>.
    *   In Mesh Anything, the VQ-VAE creates a 3D mesh vocabulary <a class="yt-timestamp" data-t="00:48:54">[00:48:54]</a>.
*   **Transformer Generation**: A decoder-only Transformer then takes an input (e.g., a point cloud) and generates sequences of these discrete shape tokens <a class="yt-timestamp" data-t="00:56:19">[00:56:19]</a>. These tokens are then decoded by the VQ-VAE to form the mesh <a class="yt-timestamp" data-t="00:49:57">[00:49:57]</a>.
*   **Noise-Resistant Decoder**: The method employs a data augmentation technique where intentionally corrupted mesh topologies are fed to the model during training <a class="yt-timestamp" data-t="00:43:43">[00:43:43]</a>. This allows the Transformer to learn what constitutes good topology, making it more robust to noise and capable of generating higher-quality meshes <a class="yt-timestamp" data-t="00:58:56">[00:58:56]</a>.

## Comparing Approaches and Future Outlook
The two approaches highlight a fundamental difference in [[generative 3D models and techniques | generative 3D models]]:
*   **MeshFormer's approach**: Uses advanced signal processing (normal maps from diffusion models) and traditional 3D pipelines (SDF to marching cubes) to achieve high geometric detail. This still leads to topology that is largely uncontrolled <a class="yt-timestamp" data-t="00:52:26">[00:52:26]</a>.
*   **Mesh Anything's approach**: Directly generates meshes token-by-token, allowing for greater control over topology and potentially leading to meshes directly usable in existing 3D production workflows <a class="yt-timestamp" data-t="00:54:31">[00:54:31]</a>.

The ability to generate meshes with good topology could signify a comeback for meshes as the standard 3D representation, maintaining momentum with existing tools rather than forcing a refactor to new representations like Nerfs or [[3d_gaussian_modeling | Gaussian Splats]] <a class="yt-timestamp" data-t="00:55:00">[00:55:00]</a>.

### The Canonical Codec Analogy: Jpeg LM
The "Jpeg LM" paper, while focused on 2D image generation, offers an interesting parallel <a class="yt-timestamp" data-t="00:59:51">[00:59:51]</a>. It demonstrates that large language models (LLMs) can generate images and videos by directly modeling them as compressed files saved on a computer via canonical codecs (like JPEG or AVC h264) <a class="yt-timestamp" data-t="01:00:41">[01:00:41]</a>.
*   Instead of encoding images into a learned, latent "visual vocabulary" (like a VQ-VAE would), Jpeg LM generates the raw byte sequence of the compressed file <a class="yt-timestamp" data-t="01:01:53">[01:01:53]</a>.
*   This approach avoids vision-specific modifications to the LLM architecture, proving that general language models can handle highly structured data formats <a class="yt-timestamp" data-t="01:08:01">[01:08:01]</a>.

This concept suggests a potential future where 3D models (e.g., STL or USD files) could be directly generated by LLMs, token by token, bypassing intermediate [[3d_representations_in_computer_vision | 3D representations]] and potentially offering direct compatibility with current 3D software <a class="yt-timestamp" data-t="01:12:49">[01:12:49]</a>. While challenging, the fact that Jpeg LM works as a proof of concept with relatively small models is highly impressive <a class="yt-timestamp" data-t="01:14:13">[01:14:13]</a>.

Ultimately, the choice between different 3D reconstruction and generation paradigms will likely depend on the desired output quality, control over properties like topology, and integration with existing industry pipelines. The advancements in using [[monocular_depth_estimation_using_diffusion_models | diffusion models]] for auxiliary inputs (normal maps) and auto-regressive generation of discrete representations (mesh tokens) represent significant steps forward in the field of [[experimental_evaluation_of_3d_reconstruction_techniques | 3D reconstruction]].