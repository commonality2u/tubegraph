---
title: Rationality and decision theory
videoId: 41SUp-TRVlg
---

From: [[dwarkesh | The Dwarkesh Podcast]]

Eliezer Yudkowsky has articulated specific views on rationality and decision theory, often contrasting them with common philosophical or academic understandings. His perspective emphasizes rationality as a means to achieve goals, rather than a fixed set of beliefs or social identity.

## Rationality as Systematized Winning

Yudkowsky defines rationality through his essay "Rationality is Systematized Winning" <a class="yt-timestamp" data-t="03:47:57">[03:47:57]</a>. This concept is intended to counter traditional philosophical notions of rationality that are not necessarily tied to achieving outcomes, or that propose flawed mathematical structures which lead to predictable mistakes <a class="yt-timestamp" data-t="03:52:39">[03:52:39]</a>.

He illustrates this with an example from Star Trek, where Spock might call a winning chess move by Kirk "irrational" or "illogical" <a class="yt-timestamp" data-t="03:53:07">[03:53:07]</a>. Yudkowsky argues this is a fundamental misunderstanding. If one perceives a "reasonable" answer as distinct from the "correct" (i.e., winning) answer, then one's concept of what is "reasonable" is flawed <a class="yt-timestamp" data-t="03:55:15">[03:55:15]</a>. The core idea is that rationality should align with the moves that lead to better results <a class="yt-timestamp" data-t="03:56:00">[03:56:00]</a>.

This definition is not about asserting that people who identify as "rationalists" will automatically be the most successful or hold the highest social status <a class="yt-timestamp" data-t="03:55:27">[03:55:27]</a>. Instead, it focuses on the underlying mathematical and cognitive structure that orients towards achieving desired outcomes <a class="yt-timestamp" data-t="03:55:52">[03:55:52]</a>. 

For more on rational approaches in technology applications, you can explore Llama 3 and AI advancements at Meta [[llama_3_and_ai_advancements_at_meta | here]].

## Common Misconceptions about Rationality

A common misconception Yudkowsky addresses is the idea that being rational will lead to losing because others are not always rational <a class="yt-timestamp" data-t="03:53:53">[03:53:53]</a>. He sees this as a misunderstanding akin to Spock's view of Kirk's winning move <a class="yt-timestamp" data-t="03:55:07">[03:55:07]</a>. This notion often leads to debates about the implications of AI on societal structures, further discussed in the Impact of AI on future technology and society [[impact_of_ai_on_future_technology_and_society | here]].

## Decision Theory

Yudkowsky critiques prevailing academic decision theories and proposes an alternative.

### Causal Decision Theory (CDT)

*   CDT is described as the contemporarily accepted decision theory in academia <a class="yt-timestamp" data-t="03:54:07">[03:54:07]</a>.
*   Yudkowsky argues that CDT can lead to situations where the "rational" agent, by CDT's definition, predictably loses, for example, in ultimatum games <a class="yt-timestamp" data-t="03:54:17">[03:54:17]</a>.

### Logical Decision Theory (LDT)

*   Yudkowsky considers Logical Decision Theory (LDT) as one of his major technical contributions <a class="yt-timestamp" data-t="01:12:00">[01:12:00]</a>.
*   LDT offers a different analysis of scenarios like the ultimatum game, where rational players (as defined by LDT) do not predictably lose <a class="yt-timestamp" data-t="03:54:34">[03:54:34]</a>. This insight into decision-making can parallel discussions on [[ai_alignment_and_safety_concerns | AI alignment and safety concerns]].
*   Further information on LDT can be found on Arbital <a class="yt-timestamp" data-t="03:54:34">[03:54:34]</a>.
*   LDT is presented as a framework where rationality aligns with achieving better outcomes, avoiding the pitfalls of CDT where "rational" actions can lead to suboptimal results <a class="yt-timestamp" data-t="03:54:43">[03:54:43]</a>.

## Rationality as a Cognitive Process

Yudkowsky emphasizes that rationality is not a creed, a banner, a way of life, a personal choice, or a social group <a class="yt-timestamp" data-t="03:48:36">[03:48:36]</a>. This perspective echoes discussions in areas such as AI scalability and breakthroughs [[ai_scalability_and_breakthroughs | here]].
*   It is a **structure of a cognitive process** <a class="yt-timestamp" data-t="03:48:48">[03:48:48]</a>.
*   Individuals can attempt to incorporate more of this cognitive structure into their thinking <a class="yt-timestamp" data-t="03:49:00">[03:49:00]</a>.
*   Social interactions or affiliations only matter insofar as they help instill more of this effective cognitive structure <a class="yt-timestamp" data-t="03:49:15">[03:49:15]</a>.
*   He believes consciously adopting Bayesian principles (e.g., not updating in a predictable direction, trying to anticipate future predictable beliefs) has personally led to "scattered bits and pieces of slightly greater sanity" <a class="yt-timestamp" data-t="03:50:31">[03:50:31]</a>.

He acknowledges the difficulty of this, stating, "Yes, there are No True Bayesians upon this planet" <a class="yt-timestamp" data-t="03:49:28">[03:49:28]</a>, highlighting that perfectly embodying this cognitive structure is an ideal rather than a common reality. This ideal can be related to broader questions on Artificial Intelligence vs Human Intelligence [[artificial_intelligence_vs_human_intelligence | here]].