---
title: Reasoning large language models and their impact
videoId: wAzBl6xllzE
---

From: [[colemedin]] <br/> 

Reasoning Large Language Models (LLMs) are a significant area of focus for AI in 2025 <a class="yt-timestamp" data-t="00:11:15">[00:11:15]</a>. These models are designed to reason with themselves about a prompt before providing a final answer <a class="yt-timestamp" data-t="00:12:23">[00:12:23]</a>. Concepts like Chain of Thought, inference time, and compute all refer to the same idea of an LLM's internal reasoning process <a class="yt-timestamp" data-t="00:12:33">[00:12:33]</a>. It's noteworthy that existing LLMs are used to create the systems that become these reasoning LLMs <a class="yt-timestamp" data-t="00:12:40">[00:12:40]</a>.

## Key Examples <a class="yt-timestamp" data-t="00:12:09">[00:12:09]</a>

Several notable reasoning LLMs have emerged:
*   03 by OpenAI <a class="yt-timestamp" data-t="00:11:57">[00:11:57]</a>
*   01 from OpenAI <a class="yt-timestamp" data-t="00:12:02">[00:12:02]</a>
*   QWQ, an open-source reasoning model from Quen <a class="yt-timestamp" data-t="00:12:05">[00:12:05]</a>
*   Gemini 2.0 Flash <a class="yt-timestamp" data-t="00:12:09">[00:12:09]</a>

## Impact and Problem Solving

Reasoning LLMs are powerful because they address two major [[limitations_of_large_language_models | challenges with large language models]]: hallucinations and poor decision-making when acting as [[ai_agents_and_large_language_models | AI agents]] <a class="yt-timestamp" data-t="00:12:17">[00:12:17]</a>. While not entirely solved, significant progress is being made in these areas <a class="yt-timestamp" data-t="00:12:20">[00:12:20]</a>.

The future of AI involves [[ai_agents_and_large_language_models | agentic workflows]] that combine reasoning LLMs with smaller, faster LLMs <a class="yt-timestamp" data-t="00:12:31">[00:12:31]</a>. This combination allows for robust and efficient systems, where reasoning LLMs handle complex, slower processes requiring more power <a class="yt-timestamp" data-t="00:12:41">[00:12:41]</a>, and smaller LLMs manage faster, less complex tasks <a class="yt-timestamp" data-t="00:12:46">[00:12:46]</a>. Incorporating domain-specific LLMs into these [[ai_agents_and_large_language_models | agent architectures]] is also expected to drive innovation in 2025 <a class="yt-timestamp" data-t="00:13:03">[00:13:03]</a>.

## Practical Skills for Reasoning LLMs

To leverage reasoning LLMs effectively, it is crucial to learn how to prompt and interact with them <a class="yt-timestamp" data-t="00:13:18">[00:13:18]</a>. Their behavior differs from standard LLMs due to their internal "chain of thought" process before providing a response <a class="yt-timestamp" data-t="00:13:27">[00:13:27]</a>. Key skills include:
*   Prompting them efficiently <a class="yt-timestamp" data-t="00:13:32">[00:13:32]</a>
*   Using them effectively as [[ai_agents_and_large_language_models | agents]] <a class="yt-timestamp" data-t="00:13:34">[00:13:34]</a>
*   Understanding when and how to integrate them into your [[ai_agents_and_large_language_models | agent architectures]] <a class="yt-timestamp" data-t="00:13:37">[00:13:37]</a>