---
title: Space Time UNet model for video generation
videoId: oyd1b5Oif84
---

From: [[hrishikeshyadav8883]] <br/> 

Lum is a new text-to-video diffusion model developed by Google Research, which has garnered attention for its advancements in realistic video generation <a class="yt-timestamp" data-t="00:00:22">[00:00:22]</a> <a class="yt-timestamp" data-t="00:00:31">[00:00:31]</a>. The model aims to synthesize videos that are realistic, diverse, and coherent in motion <a class="yt-timestamp" data-t="00:10:06">[00:10:06]</a> <a class="yt-timestamp" data-t="00:10:13">[00:10:13]</a>. Its core innovation lies in the Space Time UNet architecture <a class="yt-timestamp" data-t="00:05:57">[00:05:57]</a>.

## Key Features and Advantages

The Lum model focuses on three primary characteristics for its video output:
*   **Realistic Quality**: Videos generated by Lum are noted for their realism <a class="yt-timestamp" data-t="00:01:21">[00:01:21]</a> <a class="yt-timestamp" data-t="00:10:09">[00:10:09]</a>.
*   **High Resolution**: The generated videos exhibit higher resolution compared to other models <a class="yt-timestamp" data-t="00:01:21">[00:01:21]</a>.
*   **Consistency**: A significant advantage of Lum is its temporal consistency, which is a common challenge in text-to-video generation <a class="yt-timestamp" data-t="00:01:28">[00:01:28]</a> <a class="yt-timestamp" data-t="00:01:42">[00:01:42]</a>. For example, a sailboat sailing on a sunny day will show consistent reflections, a detail often lacking in other models <a class="yt-timestamp" data-t="00:01:34">[00:01:34]</a> <a class="yt-timestamp" data-t="00:01:42">[00:01:42]</a>.

### Addressing Inconsistency in Previous Models

Previous models like Pika, Runway Gen 2, Imagen, and Zero-shot often struggled with maintaining temporal consistency, especially when generating longer videos beyond a few seconds <a class="yt-timestamp" data-t="00:02:06">[00:02:06]</a> <a class="yt-timestamp" data-t="00:02:10">[00:02:10]</a> <a class="yt-timestamp" data-t="00:02:50">[00:02:50]</a> <a class="yt-timestamp" data-t="00:02:53">[00:02:53]</a> <a class="yt-timestamp" data-t="00:14:54">[00:14:54]</a>. This issue often arose because these models processed video generation bit-by-bit, leading to random elements appearing and disrupting the coherence of the scene <a class="yt-timestamp" data-t="00:03:35">[00:03:35]</a> <a class="yt-timestamp" data-t="00:03:40">[00:03:40]</a> <a class="yt-timestamp" data-t="00:06:16">[00:06:16]</a> <a class="yt-timestamp" data-t="00:06:18">[00:06:18]</a>. Lum's Space Time UNet architecture is designed to overcome this by generating the entire temporal duration of the video, processing full frame rates across multiple space-time cells <a class="yt-timestamp" data-t="00:10:23">[00:10:23]</a> <a class="yt-timestamp" data-t="00:10:27">[00:10:27]</a> <a class="yt-timestamp" data-t="00:11:34">[00:11:34]</a> <a class="yt-timestamp" data-t="00:11:40">[00:11:40]</a>.

### Additional Capabilities

*   **Stylized Generation**: Lum excels at stylized video generation, a capability less explored in video models compared to image-to-image models <a class="yt-timestamp" data-t="00:04:38">[00:04:38]</a> <a class="yt-timestamp" data-t="00:04:43">[00:04:43]</a> <a class="yt-timestamp" data-t="00:04:53">[00:04:53]</a>. It can apply a reference image style to create a consistent video, useful for marketing or product illustrations, such as turning an illustration reference into a GIF or creating stylized "sticker" videos <a class="yt-timestamp" data-t="00:05:00">[00:05:00]</a> <a class="yt-timestamp" data-t="00:05:14">[00:05:14]</a> <a class="yt-timestamp" data-t="00:05:30">[00:05:30]</a>.
*   **Image-to-Video Generation**: The model is strong in converting static images into videos, providing high-quality outputs <a class="yt-timestamp" data-t="00:04:02">[00:04:02]</a> <a class="yt-timestamp" data-t="00:04:07">[00:04:07]</a> <a class="yt-timestamp" data-t="00:10:06">[00:10:06]</a>.
*   **Video Stylization**: Lum can transform a source video (e.g., a person running) into different artistic styles, such as a "wooden box" or "origami" aesthetic <a class="yt-timestamp" data-t="00:06:36">[00:06:36]</a> <a class="yt-timestamp" data-t="00:06:45">[00:06:45]</a>.
*   **Video Inpainting**: It introduces a more stable version of video inpainting, where missing parts of a video can be seamlessly filled in with realistic and consistent content <a class="yt-timestamp" data-t="00:07:31">[00:07:31]</a> <a class="yt-timestamp" data-t="00:07:35">[00:07:35]</a> <a class="yt-timestamp" data-t="00:08:04">[00:08:04]</a> <a class="yt-timestamp" data-t="00:09:22">[00:09:22]</a>.
*   **Cinemagraphs**: While other models like [[Runway Gen 2 model | Runway Gen 2]] use "motion brush" for adding motion to static images, Lum offers an alternative approach for converting static images to videos <a class="yt-timestamp" data-t="00:06:56">[00:06:56]</a> <a class="yt-timestamp" data-t="00:07:02">[00:07:02]</a>.

## Architecture and Training

Lum's core is the Space Time UNet architecture, which generates the entire temporal duration of a video rather than processing it incrementally <a class="yt-timestamp" data-t="00:10:23">[00:10:23]</a> <a class="yt-timestamp" data-t="00:10:27">[00:10:27]</a>. This is a contrast to existing video models that often use methods like auto-regressive generation or temporal super-resolution, which inherently make global temporal consistency difficult to achieve <a class="yt-timestamp" data-t="00:10:38">[00:10:38]</a> <a class="yt-timestamp" data-t="00:10:47">[00:10:47]</a>.

The model directly generates a full frame rate by processing information across multiple space-time cells <a class="yt-timestamp" data-t="00:11:34">[00:11:34]</a> <a class="yt-timestamp" data-t="00:11:40">[00:11:40]</a>. It leverages pre-trained T2I (text-to-image) weighted fixed layers, with newly added temporal layers being trained <a class="yt-timestamp" data-t="00:16:19">[00:16:19]</a> <a class="yt-timestamp" data-t="00:16:24">[00:16:24]</a>.

For training, the text-to-video (T2V) model was trained on a dataset containing 30 million videos, each accompanied by text captions (prompts) <a class="yt-timestamp" data-t="00:16:54">[00:16:54]</a> <a class="yt-timestamp" data-t="00:17:06">[00:17:06]</a>. These videos were 80 frames long at 16 frames per second (5 seconds total duration) <a class="yt-timestamp" data-t="00:17:10">[00:17:10]</a> <a class="yt-timestamp" data-t="00:17:14">[00:17:14]</a>. The model was evaluated on 113 text prompts, including 18 new prompts and 95 from prior works <a class="yt-timestamp" data-t="00:17:22">[00:17:22]</a> <a class="yt-timestamp" data-t="00:17:32">[00:17:32]</a>.

## Evaluation and Comparison

Lum's performance has been evaluated against prominent text-to-video diffusion models like Imagen Video, P-Video, Stable Diffusion, and Zero-shot <a class="yt-timestamp" data-t="00:17:45">[00:17:45]</a> <a class="yt-timestamp" data-t="00:18:05">[00:18:05]</a> <a class="yt-timestamp" data-t="00:18:07">[00:18:07]</a> <a class="yt-timestamp" data-t="00:18:45">[00:18:45]</a>. Based on user study results, Lum (represented by the blue line in evaluation metrics) shows significantly higher quality matrices compared to these other models, even in areas like text alignment <a class="yt-timestamp" data-t="00:19:02">[00:19:02]</a> <a class="yt-timestamp" data-t="00:19:06">[00:19:06]</a> <a class="yt-timestamp" data-t="00:19:25">[00:19:25]</a> <a class="yt-timestamp" data-t="00:19:28">[00:19:28]</a>. Lum's capabilities in image-to-video generation, particularly with reference images, also make it stand out <a class="yt-timestamp" data-t="00:19:09">[00:19:09]</a> <a class="yt-timestamp" data-t="00:19:16">[00:19:16]</a>.

## Future Outlook

The year 2024 is seen as a pivotal year for video generation, with a strong focus on making AI-generated short films and movies mainstream <a class="yt-timestamp" data-t="00:11:08">[00:11:08]</a> <a class="yt-timestamp" data-t="00:11:14">[00:11:14]</a>. The continuous release of new models, potentially before events like Google I/O in May, signifies the rapid advancements in this field, extending to audio and robotics as well <a class="yt-timestamp" data-t="00:11:18">[00:11:18]</a> <a class="yt-timestamp" data-t="00:20:30">[00:20:30]</a> <a class="yt-timestamp" data-t="00:20:38">[00:20:38]</a>.