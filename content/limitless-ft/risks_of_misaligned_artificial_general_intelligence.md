---
title: Risks of misaligned artificial general intelligence
videoId: dam1wHetql8
---

From: [[limitless-ft]] <br/> 

The development of artificial intelligence presents humanity with three general potential outcomes <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a>. These outcomes range from a utopian future to a dystopian scenario, and finally, the existential risk posed by misaligned intelligence.

## Potential Outcomes of AI Development

### The Post-Scarcity Utopia

One optimistic scenario envisions [[impact_of_ai_on_job_replacement_and_human_labor | AI models replacing jobs]] and a significant portion of human work, generating immense value for everyone <a class="yt-timestamp" data-t="00:00:05">[00:00:05]</a>. In this future, the necessity for human beings to work for survival would be eliminated, ushering in a "post-scarcity" world <a class="yt-timestamp" data-t="00:00:14">[00:00:14]</a>. Survival would no longer be a constant struggle, allowing individuals the freedom to choose their pursuits <a class="yt-timestamp" data-t="00:00:22">[00:00:22]</a>.

### The Dystopian Feudalism

A less favorable, yet still survivable, outcome suggests that while Large Language Models (LLMs) could replace all human labor <a class="yt-timestamp" data-t="00:00:34">[00:00:34]</a>, these powerful AI resources would be owned by a small number of companies <a class="yt-timestamp" data-t="00:00:35">[00:00:35]</a>. This could lead to a new form of feudalism, where "sects" of companies controlling LLMs exert power over vast regions of the world, potentially with tight government coupling or nationalization <a class="yt-timestamp" data-t="00:00:38">[00:00:38]</a>. This scenario would create a stark division between those with access to AI resources and those without <a class="yt-timestamp" data-t="00:00:57">[00:00:57]</a>.

### The Risk of Misaligned Intelligence

The most severe potential outcome involves the creation of "misaligned intelligence," specifically misaligned Artificial General Intelligence (AGI) or misaligned [[role_of_superintelligence_and_ethical_considerations | superintelligence]] <a class="yt-timestamp" data-t="00:01:06">[00:01:06]</a>. In this scenario, for various reasons, this misaligned [[role_of_superintelligence_and_ethical_considerations | superintelligence]] could lead to the extinction of humanity <a class="yt-timestamp" data-t="00:01:12">[00:01:12]</a>.

Notable figures in the AI field, including Sam Altman and Elon Musk, estimate the "Probability of Doom" (Poom) – the risk of misaligned [[role_of_superintelligence_and_ethical_considerations | superintelligence]] or other negative consequences causing the demise of humanity – to be approximately 20% <a class="yt-timestamp" data-t="00:01:15">[00:01:15]</a>.