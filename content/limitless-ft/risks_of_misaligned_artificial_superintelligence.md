---
title: Risks of misaligned artificial superintelligence
videoId: dam1wHetql8
---

From: [[limitless-ft]] <br/> 

Among the potential future outcomes concerning [[the_potential_and_risks_of_ai_in_everyday_life | AI]], one significant concern is the creation of [[the_alignment_problem_in_ai_safety | misaligned intelligence]], [[intelligence_curse_and_agi | misaligned AGI]], or [[the_alignment_problem_in_ai_safety | misaligned potentially super intelligence]] <a class="yt-timestamp" data-t="00:01:06">[00:01:06]</a>.

## The Threat of Misalignment

The primary risk associated with [[the_alignment_problem_in_ai_safety | misaligned super intelligence]] is its potential to cause the extinction of all human beings <a class="yt-timestamp" data-t="00:01:12">[00:01:12]</a>. This outcome is considered a grave danger for humanity.

## Perceived Probability

Prominent figures in the field, such as Sam Altman and Elon Musk, have expressed concerns regarding this risk <a class="yt-timestamp" data-t="00:01:15">[00:01:15]</a>. They estimate the "doom" — the risk of [[the_alignment_problem_in_ai_safety | misaligned super intelligence]] or other negative consequences leading to the demise of all humanity — to be approximately 20% <a class="yt-timestamp" data-t="00:01:20">[00:01:20]</a>.