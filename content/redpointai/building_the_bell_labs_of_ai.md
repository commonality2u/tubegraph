---
title: Building the Bell Labs of AI
videoId: ZtY8VXswa2o
---

From: [[redpointai]] <br/> 

Answer AI, founded by Eric Ries and Jeremy Howard, aims to be the "Bell Labs of AI" <a class="yt-timestamp" data-t="00:00:01">[00:00:01]</a>. The company focuses on building smaller, cheaper, and more affordable AI models, as well as developing applications in sectors like legal and education <a class="yt-timestamp" data-t="00:00:05">[00:00:05]</a>.

## Founding Philosophy: A For-Profit R&D Lab

Answer AI operates as a for-profit R&D lab, a structure reminiscent of early industrial labs like Thomas Edison's <a class="yt-timestamp" data-t="00:14:35">[00:14:35]</a>. This approach stands in contrast to the traditional startup model, especially when considering questions of proprietary technology or product-market fit in a nascent field <a class="yt-timestamp" data-t="00:14:42">[00:14:42]</a>.

A core belief of Answer AI is that the best research occurs when the researcher is deeply coupled to the application, creating a continuous feedback loop from the customer back to the scientific inquiry <a class="yt-timestamp" data-t="00:16:02">[00:16:02]</a>. This contrasts with the hyper-specialization seen in modern academia and industry, where research (R) and development (D) are often separated <a class="yt-timestamp" data-t="00:15:35">[00:15:35]</a>. This integrated approach allows for breakthroughs driven by real-world customer needs, preventing researchers from solving problems irrelevant to the market <a class="yt-timestamp" data-t="00:17:09">[00:17:09]</a>.

### Funding and Investors

Initially, the founders were unsure who would fund such a unique R&D lab <a class="yt-timestamp" data-t="00:14:29">[00:14:29]</a>. However, they found support from investors who understood their vision, including a major AI safety advocate <a class="yt-timestamp" data-t="00:18:43">[00:18:43]</a>. Traditional investors, accustomed to the Software-as-a-Service (SaaS) stack and clear business models, often struggled to understand Answer AI's approach <a class="yt-timestamp" data-t="00:19:50">[00:19:50]</a>.

### Team Culture and Exploration

Answer AI employs a "Long Leash with Narrow Fences" approach to R&D, meaning they define a broad research thesis (the fences) and give researchers freedom to explore within those bounds (the long leash) <a class="yt-timestamp" data-t="00:25:24">[00:25:24]</a>. This fosters a team with a strong intuition for what AI technology can achieve <a class="yt-timestamp" data-t="00:32:21">[00:32:21]</a>.

To encourage exploration and deep engagement with the AI ecosystem, every team member receives a $500 monthly credit card to purchase and use any AI-related products <a class="yt-timestamp" data-t="00:33:11">[00:33:11]</a>. This leads to valuable discussions about product strengths and weaknesses, fostering appreciation for both research and commercial opportunities <a class="yt-timestamp" data-t="00:33:41">[00:33:41]</a>.

## [[Lean Startup principles applied to AI | Applying Lean Startup Principles to AI]]

Eric Ries, author of "The Lean Startup," notes that many companies in the AI world are spending large sums on models and compute before ever engaging with the market <a class="yt-timestamp" data-t="00:00:57">[00:00:57]</a>. This is partly due to the "magical" nature of AI demos, which can lead founders to believe customer testing isn't necessary <a class="yt-timestamp" data-t="00:01:36">[00:01:36]</a>. Ries asserts that fundamental principles of the [[Lean Startup principles applied to AI | Lean Startup]] still apply:
*   **Customer Needs:** It's impossible to know in advance what customers want <a class="yt-timestamp" data-t="00:01:56">[00:01:56]</a>. Companies must discover this through experimentation and "revealed actions" <a class="yt-timestamp" data-t="00:02:12">[00:02:12]</a>.
*   **Beyond the SaaS Stack:** Many AI companies incorrectly try to "copy-paste" the SaaS stack to AI, assuming similar business models <a class="yt-timestamp" data-t="00:02:22">[00:02:22]</a>. This can lead to a disconnect where AI API providers assume their customers will achieve product-market fit, even if the value chain is multiple layers deep from the model to the end-user <a class="yt-timestamp" data-t="00:02:50">[00:02:50]</a>.
*   **Understanding End Customers:** Regardless of where a company sits in the "stack," it's crucial to understand the ultimate end customer and their needs to ensure the product achieves product-market fit <a class="yt-timestamp" data-t="00:03:21">[00:03:21]</a>.
*   **Economic Differences:** The economics of AI differ significantly from traditional software, resembling physical manufacturing or deep infrastructure projects due to high computational and operating costs <a class="yt-timestamp" data-t="00:03:41">[00:03:41]</a>. This amplifies risk, requiring adaptability <a class="yt-timestamp" data-t="00:04:00">[00:04:00]</a>.
*   **Product-Market Fit First, Moats Later:** While [[Competing with tech giants in the AI market | defensibility (moats)]] is a concern, Eric Ries agrees with Arvin from Perplexity that companies should first focus on building something customers want, earning the right to think about moats later <a class="yt-timestamp" data-t="00:05:03">[00:05:03]</a>. Trying to pick "dimes in front of a steamroller" (small opportunities near large platforms) can work if fast enough, but carries significant risk <a class="yt-timestamp" data-t="00:05:19">[00:05:19]</a>.
*   **Rapid Iteration and Pivoting:** Given the high uncertainty in AI, continuous rapid iteration and the ability to pivot are more important than ever <a class="yt-timestamp" data-t="00:07:39">[00:07:39]</a>. Companies that skip this step are already running into trouble, accelerating the industry hype cycle <a class="yt-timestamp" data-t="00:08:44">[00:08:44]</a>.

## Focus on Cost Reduction and Practical Applications

Answer AI aims to reduce the cost of AI development and deployment, specifically by "bringing the price down by 10x" <a class="yt-timestamp" data-t="00:26:50">[00:26:50]</a>. This focus on resource efficiency is often seen as "tedious" by larger labs obsessed with pure performance <a class="yt-timestamp" data-t="00:26:59">[00:26:59]</a>.

However, reducing cost can create a "difference in kind," not just degree <a class="yt-timestamp" data-t="00:27:11">[00:27:11]</a>:
*   **Enabling New Use Cases:** Lower costs make previously impossible applications viable due to constraints like physical installation, power access, and HBM memory manufacturing <a class="yt-timestamp" data-t="00:27:30">[00:27:30]</a>.
*   **Continuous Fine-tuning:** If fine-tuning models becomes cheap enough, it enables "continuous pre-training" of individual agents. This allows for hyper-personalization, context, and memory, overcoming the "amnesiac" nature of current models that constantly forget previous interactions <a class="yt-timestamp" data-t="00:28:01">[00:28:01]</a>.

Answer AI's work on efficient fine-tuning of LLaMA 3 is an example of this <a class="yt-timestamp" data-t="00:23:53">[00:23:53]</a>. This breakthrough, led by one researcher, demonstrated how combining quantization and distributed computing could dramatically improve the accessibility and reduce the cost of fine-tuning <a class="yt-timestamp" data-t="00:32:44">[00:32:44]</a>.

## Key Application Areas: Legal and Education

Answer AI is particularly excited about the law and education sectors because they are heavily language-based and offer significant opportunities for societal benefit <a class="yt-timestamp" data-t="00:34:43">[00:34:43]</a>.

### Law
The legal system is often used as a "weapon" by wealthy individuals and organizations against less wealthy ones, creating injustice <a class="yt-timestamp" data-t="00:35:31">[00:35:31]</a>. By significantly reducing the cost of high-quality legal advice, AI can democratize access to justice and counter gatekeeping practices <a class="yt-timestamp" data-t="00:36:12">[00:36:12]</a>.

### Education
Education has immense potential for improvement. Current systems often force students through the same path, limiting customization <a class="yt-timestamp" data-t="00:37:48">[00:37:48]</a>. AI can enable more personalized learning experiences, allowing more people to pursue their passions and build what they envision <a class="yt-timestamp" data-t="00:37:22">[00:37:22]</a>.

## Views on AI Safety and Regulation

Jeremy Howard expressed concern about proposed legislation, such as California's SB 147, which aims to regulate the safety of AI models <a class="yt-timestamp" data-t="00:39:02">[00:39:02]</a>. While well-intentioned, such policies could be ineffective or even counterproductive, leading to a less safe situation <a class="yt-timestamp" data-t="00:40:01">[00:40:01]</a>.

The fundamental issue is that AI models, in their raw form, are "dual-use technology," similar to a pen, paper, or calculator <a class="yt-timestamp" data-t="00:40:32">[00:40:32]</a>. It's impossible to ensure the safety of the model itself, as users can fine-tune or prompt it to do anything they desire <a class="yt-timestamp" data-t="00:40:55">[00:40:55]</a>.

Strict regulation on model release would mean:
*   **Restricted Access:** Models in their raw form (like LLaMA 3) would likely not be released, only layered products (like ChatGPT) that offer limited user control <a class="yt-timestamp" data-t="00:42:37">[00:42:37]</a>.
*   **Centralization of Power:** This turns models into "extremely rivalrous goods," accessible only to large states and corporations <a class="yt-timestamp" data-t="00:43:10">[00:43:10]</a>. This fosters a competitive race for bigger models without transparency or external scrutiny <a class="yt-timestamp" data-t="00:43:46">[00:43:46]</a>.
*   **Hindering Defensive Uses:** Open access to models allows many people to use them for beneficial, defensive purposes, such as improving cybersecurity or developing vaccines <a class="yt-timestamp" data-t="00:43:34">[00:43:34]</a>. Restricting this access could ironically make the world less safe <a class="yt-timestamp" data-t="00:46:05">[00:46:05]</a>.

Eric Ries suggests that foundation model labs, while pursuing [[Path to artificial general intelligence AGI | AGI]], could benefit from re-establishing connections between their research and customers <a class="yt-timestamp" data-t="00:47:40">[00:47:40]</a>. Many valuable applications don't require [[Path to artificial general intelligence AGI | AGI]] and could be built with smaller, safer models <a class="yt-timestamp" data-t="00:45:17">[00:45:17]</a>. If these safer options aren't available, users might default to potentially unsafe frontier models for tasks <a class="yt-timestamp" data-t="00:46:37">[00:46:37]</a>.

## Overhyped and Underhyped Aspects of AI

*   **Overhyped:** AI agents, as current models' mathematical foundations are not compatible with the novel planning many attempt to do with them <a class="yt-timestamp" data-t="00:48:34">[00:48:34]</a>. Agents are strong for mixes/matches of training data, but not novel planning sequences <a class="yt-timestamp" data-t="00:22:31">[00:22:31]</a>.
*   **Underhyped:** Resource efficiency, which can dramatically increase accessibility and unlock new use cases <a class="yt-timestamp" data-t="00:48:38">[00:48:38]</a>.

## Breakthroughs That Could Change Perceptions

Two major breakthroughs could significantly alter current understanding of AI:
1.  **Reduced Energy Requirements:** A breakthrough in the amount of energy or other resource requirements for AI models would be huge <a class="yt-timestamp" data-t="00:50:31">[00:50:31]</a>.
2.  **Advanced Planning/Reasoning:** A breakthrough in planning and reasoning capability that goes beyond subgraph matching (the current "auto-regressive" word-by-word approach) <a class="yt-timestamp" data-t="00:51:10">[00:51:10]</a>. Approaches like Yann LeCun's "Jeeper-based models" or diffusion models for text could achieve this <a class="yt-timestamp" data-t="00:52:12">[00:52:12]</a>.

A profound realization might also come from understanding that the problem isn't just in scaling LLMs, but in our understanding of human cognition itself <a class="yt-timestamp" data-t="00:53:20">[00:53:20]</a>. Current methods might be a "brute force" way of finding critical cognitive algorithms, and a more direct approach could be a true breakthrough <a class="yt-timestamp" data-t="00:54:11">[00:54:11]</a>.

For more information, visit answer.ai <a class="yt-timestamp" data-t="00:54:42">[00:54:42]</a>.