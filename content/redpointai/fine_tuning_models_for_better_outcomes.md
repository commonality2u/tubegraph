---
title: Fine tuning models for better outcomes
videoId: czyHDP67CMw
---

From: [[redpointai]] <br/> 

When working with [[ai_model_selection_and_evaluation_for_businesses | AI]] models, a recommended approach is to "start small and work your way up" <a class="yt-timestamp" data-t="00:00:01">[00:00:01]</a>. This progression should be justified by rigorous Return on Investment (ROI), ensuring that progress is made on matters of significance <a class="yt-timestamp" data-t="00:00:02">[00:00:02]</a>.

## Initial Approach: Experimentation and Benchmarking

The initial phase involves testing and establishing benchmarks <a class="yt-timestamp" data-t="00:00:08">[00:00:08]</a>. It is important to acknowledge that initial benchmarks may be inadequate and will require improvement <a class="yt-timestamp" data-t="00:00:09">[00:00:09]</a>.

The journey often begins with minimal expenditure, such as spending a small amount on platforms like OpenAI or Llama on Databricks, to conduct a "litmus test" on whether [[ai_model_selection_and_evaluation_for_businesses | AI]] is suitable for a particular task <a class="yt-timestamp" data-t="00:00:12">[00:00:12]</a>. It is difficult to predict upfront if [[ai_model_selection_and_evaluation_for_businesses | AI]] will be effective for a specific use case <a class="yt-timestamp" data-t="00:00:19">[00:00:19]</a>.

Instead, the approach should be scientific:
> "You're a scientist, this is data science in the literal sense, go run an experiment and try it." <a class="yt-timestamp" data-t="00:00:25">[00:00:25]</a>

To maximize the chance of success, one should:
*   Try the experiment on the best possible model available <a class="yt-timestamp" data-t="00:00:31">[00:00:31]</a>.
*   Prompt the model directly <a class="yt-timestamp" data-t="00:00:33">[00:00:33]</a>.
*   Manually provide helpful documents into the context, even without Retrieval Augmented Generation (RAG), to observe the outcome <a class="yt-timestamp" data-t="00:00:34">[00:00:34]</a>.

## Scaling Up: Advanced Techniques and [[Finetuning and reinforcement learning techniques for AI | Fine-tuning]]

After initial experimentation, if potential value is identified, the next step involves escalating the complexity <a class="yt-timestamp" data-t="00:00:41">[00:00:41]</a>. This might necessitate the implementation of more advanced techniques, such as "hardcore RAG," to integrate proprietary data, as models cannot inherently access internal enterprise information <a class="yt-timestamp" data-t="00:00:45">[00:00:45]</a>.

If value continues to be demonstrated, [[Finetuning and reinforcement learning techniques for AI | fine-tuning]] becomes a viable option <a class="yt-timestamp" data-t="00:00:53">[00:00:53]</a>. [[Finetuning and reinforcement learning techniques for AI | Fine-tuning]] allows for the specialized knowledge to be deeply embedded ("baked") into the model <a class="yt-timestamp" data-t="00:00:55">[00:00:55]</a>.

## Benefits of [[Finetuning and reinforcement learning techniques for AI | Fine-tuning]]

While [[Finetuning and reinforcement learning techniques for AI | fine-tuning]] may involve higher upfront costs <a class="yt-timestamp" data-t="00:00:57">[00:00:57]</a>, it generally leads to improved quality outcomes <a class="yt-timestamp" data-t="00:00:58">[00:00:58]</a>.