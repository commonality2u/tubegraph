---
title: AI video tools in enterprises
videoId: FudGqDZDSx4
---

From: [[redpointai]] <br/> 

HeyGen CEO Joshua Xu discussed the company's work in [[AI transformation in creative workflows | AI video tools]] for enterprises on the Unsupervised Learning podcast <a class="yt-timestamp" data-t="00:00:11">[00:00:11]</a>. HeyGen recently secured $60 million in funding at a $500 million valuation <a class="yt-timestamp" data-t="00:00:04">[00:00:04]</a>.

## HeyGen's Core Offering

HeyGen is an [[strategic_uses_of_ai_in_enterprises | AI video platform]] designed to help users create, localize, and personalize video content <a class="yt-timestamp" data-t="00:03:50">[00:03:50]</a> <a class="yt-timestamp" data-t="00:09:42">[00:09:42]</a>. The core mission is to replace the need for physical cameras by generating footage using AI <a class="yt-timestamp" data-t="00:04:38">[00:04:38]</a> <a class="yt-timestamp" data-t="00:04:45">[00:04:45]</a>. This approach makes video production significantly faster and cheaper, enabling users who may lack expensive equipment, camera comfort, or specialized editing skills <a class="yt-timestamp" data-t="00:05:06">[00:05:06]</a> <a class="yt-timestamp" data-t="00:11:48">[00:11:48]</a>.

## The "Magic Moment" of AI Video

The company experienced a viral "magic moment" when HeyGen was used to dub the speech of Argentina's president at the World Economic Forum into different languages, highlighting the value and "magic" of AI video translation <a class="yt-timestamp" data-t="00:00:53">[00:00:53]</a> <a class="yt-timestamp" data-t="00:01:45">[00:01:45]</a>. This event demonstrated the ability for individuals to speak in different languages with natural voices and expressions in front of a camera <a class="yt-timestamp" data-t="00:01:53">[00:01:53]</a>. Internally, HeyGen believes that continuously shipping and improving the product experience, while listening to customers, leads to such market-hitting moments <a class="yt-timestamp" data-t="00:02:17">[00:02:17]</a>.

Joshua's personal "magic moment" occurred when he created his first avatar and saw himself speaking on screen <a class="yt-timestamp" data-t="00:02:36">[00:02:36]</a>. He noted the ease of using his avatar to generate product update videos from a script, eliminating the need to film himself <a class="yt-timestamp" data-t="00:03:04">[00:03:04]</a>.

## Evolution of Video Creation Workflows

Historically, video production involved filming with a camera and then extensive post-production editing <a class="yt-timestamp" data-t="00:04:03">[00:04:03]</a>. With generative AI, it is now possible to generate footage directly using AI, aiming to replace the camera <a class="yt-timestamp" data-t="00:04:36">[00:04:36]</a>. The belief that a video can be encoded as binary data implies that machines can learn and generate it <a class="yt-timestamp" data-t="00:05:41">[00:05:41]</a>.

Joshua predicts that future editing experiences will be vastly different from today's timeline editors <a class="yt-timestamp" data-t="00:06:40">[00:06:40]</a>. Timeline editors primarily exist because cameras and footage were expensive, requiring multiple takes and extensive post-production <a class="yt-timestamp" data-t="00:06:16">[00:06:16]</a>. With AI-generated footage, this dependency is removed <a class="yt-timestamp" data-t="00:06:33">[00:06:33]</a>. The future of video creation may involve generating video from text and combining various user experience elements like script writing and documentation-style editing <a class="yt-timestamp" data-t="00:07:00">[00:07:00]</a>.

### Prioritizing AI Quality

HeyGen's primary focus is on the quality of AI-generated video, ensuring that the "magic works" <a class="yt-timestamp" data-t="00:07:42">[00:07:42]</a>. Quality in AI models for video isn't solely about mathematical optimization but involves many aspects such as lighting, naturalism, and matching body motion and gestures to the script <a class="yt-timestamp" data-t="00:08:21">[00:08:21]</a>.

[[ai_production_and_evaluation_techniques | Evaluating AI models]] for subjective aesthetic quality is a challenge, requiring the internal team to develop a strong sense of what makes a good avatar <a class="yt-timestamp" data-t="00:08:55">[00:08:55]</a>. The internal benchmark is whether a team member would be happy to use the generated avatar in their day-to-day work <a class="yt-timestamp" data-t="00:09:09">[00:09:09]</a>.

## HeyGen's Customer Use Cases

HeyGen serves over 40,000 customers with three primary use cases <a class="yt-timestamp" data-t="00:09:37">[00:09:37]</a>:

1.  **Creation**: Users can create videos using avatars (either their own or stock avatars) by typing text, eliminating the need for a camera <a class="yt-timestamp" data-t="00:09:48">[00:09:48]</a>.
2.  **Localization**: Existing videos (even non-HeyGen videos) can be localized into over 175 different languages and dialects while retaining voice tone, facial expression, and lip-syncing <a class="yt-timestamp" data-t="00:10:01">[00:10:01]</a>.
3.  **Personalization**: A single video can be personalized into over 100,000 variations based on customer demographics, industry, and specific problems they face, similar to personalizing emails <a class="yt-timestamp" data-t="00:10:19">[00:10:19]</a>.

## Target Audience and User Experience

HeyGen is designed for the 99% of users who are not professional video editors and do not have access to expensive cameras or sophisticated software <a class="yt-timestamp" data-t="00:11:18">[00:11:18]</a>. The tool enables "content people" and marketers who can write scripts but may lack video production skills to produce videos <a class="yt-timestamp" data-t="00:11:29">[00:11:29]</a>. The mission is to enable visual storytelling for everyone <a class="yt-timestamp" data-t="00:11:39">[00:11:39]</a>.

Teaching users a net new way of doing things is a common challenge for AI products <a class="yt-timestamp" data-t="00:11:58">[00:11:58]</a>. HeyGen addresses this by showcasing what is possible with the technology and highlighting diverse use cases across marketing, sales, customer support, and content creation <a class="yt-timestamp" data-t="00:12:36">[00:12:36]</a>. The goal is to quickly guide new users to their specific use case and demonstrate a "magic moment" <a class="yt-timestamp" data-t="00:13:08">[00:13:08]</a>.

## Technical Aspects of Avatar Models

The key differentiator for HeyGen's avatar models is engagement <a class="yt-timestamp" data-t="00:13:40">[00:13:40]</a>. In a business context, video must effectively deliver a message, meaning it needs to be engaging to prevent viewers from disengaging quickly <a class="yt-timestamp" data-t="00:13:56">[00:13:56]</a>. Engagement goes beyond mouth movement to include head movement, eyebrow expressions, and body motion and gestures, which is the most challenging aspect to coordinate <a class="yt-timestamp" data-t="00:14:17">[00:14:17]</a>.

HeyGen's dedicated research team builds the entire video layer in-house, covering lip-syncing, body motion, and full-body rendering <a class="yt-timestamp" data-t="00:15:07">[00:15:07]</a>. Their Avatar 3.0 model can render the entire body <a class="yt-timestamp" data-t="00:15:20">[00:15:20]</a>. The process of training a good avatar model involves extensive data and continuous improvement of model architecture to capture various dimensions of human speech and integrate them <a class="yt-timestamp" data-t="00:16:06">[00:16:06]</a>.

To achieve personalization, HeyGen's "video avatar" feature requires users to submit a short video (30 seconds to 2 minutes) for the AI model to learn their unique "talking style," encompassing not just mouth movements but overall behavior <a class="yt-timestamp" data-t="00:17:02">[00:17:02]</a> <a class="yt-timestamp" data-t="00:17:23">[00:17:23]</a>. The company is working on larger models to capture different modes of speaking, like presentation mode or interview mode, and build adaptive avatar behaviors based on the script or content <a class="yt-timestamp" data-t="00:17:36">[00:17:36]</a>.

## Future of AI Video at HeyGen

### Synchronous Generation & Streaming

HeyGen has a beta version of an interactive avatar that can attend and interact in real-time Zoom meetings <a class="yt-timestamp" data-t="00:19:15">[00:19:15]</a>. The main technical challenge for synchronous generative streaming is optimizing inference speed for increasingly larger and more complex models <a class="yt-timestamp" data-t="00:19:42">[00:19:42]</a>. Joshua is optimistic that real-time AI video generation, including on-device processing, will be widespread within 12 months <a class="yt-timestamp" data-t="00:20:46">[00:20:46]</a>. This will enable new use cases, such as personalized video ads where content adapts to individual user preferences and watch history <a class="yt-timestamp" data-t="00:20:19">[00:20:19]</a>.

### Full Body Movement

Full-body rendering is crucial for creating engaging human presenters, as body motion and gestures are vital components of communication <a class="yt-timestamp" data-t="00:21:10">[00:21:10]</a>. HeyGen's 3.0 avatar version supports full-body rendering, with the inclusion of gestures as the next development step <a class="yt-timestamp" data-t="00:21:35">[00:21:35]</a>. The challenge lies in developing the right model architecture and data to capture the nuances of full-body movement <a class="yt-timestamp" data-t="00:21:48">[00:21:48]</a>.

### Integration with Text-to-Video Models

HeyGen primarily focuses on business videos and approaches video generation through an orchestration engine <a class="yt-timestamp" data-t="00:22:30">[00:22:30]</a>. This engine captures text, script, voice, sound, music, avatar footage, and background generation to create a cohesive video <a class="yt-timestamp" data-t="00:23:00">[00:23:00]</a>. This "orchestration" approach provides more control, consistency, and quality, which are crucial for brands and enterprises <a class="yt-timestamp" data-t="00:23:17">[00:23:17]</a>.

HeyGen plans to work closely with text-to-video partners (like Sora, Pika) by incorporating their outputs as components within its broader orchestration system <a class="yt-timestamp" data-t="00:24:00">[00:24:00]</a>. HeyGen acts as a service layer that directly interacts with the customer, leveraging the best available generative models as inputs <a class="yt-timestamp" data-t="00:24:12">[00:24:12]</a>.

### Brand Personalization

A significant future area is the "brand personalization layer" <a class="yt-timestamp" data-t="00:25:03">[00:25:03]</a>. Currently, AI can help write scripts in a brand's tone, but applying brand consistency to video is not yet seamless <a class="yt-timestamp" data-t="00:25:10">[00:25:10]</a>. The vision is for AI models to learn a company's color palette, style, and opening/closing video clips from existing content (e.g., a URL) and bake these elements into the final video assembly process <a class="yt-timestamp" data-t="00:25:51">[00:25:51]</a>. This involves disassembling video into components, reassembling them, and integrating user input as a "memory" to feed into the AI model <a class="yt-timestamp" data-t="00:26:31">[00:26:31]</a>.

## Startup vs. Incumbent Dynamics

HeyGen sees itself in the "creative tools stage" rather than competing directly with distribution platforms <a class="yt-timestamp" data-t="00:27:23">[00:27:23]</a>. The company believes it's not competing in the old market but opening up a new market opportunity for users who don't have access to traditional video production means <a class="yt-timestamp" data-t="00:28:27">[00:28:27]</a>.

Platforms like Snapchat and TikTok focus on enabling creators using mobile cameras <a class="yt-timestamp" data-t="00:29:12">[00:29:12]</a>. HeyGen's key value is to make the camera obsolete and enable video creation without it <a class="yt-timestamp" data-t="00:29:30">[00:29:30]</a>. This creates a dilemma for existing platforms: if AI-generated content becomes prevalent (e.g., 50% of content), it directly competes with and could reduce views for human creators, potentially necessitating new platforms specifically for AI-generated content <a class="yt-timestamp" data-t="00:30:15">[00:30:15]</a>. While HeyGen's mission is to build creative tools, not a consumption platform, they acknowledge this as a possible future opportunity <a class="yt-timestamp" data-t="00:31:10">[00:31:10]</a>.

## [[challenges_and_strategies_in_enterprise_ai_deployment | Challenges and Strategies in Enterprise AI Deployment]]

HeyGen's recent push into the enterprise market has highlighted key requirements <a class="yt-timestamp" data-t="00:31:27">[00:31:27]</a>:
*   **Higher Quality Requirements**: Enterprise customers demand much higher quality and brand consistency in video output <a class="yt-timestamp" data-t="00:31:43">[00:31:43]</a>.
*   **Integration with Workflows**: [[integration_of_ai_and_data_platforms_in_enterprises | Integrating the technology]] and product into day-to-day enterprise workflows is crucial <a class="yt-timestamp" data-t="00:32:07">[00:32:07]</a>. For marketing, this means integrating with existing CRM and go-to-market tools like HubSpot, allowing for seamless data pulling for context and easy video distribution <a class="yt-timestamp" data-t="00:32:25">[00:32:25]</a>.

## Trust & Safety in AI Video

Trust and safety are critical for HeyGen's business, especially when serving large enterprise customers <a class="yt-timestamp" data-t="00:33:45">[00:33:45]</a>. HeyGen implements a two-pronged approach:

1.  **Avatar Creation**: For every avatar created, HeyGen requires a video consent format to ensure the person in the footage is the same person giving consent <a class="yt-timestamp" data-t="00:34:11">[00:34:11]</a>. They also use dynamically generated passcodes that expire quickly to add a secure layer <a class="yt-timestamp" data-t="00:34:25">[00:34:25]</a>. This makes it nearly impossible to create an avatar without consent <a class="yt-timestamp" data-t="00:34:38">[00:34:38]</a>.
2.  **Content Moderation**: HeyGen has a platform moderation policy that prohibits hate speech, misinformation, political campaigns, and other undesirable content <a class="yt-timestamp" data-t="00:35:03">[00:35:03]</a>. This is enforced through a hybrid solution of AI model review and human moderation <a class="yt-timestamp" data-t="00:35:15">[00:35:15]</a>.

HeyGen also engages in IP partnerships with actors who build avatars on the platform <a class="yt-timestamp" data-t="00:35:49">[00:35:49]</a>. The emergence of AI-generated voices and persons could lead to new forms of IP, especially as image generation models improve consistency across different generations <a class="yt-timestamp" data-t="00:36:10">[00:36:10]</a>. This opens up possibilities for new roles like AI influencers <a class="yt-timestamp" data-t="00:36:49">[00:36:49]</a>.

## Business Model and Capital Intensity

In the AI category, the primary cost factors are GPU resources and talent <a class="yt-timestamp" data-t="00:37:32">[00:37:32]</a>. Unlike traditional software companies where marginal cost for additional customers approaches zero, AI businesses consume more GPU compute per user, meaning marginal costs are not zero <a class="yt-timestamp" data-t="00:37:56">[00:37:56]</a>.

However, Joshua noted that individual employees are becoming much more efficient with AI tools like ChatGPT <a class="yt-timestamp" data-t="00:38:24">[00:38:24]</a>. AI-native companies with AI-native teams can achieve greater efficiency <a class="yt-timestamp" data-t="00:38:54">[00:38:54]</a>. The accelerated growth trajectory and market excitement around AI mean that companies might require less capital to build great AI products than previously thought <a class="yt-timestamp" data-t="00:39:10">[00:39:10]</a> <a class="yt-timestamp" data-t="00:39:36">[00:39:36]</a>.

HeyGen offers a free tier to allow users to discover "magic moments," balancing the inference costs with the need for broad market exposure <a class="yt-timestamp" data-t="00:39:43">[00:39:43]</a>. HeyGen builds products 12 months ahead of current capabilities, anticipating future model advancements and cost reductions rather than waiting for them <a class="yt-timestamp" data-t="00:40:51">[00:40:51]</a>.

## Overhyped vs. Underhyped in AI

Joshua believes the **overhyped** aspect in AI is the speed at which AI will deliver massive value to businesses and enterprises <a class="yt-timestamp" data-t="00:41:39">[00:41:39]</a>. Conversely, the **underhyped** aspect is the ultimate impact of AI <a class="yt-timestamp" data-t="00:41:50">[00:41:50]</a>. People often overestimate short-term gains but underestimate the long-term transformative power <a class="yt-timestamp" data-t="00:41:55">[00:41:55]</a>.

## Mindset Changes and Customer Insights

Joshua's mindset shifted significantly around 2021 with the emergence of technologies like Stable Diffusion <a class="yt-timestamp" data-t="00:43:14">[00:43:14]</a>. Initially, HeyGen explored 3D model layers for video generation, aligned with the metaverse hype <a class="yt-timestamp" data-t="00:42:40">[00:42:40]</a>. However, pixel-by-pixel generation proved to be a faster path due to its ability to train on large-scale data <a class="yt-timestamp" data-t="00:43:29">[00:43:29]</a>.

Customer feedback often surprises HeyGen, particularly the detailed attention paid to avatar quality and engagement <a class="yt-timestamp" data-t="00:44:09">[00:44:09]</a>. Users have a higher bar for their own avatars compared to others <a class="yt-timestamp" data-t="00:44:26">[00:44:26]</a>. For example, a customer highlighted that while avatar gestures were fine for the first few minutes of a video, they became random later on, indicating a need for improved long-duration gesture matching <a class="yt-timestamp" data-t="00:45:30">[00:45:30]</a>.

## Vision for 2030

Joshua's vision for video creation workflows in 2030 is that everyone will have a "video agency on their pocket" <a class="yt-timestamp" data-t="00:48:33">[00:48:33]</a>. This means a product like HeyGen will interact with users like a personal video agency, guiding them from ideas to filmed footage and editing, with iterative feedback loops <a class="yt-timestamp" data-t="00:47:56">[00:47:56]</a>.

By 2030, any text, audio, or video content currently being made will be generatable by AI at a much faster rate and lower cost <a class="yt-timestamp" data-t="00:49:11">[00:49:11]</a>. The true power of creative tools is opening up entirely new use cases that are currently unimaginable, similar to how mobile cameras led to platforms like Instagram, Snapchat, and TikTok <a class="yt-timestamp" data-t="00:49:29">[00:49:29]</a>. Improving tools and lowering creation barriers will unlock a whole new world of possibilities <a class="yt-timestamp" data-t="00:49:56">[00:49:56]</a>.

Joshua's passion for the space stems from his experience at Snap, working with mobile platform evolution and witnessing the emergence of content platforms <a class="yt-timestamp" data-t="00:50:17">[00:50:17]</a>. His greatest joy comes from seeing people use the technology and tools he builds to create something on their own <a class="yt-timestamp" data-t="00:50:56">[00:50:56]</a>.

To learn more, visit [heygen.com](https://www.heygen.com/) <a class="yt-timestamp" data-t="00:51:14">[00:51:14]</a>.