---
title: Advancements and potential of video and robotics models
videoId: a0bEU83P8g8
---

From: [[redpointai]] <br/> 

Bob McGrew, former Chief Research Officer at OpenAI, discusses the current state, challenges, and future potential of [[challenges_and_future_of_video_generation_models | video models]] and [[the_future_of_ai_in_robotics_and_its_impact | robotics models]] <a class="yt-timestamp" data-t="00:00:13">[00:00:13]</a>. He highlights that while progress in [[role_of_ai_models_in_advancing_robotics_and_autonomous_driving | AI models]] might appear slow from the outside, significant advancements are underway internally within major labs <a class="yt-timestamp" data-t="00:01:00">[00:01:00]</a>.

## Advancements in Video Generation Models

The modality that has "resisted" integration into main [[future_trends_in_ai_and_multimodal_models | multimodal]] models for a long time is video <a class="yt-timestamp" data-t="00:18:54">[00:18:54]</a>.

### Sora's Role and Capabilities
Sora has been among the first to demonstrate advanced video generation capabilities <a class="yt-timestamp" data-t="00:19:00">[00:19:00]</a>. Two key aspects distinguish video from other modalities like images:
*   **Extended Sequences and User Interface:** Video involves an extended sequence of events, not just one prompt <a class="yt-timestamp" data-t="00:19:49">[00:19:49]</a>. This necessitates a comprehensive user interface to craft a story that unfolds over time <a class="yt-timestamp" data-t="00:19:54">[00:19:54]</a>. Sora's product team has focused on developing a storyboard capability, allowing users to set checkpoints to guide the generation process <a class="yt-timestamp" data-t="00:21:54">[00:21:54]</a>.
*   **High Costs:** Video generation is inherently expensive, both in terms of training and running these [[challenges_and_future_of_video_generation_models | models]] <a class="yt-timestamp" data-t="00:20:18">[00:20:18]</a>.

Sora's quality is high, and its distribution is broad, being available to OpenAI Plus and Pro account holders, setting a high bar for competitors <a class="yt-timestamp" data-t="00:20:55">[00:20:55]</a>.

### Future Outlook for Video Models
The progress in [[advancements_in_ai_for_media_production | video models]] is expected to be very direct, similar to the trajectory of LLMs <a class="yt-timestamp" data-t="00:21:23">[00:21:23]</a>:
*   **Improved Quality:** While instantaneous quality is already good, the next generations of [[challenges_and_future_of_video_generation_models | models]] will focus on achieving extended coherent generations, moving from seconds of video to potentially an hour <a class="yt-timestamp" data-t="00:21:49">[00:21:49]</a>.
*   **Reduced Cost:** Similar to how GPT-3 quality tokens became 100 times cheaper, the cost of generating high-quality, realistic videos with Sora is expected to become practically nothing <a class="yt-timestamp" data-t="00:22:25">[00:22:25]</a>.

The dream of a full-length, [[advancements_in_ai_for_media_production | AI-generated movie]] that audiences genuinely want to watch could be realized in about two years <a class="yt-timestamp" data-t="00:23:08">[00:23:08]</a>. The key will be directors leveraging [[generative_video_ai | video models]] to exercise their creative vision to produce content unachievable through traditional filming <a class="yt-timestamp" data-t="00:23:17">[00:23:17]</a>.

## Advancements in Robotics Models

Bob McGrew initially explored [[the_future_of_ai_in_robotics_and_its_impact | robotics]] in 2015, believing it was five years away from broad adoption, a prediction he now corrects to "now" <a class="yt-timestamp" data-t="00:24:50">[00:24:50]</a>. He anticipates widespread, though somewhat limited, [[ai_integration_in_consumer_devices_and_robotics | robotics adoption]] in about five years <a class="yt-timestamp" data-t="00:25:00">[00:25:00]</a>.

### Impact of Foundation Models on Robotics
Foundation models represent a significant breakthrough in robotics, enabling quicker setup and crucial generalization capabilities <a class="yt-timestamp" data-t="00:25:14">[00:25:14]</a>.
*   **Vision to Action:** The ability to use vision and translate it into plans of action comes almost "for free" with foundation models <a class="yt-timestamp" data-t="00:25:31">[00:25:31]</a>.
*   **Natural Language Interaction:** The ecosystem has developed to the point where users can simply talk to robots, making interaction much easier than typing commands <a class="yt-timestamp" data-t="00:26:01">[00:26:01]</a>.

### Key Challenges
*   **Reliability:** The most immediate challenge for agents, especially those taking actions in the real world (e.g., buying things, sending messages), is reliability <a class="yt-timestamp" data-t="00:08:57">[00:08:57]</a>. Achieving 90% to 99% reliability requires an order of magnitude increase in compute, and going from 99% to 99.9% requires another <a class="yt-timestamp" data-t="00:09:51">[00:09:51]</a>.
*   **Simulation vs. Real World Learning:**
    *   **Simulation Advantages:** Simulators are efficient for training and good at handling rigid bodies <a class="yt-timestamp" data-t="00:26:48">[00:26:48]</a>.
    *   **Real-World Necessity:** However, simulators struggle with "floppy" materials like cloth or cardboard <a class="yt-timestamp" data-t="00:27:14">[00:27:14]</a>. For general-purpose [[the_future_of_ai_in_robotics_and_its_impact | robotics]], real-world demonstrations are currently the only effective approach <a class="yt-timestamp" data-t="00:27:31">[00:27:31]</a>.

### Future of Robotics Adoption
Widespread consumer adoption of home robots is still a distant prospect due to safety concerns (robot arms can be dangerous) <a class="yt-timestamp" data-t="00:28:10">[00:28:10]</a>. However, in work environments like retail or warehouses, significant deployment of [[breakthroughs_in_autonomous_vehicle_technology | robots]] is expected within five years <a class="yt-timestamp" data-t="00:28:42">[00:28:42]</a>. For example, Amazon warehouses already utilize robots for mobility and are working on pick-and-place tasks <a class="yt-timestamp" data-t="00:28:48">[00:28:48]</a>.

### Convergence of AI Models
Frontier labs are expected to continue developing general purpose [[ai_integration_in_consumer_devices_and_robotics | models]] that perform optimally across various data types and applications <a class="yt-timestamp" data-t="00:29:41">[00:29:41]</a>. Specialization in [[the_future_of_ai_models_and_open_source_development | AI models]] primarily offers price-performance advantages <a class="yt-timestamp" data-t="00:29:55">[00:29:55]</a>. Companies can fine-tune smaller [[ai_integration_in_consumer_devices_and_robotics | models]] using data generated by the best frontier models, resulting in significantly cheaper operations, though with slightly less off-script capability <a class="yt-timestamp" data-t="00:30:21">[00:30:21]</a>.

The overarching sentiment is that progress in [[future_trends_in_ai_and_multimodal_models | AI]] will continue to be very exciting, dynamic, and constant, albeit with evolving forms and challenges <a class="yt-timestamp" data-t="01:06:55">[01:06:55]</a>.