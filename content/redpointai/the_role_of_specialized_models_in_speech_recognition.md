---
title: The role of specialized models in speech recognition
videoId: nFC3asFKlH0
---

From: [[redpointai]] <br/> 

Connor Wick, CEO of Speak, an English language learning platform, highlights the significant role of specialized models in speech recognition, particularly in the context of AI-driven language education <a class="yt-timestamp" data-t="01:12:05">[01:12:05]</a>.

## Speak's Approach to Specialized Speech Recognition

Speak has developed its own in-house speech recognition models, in addition to utilizing larger foundational models <a class="yt-timestamp" data-t="01:12:07">[01:12:07]</a>. This strategic investment in specialized models is driven by the unique requirements of their platform, which focuses on conversational fluency <a class="yt-timestamp" data-t="01:12:11">[01:12:11]</a>.

Key aspects of Speak's specialized models include:
*   **Accent Recognition** Their models are "super, super good" at understanding users speaking with various accents <a class="yt-timestamp" data-t="01:12:28">[01:12:28]</a>.
*   **Mistake Detection** The system can identify specific types of pronunciation mistakes made by learners <a class="yt-timestamp" data-t="01:12:34">[01:12:34]</a>.
*   **Speed and Reliability** These models ensure super-fast, reliable, and streaming responses back to the user's client, which is crucial for an effective product experience <a class="yt-timestamp" data-t="01:12:37">[01:12:37]</a>.
*   **Phoneme Recognition** A dedicated phoneme recognition system, built from Speak's own data, helps detect errors in pronunciation and other prosodic mistakes <a class="yt-timestamp" data-t="01:12:45">[01:12:45]</a>.

Connor emphasizes that while [[the_rise_of_multimodal_models_and_their_implications | generalized large foundational cognition models]] are expected to eventually subsume many tasks, specialized models currently offer a significant advantage for niche applications <a class="yt-timestamp" data-t="01:12:16">[01:12:16]</a>. Even if these specialized models are only used for a few years, they are a worthwhile investment in building a business <a class="yt-timestamp" data-t="01:13:07">[01:13:07]</a>.

## Rationale for Building Specialized Models

The decision to [[Building and utilizing large language models | build and utilize specialized models]] rather than solely relying on general-purpose models like [[understanding_language_models | LLMs]] is based on a strategic, long-term vision <a class="yt-timestamp" data-t="08:21:00">[08:21:00]</a>.

Connor explains the philosophy:
> "We knew in the beginning that there was a long ways to go on the technology and we couldn't perfectly predict it but the thing we did know is like over the next 5 to 10 years like with more data more compute models would get better and better and better and eventually they would like surpass humans on various tasks and eventually you know that would that would mean that we could fully replace the human in the learning process" <a class="yt-timestamp" data-t="08:23:00">[08:23:00]</a>

This long-term orientation allowed Speak to make product decisions aligned with future technological capabilities, ensuring continuous evolution <a class="yt-timestamp" data-t="08:50:00">[08:50:00]</a>.

Building these specialized models is a "really big investment" in terms of compute, team, and resources <a class="yt-timestamp" data-t="01:13:59">[01:13:59]</a>. However, it allows Speak to build a business, collect more data, and invest further <a class="yt-timestamp" data-t="01:13:28">[01:13:28]</a>.

## Specialized vs. General Models

Connor draws an analogy to the personal computer industry in the 1980s, where companies like Apple used Intel processors rather than building their own <a class="yt-timestamp" data-t="01:14:30">[01:14:30]</a>. Similarly, businesses today might use foundational [[understanding_language_models | LLMs]] <a class="yt-timestamp" data-t="01:14:26">[01:14:26]</a>. The "AI firmware" or "ML scaffolding" — the technology built to orchestrate and integrate these models with the product and backend — represents a significant and often overlooked investment that can form a long-term technological moat <a class="yt-timestamp" data-t="01:15:20">[01:15:20]</a>.

> "People are always talking about modeling... I think the modeling is definitely one investment but we're making a much bigger investment on this piece and I actually think that's like if I were going to say what's our like long-term technological mode I would actually say that probably a bigger one" <a class="yt-timestamp" data-t="01:15:42">[01:15:42]</a>

### Challenges and Opportunities
A key challenge for companies building on AI is deciding whether to build around the shortcomings of current models or wait for improvements <a class="yt-timestamp" data-t="09:47:00">[09:47:00]</a>. Speak's strategy is to continually make progress on the core problem (language learning methodology) even if it means swapping out technology later <a class="yt-timestamp" data-t="01:10:48">[01:10:48]</a>.

Connor believes there is still room for specialized, audio-only models to "win" in certain contexts <a class="yt-timestamp" data-t="01:10:07">[01:10:07]</a>. These include:
*   **Niche Use Cases** Specialized models can cater to unique needs not fully addressed by large cognition audio models <a class="yt-timestamp" data-t="01:43:22">[01:43:22]</a>.
*   **Security and On-Premise Needs** Certain applications might require specific security or on-premise solutions for speech data <a class="yt-timestamp" data-t="01:43:35">[01:43:35]</a>.
*   **Specific Vocabulary** Handling highly specialized or unusual vocabulary not commonly found on the internet <a class="yt-timestamp" data-t="01:43:42">[01:43:42]</a>.
*   **Risk-Taking** Smaller, specialized startups can take more risks than larger players <a class="yt-timestamp" data-t="01:43:55">[01:43:55]</a>.

## The Future of Audio and Multimodal AI

Connor emphasizes that the future of UI will likely be "fluid," allowing users to choose between talking, typing, or tapping <a class="yt-timestamp" data-t="02:10:00">[02:10:00]</a>. While speech isn't always superior, it's often better and will drive a huge shift, especially as [[the_rise_of_multimodal_models_and_their_implications | speech-to-speech models]] improve <a class="yt-timestamp" data-t="02:27:00">[02:27:00]</a>.

Speak is excited about multimodal audio, seeing it as a "holy grail" for their use case <a class="yt-timestamp" data-t="01:40:18">[01:40:18]</a>. The progression towards [[the_limitations_and_potential_of_current_ai_models_towards_agi | AGI]] with continuous models that integrate speech recognition, [[understanding_language_models | LLM]], and speech synthesis in one turn, would significantly reduce latency and retain more nuance <a class="yt-timestamp" data-t="01:45:38">[01:45:38]</a>. This advancement would enable more natural, human-like tutor experiences <a class="yt-timestamp" data-t="01:45:27">[01:45:27]</a>.

The ability to build specialized solutions on top of these improving multimodal models will offer "endless possibility" <a class="yt-timestamp" data-t="01:45:00">[01:45:00]</a>. These improvements are expected to lead to much smarter curriculum planning <a class="yt-timestamp" data-t="01:46:44">[01:46:44]</a>.

### Evaluation of Models
[[Use and evaluation of large language models LLMs | Model evaluation]] is crucial and often underrated <a class="yt-timestamp" data-t="01:57:57">[01:57:57]</a>. For Speak, evaluation is not just about word error rate but also about catching individual mistakes and understanding unintelligible speech, sometimes even training models to understand words humans wouldn't <a class="yt-timestamp" data-t="01:31:34">[01:31:34]</a>. A robust evaluation framework provides execution clarity for the team <a class="yt-timestamp" data-t="01:32:16">[01:32:16]</a>. When new models like GPT-4o are released, Speak runs them against numerous internal [[Use and evaluation of large language models LLMs | eval loops]] and human-in-the-loop evaluations, relying on a playbook to manage the process <a class="yt-timestamp" data-t="01:33:13">[01:33:13]</a>. They also track product metrics directly with users to gauge success <a class="yt-timestamp" data-t="01:34:12">[01:34:12]</a>.