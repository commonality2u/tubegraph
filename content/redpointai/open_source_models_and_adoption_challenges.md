---
title: Open source models and adoption challenges
videoId: P6y0gr-W2-k
---

From: [[redpointai]] <br/> 

The relevance and adoption of [[the_role_and_potential_of_open_source_models_in_ai | open source models]] have been a significant point of discussion and surprise in the AI community. While hobbyist and local Llama communities show enthusiasm, [[enterprise_ai_adoption_and_deployment_models | enterprise adoption]] has been surprisingly low and declining.

## Surprising Lack of Enterprise Adoption
The general sentiment among enterprise users regarding [[open_source_versus_closed_source_models_in_ai | open source models]] is that they are "cool" but have not fundamentally altered the adoption path of AI within businesses <a class="yt-timestamp" data-t="00:02:46">[00:02:46]</a>. A specific estimate from Anker of Brain Trust suggests that open-source model usage in enterprises is around 5% and potentially decreasing <a class="yt-timestamp" data-t="00:02:59">[00:02:59]</a>.

This limited [[enterprise_ai_adoption_challenges | enterprise adoption]] is attributed to several factors:
*   **Use Case Discovery Mode**: Enterprises are primarily focused on discovering practical use cases for AI, prioritizing the most powerful models available, regardless of their open-source status <a class="yt-timestamp" data-t="00:03:11">[00:03:11]</a>.
*   **Rapid Model Evolution**: The constant emergence of new model generations necessitates continuous re-evaluation and discovery, making it difficult for enterprises to commit to specific open-source models <a class="yt-timestamp" data-t="00:03:22">[00:03:22]</a>.
*   **Licensing Debates**: While discussions around licenses are common, they haven't translated into significant shifts in AI adoption patterns <a class="yt-timestamp" data-t="00:02:50">[00:02:50]</a>.

## Open Source Catch-Up and its Nuances
Despite the adoption challenges, there was initial surprise at how quickly [[open_source_vs_closed_source_large_language_models | open source]] models caught up to proprietary ones in terms of performance <a class="yt-timestamp" data-t="00:03:32">[00:03:32]</a>. It was initially unclear if closed-source models would maintain a compounding advantage, but the gap has felt like it's shortened <a class="yt-timestamp" data-t="00:03:37">[00:03:37]</a>.

However, this "catch-up" is nuanced:
*   **DeepSeek's Role**: The rapid advancement attributed to [[the_evolution_and_sustainability_of_open_source_projects_with_ai | open source]] is largely due to the efforts of specific entities like DeepSeek, rather than a collective "team open source" <a class="yt-timestamp" data-t="00:04:11">[00:04:11]</a>. There is now evidence that DeepSeek may cease open-sourcing its models <a class="yt-timestamp" data-t="00:04:16">[00:04:16]</a>.
*   **Distillation vs. Original Training**: Much of the perceived "catching up" involves distilling knowledge from models like DeepSeek, which is an easier task than training competitive models from scratch <a class="yt-timestamp" data-t="00:04:30">[00:04:30]</a>.
*   **Replication vs. Innovation**: Replicating existing models is significantly cheaper than creating fundamentally new ones <a class="yt-timestamp" data-t="00:04:48">[00:04:48]</a>. While DeepSeek was a strong new entrant, it didn't fundamentally introduce new concepts <a class="yt-timestamp" data-t="00:04:59">[00:04:59]</a>.

### Unique Contributions of Open Source Models
Despite these challenges, some open-source models have offered unique contributions. The R1 model, for instance, was noted for having "full traces," which is considered a net unique feature in the open-source ecosystem <a class="yt-timestamp" data-t="00:05:15">[00:05:15]</a>.

## Challenges for New Model Players
The continued emergence of new companies focused on training models is viewed as "overhyped" given the current landscape <a class="yt-timestamp" data-t="00:16:14">[00:16:14]</a>. While there might be opportunities for new model players, a clear rationale for what they would offer that is currently missing is not apparent <a class="yt-timestamp" data-t="00:16:26">[00:16:26]</a>. Many new model companies aim to pursue AGI or hit all benchmarks, which is not sustainable for all <a class="yt-timestamp" data-t="00:16:39">[00:16:39]</a>.

> "If there is something like that that these companies can latch on to and being their sort of known for being the best at maybe there's a case for that. Largely though I do agree with you that I don't think there should be at this point more model companies." <a class="yt-timestamp" data-t="00:17:21">[00:17:21]</a>

The prevailing sentiment suggests that a general-purpose model is the optimal approach, and training hyper-specific models, while potentially cheaper and faster, typically does not yield higher quality <a class="yt-timestamp" data-t="00:19:16">[00:19:16]</a>. Historical examples, such as the Bloomberg GPT model, showed that general-purpose models often surpass specialized ones, leading to the conclusion that "closed models were better" for many applications <a class="yt-timestamp" data-t="00:20:00">[00:20:00]</a>.

[!INFO] Exception: [[open_source_ecosystem_and_model_specialization | Specialized Models for Unique Datasets]]
There is still perceived opportunity for specialized models when they leverage unique datasets, such as those found in robotics, biology, or material science, where substantial new data is required <a class="yt-timestamp" data-t="00:17:34">[00:17:34]</a>. These fields involve wet labs and extensive experimentation, providing distinct data advantages <a class="yt-timestamp" data-t="00:17:53">[00:17:53]</a>. However, the core LLM agent space remains challenging for new entrants due to competition from larger players <a class="yt-timestamp" data-t="00:18:02">[00:18:02]</a>.