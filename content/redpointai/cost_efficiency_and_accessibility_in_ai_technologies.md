---
title: Cost efficiency and accessibility in AI technologies
videoId: ZtY8VXswa2o
---

From: [[redpointai]] <br/> 

Answer AI, co-founded by Eric Ries and Jeremy Howard, aims to build the "Bell Labs of AI" by focusing on creating smaller, cheaper, and more affordable AI models and applications in sectors like legal and education <a class="yt-timestamp" data-t="00:00:00">[00:00:00]</a>. This approach stands in contrast to common trends in the AI world, which often involve large funding rounds and significant spending on models and compute before market engagement <a class="yt-timestamp" data-t="00:00:57">[00:00:57]</a>.

## Applying Lean Startup Principles to AI

Eric Ries notes that the principles of "The Lean Startup" are still deeply relevant in the AI industry <a class="yt-timestamp" data-t="00:01:51">[00:01:51]</a>. While AI demos can be "magical," leading companies to believe they don't need to test with customers, the fundamental truth remains: it's impossible to know in advance what customers will want <a class="yt-timestamp" data-t="00:01:36">[00:01:36]</a>. Experimentation and discovering customer needs through "revealed actions" are crucial <a class="yt-timestamp" data-t="00:02:12">[00:02:12]</a>.

A significant [[challenges_and_strategies_in_ai_deployment | challenge]] arises when the traditional SaaS stack is "copy-pasted" to AI, assuming similar structures and economics <a class="yt-timestamp" data-t="00:02:23">[00:02:23]</a>. Many AI companies building APIs assume their customers will define [[challenges_and_strategies_in_ai_deployment | product-market fit]], leading to a potential disconnect of "two, three, or four layers deep between the model and the end product" <a class="yt-timestamp" data-t="00:03:05">[00:03:05]</a>. Ries emphasizes the importance of understanding the "end, end, end customer" regardless of one's position in the stack <a class="yt-timestamp" data-t="00:03:23">[00:03:23]</a>.

The [[economic_and_strategic_considerations_in_ai_model_deployment | economics of AI]] are "completely different" from traditional software, drawing more parallels to physical manufacturing, deep-sea oil drilling, or nuclear power plants due to real infrastructure and operating costs <a class="yt-timestamp" data-t="00:03:41">[00:03:41]</a>. This amplifies risk rather than reducing it <a class="yt-timestamp" data-t="00:04:00">[00:04:00]</a>. Despite the potential for defensibility issues (moats) in a rapidly evolving field, a strong vision combined with preparedness to pivot and rapid iteration are essential <a class="yt-timestamp" data-t="00:06:51">[00:06:51]</a>.

## Answer AI's R&D Lab Model and Efficiency Focus

Answer AI operates as a for-profit R&D lab, an unusual structure in modern venture capital <a class="yt-timestamp" data-t="00:14:35">[00:14:35]</a>. This model prioritizes integrating research ("R") and development ("D"), believing that the best research is conducted when the researcher is "coupled to the application" <a class="yt-timestamp" data-t="00:16:02">[00:16:02]</a>. This continuous feedback loop from customer needs back into scientific inquiry drives breakthroughs <a class="yt-timestamp" data-t="00:16:21">[00:16:21]</a>.

Answer AI's focus on [[cost_optimization_and_economic_considerations_for_ai_model_deployment | resource efficiency]] is a core mission <a class="yt-timestamp" data-t="00:25:52">[00:25:52]</a>. They believe there's an "overinvestment in training Foundation models from scratch" and in using "gold-plated super expensive hardware," leading to an "underinvestment on like the real world, which is resource constrained" <a class="yt-timestamp" data-t="00:25:40">[00:25:40]</a>. Their Llama 3 fine-tuning breakthrough, achieved by one researcher, Karam, exemplifies this focus, demonstrating how to significantly reduce costs and improve accessibility <a class="yt-timestamp" data-t="00:24:10">[00:24:10]</a>.

For many large AI labs, simply making something cheaper is "tedious" <a class="yt-timestamp" data-t="00:27:00">[00:27:00]</a>. However, Jeremy Howard and Eric Ries argue that a "difference in degree becomes a difference in kind" <a class="yt-timestamp" data-t="00:27:11">[00:27:11]</a>. Reducing inference costs doesn't just improve margins; it makes entirely new applications possible <a class="yt-timestamp" data-t="00:27:23">[00:27:23]</a>. The software industry is now dealing with physical supply chain and power constraints, making [[cost_optimization_and_economic_considerations_for_ai_model_deployment | efficiency optimizations]] critical <a class="yt-timestamp" data-t="00:27:30">[00:27:30]</a>.

### The Vision of Continuous Fine-Tuning

A future where it's "so cheap to fine-tune a model that you can do it continuously" is envisioned <a class="yt-timestamp" data-t="00:27:58">[00:27:58]</a>. This contrasts with current "amnesiac models" that lack memory <a class="yt-timestamp" data-t="00:28:06">[00:28:06]</a>. Continuous pre-training of individual agents on inexpensive virtual machines could unlock hyper-personalization and context-aware use cases, where dedicated resources per customer become feasible <a class="yt-timestamp" data-t="00:28:45">[00:28:45]</a>.

This focus on "manufacturability" and practical deployment over "splashy demos" is compared to Thomas Edison's approach to electricity: making the technology practical and usable for widespread application <a class="yt-timestamp" data-t="00:29:15">[00:29:15]</a>.

## Applications for Societal Benefit

Answer AI prioritizes areas where the ability to compute and interact with language can have clear societal benefits <a class="yt-timestamp" data-t="00:34:33">[00:34:33]</a>. Two key areas are:

*   **Law:** Described as a "very large language model" (text in, text out), the law is often used as a "weapon by wealthy people and organizations" against less wealthy ones <a class="yt-timestamp" data-t="00:35:05">[00:35:05]</a>. By bringing down the [[cost_optimization_and_economic_considerations_for_ai_model_deployment | cost]] of high-quality legal advice, AI can combat this injustice and reduce gatekeeping in regulated markets <a class="yt-timestamp" data-t="00:36:12">[00:36:12]</a>.
*   **Education:** As a homeschooling dad, Jeremy Howard sees immense opportunities to improve education <a class="yt-timestamp" data-t="00:37:07">[00:37:07]</a>. AI can help remove the "constrained environment" where all children follow the same path, enabling more personalized learning and allowing people to "be the people they want to be" <a class="yt-timestamp" data-t="00:37:48">[00:37:48]</a>.

## AI Safety, Centralization, and Openness

Jeremy Howard expressed concerns about proposed AI safety regulations, such as California's SB147, which aims to ensure the safety of AI models <a class="yt-timestamp" data-t="00:39:02">[00:39:02]</a>. He argues that such policies, while well-intentioned, could be "uneffective" and even cause the "opposite result," creating a less safe situation <a class="yt-timestamp" data-t="00:40:01">[00:40:01]</a>.

The core issue is that AI models, like pens or calculators, are "dual-use technology" <a class="yt-timestamp" data-t="00:40:32">[00:40:32]</a>. It's impossible to ensure their safety in a way that prevents misuse if the raw model is accessible <a class="yt-timestamp" data-t="00:40:46">[00:40:46]</a>. Regulatory attempts to ensure safety by restricting model releases (allowing only products on top of them) would transform models into "extremely rivalrous goods" <a class="yt-timestamp" data-t="00:43:10">[00:43:10]</a>. This would lead to [[trends_and_challenges_in_ai_infrastructure | massive centralization of power]] and reduced transparency, hindering independent study and defensive applications like cybersecurity or vaccine development <a class="yt-timestamp" data-t="00:43:46">[00:43:46]</a>.

Instead of focusing solely on frontier AGI models, which can pose safety risks when deployed, Eric Ries advocates for building a "huge class of valuable applications that are intrinsically safe" using smaller, properly fine-tuned models <a class="yt-timestamp" data-t="00:46:32">[00:46:32]</a>. He believes that if these safer options aren't provided, people will default to less safe alternatives <a class="yt-timestamp" data-t="00:46:38">[00:46:38]</a>.

## Overhyped, Underhyped, and Future Breakthroughs

Jeremy Howard considers "agents" to be overhyped because current attempts to use them are often "not compatible with the mathematical foundations of the models" <a class="yt-timestamp" data-t="00:48:40">[00:48:40]</a>. Conversely, he sees [[cost_optimization_and_economic_considerations_for_ai_model_deployment | resource efficiency]] as underhyped <a class="yt-timestamp" data-t="00:48:38">[00:48:38]</a>.

For [[future_development_areas_for_ai_and_efficiency_optimizations | future breakthroughs]] that could fundamentally change AI, two areas are highlighted:
*   **Energy and Resource Requirements:** A breakthrough in reducing the "massive energy requirements" of models would overcome a direct "economic or even physical obstacle" <a class="yt-timestamp" data-t="00:50:34">[00:50:34]</a>.
*   **Planning and Reasoning Capabilities:** A breakthrough that moves beyond "subgraph matching" (the current "auto-regressive" word-by-word generation) into true planning and reasoning, possibly through approaches like "jeeper-based models" or "diffusion models for text," would be transformative <a class="yt-timestamp" data-t="00:51:14">[00:51:14]</a>.

Eric Ries adds that a breakthrough in understanding human cognition itself, revealing a more efficient way to build intelligence than the current "brute force" methods (like building a calculator out of Minecraft blocks by trying every combination), would be mind-blowing <a class="yt-timestamp" data-t="00:53:12">[00:53:12]</a>.

More information can be found at Answer.AI <a class="yt-timestamp" data-t="00:54:42">[00:54:42]</a>.