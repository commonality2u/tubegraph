---
title: AI advancements in scientific research and social sciences
videoId: OoL8K_AFqkw
---

From: [[redpointai]] <br/> 

AI models, particularly those developed by OpenAI, are showing significant potential for [[AI advancements in coding and software engineering | advancing scientific research]] and exploring social sciences <a class="yt-timestamp" data-t="00:42:10">[00:42:10]</a>. These models are transitioning from being broadly capable to potentially surpassing expert humans in various domains <a class="yt-timestamp" data-t="00:42:26">[00:42:26]</a>.

## Current Capabilities and Applications

OpenAI's O1 model is at the forefront of reasoning in Large Language Models (LLMs) <a class="yt-timestamp" data-t="00:36:03">[00:36:03]</a>. It is described as "more intelligent" and "extremely good" for tackling very hard problems <a class="yt-timestamp" data-t="00:15:24">[00:15:24]</a>.

### Accelerating Scientific Research
O1 can tackle hard research questions that would normally require someone with a PhD to handle <a class="yt-timestamp" data-t="00:15:40">[00:15:40]</a>. The hope is that these models can act as partners to researchers, enabling tasks that were previously impossible or doing them much faster <a class="yt-timestamp" data-t="00:42:51">[00:42:51]</a>.

Specific areas where O1 has shown impressive results include:
*   **Math** <a class="yt-timestamp" data-t="00:43:59">[00:43:59]</a>: The model can multiply large numbers by working through arithmetic steps like carrying digits <a class="yt-timestamp" data-t="00:20:15">[00:20:15]</a>. However, it's more optimal for it to use a calculator tool or write a Python script for such tasks <a class="yt-timestamp" data-t="00:20:31">[00:20:31]</a>.
*   [[AI advancements in coding and software engineering | **Coding**]] <a class="yt-timestamp" data-t="00:43:59">[00:43:59]</a>: O1 is expected to perform much better in coding than its preview version, with the potential to significantly change the field <a class="yt-timestamp" data-t="00:22:27">[00:22:27]</a>. Internally, it is used for difficult coding tasks or when a large amount of code needs to be written <a class="yt-timestamp" data-t="00:23:04">[00:23:04]</a>.

While it's currently unclear if O1 can improve the state of chemistry, biology, or theoretical mathematics research, releasing the model to experts in these fields will provide valuable feedback <a class="yt-timestamp" data-t="00:43:24">[00:43:24]</a>.

### Exploring Social Sciences
AI models like O1 are promising for [[experimentation_in_ai_and_data_science | social science experiments]] and neuroscience research <a class="yt-timestamp" data-t="00:36:09">[00:36:09]</a>. They can provide insights into human behavior by imitating humans, which can be more scalable and cheaper than using human subjects <a class="yt-timestamp" data-t="00:36:17">[00:36:17]</a>.

An example of this application is in **Game Theory**:
*   Models can be used for experiments like the Ultimatum Game <a class="yt-timestamp" data-t="00:38:01">[00:38:01]</a>, where participants decide on splitting money <a class="yt-timestamp" data-t="00:38:04">[00:38:04]</a>.
*   Historically, such experiments at higher stakes involved significant costs or ethical concerns, sometimes requiring studies in very poor communities <a class="yt-timestamp" data-t="00:38:59">[00:38:59]</a>. AI models could offer insights into how people might react in cost-prohibitive or ethically sensitive situations <a class="yt-timestamp" data-t="00:39:19">[00:39:19]</a>.
*   The ability to quantify how closely models match human behavior in these settings will be important <a class="yt-timestamp" data-t="00:39:56">[00:39:56]</a>. As models become more capable, they are expected to better imitate human actions <a class="yt-timestamp" data-t="00:40:08">[00:40:08]</a>.

### AI in Education and Human Interaction
The development of LLMs has effectively solved the problem of AI communication, as they possess a language that humans also use <a class="yt-timestamp" data-t="00:40:43">[00:40:43]</a>. This means AI agents can now interact and negotiate with other AI agents and humans <a class="yt-timestamp" data-t="00:40:55">[00:40:55]</a>. This capability has implications for [[ai_in_education_and_human_interaction | AI in education]] and [[ai_applications_in_legal_and_education | legal applications]], though these are not explicitly detailed here.

## Overcoming Challenges in AI Research

Previously, the hardest unsolved research question was finding a general way to scale inference compute <a class="yt-timestamp" data-t="00:00:11">[00:00:11]</a>. This was expected to take at least a decade but was achieved in 2-3 years <a class="yt-timestamp" data-t="00:01:15">[00:01:15]</a>. This breakthrough is largely attributed to advancements in "test time compute" <a class="yt-timestamp" data-t="00:03:15">[00:03:15]</a>.

Nome, a research scientist at OpenAI, was a key part of the work on O1 <a class="yt-timestamp" data-t="00:00:26">[00:00:26]</a>. His background in search and planning for games like poker and diplomacy influenced this approach <a class="yt-timestamp" data-t="00:00:33">[00:00:33]</a>. The shift in mindset was from extending specific algorithms to more domains to starting with a general domain (like language) and figuring out how to scale test-time compute for it <a class="yt-timestamp" data-t="00:18:34">[00:18:34]</a>.

This approach has led to models like O1 capable of emergent behaviors <a class="yt-timestamp" data-t="00:14:04">[00:14:04]</a>, such as trying different strategies, breaking down problems, recognizing mistakes, and correcting them, simply by being allowed to "think for longer" <a class="yt-timestamp" data-t="00:13:43">[00:13:43]</a>. This qualitative change in behavior gave conviction that the approach would be significant <a class="yt-timestamp" data-t="00:14:38">[00:14:38]</a>.

## [[challenges_in_using_ai_for_research_advancements | Challenges and Future Directions]]

### The "Soft Wall" of Pre-training
Scaling pre-training models further incurs increasing costs, moving from thousands to hundreds of millions of dollars <a class="yt-timestamp" data-t="00:02:07">[00:02:07]</a>. While models continue to improve with more resources, there's an eventual "soft wall" where the cost becomes economically intractable, reaching billions or tens of billions of dollars <a class="yt-timestamp" data-t="00:02:37">[00:02:37]</a>.

### Importance of Test-Time Compute
Test-time compute is seen as having significant "runway" for further scaling <a class="yt-timestamp" data-t="00:03:37">[00:03:37]</a>. The potential for algorithmic improvements in this area is substantial <a class="yt-timestamp" data-t="00:03:42">[00:03:42]</a>.
*   Current ChatGPT queries cost around a penny <a class="yt-timestamp" data-t="00:03:33">[00:03:33]</a>.
*   For highly important problems, people might be willing to pay millions of dollars per query <a class="yt-timestamp" data-t="00:04:48">[00:04:48]</a>, indicating about eight orders of magnitude of room to push test-time compute further <a class="yt-timestamp" data-t="00:05:00">[00:05:00]</a>.

### The Bitter Lesson and Scaffolding
Richard Sutton's "Bitter Lesson" is cited, suggesting that techniques that scale well with more compute and data will ultimately succeed over human-coded knowledge or "scaffolding" <a class="yt-timestamp" data-t="00:26:05">[00:26:05]</a>.
*   While scaffolding and prompting tricks can push models slightly further in the short term, they don't scale well with more data and compute <a class="yt-timestamp" data-t="00:26:48">[00:26:48]</a>.
*   Models like O1, which scale well with more data and compute, are expected to make these scaffolding techniques obsolete in the long run <a class="yt-timestamp" data-t="00:27:15">[00:27:15]</a>. This poses a challenge for startups that might invest heavily in such temporary solutions <a class="yt-timestamp" data-t="00:27:37">[00:27:37]</a>.

### Role of Academia in [[AI research and innovation | AI Research]]
Academia faces [[challenges_in_using_ai_for_research_advancements | challenges in competing with industry labs]] due to the dependence on data and compute resources <a class="yt-timestamp" data-t="00:29:04">[00:29:04]</a>.
*   There's an incentive to add "clever prompting or tricks" to achieve marginal performance gains for papers, which may not lead to impactful long-term research <a class="yt-timestamp" data-t="00:29:20">[00:29:20]</a>.
*   It's suggested that academia should focus on investigating novel architectures or approaches that demonstrate promising scaling trends with more data and compute, even if they don't achieve state-of-the-art performance immediately <a class="yt-timestamp" data-t="00:30:21">[00:30:21]</a>. Industry labs are interested in such foundational work <a class="yt-timestamp" data-t="00:31:09">[00:31:09]</a>.

### Multimodal Models and Hardware
O1 accepts images as input <a class="yt-timestamp" data-t="00:36:38">[00:36:38]</a>, and there are no perceived blockers to making these models as multimodal as others like GPT-4o <a class="yt-timestamp" data-t="00:16:48">[00:16:48]</a>.

The development of O1 signals a shift in hardware thinking, moving from a focus on massive pre-training runs to optimizing for inference compute <a class="yt-timestamp" data-t="00:35:09">[00:35:09]</a>. This creates an opportunity for hardware innovation to adapt to this new paradigm <a class="yt-timestamp" data-t="00:35:27">[00:35:27]</a>.

### Outlook
Progress in [[AI research and innovation | AI research]] is expected to accelerate <a class="yt-timestamp" data-t="00:45:29">[00:45:29]</a>. The shift in perspective regarding the rapid progress of AI, particularly with the general scaling of test-time compute, has increased optimism about achieving highly intelligent models sooner than previously thought <a class="yt-timestamp" data-t="00:34:12">[00:34:12]</a>.