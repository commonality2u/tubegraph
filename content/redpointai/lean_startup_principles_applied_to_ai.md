---
title: Lean Startup principles applied to AI
videoId: ZtY8VXswa2o
---

From: [[redpointai]] <br/> 

Eric Ries, author of *The Lean Startup*, and Jeremy Howard are building Answer AI, aiming to create the "Bell Labs of AI" <a class="yt-timestamp" data-t="00:00:01">[00:00:01]</a>. They focus on building smaller, cheaper, more affordable models and applications, particularly in the legal and education sectors <a class="yt-timestamp" data-t="00:00:05">[00:00:05]</a>. Ries observes that while his Lean Startup lessons defined how many in tech approach building and innovating, the [[the_evolution_of_ai_startup_strategies | AI world]] sometimes presents different scenarios <a class="yt-timestamp" data-t="00:00:50">[00:00:50]</a>.

## Unique Challenges in Applying Lean Startup to AI

Ries notes that [[challenges_in_ai_product_development | AI makes unbelievably good demos]], which can lead companies to believe they are the exception and don't need to test with customers <a class="yt-timestamp" data-t="00:01:32">[00:01:32]</a>. This often results in significant spending on models and compute long before a product touches the market <a class="yt-timestamp" data-t="00:01:00">[00:01:00]</a>.

Another challenge is the tendency to simply copy-paste the existing SAS (Software as a Service) stack to AI, assuming everything will be the same <a class="yt-timestamp" data-t="00:02:23">[00:02:23]</a>. Many AI companies creating APIs push the product-market fit question down the "stack" to their customers, who then must define it with *their* customers <a class="yt-timestamp" data-t="00:02:45">[00:02:45]</a>. This value chain can be two, three, or four layers deep between the model and the end product, potentially leading to "carnage" in applications if the stack is not assembled differently <a class="yt-timestamp" data-t="00:03:05">[00:03:05]</a>.

Ries highlights that the economics of AI are "completely different" from traditional software, drawing more parallels to physical manufacturing, deep-sea oil drilling, or nuclear power plants due to their infrastructure, operating costs, and market risk <a class="yt-timestamp" data-t="00:03:41">[00:03:41]</a>.

## Core Lean Startup Principles Remain Essential

Despite the unique aspects of AI, fundamental Lean Startup principles still apply <a class="yt-timestamp" data-t="00:01:51">[00:01:51]</a>:
*   **Customer-Centricity**: "You can't know in advance what customers are going to want" <a class="yt-timestamp" data-t="00:01:56">[00:01:56]</a>. Instead of asking customers what they want, businesses should experiment and discover needs through "revealed actions" <a class="yt-timestamp" data-t="00:02:12">[00:02:12]</a>. Eric Ries emphasizes understanding the "end, end, end customer" regardless of a company's position in the stack <a class="yt-timestamp" data-t="00:03:25">[00:03:25]</a>. As Peter Drucker stated, "a business is an entity that exists to create a customer," and this is unchanged by AI; AI agents are not the customer, human beings are <a class="yt-timestamp" data-t="00:04:15">[00:04:15]</a>.
*   **Rapid Iteration and Pivoting**: Given the high level of uncertainty in AI, especially regarding future model capabilities and consumer desires, it's crucial to build in a way that allows for rapid iteration and the ability to pivot <a class="yt-timestamp" data-t="00:07:39">[00:07:39]</a>. This means staying alert to the possibility that assumptions could be wrong <a class="yt-timestamp" data-t="00:07:42">[00:07:42]</a>. The pace of change in AI makes quick adaptation and feedback more important than ever <a class="yt-timestamp" data-t="00:08:27">[00:08:27]</a>.

## Moats and Defensibility in AI

Some entrepreneurs are "paralyzed into action" by concerns about "moats" and defensibility in the AI space <a class="yt-timestamp" data-t="00:04:44">[00:04:44]</a>. Jeremy Howard suggests that the priority should be to build something customers want, and then "earn the right" to think about a moat <a class="yt-timestamp" data-t="00:05:04">[00:05:04]</a>.

Ries describes this as "picking up dimes in front of a steamroller" <a class="yt-timestamp" data-t="00:05:20">[00:05:20]</a>. While large platforms like OpenAI could theoretically "nuke you from orbit," they have limited focus <a class="yt-timestamp" data-t="00:05:37">[00:05:37]</a>. The strategy is to quickly jump in, grab a use case (the "dime"), and jump out <a class="yt-timestamp" data-t="00:05:51">[00:05:51]</a>. This requires speed and the ability to pivot if needed <a class="yt-timestamp" data-t="00:05:51">[00:05:51]</a>.

## Answer AI's R&D Lab Approach

Answer AI operates as a for-profit R&D lab, an unconventional model for venture-backed startups <a class="yt-timestamp" data-t="00:14:35">[00:14:35]</a>. Eric Ries argues that the best research occurs when the researcher is "coupled to the application" <a class="yt-timestamp" data-t="00:16:07">[00:16:07]</a>. This contrasts with the modern trend of hyper-specialization where research (R) and development (D) are separated, leading to research untethered from practical applications <a class="yt-timestamp" data-t="00:15:35">[00:15:35]</a>.

### The Edison Approach
Answer AI aims to emulate Thomas Edison's lab, where the iteration loop extends from the customer all the way into scientific inquiry and back <a class="yt-timestamp" data-t="00:16:21">[00:16:21]</a>.

For instance, in a corporate setting, Ries witnessed researchers focused on winning a Nobel Prize for energy efficiency in data centers, only to find through an MVP that customers (data center builders) cared more about physical footprint <a class="yt-timestamp" data-t="00:16:54">[00:16:54]</a>. Bringing this customer feedback back to the lab immediately redirected research toward a relevant problem <a class="yt-timestamp" data-t="00:17:58">[00:17:58]</a>.

### "Long Leash with Narrow Fences"
Inspired by historian Eric Gilliam, Answer AI adopts a "long leash with narrow fences" approach <a class="yt-timestamp" data-t="00:25:24">[00:25:24]</a>. Jeremy Howard and Eric Ries establish "narrow fences" â€“ a research thesis focusing on specific areas like efficiency and reasonable cost, countering overinvestment in large, expensive models <a class="yt-timestamp" data-t="00:25:29">[00:25:29]</a>. Within these fences, team members have a "long leash" to pursue projects they believe in <a class="yt-timestamp" data-t="00:24:43">[00:24:43]</a>.

An example is Karam's breakthrough in efficient fine-tuning of Llama 3 <a class="yt-timestamp" data-t="00:24:10">[00:24:10]</a>. He independently pursued this project, driven by the team's shared focus on reducing costs and increasing accessibility <a class="yt-timestamp" data-t="00:24:43">[00:24:43]</a>. This focus on "the same thing but cheaper" is a major part of Answer AI's mission to make AI more accessible, which is often dismissed as "tedious" by larger labs <a class="yt-timestamp" data-t="00:26:59">[00:26:59]</a>.

## Efficiency and Accessibility as Breakthroughs

Ries argues that a "difference in degree becomes a difference in kind" <a class="yt-timestamp" data-t="00:27:15">[00:27:15]</a>. Making inference costs cheaper doesn't just improve margins; it enables new applications <a class="yt-timestamp" data-t="00:27:23">[00:27:23]</a>. The software industry is now dealing with "actual supply chain constraints" like physical installation, power access, and HBM memory manufacturing limits <a class="yt-timestamp" data-t="00:27:30">[00:27:30]</a>.

Lower costs could enable continuous fine-tuning, allowing models to have "memory" and hyper-personalization, unlike current "amnesiac models" <a class="yt-timestamp" data-t="00:28:01">[00:28:01]</a>. This opens up use cases requiring dedicated, customized models per customer <a class="yt-timestamp" data-t="00:29:02">[00:29:02]</a>.

Just like electricity's practical applications were not always obvious but required "hundreds of thousands of individual experiments painstakingly done," AI needs to focus on "manufacturability," deployability, and usability <a class="yt-timestamp" data-t="00:29:22">[00:29:22]</a>.

## Applications in Legal and Education

Answer AI specifically targets the legal and education sectors, which are heavily language-based and offer significant opportunities for societal improvement <a class="yt-timestamp" data-t="00:34:43">[00:34:43]</a>.

*   **Legal**: The law is often used as a "weapon by wealthy people and organizations" <a class="yt-timestamp" data-t="00:35:36">[00:35:36]</a>. Reducing the cost of high-quality legal advice can combat injustice and gatekeeping, enabling more people to pursue their ideas <a class="yt-timestamp" data-t="00:36:14">[00:36:14]</a>.
*   **Education**: Jeremy Howard, a homeschooling dad, believes AI can improve education by removing constraints like standardized paths, allowing for more customized learning <a class="yt-timestamp" data-t="00:37:06">[00:37:06]</a>.

Both fields involve significant "language in, language out" <a class="yt-timestamp" data-t="00:38:16">[00:38:16]</a>.

## [[challenges_and_strategies_in_enterprise_ai_deployment | Challenges and Strategies in Enterprise AI Deployment]]

Jeremy Howard expresses concern over proposed legislation like California's SB147, which aims to ensure AI model safety through regulation <a class="yt-timestamp" data-t="00:39:02">[00:39:02]</a>. His research, including interviews with over 70 experts, suggests such policies would be ineffective and could even lead to less safe situations <a class="yt-timestamp" data-t="00:39:43">[00:39:43]</a>.

The core issue is that AI models are "dual use technology," like a pen, paper, or calculator <a class="yt-timestamp" data-t="00:40:32">[00:40:32]</a>. A model deemed "safe" can still be fine-tuned or prompted to do unsafe things by users <a class="yt-timestamp" data-t="00:40:55">[00:40:55]</a>.
*   **Implications of Regulation**: If safety must be "ensured," it effectively means models cannot be released in their raw form <a class="yt-timestamp" data-t="00:41:34">[00:41:34]</a>. Instead, only "products on top of them" can be released (e.g., ChatGPT versus downloadable Llama 3) <a class="yt-timestamp" data-t="00:42:01">[00:42:01]</a>.
*   **Centralization of Power**: Raw models are "much more powerful" due to the ability to fine-tune, study weights, control, and cache <a class="yt-timestamp" data-t="00:42:42">[00:42:42]</a>. Restricting their release makes them an "extremely rivalrous good," accessible only to big states and companies, leading to "massive centralization of power" and reduced transparency <a class="yt-timestamp" data-t="00:43:03">[00:43:03]</a>. This prevents widespread use for defensive purposes like cybersecurity or vaccine development <a class="yt-timestamp" data-t="00:43:34">[00:43:34]</a>.
*   **Safety through Accessibility**: Eric Ries argues that focusing on unlocking the "unbelievable reservoir of applications that don't require AGI" <a class="yt-timestamp" data-t="00:45:14">[00:45:14]</a> leads to building intrinsically safe applications with smaller, properly fine-tuned models <a class="yt-timestamp" data-t="00:46:15">[00:46:15]</a>. If these options aren't available, people default to potentially unsafe uses of large frontier models <a class="yt-timestamp" data-t="00:46:38">[00:46:38]</a>.

Ries also points out that large foundation labs can become "schizophrenic," with commercial teams disconnected from or uninterested in the safety agenda of the research teams, which can lead to internal "rival tensions" <a class="yt-timestamp" data-t="00:47:08">[00:47:08]</a>. He suggests these labs re-establish the connection between research and the customer by affirmatively seeing their customers succeed and using the "Toyota production system Playbook" to "go and see for yourself" <a class="yt-timestamp" data-t="00:48:06">[00:48:06]</a>.

## Overhyped and Underhyped in AI

*   **Overhyped**: Agents <a class="yt-timestamp" data-t="00:48:34">[00:48:34]</a>. Jeremy Howard believes current agent capabilities are not compatible with the mathematical foundations of language models, especially for novel planning <a class="yt-timestamp" data-t="00:48:42">[00:48:42]</a>. Agents excel at "various mixes and matches of stuff that you've seen in the training data" (e.g., answering emails, adding CSS) but struggle with novel algorithms or research breakthroughs not in their training data <a class="yt-timestamp" data-t="00:22:31">[00:22:31]</a>.
*   **Underhyped**: Resource efficiency <a class="yt-timestamp" data-t="00:48:38">[00:48:38]</a>.

## Future Breakthroughs and Cognition

Jeremy Howard highlights two key breakthroughs that would change his mind on model capabilities:
1.  A significant breakthrough in **energy or other resource requirements** <a class="yt-timestamp" data-t="00:51:03">[00:51:03]</a>.
2.  A breakthrough in **planning and reasoning capability** that goes "past subgraph matching," like Yean LeCun's "Jeeper based models" or "diffusion models for text" <a class="yt-timestamp" data-t="00:51:14">[00:51:14]</a>. Current auto-regressive models are limited by picking words sequentially <a class="yt-timestamp" data-t="00:51:51">[00:51:51]</a>.

Eric Ries notes that the most interesting aspect of LLMs is how they've changed the perception of human intelligence, suggesting "far more of human intelligence is obviously encoded in language and linguistic processing than we previously thought" <a class="yt-timestamp" data-t="00:52:36">[00:52:36]</a>. He hopes for a breakthrough that reveals "the problem is not in our understanding of the scaling of LLMs, the problem is in our understanding of cognition itself in human cognition" <a class="yt-timestamp" data-t="00:53:20">[00:53:20]</a>. He uses the analogy of a calculator built with Minecraft blocks: the current approach might be a brute-force, inefficient way of discovering a "critical algorithm in cognition" that could eventually be done directly <a class="yt-timestamp" data-t="00:53:32">[00:53:32]</a>.