---
title: Evaluating AI progress with ROI
videoId: czyHDP67CMw
---

From: [[redpointai]] <br/> 

Measuring progress and justifying investment in AI initiatives requires a rigorous approach focused on [[the_economic_impact_of_ai_investments_and_roi_expectations | Return on Investment (ROI)]] <a class="yt-timestamp" data-t="00:00:02">[00:00:02]</a>. The journey should begin small and incrementally scale up <a class="yt-timestamp" data-t="00:00:01">[00:00:01]</a>.

## Starting Small: The Experimental Approach

Given the difficulty in predicting AI's [[effectiveness_of_ai_agents_in_specific_tasks | effectiveness]] for specific use cases <a class="yt-timestamp" data-t="00:00:18">[00:00:18]</a>, it's recommended to adopt a scientific, experimental mindset <a class="yt-timestamp" data-t="00:00:25">[00:00:25]</a>.

### Initial Steps

The initial phase can be as simple as:
*   Spending a minimal amount (e.g., 20 cents) on platforms like [[comparisons_and_partnerships_with_ai_model_providers | OpenAI]] or Llama on Databricks to "litmus test" AI's capabilities for a particular task <a class="yt-timestamp" data-t="00:00:12">[00:00:12]</a>.
*   Setting up experiments to maximize chances of success, trying the "best possible model" available <a class="yt-timestamp" data-t="00:00:30">[00:00:30]</a>.
*   For preliminary testing, even basic prompting or manually supplying a few relevant documents to the model can suffice, rather than immediately implementing complex systems like RAG (Retrieval Augmented Generation) <a class="yt-timestamp" data-t="00:00:34">[00:00:34]</a>.

The goal is to determine if there's any inherent value or "there there" before investing further <a class="yt-timestamp" data-t="00:00:41">[00:00:41]</a>.

## Iterative Progress and Benchmarking

As you progress, ensure that advancements are justified by demonstrable [[the_economic_impact_of_ai_investments_and_roi_expectations | ROI]] and contribute to "things that matter" <a class="yt-timestamp" data-t="00:00:03">[00:00:03]</a>.

### Evaluation and Refinement

*   **Establish Benchmarks:** Create specific benchmarks to test AI's performance <a class="yt-timestamp" data-t="00:00:08">[00:00:08]</a>.
*   **Iterative Improvement:** Recognize that initial benchmarks may be inadequate and continuously build better ones <a class="yt-timestamp" data-t="00:00:09">[00:00:09]</a>. This iterative process is crucial for [[evaluation_methodologies_and_user_feedback_for_ai_models | evaluating AI systems]].

### Scaling Up Complexity

If initial experiments show promise, the next steps involve increasing complexity:
*   **Implementing RAG:** If the model requires access to internal data, it's time to implement a robust RAG system, as the model won't inherently know enterprise-specific information <a class="yt-timestamp" data-t="00:00:43">[00:00:43]</a>.
*   **Fine-Tuning:** If significant value is being generated, fine-tuning the model can embed more specific knowledge, leading to better quality, though it incurs more upfront [[cost_efficiency_and_accessibility_in_ai | cost]] <a class="yt-timestamp" data-t="00:00:53">[00:00:53]</a>. This falls under [[ai_production_and_evaluation_techniques | AI production techniques]].

> [!NOTE]
> The principle is to constantly test, evaluate, and scale up only when justified by tangible progress and value creation.