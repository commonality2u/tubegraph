---
title: Evaluating AI through rigorous ROI testing
videoId: czyHDP67CMw
---

From: [[redpointai]] <br/> 

Evaluating AI applications should begin with small-scale implementations and gradually escalate, with each step justified by a rigorous Return on Investment (ROI) analysis <a class="yt-timestamp" data-t="00:00:01">[00:00:01]</a>. The objective is to ensure that progress is being made on metrics that are important to the user <a class="yt-timestamp" data-t="00:00:03">[00:00:03]</a>.

## Initial Testing and Benchmarking
The process involves establishing benchmarks and testing the AI against them <a class="yt-timestamp" data-t="00:00:08">[00:00:08]</a>. It's expected that initial benchmarks may be suboptimal, necessitating iterative improvement to build more effective ones <a class="yt-timestamp" data-t="00:00:09">[00:00:09]</a>.

The journey can start with minimal financial outlay, such as spending as little as 20 cents on platforms like OpenAI or LLaMA on Databricks <a class="yt-timestamp" data-t="00:00:12">[00:00:12]</a>. This initial "litmus test" helps determine if AI is suitable for a particular task <a class="yt-timestamp" data-t="00:00:16">[00:00:16]</a>.

## The Experimental Approach
There is currently a lack of good [[understanding_predictability_and_effectiveness_of_ai | predictability]] regarding whether AI will be highly effective for a specific use case <a class="yt-timestamp" data-t="00:00:19">[00:00:19]</a>. Therefore, the recommended approach is to act as a scientist, engaging in "data science in the literal sense" by running [[experimenting_and_testing_ai_use_cases | experiments]] <a class="yt-timestamp" data-t="00:00:25">[00:00:25]</a>.

To maximize the chance of success, one should:
*   Set up experiments thoughtfully <a class="yt-timestamp" data-t="00:00:30">[00:00:30]</a>.
*   Utilize the best possible model available <a class="yt-timestamp" data-t="00:00:31">[00:00:31]</a>.
*   Start with basic prompting or manually supplying relevant documents into the context, rather than immediately implementing complex methods like Retrieval-Augmented Generation (RAG) <a class="yt-timestamp" data-t="00:00:34">[00:00:34]</a>.

After observing the results of these initial tests, the next steps can be determined <a class="yt-timestamp" data-t="00:00:41">[00:00:41]</a>.

## Scaling Up and Fine-Tuning
If initial experiments show promise, more advanced techniques such as "hardcore RAG" might be considered to bring in proprietary data, as models do not possess innate knowledge of internal enterprise information <a class="yt-timestamp" data-t="00:00:45">[00:00:45]</a>.

If value continues to be demonstrated, fine-tuning the model becomes an option <a class="yt-timestamp" data-t="00:00:53">[00:00:53]</a>. Fine-tuning integrates specific knowledge directly into the model, which involves a higher [[cost_optimization_and_economic_considerations_for_ai_model_deployment | upfront cost]] but typically leads to better quality outputs <a class="yt-timestamp" data-t="00:00:54">[00:00:54]</a>.