---
title: Future of interactive avatars and synchronous streaming
videoId: FudGqDZDSx4
---

From: [[redpointai]] <br/> 

Haiyan, an [[ai_avatar_technology_and_advancements | AI video platform]] that recently secured $60 million in funding at a $500 million valuation, is at the forefront of revolutionizing video creation. Led by CEO Joshua, the company's mission is to make the camera obsolete, enabling visual storytelling for everyone by making video production 10 times faster and cheaper using [[ai_avatar_technology_and_advancements | AI video tools]] <a class="yt-timestamp" data-t="00:04:45">[00:04:45]</a>, <a class="yt-timestamp" data-t="00:11:38">[00:11:38]</a>. The platform helps over 40,000 customers create, localize, and personalize video content <a class="yt-timestamp" data-t="00:09:37">[00:09:37]</a>.

## Current State of Interactive Avatars

Haiyan currently offers an interactive avatar beta version that allows users to send their avatars to participate in Zoom meetings and interact in real time <a class="yt-timestamp" data-t="00:19:15">[00:19:15]</a>, <a class="yt-timestamp" data-t="00:19:26">[00:19:26]</a>. This capability represents a significant step towards more dynamic and immersive [[ai_avatar_technology_and_advancements | AI avatar technology]].

## Technical Challenges for Synchronous Streaming

Achieving seamless synchronous (real-time) streaming with [[ai_avatar_technology_and_advancements | AI avatars]] presents considerable technical hurdles:
*   **Model Complexity:** As [[ai_avatar_technology_and_advancements | AI models]] grow larger and more intricate, optimizing their performance for real-time inference becomes increasingly challenging <a class="yt-timestamp" data-t="00:19:42">[00:19:42]</a>.
*   **Inference Speed:** The primary challenge is navigating through complex model architectures to maintain high inference speeds necessary for live interaction <a class="yt-timestamp" data-t="00:19:51">[00:19:51]</a>.

Despite these challenges, there is strong optimism that these technologies will be capable of running in real time, even on devices, within the next 12 months <a class="yt-timestamp" data-t="00:20:46">[00:20:46]</a>, <a class="yt-timestamp" data-t="00:20:54">[00:20:54]</a>.

## [[Future Directions for Video Models | Future Vision for Synchronous Streaming]]

The advent of real-time [[ai_avatar_technology_and_advancements | AI avatars]] promises to unlock entirely new use cases <a class="yt-timestamp" data-t="00:19:57">[00:19:57]</a>. One significant area of impact is advertising, where platforms like Facebook and Google currently show the same video ads to all users. In the [[future of generative models in media | future]], it will be possible for users to view different video ads based on their individual preferences and watch history, leading to highly personalized experiences <a class="yt-timestamp" data-t="00:20:00">[00:20:00]</a>, <a class="yt-timestamp" data-t="00:20:17">[00:20:17]</a>. This aligns with a broader [[vision_for_the_future_of_aipowered_content_personalization | vision for AI-powered content personalization]] and [[future_trends_in_ai_and_personalization | future trends in AI and personalization]].

## Evolution of Avatar Models

Haiyan's focus is on building "magic moments" by inventing new ways for customers to create video <a class="yt-timestamp" data-t="00:01:32">[00:01:32]</a>, <a class="yt-timestamp" data-t="00:01:45">[00:01:45]</a>, <a class="yt-timestamp" data-t="00:07:33">[00:07:33]</a>. The core of this is the quality of the [[ai_avatar_technology_and_advancements | AI models]], which goes beyond mathematical optimization to focus on how great the video looks <a class="yt-timestamp" data-t="00:08:01">[00:08:01]</a>. Key aspects of an engaging avatar include <a class="yt-timestamp" data-t="00:13:40">[00:13:40]</a>:
*   **Facial Expression and Voice Tone:** Maintaining the speaker's unique voice tone and facial expressions during localization <a class="yt-timestamp" data-t="00:10:10">[00:10:10]</a>.
*   **Body Motion and Gesture:** Coordinating head movement, eyebrow movement, and body gestures to match the script and content <a class="yt-timestamp" data-t="00:14:19">[00:14:19]</a>, <a class="yt-timestamp" data-t="00:21:25">[00:21:25]</a>.
*   **Full Body Rendering:** Haiyan's [[ai_avatar_technology_and_advancements | AI 3.0 model]] can render the entire body, with future plans to include gestures <a class="yt-timestamp" data-t="00:15:20">[00:15:20]</a>, <a class="yt-timestamp" data-t="00:21:32">[00:21:32]</a>.
*   **Capturing Diverse Modes:** The aim is to create models that can capture different speaking modes, such as presentation mode or interview mode, from just 30 seconds to 2 minutes of video footage <a class="yt-timestamp" data-t="00:17:23">[00:17:23]</a>, <a class="yt-timestamp" data-t="00:17:31">[00:17:31]</a>.

## Haiyan's Approach to Video Generation

Haiyan primarily focuses on business videos <a class="yt-timestamp" data-t="00:22:30">[00:22:30]</a>. It pursues an "orchestration engine" approach to video generation, which involves capturing and integrating text, script, voice, sound, music, avatar footage, and background generation <a class="yt-timestamp" data-t="00:23:00">[00:23:00]</a>, <a class="yt-timestamp" data-t="00:24:10">[00:24:10]</a>. This method is preferred over pixel-by-pixel generation (like text-to-video models such as Sora) because it offers greater control, consistency, and quality, which are crucial for brands and enterprises <a class="yt-timestamp" data-t="00:23:17">[00:23:17]</a>. Haiyan aims to integrate with and build upon text-to-video technologies as a foundational layer <a class="yt-timestamp" data-t="00:24:00">[00:24:00]</a>.

## Impact on Content Creation and Traditional Platforms

AI-generated content introduces a dilemma for existing platforms like TikTok, which are built around human content creators <a class="yt-timestamp" data-t="00:29:40">[00:29:40]</a>. If [[ai_avatar_technology_and_advancements | AI-generated content]] becomes prevalent, platforms will face challenges in ranking and recommending it alongside traditional camera-based content without diminishing the reach of human creators <a class="yt-timestamp" data-t="00:30:00">[00:30:00]</a>, <a class="yt-timestamp" data-t="00:30:18">[00:30:18]</a>. This could lead to the emergence of new platforms specifically for [[the future of generative models in media | AI-generated content]] <a class="yt-timestamp" data-t="00:30:37">[00:30:37]</a>. Haiyan, however, intends to remain a creative tool provider, not a consumption platform <a class="yt-timestamp" data-t="00:31:10">[00:31:10]</a>.

## Future of AI-Generated Content and IPs

The ability to generate new voices and [[ai_avatar_technology_and_advancements | AI-generative people]] could lead to the creation of entirely new intellectual properties (IPs) <a class="yt-timestamp" data-t="00:36:10">[00:36:10]</a>. With advancements in image generation allowing for consistent character persistence across different generations, extending this to video opens up possibilities for new forms of AI influencers and digital personas <a class="yt-timestamp" data-t="00:36:27">[00:36:27]</a>, <a class="yt-timestamp" data-t="00:36:48">[00:36:48]</a>.

## Funding and Costs in AI

The [[the current and future trajectory of agi_development | AI category]] is uniquely capital-intensive due to the high costs associated with GPUs and talent <a class="yt-timestamp" data-t="00:37:32">[00:37:32]</a>. Unlike traditional software companies where marginal costs approach zero, serving additional customers in AI involves significant GPU consumption <a class="yt-timestamp" data-t="00:37:56">[00:37:56]</a>. However, [[the future of voice ai and its impact | AI also]] makes individual employees much more efficient, potentially reducing the overall capital required to build a great AI company <a class="yt-timestamp" data-t="00:38:24">[00:38:24]</a>, <a class="yt-timestamp" data-t="00:39:36">[00:39:36]</a>. AI-native teams leveraging AI tools themselves can operate with greater efficiency <a class="yt-timestamp" data-t="00:38:50">[00:38:50]</a>.

## [[Future Directions for Video Models | Future of Video Creation Workflows]]

By 2030, the vision is that everyone will have a "video agency on their pocket" <a class="yt-timestamp" data-t="00:48:30">[00:48:30]</a>. This AI-powered agency would allow users to interact with a product like Haiyan as if speaking to a personal video agency, guiding them through the entire video creation process from ideas to final editing <a class="yt-timestamp" data-t="00:48:04">[00:48:04]</a>, <a class="yt-timestamp" data-t="00:48:41">[00:48:41]</a>.

This transformation will lead to new use cases that are currently unimaginable, much like how the mobile camera in 2012 led to platforms like Instagram, Snapchat, and TikTok <a class="yt-timestamp" data-t="00:49:29">[00:49:29]</a>. Lowering the barrier to content creation through advanced tools will unlock a new world of possibilities for visual storytelling <a class="yt-timestamp" data-t="00:50:01">[00:50:01]</a>.