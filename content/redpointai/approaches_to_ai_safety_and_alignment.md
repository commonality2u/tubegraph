---
title: Approaches to AI safety and alignment
videoId: MvxtIIqJRUQ
---

From: [[redpointai]] <br/> 

OpenAI, a leading AI firm, focuses on building safe [[ai_safety_and_alignment | AI safety and alignment]] systems that benefit humanity <a class="yt-timestamp" data-t="02:30:00">[02:30:00]</a>. Peter Welinder, VP of Product and Partnerships at OpenAI, highlights their approach to mitigating risks and ensuring responsible AI development.

## Addressing AI Risks

While many risks associated with AI, such as misinformation, deepfakes, and bias, are seen as surmountable, the most significant concern is the potential for superintelligence <a class="yt-timestamp" data-t="00:30:53">[00:30:53]</a>.

### Surmountable Risks

*   **Misinformation and Deepfakes**: These issues become problematic at scale and often rely on existing distribution channels like social media or email. Infrastructure is already in place to protect against such abuses <a class="yt-timestamp" data-t="00:31:07">[00:31:07]</a>.
*   **Bias**: It is considered impossible to entirely eliminate bias in models. OpenAI's goal is to provide [[developers_approach_to_ai_models_and_agents | developer tools]] that allow product developers and users to instruct models to adopt desired biases within certain bounds <a class="yt-timestamp" data-t="00:31:34">[00:31:34]</a>. Models should not have a particular political orientation; users should be able to define the model's behavior <a class="yt-timestamp" data-t="00:31:58">[00:31:58]</a>.

### Existential Risks: The Challenge of Superintelligence

The risk of superintelligence, where AI models become significantly smarter than humans, receives surprisingly little research attention outside of select organizations like OpenAI <a class="yt-timestamp" data-t="00:32:19">[00:32:19]</a>. This is a critical area that could pose an existential threat to humanity <a class="yt-timestamp" data-t="00:32:51">[00:32:51]</a>.

Key aspects to address include:
*   **Technical Alignment**: Ensuring that these models are aligned with human values and that humans can control them <a class="yt-timestamp" data-t="00:33:05">[00:33:05]</a>.
*   **Regulation and Governance**: Governments worldwide need to understand when superintelligence is approaching, including tracking factors like the amount of compute used to train models that could exceed [[ai_safety_and_alignment | AGI]] <a class="yt-timestamp" data-t="00:33:16">[00:33:16]</a>.

## OpenAI's Strategy for Safety and Development

OpenAI's strategy revolves around gradually releasing models when stakes are low to learn from emergent risks <a class="yt-timestamp" data-t="00:38:42">[00:38:42]</a>. This approach aims to build the necessary organizational processes and frameworks for safety <a class="yt-timestamp" data-t="00:38:31">[00:38:31]</a>.

*   **Gradual Deployment**: By releasing models with lower stakes, such as those related to misinformation or bias, OpenAI can learn how to tackle these issues before confronting more powerful systems <a class="yt-timestamp" data-t="00:38:46">[00:38:46]</a>.
*   **Pausing Releases**: OpenAI has demonstrated caution, for example, holding back the release of GPT-4 for nearly half a year to gain clarity on potential downsides <a class="yt-timestamp" data-t="00:39:40">[00:39:40]</a>. This sets an example for others in the field, fostering accountability <a class="yt-timestamp" data-t="00:40:02">[00:40:02]</a>.
*   **Balancing Upside and Risk**: While acknowledging the risks, OpenAI emphasizes the immense upside potential of superintelligence to solve global challenges like climate change, cancer, and aging, leading to greater abundance and a higher standard of living <a class="yt-timestamp" data-t="00:40:20">[00:40:20]</a>.

## Areas for Increased Research and Investment

Greater investment is needed in several areas related to superintelligence safety:

*   **Model Interpretability**: Understanding the internal workings of these "black box" models is crucial. Research into why specific activations occur within deep neural networks can provide insights into model behavior <a class="yt-timestamp" data-t="00:41:18">[00:41:18]</a>.
*   **Defining Alignment and Guardrails**: There is a need for clearer and more precise definitions of what "alignment" means, how to specify goals, and how to establish effective guardrails for AI systems <a class="yt-timestamp" data-t="00:42:16">[00:42:16]</a>. This requires collaboration between technical experts, social scientists, and philosophers <a class="yt-timestamp" data-t="00:42:45">[00:42:45]</a>.
*   **Technical Approaches**: Exploring various methods to ensure safe AI behavior, including:
    *   Shaping reward functions used in reinforcement learning during model training <a class="yt-timestamp" data-t="00:43:02">[00:43:02]</a>.
    *   Implementing oversight mechanisms, such as one model monitoring and reporting on the actions of another <a class="yt-timestamp" data-t="00:43:08">[00:43:08]</a>.

OpenAI encourages more resources and incentives for smart individuals to tackle these complex problems and develop solutions <a class="yt-timestamp" data-t="00:43:36">[00:43:36]</a>.