---
title: AI policy and regulation implications
videoId: _N2KPEdh69s
---

From: [[redpointai]] <br/> 

[[ai_policy_and_ethics | AI policy]] and regulation are critical considerations in the evolving landscape of artificial intelligence. Arthur Mensch, CEO and co-founder of Mistral, shared his perspective on how these areas should evolve and the challenges associated with current approaches <a class="yt-timestamp" data-t="00:17:04">[00:17:04]</a>.

## Approach to AI Safety and Regulation

Mensch believes that [[concerns_and_considerations_for_ai_safety_and_regulation | AI safety]] should be addressed through a product safety perspective, similar to how software safety has been managed <a class="yt-timestamp" data-t="00:17:19">[00:17:19]</a>. This approach focuses on evaluating the product's expected functionality and ensuring it performs as intended <a class="yt-timestamp" data-t="00:17:25">[00:17:25]</a>.

### The EU AI Act
Initially, the EU AI Act aligned with this product safety view <a class="yt-timestamp" data-t="00:17:32">[00:17:32]</a>. However, lobbying efforts led to the introduction of technology-specific regulations, including forced evaluation and "red-teaming" based on flop thresholds for large language models (LLMs) <a class="yt-timestamp" data-t="00:17:48">[00:17:48]</a>.

While manageable for companies like Mistral—who already red-team and evaluate their models <a class="yt-timestamp" data-t="00:17:58">[00:17:58]</a>—Mensch argues this approach is ill-directed <a class="yt-timestamp" data-t="00:18:32">[00:18:32]</a>. LLMs, like coding languages, can be used for various purposes, making it difficult to certify product safety solely based on model evaluation <a class="yt-timestamp" data-t="00:18:07">[00:18:07]</a>. The core problem of ensuring an AI product is safe remains unsolved because it requires rethinking continuous integration and verification for stochastic models <a class="yt-timestamp" data-t="00:19:17">[00:19:17]</a>.

### Transparency of Training Data
Discussions around [[the_evolving_landscape_of_ai_regulation_and_safety | transparency of training data sets]] are ongoing, with a caveat for protecting trade secrets due to the competitive landscape <a class="yt-timestamp" data-t="00:18:45">[00:18:45]</a>. Similar discussions are evolving in the US <a class="yt-timestamp" data-t="00:19:00">[00:19:00]</a>.

### Regulating the Application Layer
Mensch suggests that policymakers should pressure application makers to verify that their AI solutions effectively solve the intended task <a class="yt-timestamp" data-t="00:21:17">[00:21:17]</a>. This would create a "second-order pressure" on foundational model makers to provide tools and models that can be effectively controlled and verified by application developers <a class="yt-timestamp" data-t="00:21:34">[00:21:34]</a>.

This approach promotes healthy competition, as application makers would choose models that offer the best control <a class="yt-timestamp" data-t="00:22:21">[00:22:21]</a>. In contrast, directly regulating the technology can favor large players who have the resources to influence regulators and standard-setting bodies <a class="yt-timestamp" data-t="00:22:43">[00:22:43]</a>.

## Global and Geopolitical Implications
The emergence of foundation models for different countries, such as in India and Japan, is a notable trend <a class="yt-timestamp" data-t="00:23:01">[00:23:01]</a>. Mensch believes that enabling countries and developers to deploy AI technology where they want is crucial, with portability being Mistral's approach to national sovereignty <a class="yt-timestamp" data-t="00:23:23">[00:23:23]</a>.

Another critical aspect is language. Current models perform significantly better in English than in other languages <a class="yt-timestamp" data-t="00:23:41">[00:23:41]</a>. Mistral aims to create models that are proficient in every language, starting with French <a class="yt-timestamp" data-t="00:23:51">[00:23:51]</a>. This focus on multilingualism ensures that generative AI benefits the entire world and that the technology is ubiquitous <a class="yt-timestamp" data-t="00:24:08">[00:24:08]</a>.

While it's technologically optimal to have a few global LLM providers, the political question of countries wanting their own homegrown AI companies remains <a class="yt-timestamp" data-t="00:25:02">[00:25:02]</a>. Mensch suggests that if companies provide portable and multilingual technology, allowing countries to modify and control it, this should address sovereignty concerns <a class="yt-timestamp" data-t="00:25:13">[00:25:13]</a>. A situation with only a few companies offering Software-as-a-Service (SaaS) models would indeed pose a sovereignty problem, which many countries have already identified <a class="yt-timestamp" data-t="00:25:41">[00:25:41]</a>.