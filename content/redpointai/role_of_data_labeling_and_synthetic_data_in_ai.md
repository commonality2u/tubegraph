---
title: Role of data labeling and synthetic data in AI
videoId: sLsYzhk_4CY
---

From: [[redpointai]] <br/> 

## Current State of Data Labeling in AI
The type of data labeling that will matter for future model improvement involves expert data labelers who encode more reasoning tasks into models <a class="yt-timestamp" data-t="00:00:08">[00:00:08]</a>. The initial wave of post-ChatGPT moments focused on RHHF data, which has now shifted towards expert data labelers <a class="yt-timestamp" data-t="00:12:43">[00:12:43]</a>.

### Human Role in Evaluation
Humans remain the gold standard for evaluating AI models <a class="yt-timestamp" data-t="00:13:03">[00:13:03]</a>. It is not yet possible to remove humans from the evaluation loop <a class="yt-timestamp" data-t="00:13:15">[00:13:15]</a>. The only way to potentially remove humans from the loop would be to have an expert observing the model who is better than the current model <a class="yt-timestamp" data-t="00:13:23">[00:13:23]</a>.

### Challenges of Human Data Generation
While human data is still necessary for data generation, it is prohibitively expensive <a class="yt-timestamp" data-t="00:13:39">[00:13:39]</a>. For example, teaching a model general conversation or "chitchat" was viable by collecting data from 100,000 average people <a class="yt-timestamp" data-t="00:14:00">[00:14:00]</a>. However, it is not a viable strategy to teach a model medicine by finding 100,000 doctors <a class="yt-timestamp" data-t="00:14:51">[00:14:51]</a>.

## The Rise of Synthetic Data
The ability to chitchat and converse, initially taught through human data, has unlocked a degree of freedom in synthetic data generation <a class="yt-timestamp" data-t="00:14:16">[00:14:16]</a>. This allows AI developers to apply synthetic data to specific domains like medicine, using a much smaller pool of human data <a class="yt-timestamp" data-t="00:14:24">[00:14:24]</a>. For instance, one might go to 100 doctors to get some lessons, then use that trustworthy data to generate a thousandfold of synthetic lookalike data <a class="yt-timestamp" data-t="00:14:37">[00:14:37]</a>.

### Synthetic Data in Verifiable Domains
In verifiable domains like code and math, it is significantly easier to check the results of synthetic data, allowing for effective filtering of garbage and discovery of useful information <a class="yt-timestamp" data-t="00:14:49">[00:14:49]</a>. This process remains viable even in more complex domains <a class="yt-timestamp" data-t="00:15:02">[00:15:02]</a>. Currently, an overwhelming majority of data generated by Cohere for new models is synthetic <a class="yt-timestamp" data-t="00:15:15">[00:15:15]</a>.

## Data Beyond English and the Web
While the web contains extensive information about humanity, history, culture, and science, certain fundamental contexts about specific businesses or domains are missing from models built solely on web data <a class="yt-timestamp" data-t="00:10:57">[00:10:57]</a>. This includes data such as:
*   Manufacturing data <a class="yt-timestamp" data-t="00:11:30">[00:11:30]</a>
*   Customer transactions <a class="yt-timestamp" data-t="00:11:33">[00:11:33]</a>
*   Detailed personal health records <a class="yt-timestamp" data-t="00:11:35">[00:11:35]</a>

To address these gaps, companies like Cohere partner with organizations that possess this proprietary data to create custom models that are highly effective in those specific domains <a class="yt-timestamp" data-t="00:11:43">[00:11:43]</a>.

### Multilingual Data Collection
For AI technology to be useful globally, it must speak local languages and understand local cultures <a class="yt-timestamp" data-t="00:30:52">[00:30:52]</a>. Cohere's open-source "Ya project" was the largest data collection effort for any machine learning project, involving thousands of native speakers contributing data in over 100 different languages <a class="yt-timestamp" data-t="00:30:27">[00:30:27]</a>. This data was open-sourced to benefit all language models, not just Cohere's <a class="yt-timestamp" data-t="00:30:37">[00:30:37]</a>. Cohere is deeply committed to ensuring their technology works as well in Japanese and Korean as it does in English, through partnerships with companies like Fujitsu and LG <a class="yt-timestamp" data-t="00:29:42">[00:29:42]</a>.

### Siloed Data as a Challenge
For specialized domains like cancer research, the issue is not necessarily a token scarcity in data, but rather that existing data is siloed and locked up in numerous places that refuse to share or communicate with each other <a class="yt-timestamp" data-t="00:34:41">[00:34:41]</a>. For example, cancer data is often not well-structured or linked to outcomes <a class="yt-timestamp" data-t="00:34:02">[00:34:02]</a>. While bio foundation model companies often spin up labs to generate more data, and the [[role_of_ai_models_in_advancing_robotics_and_autonomous_driving | robotics]] sector faces similar data issues, the problem in areas like cancer research is more of a human problem of data access rather than a data generation problem <a class="yt-timestamp" data-t="00:34:09">[00:34:09]</a>.
