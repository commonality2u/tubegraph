---
title: The role of AI in enhancing patientprovider communication
videoId: VUBfFv-Un3c
---

From: [[redpointai]] <br/> 

AI, particularly Large Language Models (LLMs), is poised to significantly impact healthcare by enhancing communication between patients and providers <a class="yt-timestamp" data-t="01:36:19">[01:36:19]</a>. This advancement is crucial given healthcare's unique blend of highly formal and deeply human language <a class="yt-timestamp" data-t="02:14:02">[02:14:02]</a>.

## Core Capabilities of LLMs

LLMs excel at transforming informal language into formal language and vice versa <a class="yt-timestamp" data-t="01:40:24">[01:40:24]</a>. This capability is uniquely suited for healthcare, where highly structured codes (e.g., ICD-10, CPT) and regulations coexist with complex human conversations between patients and providers <a class="yt-timestamp" data-t="02:02:10">[02:02:10]</a>.

## Administrative Use Cases

Initially, many AI applications in healthcare will focus on administrative tasks <a class="yt-timestamp" data-t="03:11:04">[03:11:04]</a>. Oscar Health, a public health insurance company, has prioritized several administrative use cases to improve efficiency and transparency <a class="yt-timestamp" data-t="03:09:07">[03:09:07]</a>:

*   **Claims Explainers** AI can translate the complex, formal "trace" of how a claim was processed—detailing which rules were applied and why—into understandable, informal language for a layperson <a class="yt-timestamp" data-t="03:37:37">[03:37:37]</a>. This aims to make processes like claim denials much clearer to members <a class="yt-timestamp" data-t="03:47:04">[03:47:04]</a>.
*   **Call Summarization** LLMs are increasingly used to summarize customer service calls, phasing out manual note-taking by care guides <a class="yt-timestamp" data-t="07:19:57">[07:19:57]</a>.
*   **Medical Record Generation** AI can generate medical records from secure messaging conversations <a class="yt-timestamp" data-t="07:46:04">[07:46:04]</a>.
*   **Lab Test Summarization** LLMs summarize lab test results for medical group staff <a class="yt-timestamp" data-t="07:41:43">[07:41:43]</a>.

These [[administrative_and_clinical_use_cases_of_ai_in_health_insurance | administrative uses]] contribute to making the healthcare system more transparent, allowing patients to understand costs and alternatives in real-time <a class="yt-timestamp" data-t="04:08:24">[04:08:24]</a>.

## Clinical Use Cases and Challenges

While administrative applications are more straightforward, AI also aims to enhance communication in clinical settings.

### Summarizing Clinical Conversations
LLMs can summarize conversations between providers and patients (e.g., in virtual primary care settings) and generate medical record notes from them <a class="yt-timestamp" data-t="07:02:05">[07:02:05]</a>. This allows for adapting communication based on the audience, whether it's doctor-to-doctor or doctor-to-patient, by transforming the same data into different information levels <a class="yt-timestamp" data-t="06:05:05">[06:05:05]</a>.

### Limitations and Nuance
A key challenge for LLMs in clinical contexts is capturing subtle contextual knowledge that human providers possess but may not explicitly record <a class="yt-timestamp" data-t="07:17:21">[07:17:21]</a>. This includes remembering previous unrecorded conversations with a patient or understanding local geographical context (e.g., weather conditions impacting travel to a physician) <a class="yt-timestamp" data-t="09:06:08">[09:06:08]</a>. This creates an "unfair playing field" for LLMs when inputs and outputs are less structured <a class="yt-timestamp" data-t="07:38:15">[07:38:15]</a>.

For instance, an LLM might struggle with highly specific medical definitions compared to common layman's understanding, leading to false positives (e.g., incorrectly identifying "post-traumatic injury" in a specific clinical context) <a class="yt-timestamp" data-t="03:49:50">[03:49:50]</a>. To address this, strategies like "self-consistency questionnaires" or "chain of thought" prompting are used, where the LLM is guided to generate and evaluate multiple perspectives or break down complex tasks into independent steps <a class="yt-timestamp" data-t="03:52:13">[03:52:13]</a>.

## Regulatory and Trust Requirements

[[implementing_ai_in_healthcare_systems | Implementing AI in healthcare systems]] is subject to strict regulations, most notably HIPAA, which prohibits sharing patient-specific information without proper agreements <a class="yt-timestamp" data-t="02:08:59">[02:08:59]</a>. AI providers must sign Business Associate Agreements (BAAs) to handle protected health information <a class="yt-timestamp" data-t="02:05:32">[02:05:32]</a>. New models from providers like Google or OpenAI are not immediately covered by these agreements, requiring the use of synthetic or anonymized test data for initial evaluations <a class="yt-timestamp" data-t="02:51:30">[02:51:30]</a>.

Beyond compliance, gaining the trust of hospitals and healthcare systems is paramount <a class="yt-timestamp" data-t="02:44:03">[02:44:03]</a>. This often involves lengthy security and policy reviews, and building relationships through collaboration rather than just product development <a class="yt-timestamp" data-t="02:56:16">[02:56:16]</a>.

## The [[the_future_of_ai_in_human_communication | Future of AI in Human Communication]]

### AI Doctors and Clinical Automation
The long-term goal is to replace caregivers and clinical intelligence with machine intelligence, potentially reducing the cost of doctor visits significantly <a class="yt-timestamp" data-t="04:30:11">[04:30:11]</a>. Medicine is highly algorithmic, making it theoretically suitable for AI to map existing knowledge and infer based on data points <a class="yt-timestamp" data-t="00:57:15">[00:57:15]</a>.

### Challenges in Virtual Care
Despite the potential for AI in clinical settings, several [[challenges_in_virtual_healthcare_and_ai_doctors | challenges remain for virtual healthcare and AI doctors]] <a class="yt-timestamp" data-t="00:57:36">[00:57:36]</a>:
*   **Safety**: Ensuring AI outputs are safe and don't hallucinate or contain biases is critical for direct patient interaction <a class="yt-timestamp" data-t="04:50:09">[04:50:09]</a>. Currently, human oversight (human-in-the-loop) is necessary for sensitive use cases <a class="yt-timestamp" data-t="05:01:02">[05:01:02]</a>.
*   **Physical Interaction**: Many medical needs still require in-person physical interaction, such as lab tests or foot exams for diabetics <a class="yt-timestamp" data-t="05:57:52">[05:57:52]</a>. This "leakage" to in-person care can disrupt the continuity of virtual AI-driven care <a class="yt-timestamp" data-t="05:59:36">[05:59:36]</a>.
*   **Business Model**: Current healthcare system incentives do not always align with adopting lower-cost virtual care channels, as it can lead to reduced reimbursement and capacity <a class="yt-timestamp" data-t="05:59:50">[05:59:50]</a>.

### Overhyped vs. Underhyped
*   **Overhyped**: Clinical chatbots generally are considered overhyped at present <a class="yt-timestamp" data-t="01:00:49">[01:00:49]</a>.
*   **Underhyped**: Voice outputs are seen as underhyped, offering significant potential for communication, as long as they are not used for clinical advice <a class="yt-timestamp" data-t="01:00:56">[01:00:56]</a>.

Ultimately, general-purpose LLMs like GPT-4 are often preferred over specialized healthcare models because they maintain better alignment and instruction-following capabilities, even if specialized models might seem contextually more appropriate <a class="yt-timestamp" data-t="00:43:57">[00:43:57]</a>. Combining approaches like RAG (Retrieval Augmented Generation) and fine-tuning can independently improve performance <a class="yt-timestamp" data-t="00:45:11">[00:45:11]</a>.