---
title: Future of artificial general intelligence AGI
videoId: MvxtIIqJRUQ
---

From: [[redpointai]] <br/> 

Peter Welinder, VP of Product and Partnerships at OpenAI, shared insights into the company's vision for the [[path_to_artificial_general_intelligence_agi | future of AGI]] and superintelligence, emphasizing OpenAI's strategic focus and the broader implications for humanity <a class="yt-timestamp" data-t="00:00:32">[00:00:32]</a>.

## Defining AGI and Superintelligence

OpenAI defines [[path_to_artificial_general_intelligence_agi | AGI]] as autonomous systems capable of performing economically valuable work at the level of humans <a class="yt-timestamp" data-t="03:43:41">[03:43:41]</a>. Welinder states that the next step beyond [[path_to_artificial_general_intelligence_agi | AGI]] is "super intelligence," where models become significantly smarter than humans <a class="yt-timestamp" data-t="03:19:19">[03:19:19]</a>.

## Timeline Predictions

Welinder speculates that humanity has a "shot at doing that [reaching AGI] before 2030" <a class="yt-timestamp" data-t="03:52:00">[03:52:00]</a>. He notes that the current pace of innovation feels "very different now than how it felt 15-20 years ago" <a class="yt-timestamp" data-t="03:54:00">[03:54:00]</a>, suggesting that the field is on a "trajectory" towards [[path_to_artificial_general_intelligence_agi | AGI]] <a class="yt-timestamp" data-t="03:50:00">[03:50:00]</a>. He wouldn't be surprised to see "something that resembles AGI by the end of this decade" <a class="yt-timestamp" data-t="03:38:00">[03:38:00]</a>.

For superintelligence, Welinder imagines that "2030 is probably around the time where I imagine that we will start really seeing kind of the early signs of super intelligence" <a class="yt-timestamp" data-t="03:48:00">[03:48:00]</a>. However, he also acknowledges that it might be too expensive to run [[path_to_artificial_general_intelligence_agi | AGI]] economically, potentially delaying its widespread adoption <a class="yt-timestamp" data-t="03:30:00">[03:30:00]</a>.

## OpenAI's Strategy and Role in AGI Development

OpenAI's mission is to "build AGI, make sure it's safe, [and] make sure it benefits all humanity" <a class="yt-timestamp" data-t="02:35:00">[02:35:00]</a>. Their strategy is centered on enabling as many builders as possible to create products on top of their technology <a class="yt-timestamp" data-t="02:44:00">[02:44:00]</a>.

OpenAI has made a "conscious choice to make sure that we don't have a lot of value extraction at the model infrastructure layer" <a class="yt-timestamp" data-t="02:52:00">[02:52:00]</a>, aiming to keep prices low to encourage development <a class="yt-timestamp" data-t="02:59:00">[02:59:00]</a>. A key part of their research and engineering effort is to drive down model prices, broadening access and applicability <a class="yt-timestamp" data-t="03:22:00">[03:22:00]</a>.

The company prioritizes developing the core models, which are "incredibly flexible" <a class="yt-timestamp" data-t="10:03:00">[10:03:00]</a>. While they offer general applications like ChatGPT, they intend to remain a platform layer, empowering other companies to build specialized applications <a class="yt-timestamp" data-t="06:22:00">[06:22:00]</a>. This platform approach is exemplified by features like plugins in ChatGPT, which allow connection to external services <a class="yt-timestamp" data-t="07:28:00">[07:28:00]</a>.

### Focus on Models, Not Tools

OpenAI acknowledges that the "right tools for this technology just haven't been invented yet" <a class="yt-timestamp" data-t="03:56:00">[03:56:00]</a> and believes the developer ecosystem should determine and build the best tools <a class="yt-timestamp" data-t="04:07:00">[04:07:00]</a>. Their core focus remains on training and ensuring robust inference for models, where they believe they can provide the most value <a class="yt-timestamp" data-t="29:01:00">[29:01:00]</a>.

## Risks and Safety Concerns

Welinder differentiates between several risks associated with AI:

*   **Sufficiently Solvable Risks**: Misinformation, deepfakes, and bias are seen as "pretty surmountable" challenges <a class="yt-timestamp" data-t="30:53:00">[30:53:00]</a>. Misinformation, for example, often relies on existing distribution channels that already have infrastructure to protect against it <a class="yt-timestamp" data-t="31:07:00">[31:07:00]</a>. Regarding bias, OpenAI aims to provide tools for developers to "instruct the model to have the biases that you want" within bounds, allowing users to select the model's behavior <a class="yt-timestamp" data-t="31:38:00">[31:38:00]</a>.
*   **Hallucinations**: This is identified as the "biggest gap" preventing full enterprise adoption <a class="yt-timestamp" data-t="26:17:00">[26:17:00]</a>. OpenAI is actively working to make models more robust to hallucinations <a class="yt-timestamp" data-t="26:42:00">[26:42:00]</a>. A common workaround involves grounding models in external data via techniques like embeddings and vector search <a class="yt-timestamp" data-t="26:55:00">[26:55:00]</a>.
*   **Existential Risk from Superintelligence**: Welinder believes society is "paying too little attention" to the risk of superintelligence <a class="yt-timestamp" data-t="32:10:00">[32:10:00]</a>, which could potentially be "existential for humanity" <a class="yt-timestamp" data-t="32:54:00">[32:54:00]</a>. He highlights the surprising lack of dedicated research into how to ensure a beneficial outcome from superintelligence <a class="yt-timestamp" data-t="33:36:00">[33:36:00]</a>.

### Addressing Superintelligence Safety

Welinder suggests a need for more serious debates and investments in this area <a class="yt-timestamp" data-t="34:01:00">[34:01:00]</a>. He believes humanity will figure this out through "self-preservation" <a class="yt-timestamp" data-t="38:14:00">[38:14:00]</a>, requiring:
*   **Technical Aspects**: Research into interpretability of models (understanding what's happening inside them) <a class="yt-timestamp" data-t="41:19:00">[41:19:00]</a> and defining alignment, including specifying goals and guardrails more crisply, potentially through collaboration between technical people, social scientists, and philosophers <a class="yt-timestamp" data-t="42:16:00">[42:16:00]</a>. Technical approaches include shaping reward functions for reinforcement learning and having one model oversee another's actions <a class="yt-timestamp" data-t="43:02:00">[43:03:00]</a>.
*   **Organizational Processes and Regulation**: Building the right organizational structures for decision-making on deployment and safety safeguards <a class="yt-timestamp" data-t="39:04:00">[39:04:00]</a>. He stresses that this cannot be figured out *after* [[path_to_artificial_general_intelligence_agi | AGI]] is trained <a class="yt-timestamp" data-t="39:19:00">[39:19:00]</a>. Governments also need to understand when the world is nearing superintelligence and which companies are involved <a class="yt-timestamp" data-t="33:16:00">[33:16:00]</a>.

OpenAI's approach has been to release models when stakes are low to learn about risks like misinformation and bias <a class="yt-timestamp" data-t="38:42:00">[38:42:00]</a>. They demonstrated this caution by holding back GPT-4 for almost half a year to gain clarity on potential downsides <a class="yt-timestamp" data-t="39:40:00">[39:40:00]</a>. Welinder believes this example from a field leader adds accountability to others <a class="yt-timestamp" data-t="40:04:00">[40:04:00]</a>.

## Benefits of AGI

The upside of achieving superintelligence includes solving major global problems such as climate change, cancer, and aging, leading to "more abundance and higher standard of living for everyone" <a class="yt-timestamp" data-t="40:32:00">[40:32:00]</a>.

## Competition with Open Source Models

Welinder holds a "more unpopular opinion" <a class="yt-timestamp" data-t="15:21:00">[15:21:00]</a> that while open-source models will improve, proprietary models will likely remain "way way better" for the foreseeable future, similar to how desktop Linux hasn't caught up to Mac OS or Windows <a class="yt-timestamp" data-t="16:17:00">[16:17:00]</a>. The significant capital and engineering required for training and inference make it hard to replicate at an open-source level <a class="yt-timestamp" data-t="17:02:00">[17:02:00]</a>.

However, he is "very excited about the open source development" <a class="yt-timestamp" data-t="17:59:00">[17:59:00]</a>, recognizing its utility in pushing research forward and for specific applications like on-device deployment or on-premise solutions due to latency or control needs <a class="yt-timestamp" data-t="18:10:00">[18:10:00]</a>. OpenAI open-sources models like Whisper to enable more use cases that feed into larger language models <a class="yt-timestamp" data-t="24:39:00">[24:39:00]</a>.

For applications requiring the "smartest model" and reliability, proprietary models will be the rational choice <a class="yt-timestamp" data-t="18:56:00">[18:56:00]</a>, as "most of the value is ultimately going to be in the smartest models" <a class="yt-timestamp" data-t="21:54:00">[21:54:00]</a>, allowing companies to tackle the most economically valuable problems <a class="yt-timestamp" data-t="22:42:00">[22:42:00]</a>.

## Internal Use of ChatGPT at OpenAI

Internally, OpenAI employees use ChatGPT extensively for [[future_of_coding_and_ai_integration | coding]], including debugging and handling stack traces <a class="yt-timestamp" data-t="44:30:00">[44:30:00]</a>. Welinder personally uses it to help with writing, overcoming writer's block, improving prose, and generating first drafts of emails <a class="yt-timestamp" data-t="44:40:00">[44:40:00]</a>.

## Most Important Disagreement

Welinder believes his strongest and most important belief about AI's future, which most people would disagree with, is the urgent need to start seriously considering the implications of superintelligence now <a class="yt-timestamp" data-t="45:30:00">[45:30:00]</a>. He feels that people are only now grasping the current capabilities of models and underestimate the timeline for [[path_to_artificial_general_intelligence_agi | AGI]] and superintelligence <a class="yt-timestamp" data-t="45:50:00">[45:50:00]</a>.

## Potential Challenges for OpenAI's Leadership

The biggest risk to OpenAI's leading position is losing touch with its users and developers <a class="yt-timestamp" data-t="47:10:00">[47:10:00]</a>. Welinder acknowledges the "tension" where models improve and sometimes replace functionality built by developers <a class="yt-timestamp" data-t="47:40:00">[47:40:00]</a>. Scaling the "great customer experience" from knowing all customers by name to millions of users is a significant concern for maintaining developer embrace and fulfilling their mission <a class="yt-timestamp" data-t="48:35:00">[48:35:00]</a>.