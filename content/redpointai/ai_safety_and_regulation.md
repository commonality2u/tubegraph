---
title: AI safety and regulation
videoId: ZtY8VXswa2o
---

From: [[redpointai]] <br/> 

## Introduction to AI Safety and Regulation Concerns
Eric Reese and Jeremy Howard discuss the landscape of [[AI safety and alignment]] and [[AI policy and regulation]], particularly concerning the development and deployment of AI models <a class="yt-timestamp" data-t="00:00:19">[00:00:19]</a>. They highlight how current trends in AI development, particularly large investments in models and compute before market interaction, sometimes deviate from traditional Lean Startup principles <a class="yt-timestamp" data-t="00:00:57">[00:00:57]</a>.

## The Dual-Use Nature of AI Models
Jeremy Howard emphasizes that AI models are a "purely kind of dual use technology" <a class="yt-timestamp" data-t="00:40:32">[00:40:32]</a>, comparable to a pen, paper, calculator, or the internet <a class="yt-timestamp" data-t="00:40:36">[00:40:36]</a>. He explains that it's impossible to ensure the inherent safety of a raw AI model because it can be fine-tuned or prompted to perform any desired function, regardless of its initial safety testing <a class="yt-timestamp" data-t="00:40:55">[00:40:55]</a>.

## Critiques of Current Regulatory Approaches
Howard specifically discusses the proposed California state law, SB147, which aims to place limitations and [[AI regulations and geopolitical impact|regulatory checks]] on training foundation models <a class="yt-timestamp" data-t="00:38:25">[00:38:25]</a>. While acknowledging the good intentions and some positive features of such laws, he points out a significant flaw: attempts to ensure model safety by law are likely to be ineffective and could even lead to less safe situations <a class="yt-timestamp" data-t="00:40:01">[00:40:01]</a>.

> "Counterintuitively...not only is it likely such a policy would be uneffective but in fact it would be likely to cause the opposite uh result it would actually be likely to to create a less safe situation" <a class="yt-timestamp" data-t="00:39:58">[00:39:58]</a>.

### Consequences of "Ensuring Safety" through Regulation
According to Howard, regulating models to "ensure safety" in their raw form practically means preventing their release <a class="yt-timestamp" data-t="00:41:36">[00:41:36]</a>. Instead, only products built *on top* of models (like ChatGPT) would be released, where the product's safety can be controlled, but not the underlying model <a class="yt-timestamp" data-t="00:42:04">[00:42:04]</a>. This approach has several negative implications:
*   **Centralization of Power**: It makes models a "rival risk good" â€“ a "jealously guarded secret" only available to big states and large companies <a class="yt-timestamp" data-t="00:43:10">[00:43:10]</a>.
*   **Reduced Transparency**: It decreases the ability of independent researchers to study how models work, potentially hindering the development of defensive applications <a class="yt-timestamp" data-t="00:43:57">[00:43:57]</a>.
*   **Hindered Innovation**: It limits widespread access to powerful models that could be used for beneficial applications, such as improving cybersecurity or developing vaccines <a class="yt-timestamp" data-t="00:43:34">[00:43:34]</a>.

## The Role of Open Source and Smaller Models in Safety
Both Howard and Reese advocate for the importance of providing options for "intrinsically safe" applications <a class="yt-timestamp" data-t="00:46:34">[00:46:34]</a>. Eric Reese argues that there's an "unbelievable reservoir of applications that don't require AGI to unlock" <a class="yt-timestamp" data-t="00:45:14">[00:45:14]</a> that are not being built because fundraising gravity pushes entrepreneurs towards "science fiction and speculative stuff" <a class="yt-timestamp" data-t="00:45:28">[00:45:28]</a>.

He notes that if only frontier models (the closest thing to AGI) are widely available, their deployment into real-world systems risks "a lot of unsafe things to happen" <a class="yt-timestamp" data-t="00:46:10">[00:46:10]</a>. Conversely, smaller, properly fine-tuned models are "safer by definition" <a class="yt-timestamp" data-t="00:46:20">[00:46:20]</a>. If such options are not provided, people will default to using less safe, larger models <a class="yt-timestamp" data-t="00:46:38">[00:46:38]</a>.

From the perspective of [[AI policy and societal trust]], they stress that preventing open-source research, fine-tuning, or building applications from small models does not advance the cause of safety <a class="yt-timestamp" data-t="00:46:43">[00:46:43]</a>.

## Internal Organizational Challenges in Foundation Labs
Reese observes that large foundation model labs can become "schizophrenic" <a class="yt-timestamp" data-t="00:47:08">[00:47:08]</a>, with the original safety-focused AGI mission clashing with the commercial apparatus <a class="yt-timestamp" data-t="00:47:13">[00:47:13]</a>. He suggests that these commercial teams often have only a "faintest idea" of the safety agenda or may not care, leading to internal tensions that can undermine coherence and alignment <a class="yt-timestamp" data-t="00:47:18">[00:47:18]</a>.

Reese advises these labs to reestablish the connection between research and the customer <a class="yt-timestamp" data-t="00:47:43">[00:47:43]</a>, taking responsibility for customer success in technology deployment and actively seeking feedback from end-users <a class="yt-timestamp" data-t="00:47:52">[00:47:52]</a>.

## Future Breakthroughs and [[Approaches to AI safety and alignment]]
A significant breakthrough in AI could be a reduction in the "massive energy requirements" for models, which currently pose an economic and physical obstacle <a class="yt-timestamp" data-t="00:50:34">[00:50:34]</a>. Even more transformative would be a breakthrough in "planning and reasoning capability" beyond current "subgraph matching" <a class="yt-timestamp" data-t="00:51:16">[00:51:16]</a>. This would move beyond the current auto-regressive model where models pick the next word, allowing for more complex reasoning where important words might appear much later in a sequence <a class="yt-timestamp" data-t="00:51:50">[00:51:50]</a>.

Eric Reese highlights that the most interesting aspect of LLMs is how they've changed perceptions of human intelligence, revealing more of it is encoded in language than previously thought <a class="yt-timestamp" data-t="00:52:36">[00:52:36]</a>. He ponders a future breakthrough where it's realized that current methods of emulating cognition are extremely inefficient brute-force approaches, and a more direct, efficient algorithm in cognition is discovered <a class="yt-timestamp" data-t="00:53:15">[00:53:15]</a>.