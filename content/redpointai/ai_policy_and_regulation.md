---
title: AI policy and regulation
videoId: _N2KPEdh69s
---

From: [[redpointai]] <br/> 

Arthur Mensch, CEO and co-founder of Mistral, shared his perspective on [[AI policy and societal trust | AI policy and regulation]], particularly regarding the EU AI Act and the broader future of AI oversight <a class="yt-timestamp" data-t="00:17:04">[00:17:04]</a>.

## Approach to AI Safety and Regulation

Mistral's position is that [[ai_safety_and_regulation | AI safety and regulation]] should be addressed from a "product safety perspective" <a class="yt-timestamp" data-t="00:17:17">[00:17:17]</a>. This approach mirrors how software safety has traditionally been managed, focusing on the end product, its expected performance, and methods for evaluation <a class="yt-timestamp" data-t="00:17:21">[00:17:21]</a>.

Mensch believes that regulators should apply pressure on application makers to ensure their products function correctly and safely <a class="yt-timestamp" data-t="00:21:12">[00:21:12]</a>. This would create a "second order pressure" on foundational model developers, as application makers would demand models that are easier to control and verify for specific tasks <a class="yt-timestamp" data-t="00:21:34">[00:21:34]</a>. He suggests providing application makers with evaluation tools and methods for continuous integration and verification <a class="yt-timestamp" data-t="00:22:01">[00:22:01]</a>.

### Concerns with Current Regulatory Trends

Mensch highlighted that the current trend in regulation, influenced by lobbying, introduces "technology regulation" based on factors like flop thresholds, which he views as an "ill-directed burden" <a class="yt-timestamp" data-t="00:17:48">[00:17:48]</a>. While manageable for Mistral as they already perform evaluations and documentation, he argues it doesn't solve the core product safety problem <a class="yt-timestamp" data-t="00:18:02">[00:18:02]</a>.

The challenge lies in making an AI product safe, which is a difficult problem given the stochastic nature of models <a class="yt-timestamp" data-t="00:19:22">[00:19:22]</a>. He emphasizes that rethinking evaluation and continuous verification is a technological and product problem, not primarily a regulatory one <a class="yt-timestamp" data-t="00:19:47">[00:19:47]</a>.

Furthermore, he expresses concern that direct regulation on technology, rather than applications, favors larger players who can deploy "an army of lawyers" to influence regulators and standard-setting bodies, hindering healthy competition <a class="yt-timestamp" data-t="00:22:40">[00:22:40]</a>.

### Transparency of Training Data

There are ongoing discussions around the transparency of training data sets, which Mistral would like to enable, with the caveat of needing to protect "trading secrets" due to the competitive landscape <a class="yt-timestamp" data-t="00:18:45">[00:18:45]</a>. This issue also applies to regulations evolving in the US <a class="yt-timestamp" data-t="00:18:59">[00:18:59]</a>.

## Global and Geopolitical Implications

The emergence of foundation models for specific countries (e.g., India, Japan) suggests a trend toward localized AI capabilities <a class="yt-timestamp" data-t="00:23:03">[00:23:03]</a>. Mensch believes that enabling countries and developers to deploy AI technology where they want — through "portability" — is the best approach to sovereignty <a class="yt-timestamp" data-t="00:23:25">[00:23:25]</a>.

The importance of language-specific models is also highlighted, as current models perform significantly better in English <a class="yt-timestamp" data-t="00:23:43">[00:23:43]</a>. Mistral aims to create models that excel in every language, starting with French, which is largely handled at the pre-training stage <a class="yt-timestamp" data-t="00:23:51">[00:23:51]</a>. Mistral's strategy is to be a global, portable, and multilingual company, ensuring their technology is ubiquitous <a class="yt-timestamp" data-t="00:24:14">[00:24:14]</a>.

Mensch considers the proliferation of country-specific LLM companies more of a political than a technological question <a class="yt-timestamp" data-t="00:25:00">[00:25:00]</a>. If countries can access and modify technology as desired, it should foster confidence and control <a class="yt-timestamp" data-t="00:25:13">[00:25:13]</a>. However, if only a few companies offer AI as a Software-as-a-Service (SaaS), a "sovereignty problem" arises, which many countries have already identified <a class="yt-timestamp" data-t="00:25:41">[00:25:41]</a>.