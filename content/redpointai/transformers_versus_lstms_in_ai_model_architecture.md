---
title: Transformers versus LSTMs in AI model architecture
videoId: 7-3IxVvWoxc
---

From: [[redpointai]] <br/> 

Jonathan Frankle, Chief AI Scientist at Databricks, has a notable bet regarding the future dominance of the Transformer architecture in AI models <a class="yt-timestamp" data-t="00:02:43">[00:02:43]</a>. He maintains a long-term perspective on this subject <a class="yt-timestamp" data-t="00:02:59">[00:02:59]</a>.

## The Dominance of Transformers
Frankle notes that in the days following the release of the Transformer paper and Bert, countless papers emerged proposing minor tweaks to Bert <a class="yt-timestamp" data-t="00:03:08">[00:03:08]</a>. Ultimately, the models trained today are largely based on the original "Vaswani et al. Transformer" architecture, often with different positional encodings and using only the decoder <a class="yt-timestamp" data-t="00:03:17">[00:03:17]</a>. This suggests that the original Transformer found a "sweet spot" in the hyperparameter space, making significant architectural changes less appealing <a class="yt-timestamp" data-t="00:03:25">[00:03:25]</a>.

## Comparison with LSTMs
Before the Transformer, the state-of-the-art architecture for natural language processing (NLP) was [[evolution_of_ai_models_and_infrastructure | recurrent neural networks]], specifically LSTMs (Long Short-Term Memory networks) <a class="yt-timestamp" data-t="00:03:37">[00:03:37]</a>. Frankle points out that he is actually one year older than LSTMs <a class="yt-timestamp" data-t="00:03:50">[00:03:50]</a>.

Key comparisons include:
*   **Difficulty of Discovery**: Good architectures are exceptionally difficult to find, taking roughly a generation to progress from older state-of-the-art architectures to current ones <a class="yt-timestamp" data-t="00:03:53">[00:03:53]</a>.
*   **Hypothetical Scaling**: There's an "alternate journey" that wasn't taken, where LSTMs could have been scaled extensively <a class="yt-timestamp" data-t="00:04:03">[00:04:03]</a>. It's unclear if Transformers are fundamentally superior or if their success is due to where collective energy was focused <a class="yt-timestamp" data-t="00:04:11">[00:04:11]</a>.
*   **Simplicity**: Transformers are generally simpler than LSTMs <a class="yt-timestamp" data-t="00:04:21">[00:04:21]</a>.

## Architectural Evolution and Future Outlook
Frankle cautions against the common newcomer belief that a "next big thing" in AI architecture is always just around the corner <a class="yt-timestamp" data-t="00:04:26">[00:04:26]</a>. He emphasizes that science tends to advance in "big leaps" followed by periods of consolidation <a class="yt-timestamp" data-t="00:04:42">[00:04:42]</a>. From this perspective, the notion that something will suddenly "blow the Transformer out of the water" seems "ahistorical" <a class="yt-timestamp" data-t="00:04:52">[00:04:52]</a>.

Despite the emergence of new [[limitations_of_current_ai_models_and_future_architecture | AI models and infrastructure]], such as OpenAI's o1 model, Frankle suggests that breakthroughs are often only recognized with hindsight <a class="yt-timestamp" data-t="00:42:15">[00:42:15]</a>. He highlights that while new ideas are constantly circulating, the true achievement lies in scaling them effectively, which is often an engineering feat as much as a scientific one <a class="yt-timestamp" data-t="00:43:15">[00:43:15]</a>.

Frankle remains "very good" about his bet on the Transformer's continued dominance <a class="yt-timestamp" data-t="00:04:59">[00:04:59]</a>.