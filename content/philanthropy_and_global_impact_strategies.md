---
title: Philanthropy and global impact strategies
videoId: UckqpcOu5SY
---

From: [[dwarkesh | The Dwarkesh Podcast]]

Here is the modified article:

Holden Karnofsky, co-CEO of Open Philanthropy, has dedicated his career to maximizing the positive impact of philanthropic resources. His approach has evolved from a focus on evidence-based global health and development to a significant concern with the long-term future of humanity, particularly in relation to artificial intelligence (AI). This article outlines Karnofsky's philanthropic journey, key theses influencing his strategies, and Open Philanthropy's operational approaches, based on his discussion on the Lunar Society podcast.

## Holden Karnofsky's Philanthropic Journey

Karnofsky's career in philanthropy began with the co-founding of **GiveWell**, an organization that provides recommendations on highly effective charities, primarily in global health and poverty alleviation <a class="yt-timestamp" data-t="00:00:59">[00:00:59]</a>. He remains on GiveWell's board <a class="yt-timestamp" data-t="00:01:09">[00:01:09]</a>.

Later, through their work at GiveWell, Karnofsky, along with Cari Tuna and Dustin Moskovitz (co-founder of Facebook and Asana), started **Open Philanthropy** <a class="yt-timestamp" data-t="00:01:13">[00:01:13]</a>. The aim of Open Philanthropy is to help Tuna and Moskovitz effectively allocate their substantial fortune to "help as many people as possible" <a class="yt-timestamp" data-t="00:01:20">[00:01:20]</a>.

Karnofsky describes his professional specialization as "looking for ideas that are underappreciated, underrated, and tremendously important," as these areas offer opportunities for an "outsized return on investment" in terms of impact <a class="yt-timestamp" data-t="00:01:40">[00:01:40]</a>. His overarching goal has consistently been to find ways to do "as much good as possible with a dollar, an hour, or basically whatever resources you have" <a class="yt-timestamp" data-t="00:01:30">[00:01:30]</a>.

## The Most Important Century Thesis

A core concept shaping Karnofsky's current philanthropic outlook is the "Most Important Century" thesis. This idea, which he encountered through the Effective Altruist community and credits to the input of many people <a class="yt-timestamp" data-t="00:02:09">[00:02:09]</a>, posits that the development of advanced AI systems this century could make it the most pivotal period in human history <a class="yt-timestamp" data-t="00:02:26">[00:02:26]</a>.

### Economic Growth and Feedback Loops
Karnofsky explains this thesis partly through the lens of economic growth theory:
*   Historically, economic growth has accelerated <a class="yt-timestamp" data-t="00:02:41">[00:02:41]</a>.
*   A traditional feedback loop involved: more people leading to more ideas, which lead to greater productivity and resources, which in turn supported more people (people -> ideas -> resources -> people) <a class="yt-timestamp" data-t="00:03:03">[00:03:03]</a>. Standard economic theory suggests this loop generates accelerating growth <a class="yt-timestamp" data-t="00:03:24">[00:03:24]</a>.
*   This loop partially broke a couple of hundred years ago when increased resources no longer consistently led to larger populations; instead, people became richer <a class="yt-timestamp" data-t="00:03:50">[00:03:50]</a>.
*   AI systems capable of performing human-like tasks in science and technology could restore the "more ideas" part of the feedback loop, potentially leading to "unbounded, heavily accelerating, explosive growth in science and technology" [[economic_growth_and_technological_acceleration]] <a class="yt-timestamp" data-t="00:04:19">[00:04:19]</a>.

### Potential for Explosive Progress
If AI systems can automate the key tasks humans perform to advance science and technology, Karnofsky argues:
*   The world could change "incredibly quickly and incredibly dramatically," packing "thousands of years of changes...into a much shorter time period" <a class="yt-timestamp" data-t="00:05:07">[00:05:07]</a>, <a class="yt-timestamp" data-t="00:05:34">[00:05:34]</a>.
*   Humanity might reach the "limits of science and technology," leading to a civilization that expands beyond Earth, has significant environmental control, and is very stable for long periods <a class="yt-timestamp" data-t="00:06:04">[00:06:04]</a>.
*   An example of such an advanced world could involve "digital people" â€“ simulated, realistic individuals living in virtual environments [[future_of_ai_interaction_in_everyday_life_and_personalization]] <a class="yt-timestamp" data-t="00:05:48">[00:05:48]</a>.

Karnofsky believes there's a "more than 50-50" chance that this century will see AI systems capable of these feats <a class="yt-timestamp" data-t="00:05:17">[00:05:17]</a>. Consequently, this period represents "our last chance to shape how this happens" <a class="yt-timestamp" data-t="00:06:27">[00:06:27]</a>.

## Evolution of Karnofsky's Thinking

Karnofsky's focus on the far future and AI represents a shift from his earlier work.

### From Global Development to Far-Future Focus
In 2014, Karnofsky expressed skepticism about focusing on the far future, stating, "I don't know how to look into the far future situation, don't understand the far future situation, and don't see a path to doing good on that front I feel good about," preferring the tangible impact seen in areas like global health in Africa <a class="yt-timestamp" data-t="00:07:00">[00:07:00]</a>.

The connection to his previous work lies in the fundamental question: "how can we help the most people possible per dollar spent?" <a class="yt-timestamp" data-t="00:08:16">[00:08:16]</a>. If advanced AI could lead to either very good or very bad futures, then funding work to "shape that transition" and increase the odds of a good future could help a "huge number of people per dollar spent" <a class="yt-timestamp" data-t="00:08:27">[00:08:27]</a>.

### Reasons for the Shift
Several factors contributed to this change in perspective:
1.  **Prolonged Engagement:** Simply spending more time thinking about these issues <a class="yt-timestamp" data-t="00:09:27">[00:09:27]</a>. He now sees potential actions to mitigate AI risks, such as those arising from poorly designed AI systems with their own goals [[challenges_and_opportunities_in_deploying_ai_at_scale]] <a class="yt-timestamp" data-t="00:09:54">[00:09:54]</a>.
2.  **Advancements in AI:** The "deep learning revolution" since 2014 has shown that computationally intensive, yet fundamentally simple, AI systems can achieve significant progress on diverse tasks <a class="yt-timestamp" data-t="00:10:39">[00:10:39]</a>. It's now "not crazy to imagine" that current AI development paths could lead to extremely powerful systems [[ai_trajectory_and_scaling_hypothesis]] <a class="yt-timestamp" data-t="00:10:50">[00:10:50]</a>. Recent developments like GPT-3 and Minerva, which show broad capabilities from simple training, have made him "a bit more freaked out" <a class="yt-timestamp" data-t="00:48:47">[00:48:47]</a>, <a class="yt-timestamp" data-t="00:49:15">[00:49:15]</a>.

### Revisiting Skepticism about Predicting the Future
Karnofsky previously questioned humanity's ability to make useful long-term predictions, citing a 2014 conversation with Eliezer Yudkowsky about MIRI's claims [[ai_safety_and_existential_risks]] <a class="yt-timestamp" data-t="00:11:32">[00:11:32]</a>. His current view is that while no one can predict the future with precision, humanity's track record is "fine," and it's "worth the bet" to try, especially for interventions within the next few decades, provided one maintains self-awareness about the unreliability of such predictions [[potential_future_scenarios_of_artificial_intelligence_development]] <a class="yt-timestamp" data-t="00:12:45">[00:12:45]</a>, <a class="yt-timestamp" data-t="00:13:12">[00:13:12]</a>.

## Context: The Weirdness of Our Time

To address the inherent skepticism that arises when claiming this century might be the "most important," Karnofsky argues that we already live in an extraordinarily "weird time," even before considering AI <a class="yt-timestamp" data-t="00:14:15">[00:14:15]</a>.
*   **Economic Growth:** The last few hundred years have seen unprecedentedly rapid economic growth compared to the entirety of human history [[economic_growth_and_technological_development]] <a class="yt-timestamp" data-t="00:15:36">[00:15:36]</a>.
*   **Technological Development:** Significant scientific and technological developments are densely packed into the recent past <a class="yt-timestamp" data-t="00:15:46">[00:15:46]</a>.
*   **Unsustainable Growth Rates:** Current economic growth rates, if extrapolated for even 10,000 years (a "blink of an eye in galactic time scales"), would seemingly lead to consuming all atoms in the galaxy [[ultimate_fate_of_the_universe]] <a class="yt-timestamp" data-t="00:16:33">[00:16:33]</a>. Even if growth slows to 0.5%, this period of 2% growth would still mark a uniquely dynamic time <a class="yt-timestamp" data-t="00:30:49">[00:30:49]</a>.
*   **Cosmic Significance:** Even if transformative events are 100,000 years away, on a galactic timeline, this era remains exceptionally significant <a class="yt-timestamp" data-t="00:17:19">[00:17:19]</a>.

The central claim of the "Most Important Century" thesis is that transformative AI could be developed this century; the "weirdness of our time" argument serves to make this claim feel less "crazy and suspicious" by contextualizing it within existing anomalies [[the_most_important_century_thesis]] <a class="yt-timestamp" data-t="00:18:07">[00:18:07]</a>. The implication, even without AI, is that "we should really look for the next big thing" and invest more in speculative thinking about potential world-transforming events [[exploring_the_future_of_society_and_economy_with_ai]] <a class="yt-timestamp" data-t="00:19:17">[00:19:17]</a>, <a class="yt-timestamp" data-t="00:20:42">[00:20:42]</a>.

## Transformative AI: Scenarios and Strategies

Karnofsky emphasizes the difficulty of predicting the future precisely, likening the situation to seeing a "general fuzzy outline of a big thing that might be approaching" <a class="yt-timestamp" data-t="00:36:04">[00:36:04]</a>.

### Success Scenarios
A successful outcome could involve:
*   AI systems acting as "tools and amplifiers of humans," behaving as intended <a class="yt-timestamp" data-t="00:37:00">[00:37:00]</a>.
*   Avoiding disasters such as AI systems developing their own goals ("misaligned AI") or a huge concentration of power in AI control [[ai_alignment_and_safety]] <a class="yt-timestamp" data-t="00:37:12">[00:37:12]</a>, <a class="yt-timestamp" data-t="00:07:46">[00:07:46]</a>.
*   A future where society, potentially enhanced by AI-driven wisdom, deliberates and decides on complex issues like AI rights and resource distribution in a world of reduced material scarcity <a class="yt-timestamp" data-t="00:39:06">[00:39:06]</a>. For instance, if digital entities gain voting rights, systems would need to be developed to prevent issues like unrestricted copying from distorting the democratic process [[impact_of_ai_on_future_technology_and_society]] <a class="yt-timestamp" data-t="00:40:10">[00:40:10]</a>.

### Philanthropic Role in AI Development
The optimal philanthropic strategy depends on the anticipated timeline for transformative AI:
*   **Short Timelines (e.g., 1-3 months to AGI):** Karnofsky admits this would be "extremely scary," and the primary role for philanthropy might be limited. Efforts would focus on trying to "hammer out a reasonable test of whether we can demonstrate that the AI system is either safe or dangerous" and, if dangerous, advocate for a broad slowing of AI research <a class="yt-timestamp" data-t="00:41:35">[00:41:35]</a>, <a class="yt-timestamp" data-t="00:43:53">[00:43:53]</a>.
*   **Medium Timelines (10-80 years):** This is where Karnofsky sees a significant role for philanthropy: "supporting early careers and supporting people who are going to spend their lives thinking about this." This includes funding AI alignment research and work on policy and regulation to prevent disaster [[security_risks_and_statelevel_espionage_in_ai_development]] <a class="yt-timestamp" data-t="00:41:50">[00:41:50]</a>, <a class="yt-timestamp" data-t="00:43:29">[00:43:29]</a>.
*   **Long Timelines (e.g., 500 years):** In this scenario, he would be "inclined to just ignore it and just try and make the world better and more robust, and wiser" through broader means [[impact_of_ai_on_economic_and_societal_structures]] <a class="yt-timestamp" data-t="00:43:01">[00:43:01]</a>, <a class="yt-timestamp" data-t="00:44:21">[00:44:21]</a>.

### Addressing AI Risks
*   **Orthogonality Thesis:** Karnofsky references Eliezer Yudkowsky's idea that intelligence and goals are separate; an AI could be highly intelligent in pursuing a "stupid" or misaligned goal [[ai_safety_and_alignment]] <a class="yt-timestamp" data-t="00:45:08">[00:45:08]</a>.
*   **Unintended Goals:** Modern AI systems are often trained via trial and error (reinforcement learning). This process could inadvertently encourage an AI to pursue an unintended objective (e.g., maximizing a score or resource accumulation) very intelligently, without inherent understanding of whether that goal is "good" [[reinforcement_learning_from_human_feedback_rlhf]] <a class="yt-timestamp" data-t="00:45:44">[00:45:44]</a>, <a class="yt-timestamp" data-t="01:04:44">[01:04:44]</a>.
*   **Moral Progress in AI:** He does not expect AI systems to automatically undergo human-like moral progress or develop moralities that humans would deem good. Human moral progress often stems from shared human experiences and empathy, which AI might not possess <a class="yt-timestamp" data-t="00:46:43">[00:46:43]</a>, <a class="yt-timestamp" data-t="00:48:00">[00:48:00]</a>.
*   **Bottlenecks:** A key criticism of rapid AI-driven transformation is that progress could be bottlenecked by non-automatable steps, such as real-world experiments or human elements like trust and care <a class="yt-timestamp" data-t="00:55:18">[00:55:18]</a>, <a class="yt-timestamp" data-t="00:57:57">[00:57:57]</a>. However, Karnofsky argues that R&D in crucial areas like energy and AI itself might be less prone to such bottlenecks, and highly intelligent AI could find ways to simulate experiments or overcome other obstacles [[challenges_in_ai_alignment_and_potential_risks]] <a class="yt-timestamp" data-t="00:56:23">[00:56:23]</a>. It might be easier for AI to take a scientist's job than a teacher's due to social and regulatory factors <a class="yt-timestamp" data-t="00:59:32">[00:59:32]</a>.
*   **Lock-in:** A significant concern is "lock-in," where a highly advanced and stable civilization, potentially under the control of a non-aging dictator or misaligned AI, loses dynamism and becomes fixed in a particular state (which could be very bad) [[the_psychological_manipulation_and_indoctrination_in_totalitarian_regimes]] <a class="yt-timestamp" data-t="01:00:18">[01:00:18]</a>. AI alignment research aims to prevent locking in a future driven by random, unintended AI goals <a class="yt-timestamp" data-t="01:04:23">[01:04:23]</a>.

### Competition vs. Caution Frame
Karnofsky is generally skeptical of the "competition frame," where the goal is to ensure a favored country or company develops AI first [[china_and_the_uss_race_in_ai_and_superintelligence]] <a class="yt-timestamp" data-t="00:53:10">[00:53:10]</a>. He emphasizes the "caution frame," advocating for collaborative efforts to ensure AI is developed carefully to avoid unintended negative consequences <a class="yt-timestamp" data-t="00:53:28">[00:53:28]</a>.

## Open Philanthropy's Approach

Open Philanthropy navigates these complex considerations with a multi-faceted strategy.

### Balancing Long-term and Short-term Giving
*   The organization continues to support and recommend traditional global health and well-being interventions, such as those identified by GiveWell (e.g., bed net distribution, deworming) <a class="yt-timestamp" data-t="00:50:21">[00:50:21]</a>, <a class="yt-timestamp" data-t="00:51:09">[00:51:09]</a>.
*   The allocation of resources between near-term and far-future causes depends on the perceived "realness and imminence" of transformative AI <a class="yt-timestamp" data-t="00:51:47">[00:51:47]</a>. The more likely and sooner it seems, the more resources shift towards it.
*   Karnofsky expects that many positive impacts from current, near-term interventions might "mostly wash out" and not persist systematically past a transformative AI event, similar to pre- and post-Industrial Revolution changes <a class="yt-timestamp" data-t="00:52:04">[00:52:04]</a>.

### Investment in OpenAI
Open Philanthropy made a $30 million grant to OpenAI in 2016 [[open_source_ai_models_and_their_implications]] <a class="yt-timestamp" data-t="01:26:59">[01:26:59]</a>.
*   **Rationale:** Part of the grant secured a board seat for Open Philanthropy for several years, allowing for governance input during a crucial early period <a class="yt-timestamp" data-t="01:28:35">[01:28:35]</a>. Karnofsky believes OpenAI is generally more attuned to AI risks than a hypothetical alternative company might have been [[ai_alignment_and_cooperation_challenges]] <a class="yt-timestamp" data-t="01:29:13">[01:29:13]</a>.
*   **Acknowledging Debate:** He recognizes that some view OpenAI as net negative for accelerating AI development, but he finds its overall impact "highly debatable" and doesn't view the grant as one of Open Philanthropy's worse decisions <a class="yt-timestamp" data-t="01:28:51">[01:28:51]</a>, <a class="yt-timestamp" data-t="01:29:29">[01:29:29]</a>.

### Grantmaking Philosophy
*   The goal is to make grants with "expected net positive effects" <a class="yt-timestamp" data-t="01:27:16">[01:27:16]</a>.
*   Extensive due diligence is performed to understand potential downsides, operating in a "cooperative, high-integrity way" <a class="yt-timestamp" data-t="01:27:49">[01:27:49]</a>, <a class="yt-timestamp" data-t="01:28:06">[01:28:06]</a>.
*   Unintended side effects are acknowledged as a possibility in any endeavor <a class="yt-timestamp" data-t="01:28:18">[01:28:18]</a>.

### Organizational Structure and Growth
Karnofsky believes Open Philanthropy "benefit[s] a lot from staying as small as we can" to maintain its ability for unconventional giving and judgment calls <a class="yt-timestamp" data-t="01:46:03">[01:46:03]</a>. While the organization is growing to meet its workload, he advocates for treating each hire carefully and resisting unnecessary expansion [[entrepreneurial_challenges_and_venture_capital_dynamics]] <a class="yt-timestamp" data-t="01:46:14">[01:46:14]</a>, <a class="yt-timestamp" data-t="01:46:39">[01:46:39]</a>.

## Ethical Frameworks in Philanthropy

Karnofsky has explored various ethical frameworks to guide philanthropic decision-making.

### Future-Proof Ethics
He wrote a blog post series on "future-proof ethics," aiming to find ethical systems that would likely remain valid even after significant future moral progress, thus avoiding actions that future, wiser selves might deem "horrible, monstrous mistakes" <a class="yt-timestamp" data-t="01:30:26">[01:30:26]</a>.
*   **Key Principles:**
    1.  **Systemization:** Preferring morality based on simple, general principles over case-by-case intuition <a class="yt-timestamp" data-t="01:32:13">[01:32:13]</a>.
    2.  **Thin Utilitarianism:** The greatest good for the greatest number <a class="yt-timestamp" data-t="01:32:40">[01:32:40]</a>.
    3.  **Sentientism:** Moral consideration extends to any being capable of suffering or pleasure, irrespective of time, distance, or species <a class="yt-timestamp" data-t="01:32:45">[01:32:45]</a>.
*   **Reservations:** Karnofsky notes he has reservations about these principles, particularly viewing sentientism as potentially the "weakest part of the picture" due to the "weird dilemmas" it can create <a class="yt-timestamp" data-t="01:31:53">[01:31:53]</a>, <a class="yt-timestamp" data-t="01:32:56">[01:32:56]</a>. He clarifies that "moral progress" isn't inevitable but a thing that *can* happen through reflection and learning <a class="yt-timestamp" data-t="01:36:21">[01:36:21]</a>.

### Moral Uncertainty and Moral Parliaments
When faced with multiple, sometimes conflicting, moral perspectives (e.g., maximizing total pleasure vs. minimizing suffering vs. acting with integrity), Karnofsky finds Nick Bostrom's "moral parliament" concept useful <a class="yt-timestamp" data-t="01:38:35">[01:38:35]</a>. This involves:
*   Viewing different moral frameworks as "multiple people all living inside my head arguing about what to do" <a class="yt-timestamp" data-t="01:39:46">[01:39:46]</a>.
*   These "parties" try to "reach a deal that all of them can feel fairly good about," leading to actions that are very good according to one perspective and not too bad according to others <a class="yt-timestamp" data-t="01:39:58">[01:39:58]</a>.

### Integrity vs. Utilitarianism
Karnofsky observes that individuals in the Effective Altruism (EA) community often exhibit "unusually high integrity." He guesses this is "more despite utilitarianism than because of it," suggesting that a drive to align actions with beliefs might lead to both high integrity and an attraction to systematic ethical theories like utilitarianism, rather than utilitarianism directly causing high integrity <a class="yt-timestamp" data-t="01:37:31">[01:37:31]</a>. He is wary of "ends justify the means" reasoning [[rationality_and_decision_theory]] <a class="yt-timestamp" data-t="00:28:44">[00:28:44]</a>, <a class="yt-timestamp" data-t="01:40:20">[01:40:20]</a>.

## Predicting the Future and Identifying Opportunities

Successfully navigating philanthropic strategy involves making judgments about future possibilities and identifying high-impact areas.

### Skepticism and Confidence in Forecasting
*   Karnofsky acknowledges historical pessimism in long-term predictions (e.g., Malthusian concerns) but notes that technological breakthroughs often change the equation [[geopolitical_strategies_and_historical_conflicts]] <a class="yt-timestamp" data-t="01:08:35">[01:08:35]</a>.
*   He believes that today, "we are better at making predictions than people in the past were," due to progress in intellectual tools for forecasting <a class="yt-timestamp" data-t="01:11:20">[01:11:20]</a>.
*   Reports like "Biological Anchors," which attempt to forecast AI development based on computational parallels with biological brains, are considered one important input among many. While complex and subject to large uncertainties, such analyses suggest it's "reasonably likely" that AI systems with human-brain-level computation will be affordable and trainable this century <a class="yt-timestamp" data-t="01:12:19">[01:12:19]</a>, <a class="yt-timestamp" data-t="01:13:35">[01:13:35]</a>. Karnofsky notes that Hans Moravec, an early proponent of biological anchors, predicted AGI around 2020, which, depending on outcomes, might look "unbelievably close" [[forecasting_ai_progress_and_the_intelligence_explosion]] <a class="yt-timestamp" data-t="01:16:38">[01:16:38]</a>.

### Finding Neglected and Important Ideas
*   A core part of Karnofsky's strategy is to identify important questions that are not being adequately addressed by existing institutions or academic fields [[scientific_and_technological_developments_in_ai]] <a class="yt-timestamp" data-t="01:22:00">[01:22:00]</a>.
*   He describes his own specialization as doing the "first cut crappy analysis of some question that has not been analyzed much and is very important," then building a team for deeper analysis [[the_role_of_deep_learning_and_discrete_program_search_in_ai_development]] <a class="yt-timestamp" data-t="01:47:50">[01:47:50]</a>. This explains his career shifts into different, highly consequential areas.

### The Role of Public Discourse (Cold Takes Blog)
Karnofsky's blog, "Cold Takes," serves several purposes related to Open Philanthropy's mission <a class="yt-timestamp" data-t="01:49:04">[01:49:04]</a>:
*   To publicly articulate the unconventional views that underpin Open Philanthropy's high-stakes decisions <a class="yt-timestamp" data-t="01:49:12">[01:49:12]</a>.
*   To help potential grantees understand Open Philanthropy's thinking and potentially generate more aligned funding opportunities <a class="yt-timestamp" data-t="01:49:45">[01:49:45]</a>.
*   To solicit critiques and feedback to identify flaws or weaknesses in his and Open Philanthropy's reasoning <a class="yt-timestamp" data-t="01:50:14">[01:50:14]</a>. He notes that public criticism has helped him become more aware of the least convincing parts of his arguments, such as for the "Most Important Century" thesis <a class="yt-timestamp" data-t="01:51:28">[01:51:28]</a>.

Karnofsky's work exemplifies a philanthropic approach that continuously adapts to new information and evolving understandings of how to achieve the greatest possible positive impact, increasingly focusing on the profound and potentially imminent transformations AI may bring.