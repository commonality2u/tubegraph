---
title: Ethical considerations and deployment of AI
videoId: Wo95ob_s_NI
---

From: [[dwarkesh | The Dwarkesh Podcast]]

The rapid advancement of Artificial Intelligence (AI), particularly Large Language Models (LLMs), presents significant ethical challenges and necessitates careful consideration regarding its deployment. John Schulman, co-founder of OpenAI, has discussed various facets of these issues, focusing on safety, alignment, and the societal impact of increasingly capable AI systems.

## Preparing for Advanced AI and Potential AGI

The possibility of Artificial General Intelligence (AGI) emerging relatively soon, potentially within the next few years, underscores the need for proactive measures. This is particularly crucial as the [[challenges_and_advancements_in_ai_training_techniques | challenges and advancements in AI training techniques]] continue to evolve.

### Caution and Safety Measures
If AGI or significantly advanced AI arrives sooner than anticipated, a cautious approach to further training and deployment is crucial until its safety can be reasonably assured. Schulman emphasizes that current understanding of these advanced systems is still "rudimentary in a lot of ways" and highlights the need for [[ai_alignment_and_safety_research | AI alignment and safety research]].

### International Coordination
Given the global nature of AI development, Schulman suggests that coordination among the major entities training the largest models would be necessary. This coordination could involve avoiding "race dynamics" where safety might be compromised in the pursuit of staying ahead, which echoes concerns discussed in [[the_geopolitical_stakes_of_agi_development | the geopolitical stakes of AGI development]].  

The ultimate goal of such a pause or slowdown would be to ensure that technical problems around AI alignment are sufficiently solved. This aligns with the objectives outlined in [[ai_alignment_and_existence_risks | AI alignment and existential risks]].

## Deployment Strategies and Safety Protocols

Schulman advocates for a deployment strategy centered on incremental progress and robust safety checks. This aligns with the insights shared in [[ai_safety_and_alignment | AI Safety and Alignment]] regarding gradual deployment protocols.

### Incremental Deployment and Continuous Improvement
The preferred scenario is a continuous release of AI systems that are incrementally smarter than their predecessors. This approach allows for safety and alignment improvements to be made in correspondence with capability enhancements, resonating with [[ai_scalability_and_breakthroughs | AI scalability and breakthroughs]].

### Handling Discontinuous Jumps in Capability
If a discontinuous jump in AI capability occurs, more stringent measures would be required before release, emphasizing extensive testing and stringent [[cybersecurity_and_ai_vulnerabilities | cybersecurity and AI vulnerabilities]] as crucial components.

### Monitoring During Training
Especially for techniques like long-horizon Reinforcement Learning (RL), careful monitoring during the training process is vital to ensure alignment and prevent potential risks, akin to those discussed in [[ai_alignment_challenges_and_ethical_considerations | AI alignment challenges and ethical considerations]].

## AI in Society: Humans in the Loop and Alignment

As AI becomes more integrated into society, questions about human oversight and AI alignment become paramount amidst economic pressures. This is consistent with insights on [[impact_of_ai_on_economic_and_societal_structures | impact of AI on economic and societal structures]].

### The Role of Humans in an AI-Driven Economy
Initially, the preference is for humans to oversee important decisions, even if AI systems become capable of handling them independently. However, practical considerations might also influence the role of humans, as highlighted in [[ai_economic_impact_and_the_future_of_whitecollar_work_automation | AI's economic impact and the future of white-collar work automation]].

### AI Alignment and Stakeholder Considerations
For AI to be used in high-stakes scenarios, current RLHF methods might need to evolve, considering the broad spectrum of [[ai_alignment_and_cooperation_challenges | AI alignment and cooperation challenges]]. OpenAI's "Model Spec" is an effort to address this, by balancing diverse stakeholder requirements and ensuring alignment.

### Articulating Preferences for Future AI
As models like a hypothetical GPT-6 or GPT-7 become more advanced, how human preferences are communicated to them is an open question, reflecting the ongoing [[challenge_and_limitation_in_ai_interpretability_and_safety | challenges and limitations in AI interpretability and safety]].