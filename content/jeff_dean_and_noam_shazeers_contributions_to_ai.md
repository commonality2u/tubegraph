---
title: Jeff Dean and Noam Shazeer's Contributions to AI
videoId: v0gjI__RyCY
---

From: [[dwarkesh | The Dwarkesh Podcast]]

Jeff Dean and Noam Shazeer are prominent figures in the field of Artificial Intelligence, particularly known for their foundational work at Google. Dean, as Google's Chief Scientist, has been instrumental in developing transformative systems in modern computing over his 25-year tenure, including MapReduce, BigTable, Tensorflow, and Gemini [[25_years_at_google_evolution_and_impact | 25-year tenure]]. Shazeer is widely recognized as a key inventor behind the current AI revolution, having invented or co-invented core architectures and techniques for modern Large Language Models (LLMs) such as the Transformer, Mixture of Experts, and Mesh Tensorflow. Both are co-leads of the Gemini project at Google DeepMind.

## Early Careers and Recruitment at Google

### Joining Google
Noam Shazeer joined Google around the end of 2000. He initially encountered Google at a job fair in 1999 but assumed it was already too large to join, as many of his peers at UC Berkeley were already using it. He sent his resume in 2000 on a whim, attracted by it being his favorite search engine and the prospect of working with smart people on interesting problems, exemplified by a crayon chart on the wall showing exponential growth in search queries. His initial plan was to earn enough money to then independently work on AI, a goal he was already considering in 2000.

Jeff Dean, who had joined when Google had around 25 employees, actually reached out to Google himself.

### Early Experiences
When Shazeer joined, he was assigned Jeff Dean as his mentor. At the time, Shazeer felt Dean "knew everything" because he had "basically written everything." Dean described the phases of growth at Google: from knowing everyone's name, to knowing everyone in software engineering, to knowing all the projects, and finally, to hearing about major project launches like "Project Platypus" without prior awareness. He emphasized the importance of maintaining a high-level understanding of the company's activities and building a strong internal network.

Shazeer briefly left Google but rejoined in 2012 to work with the early Google Brain team after a chance lunch encounter with Dean. He noted a pattern of rejoining Google every 12 years (2000, 2012, 2024).

## Pioneering Work in AI and Systems

### Early Neural Network Research (Jeff Dean)
In 1990, for his senior undergraduate thesis, Jeff Dean worked on parallelizing backpropagation for neural networks. He implemented both model parallelism (which he called "pattern partitioning") and data parallelism on a 32-processor Hypercube machine. At the time, he naively thought 32 processors would be sufficient for training "really awesome neural nets," but later realized about a million times more compute was needed for them to work on real problems. This threshold was reached around 2008-2010, reigniting his interest in the field. His thesis was notably concise, comprising four pages of text and 30 pages of C code.

### Large-Scale N-gram Models (Jeff Dean & Team)
In 2007, Dean and Shazeer were involved in training a two-trillion token N-gram model for language modeling, primarily for machine translation. This work originated from a DARPA machine translation contest where Google's entry, while winning, was impractical for real-world use, taking 12 hours to translate a single sentence due to its reliance on extensive disk seeks (100,000 per word) in its language model.
Dean spent months with the team designing an in-memory compressed representation for 5-gram data (statistics of five-word sequences) from a corpus of 2 trillion words processed from the web. This system, distributed across 200 machines with a batched API, reduced sentence translation time from hours to about 100 milliseconds. This infrastructure for serving large language models was later used for other applications like query completion.

### Early Language Modeling and Spelling Correction (Noam Shazeer)
Around 2000-2001, Shazeer developed an influential spelling correction system at Google that utilized language models. This system ran in-memory on a single machine. Dean recalled being highly impressed by its ability to correct severely misspelled queries, such as "scrumbled uggs Bundict" for "scrambled eggs benedict." Shazeer acknowledged this as an early form of language modeling.

### The "Cat Neuron" and Unsupervised Learning (Google Brain)
In the early days of the Google Brain team (around 2012), they focused on building infrastructure to train very large neural networks on CPUs, as GPUs were not yet prevalent in their data centers. They developed a system for unsupervised learning using 10 million randomly selected YouTube frames, training a model on 2,000 computers (16,000 cores). This model, without explicit labeling, learned to develop high-level representations, famously including a neuron that became selectively activated by images of cats (specifically head-on facial views). Other neurons specialized in human faces or pedestrians. This work demonstrated the power of scaling and unsupervised learning, leading to a 60% relative improvement on the ImageNet 20,000 category challenge at the time. Dean noted this success reinforced the idea that scaling up neural nets was a promising direction [[ai_scalability_and_breakthroughs | scaling up neural nets]].

### Evolution of Hardware and Moore's Law
Dean observed that Moore's Law's impact has shifted. Two decades ago, hardware rapidly improved every 18 months without needing algorithmic changes. More recently, general-purpose CPU scaling has slowed, with fabrication improvements taking longer and architectural boosts diminishing. However, specialized hardware like TPUs (Tensor Processing Units) and ML-focused GPUs have provided significant performance gains for modern computations.

Shazeer added that algorithms have followed hardware trends, where arithmetic has become cheap and data movement expensive, leading to the success of deep learning methods built on matrix multiplications. Dean highlighted the pivot to hardware like TPUs, which are essentially reduced-precision linear algebra machines, as a critical transition. This involved filling chip area with many low-precision arithmetic units, a concept Shazeer attributed to identifying opportunity costs.

#### Reduced Precision and Co-design
A key trend in hardware like TPUs has been the increasing use of reduced precision. TPUv1 was designed around 8-bit integers for inference, a decision made with early, somewhat uncertain evidence of its feasibility. Since then, precision has continued to drop for both training and inference, with INT4, FP4, and even 1-bit or 2-bit quantization becoming common. Shazeer emphasized that such advancements require co-design: algorithm designers must be aware of the performance benefits of lower precision to overcome the perceived risks and irritation, and chip designers need to see the whole picture to realize the throughput-to-cost gains [[innovations_and_challenges_in_ai_hardware | co-design]].

## Gemini and the Future of AI

### Google's Evolving Mission
Dean framed Google's mission as "organizing the world's information and making it universally accessible and useful," a mandate broad enough to inherently require advanced AI. He sees current AI, like Gemini, extending this mission beyond information retrieval to include creation and synthesis, such as drafting letters or summarizing videos. This includes understanding information in all modalities (text, images, video, audio, genomic data, sensor data) and transforming it into useful insights. Shazeer added that the goal is to create value, potentially orders of magnitude more than currently, by enabling systems to perform tasks like writing code or solving complex problems [[impact_of_ai_on_future_technology_and_society | future of AI]].

### Long Context Windows
A significant research direction is expanding the context window of LLMs. Dean explained that while model parameters store vast amounts of "squishy" information from training data, information in the active context window is "sharp and clear" due to attention mechanisms. Current models can handle millions of tokens (hundreds of PDF pages, hours of video), but the goal is to attend to trillions of tokens, potentially the entire internet or a user's personal data (emails, documents, photos). This presents a computational challenge as naive attention is quadratic, necessitating algorithmic approximations [[understanding_and_leveraging_long_context_lengths_in_llms | long context windows]]. Shazeer pointed out the memory inefficiency of context tokens (potentially kilobytes per token) compared to model parameters (one fact per parameter), highlighting ongoing innovation in minimizing this and selectively accessing information.

### AI in Software Development and Research
Models like Gemini are already being used internally at Google to enhance developer productivity. A Gemini model further trained on Google's internal codebase contributes to 25% of characters checked into the codebase, with human oversight.
Looking ahead, they envision AI models significantly accelerating research. Dean suggested researchers could specify high-level ideas (e.g., "explore this idea similar to this paper, but make it convolutional"), and the system would generate experimental code for review and execution. Shazeer mentioned the possibility of trying millions of ideas, especially since ML experiments often have low success rates; massive parallel exploration could lead to breakthroughs. Dean noted that current experimental coding models can already generate complex systems like a basic SQL database in C from a paragraph prompt [[impact_of_ai_on_software_development_and_productivity | AI's impact on software development]]. This automated exploration could vet more ideas and integrate them into production training much faster.

### Accelerating Chip Design
Dean is excited about using AI to dramatically speed up the chip design process, which currently takes about 18 months from conception to tape-out, followed by ~4 months for fabrication. If AI could shrink the design phase from 12-18 months with 150 people to a few people with an automated search process, it would enable more rapid iteration and specialization, aligning hardware better with ML algorithms that are 6-9 months out, rather than 2-2.5 years [[emerging_trends_in_memory_and_chip_design | accelerating chip design]]. The fabrication time (3-5 months for leading-edge nodes) would become the dominant part of the cycle.

### Inference Time Compute and Scaling
Shazeer highlighted the vast potential for increasing compute at inference time. Current operations are incredibly cheap (e.g., a million tokens per dollar), making LLM interaction much cheaper than activities like reading a paperback or hiring human experts. This headroom allows for spending more compute to make models smarter [[ai_scalability_and_breakthroughs | compute and scaling]]. Dean added that techniques allowing users to dial up inference compute for better answers (at higher cost) are valuable, enabling a trade-off based on the problem's importance. This involves active exploration and iterative problem-solving by the model. Rich Sutton's "Bitter Lesson" (learning and search are highly effective scalable techniques) is relevant here.

### Continual Learning and Modular AI (Pathways Vision)
Dean has long been an advocate for sparse, modular models where different parts specialize in different tasks, inspired by biological brains but adapted for silicon. Current Mixture-of-Experts (MoE) models (like Gemini 1.5 Pro) are a step in this direction, activating only relevant parts of a large-capacity model for efficiency. However, he envisions more organic, less rigidly structured models where components can be developed somewhat independently and then integrated. This could allow specialized teams to improve modules for specific languages (e.g., Southeast Asian languages) or domains (e.g., Haskell code reasoning) and then connect them to a larger model. This "Pathways" vision, for which infrastructure has been built and is used for Gemini, supports such "twisty, weird models with asynchronous updates." This modularity could also aid data control, allowing personal modules trained on private data or modules with restricted data usage (e.g., YouTube data only for YouTube products) [[ai_alignment_and_safety_concerns | alignment and safety]].

Shazeer noted that MoE experts can be surprisingly interpretable (e.g., experts for cylindrical objects or dates). Distillation is seen as a key tool to manage these complex models, transforming them into different architectures or smaller, efficient versions for serving, [[mechanistic_interpretability_in_ai | interpretability and reasoning]].

### Data Efficiency and Training Objectives
Both see potential for improving data efficiency. Humans learn effectively from far fewer tokens (billions) than current LLMs. Dean suggests changing training objectives beyond next-token prediction to more complex tasks like answering questions after reading a chapter, or learning more from visual data and interactive experiences (like an infant learning gravity by dropping objects). Shazeer pointed to learning from self-play (e.g., chess) or even thought experiments without new external data. Dean also believes more value can be extracted from existing text data by making models "work harder" on certain tokens or using techniques like dropout over many epochs [[challenges_and_methodologies_in_ai_training_and_data_usage | data efficiency]].

## Reflections and Perspectives

### On Publishing Research (e.g., Transformer)
Regarding the open publication of the Transformer paper, which heavily benefited competitors, Shazeer acknowledged it's a good question, suggesting that seeing the opportunity size (often reflected by competitor actions) was important and that the AI field is far from a fixed pie. He noted that Google is doing well, and these days they publish "a little less." Dean elaborated on the varying publication strategies: some critical developments might not be published, some are released in products first then detailed in papers (like Pixel computational photography), and others are published openly to advance the field [[open_source_ai_models_and_their_implications | open source AI models]].

### Addressing AI Safety and Potential Misalignment
Dean expressed a view between extreme optimism and extreme alarmism regarding AI capabilities. He co-authored a paper on "Shaping AI," arguing for actively steering AI development towards beneficial applications (education, healthcare) and away from harmful ones, using technical safeguards and policy. He likened this to rigorous software engineering for safety-critical systems like airplane software. Shazeer believes that LLMs' ability to analyze text (which is easier than generating it) will be key to solving control issues, with models checking themselves and others. Dean emphasized that as systems become more powerful, caution increases, and human oversight in AI-driven research (e.g., algorithmic exploration) is a safeguard [[ai_alignment_and_safety_research | AI safety]].

### Secrets to Longevity and Breadth
Dean attributed his ability to work across diverse fields to a willingness to learn new areas, often by collaborating with experts from different domains. This collective work allows for achievements beyond individual capabilities and leads to a cross-pollination of expertise.
Shazeer emphasized humility and the willingness to drop an idea when a better one emerges, whether from oneself or others. He also discussed the importance of incentive structures that encourage both dropping unpromising work (as seen in Google Brain's early "UBI" for chip allocation) and fostering collaboration (as in Gemini's more top-down approach). Dean added the value of articulating "wacky ideas" to bootstrap new directions [[historical_influences_on_leadership_and_innovation | influences on leadership and innovation]].