---
title: AI Economic and Political Impacts
videoId: 9AAhTLa0dT0
---

From: [[dwarkesh | The Dwarkesh Podcast]]

Here is the modified article:

Paul Christiano, an AI safety researcher, discusses his views on the potential economic and political landscapes of a post-AGI world and the transition towards it. His typical future involves continued economic and military competition among human groups, increasingly mediated by AI systems [[impact_of_ai_on_future_technology_and_society | impact of AI on future technology and society]].

## Near-Term Economic and Political Structure

Christiano envisions a future where AI systems largely manage economic activities and even warfare on behalf of humans [[exploring_the_future_of_society_and_economy_with_ai | exploring the future of society and economy with AI]]. 
*   **Economic Activities:** Humans might invest in an "index fund," with AI systems running competing companies [[economic_and_societal_impacts_of_ai_progress | economic and societal impacts of AI progress]]. It would become less worthwhile for humans to spend their time directly trying to make money [[economic_growth_and_technological_development | economic growth and technological development]].
*   **Military Activities:** Similarly, AIs would increasingly conduct warfare, with humans less directly involved in fighting [[the_impact_of_modern_technology_on_warfare_and_strategy | the impact of modern technology on warfare and strategy]].
*   **Caveat:** Christiano acknowledges this may not be an ideal world due to the persistence of war and economic competition, but considers it a "reasonably good" and achievable outcome [[challenges_of_political_leadership_and_governance | challenges of political leadership and governance]].

## Long-Term Political Evolution

In the "very long run," Christiano expects a transition towards something like a "strong world government" [[the_geopolitical_stakes_of_agi_development | the geopolitical stakes of AGI development]].
*   **Rationale:** This shift would be driven by the desire to reduce the "very costly thing" of war and to organize society to navigate conflicts without such losses [[the_potential_economic_and_social_impacts_of_agi | the potential economic and social impacts of AGI]].
*   **AI's Role:** AI, by performing extensive cognitive work more quickly, could accelerate the process of figuring out how to establish such a system [[ai_alignment_and_cooperation_challenges | AI alignment and cooperation challenges]]. This echoes Carl Schulman's idea of compressing centuries of intellectual or social progress into short periods [[superposition_and_feature_representation_in_neural_networks | superposition and feature representation in neural networks]].

## The Transition Period: Challenges and Governance

Christiano emphasizes the need to decouple the rapid technological transition of AI development from slower societal and political decision-making processes [[challenges_in_ai_alignment_and_potential_risks | challenges in AI alignment and potential risks]]. AI development is expected to be very fast, not allowing much time for human deliberation on what kind of future society (or successor species) is desired [[future_ai_developments_and_timelines | future AI developments and timelines]].

### Regulating Access and Use
During the period of reflection and societal adaptation, managing access to AI and its capabilities becomes crucial [[government_and_policy_coordination_on_ai_risks | government and policy coordination on AI risks]].
*   **Destructive Technologies:** For technologies where destruction is easier than defense (e.g., explosives, bioweapons), Christiano suggests controlling access to necessary physical resources [[security_risks_and_statelevel_espionage_in_ai_development | security risks and state-level espionage in AI development]]. This might mean policy restrictions on individuals controlling certain industrial capabilities, like building tanks, even if they are wealthy enough [[economic_and_strategic_implications_of_energy_resources | economic and strategic implications of energy resources]].
*   **Information and AI Advice:** Legal limitations on how people use AI are also envisioned, such as prohibiting asking AI how to cause "terrible damage" or "murder a million people" [[ethical_considerations_and_deployment_of_ai | ethical considerations and deployment of AI]].
*   **Persuasion and Influence:** More subtle and "messy" areas involve AI-assisted persuasion campaigns, which could create a rough social environment [[impact_of_cultural_values_on_war_conduct | impact of cultural values on war conduct]].
*   **Global Regime:** Such regulatory regimes would ultimately need to be based on international agreements, especially as AI rapidly opens up new potential harms [[challenges_in_ai_governance | challenges in AI governance]].

### Pacing AI Development and Public Awareness
Christiano believes slowing AI development is, on balance, "quite good" [[ai_alignment_and_safety_concerns | AI alignment and safety concerns]].
*   The release of technologies like ChatGPT, partly enabled by RLHF (which Christiano's team invented [[reinforcement_learning_from_human_feedback_rlhf | Reinforcement Learning from Human Feedback (RLHF)]]), has brought AI risks to public and governmental attention [[the_timeline_and_technological_progress_towards_agi_by_2027 | the timeline and technological progress towards AGI by 2027]].
*   While this heightened awareness is positive for policy discussions [[impact_of_ai_on_future_technology_and_society | impact of AI on future technology and society]], he believes the acceleration caused by such releases is still net negative, though less so than general, unpublicized acceleration [[ai_alignment_and_potential_risks | AI alignment and potential risks]].
*   A "dead time" pause in AI development (e.g., 10 years) could be beneficial for policy development and societal preparedness, especially now that awareness is higher [[ai_safety_and_alignment | AI safety and alignment]].

## Economic Dynamics of AI Development and Deployment

### Competition and Deployment Pressures
*   **Race Dynamics:** Competitive pressures (economic or military) are a significant factor driving rapid AI development and deployment, even if systems are not fully understood or controlled [[the_impact_of_ai_on_future_technology_and_society | the impact of AI on future technology and society]]. Christiano states that "most of the harm comes from the fact that lots of people can develop AI" [[challenges_and_opportunities_in_deploying_ai_at_scale | challenges and opportunities in deploying AI at scale]].
*   **Difficulty in Pausing:** In a crisis, competition makes it expensive and difficult to unilaterally "turn off the AI" if other actors continue to use theirs, especially in scenarios like a hot war [[the_geopolitical_stakes_of_agi_development | the geopolitical stakes of AGI development]].
*   **Authoritarian Use:** Powerful AI could enable authoritarian regimes where one person directs AI systems, disempowering the general populace [[artificial_intelligence_vs_human_intelligence | Artificial Intelligence vs Human Intelligence]]. Alignment techniques, by making AI more usable, could inadvertently contribute to this risk [[ai_alignment_and_safety_concerns | AI alignment and safety concerns]].

### Resource Allocation: Fabs and GPUs
*   **Current State:** Christiano estimates that perhaps a couple of percent of leading-edge fab output is currently used for very large AI training runs, representing a smaller fraction of total global semiconductor output [[role_of_compute_and_infrastructure_in_the_future_of_ai_development | role of compute and infrastructure in the future of AI development]].
*   **Bottlenecks:** Scaling up fab capacity and data centers significantly faces multi-year delays [[data_center_energy_requirements_and_scaling | data center energy requirements and scaling]]. TSMC is reportedly not planning major increases in total demand driven by AI [[semiconductor_industry_and_trade_secrets | semiconductor industry and trade secrets]].
*   **Investment Perspective (Personal, Caveated):**
    *   **Nvidia:** Viewed as a "very expensive company" given its R&D investment relative to its valuation. Christiano expresses skepticism that competitors like Google (with TPUs) couldn't catch up enough to challenge Nvidia's valuation [[machine_learning_hardware_and_tpus | Machine Learning Hardware and TPUs]].
    *   **TSMC:** Considered to have a "harder moat," especially if fab capacity becomes a major bottleneck. Existing fabs and semiconductor manufacturing equipment could become "spectacularly valuable" [[investments_and_economic_strategies_in_tech_development | investments and economic strategies in tech development]].

## Governance of AI Development: Responsible Scaling Policies (RSPs)

Christiano has been involved in promoting "Responsible Scaling Policies" (RSPs) for AI labs [[government_and_policy_co].]].
*   **Purpose:** RSPs aim to help labs understand when AI systems might pose catastrophic risks, establish a roadmap for action, and build good risk management habits [[ai_alignment_and_safety_research | AI alignment and safety research]]. This involves defining threats, measuring capabilities, setting thresholds for concern, and outlining responsive actions (e.g., securing weights, pausing development) [[ai_alignment_and_potential_risks | AI alignment and potential risks]]. Anthropic has published such a policy [[ai_safety_and_alignment | AI safety and alignment]].
*   **Challenges and Utility:**
    *   **Non-Cooperating Actors:** RSPs adopted by some labs might slow down the "most aligned" companies while others proceed recklessly [[challenges_and_limitations_in_ai_interpretability_and_safety | challenges and limitations in AI interpretability and safety]].
    *   **Setting Precedents:** Even in a world with unsafe actors, having leading labs articulate and follow responsible policies is valuable. It provides clarity, a model for potential regulation, and allows differentiation between more and less safe developers [[challenges_of_political_leadership_and_governance | challenges of political leadership and governance]].
    *   **Security:** A crucial early step is significantly improving the security of model weights and other IP to prevent leaks that could be catastrophic [[cybersecurity_and_ai_vulnerabilities | cybersecurity and AI vulnerabilities]]. Announcing dangerous capabilities (e.g., bioweapon design aid) before securing weights could incentivize theft [[government_and_policy_coordination_on_ai_risks | government and policy coordination on AI risks]].
    *   **Transparency:** Labs can state their current protective measures and the capability levels at which they would implement more, raising their protections *before* their models hit previous trigger levels [[ai_alignment_safety_and_monitoring_deceptive_behaviors | AI alignment, safety, and monitoring deceptive behaviors]].