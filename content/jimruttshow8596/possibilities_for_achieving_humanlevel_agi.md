---
title: Possibilities for achieving humanlevel AGI
videoId: isIrLmYTdvU
---

From: [[jimruttshow8596]] <br/> 

## Introduction to [[Artificial General Intelligence AGI|AGI]]

[[Artificial General Intelligence AGI|AGI]] is an imprecise and informal term referring to computer systems capable of performing tasks that are considered intelligent when humans do them, particularly those they were not specifically programmed or trained for <a class="yt-timestamp" data-t="01:52:00">[01:52:00]</a>. This contrasts with [[Artificial General Intelligence AGI vs Narrow AI|narrow AI]], which excels at highly particular tasks based on programming or data-driven training <a class="yt-timestamp" data-t="02:35:00">[02:35:00]</a>. Humans can generalize and improvise in new domains, unlike narrow AI systems like AlphaFold, which struggles with protein types outside its specific training data <a class="yt-timestamp" data-t="02:47:00">[02:47:00]</a>.

The concept of computers being generally intelligent is gaining mainstream acceptance, though practical achievement remains distant <a class="yt-timestamp" data-t="05:46:00">[05:46:00]</a>. There is no current robot or [[Artificial General Intelligence AGI|AI]] that could perform a simple, everyday task requiring general intelligence, such as making coffee in a random kitchen <a class="yt-timestamp" data-t="06:27:00">[06:27:00]</a>. This highlights the "AGI-hard" problems, which demand a level of generalization beyond current capabilities <a class="yt-timestamp" data-t="06:56:00">[06:56:00]</a>.

## Critique of Current [[Generative AI vs AGI|Generative AI]] and Deep Neural Nets (DNNs)

Ben Goertzel, who coined the term [[Artificial General Intelligence AGI|Artificial General Intelligence]], believes that Deep Neural Networks (DNNs) and other machine learning algorithms, which currently dominate [[Generative Artificial Intelligence|AI]] research, are fundamentally unsuited for achieving human-level [[Artificial General Intelligence AGI|AGI]] <a class="yt-timestamp" data-t="13:04:00">[13:04:00]</a>.

He argues that while DNNs are useful components within an [[Artificial General Intelligence AGI|AGI]] architecture, they lack many key aspects necessary for human-level intelligence <a class="yt-timestamp" data-t="15:38:00">[15:38:00]</a>. DNNs operate largely as clever lookup tables, recording and indexing vast amounts of data to supply responses based on relevant historical information <a class="yt-timestamp" data-t="16:27:00">[16:27:00]</a>. This approach focuses on "shallow patterns" rather than building a conceptual model of the underlying reality <a class="yt-timestamp" data-t="17:47:00">[17:47:00]</a>.

An example of this limitation is a DNN suggesting a "table saw" to fit a large table through a small door, assuming it's a saw *for* tables, rather than a saw *on* a table <a class="yt-timestamp" data-t="18:23:00">[18:23:00]</a>. This illustrates a knowledge representation issue where information is stored as contextualized particulars without abstract understanding <a class="yt-timestamp" data-t="21:29:00">[21:29:00]</a>. Unlike humans, who can generalize from minimal examples (one-shot or few-shot learning), current DNNs rely on massive datasets and do not intrinsically form concise abstractions of experience <a class="yt-timestamp" data-t="21:50:00">[21:50:00]</a>.

The current [[Generative Artificial Intelligence|AI]] industry's focus on DNNs is driven by their immediate commercial value, as they excel at optimizing well-defined metrics and repeating operations predictably <a class="yt-timestamp" data-t="27:40:00">[27:40:00]</a>. This focus steers resources away from [[Artificial General Intelligence AGI|AGI]] research, which requires more imaginative, creative, and unpredictable outcomes <a class="yt-timestamp" data-t="28:32:00">[28:32:00]</a>.

## Three Viable Paths to True [[Artificial General Intelligence AGI|AGI]]

Ben Goertzel outlines three main [[Potential trajectories of AI advancements|approaches]] to achieving human-level [[Artificial General Intelligence AGI|AGI]]:

### 1. Cognitive Level Approach: Hybrid Neural-Symbolic Evolutionary Metagraph-based [[Artificial General Intelligence AGI|AGI]]

This approach, exemplified by Goertzel's OpenCog project, aims to emulate the human mind's high-level functions using advanced computer science algorithms, without strictly mimicking biological details <a class="yt-timestamp" data-t="33:30:00">[33:30:00]</a>. It's akin to designing an airplane by observing birds but not replicating feather-flapping <a class="yt-timestamp" data-t="34:10:00">[34:10:00]</a>.

Key aspects include:
*   **Modular Functions**: Designing effective computer science algorithms for distinct cognitive functions like perception, action, planning, working memory, and long-term memory <a class="yt-timestamp" data-t="35:48:00">[35:48:00]</a>.
*   **[[Cognitive Synergy in AGI development|Cognitive Synergy]]**: Integrating these algorithms so that semi-discrete functions can transparently interact and help each other out <a class="yt-timestamp" data-t="36:36:00">[36:36:00]</a>.
*   **Knowledge Graph (Metagraph)**: Centering the system on a large, distributed knowledge graph (hypergraph or metagraph) where different [[Generative Artificial Intelligence|AI]] algorithms act on common knowledge <a class="yt-timestamp" data-t="37:57:00">[37:57:00]</a>. OpenCog Hyperon is the new version of this system, incorporating new mathematical approaches to unify learning and reasoning algorithms <a class="yt-timestamp" data-t="39:10:00">[39:10:00]</a>.
*   **Distinction from "Good Old-Fashioned [[Generative Artificial Intelligence|AI]]" (GOFAI)**: Unlike GOFAI, this approach:
    *   Uses advanced fuzzy, probabilistic, and paraconsistent logic to handle uncertainty and contradictions, rather than crisp logic <a class="yt-timestamp" data-t="44:20:00">[44:20:20]</a>.
    *   Does not rely on hand-coding common sense knowledge, instead focusing on learning from data <a class="yt-timestamp" data-t="44:45:00">[44:45:00]</a>.
    *   Integrates learning deeply, even allowing for logical theorem proving to be used for unsupervised learning on low-level sensory data <a class="yt-timestamp" data-t="45:00:00">[45:00:00]</a>.
*   **Role of Evolution**: Evolution is implicitly present in the system's distributed parallel processes, where "fitness values" (importance in the knowledge graph) drive selection and "logical inference" acts as crossover/mutation <a class="yt-timestamp" data-t="47:17:00">[47:17:00]</a>. Explicit genetic algorithms are also used for procedure learning and creativity, such as evolving new logical predicates <a class="yt-timestamp" data-t="49:26:00">[49:26:00]</a>.

### 2. Brain Level Approach: Large-Scale Non-Linear Dynamical Brain Simulation

This approach involves simulating the brain's complex, non-linear dynamics at a detailed level. Goertzel notes that current DNNs are not true brain simulations <a class="yt-timestamp" data-t="51:31:00">[51:31:00]</a>.

Challenges and Opportunities:
*   **Measurement Limitations**: A key barrier is the lack of instruments to sufficiently measure time series of neural activity across large cortical areas to reverse-engineer brain processes, particularly concerning abstraction formation <a class="yt-timestamp" data-t="52:52:00">[52:52:00]</a>.
*   **Sophisticated Neuron Models**: Work by Gerald Edelman and Eugene Izhikevich on chaos theory-based neuron models, more biologically realistic than those in modern DNNs, shows promise for understanding how disparate neurons bind together <a class="yt-timestamp" data-t="55:31:00">[55:31:00]</a>.
*   **Predictive Coding-based Learning**: Alex Ororbia's work on a "back propagation killer" for deep neural networks, using predictive coding, offers a new learning method that is more biologically realistic and could potentially lead to better generalization by allowing for more realistic neuron models and glial cells <a class="yt-timestamp" data-t="57:41:00">[57:41:00]</a>. This could enable neural nets to automatically learn structured semantic representations that interface cleanly with logic-based systems like OpenCog <a class="yt-timestamp" data-t="01:00:00">[01:00:00]</a>.
*   **Hardware Bottleneck**: Detailed brain simulations are a terrible fit for traditional Von Neumann (serial) computing architectures <a class="yt-timestamp" data-t="01:01:34">[01:01:34]</a>. While GPUs offer some parallelization for simpler DNNs, more complex brain models require inherently parallel hardware <a class="yt-timestamp" data-t="01:02:01">[01:02:01]</a>.
    *   **Specialized Chips**: The next 3-5 years are expected to see the emergence of specialized chips for different [[Generative Artificial Intelligence|AI]] algorithms, beyond just GPUs <a class="yt-timestamp" data-t="01:03:50">[01:03:50]</a>. This includes chips optimized for Izhikevich neurons or Multi-Instruction, Multiple Data (MIMD) parallel processor-in-RAM architectures for graph pattern matching, as is being explored for OpenCog <a class="yt-timestamp" data-t="01:05:15">[01:05:15]</a>. The declining cost of custom chip design makes "AGI boards" combining different specialized chips viable <a class="yt-timestamp" data-t="01:07:25">[01:07:25]</a>.

### 3. Chemistry Level Approach: Massively Distributed [[Artificial General Intelligence AGI|AI]] Optimized Artificial Chemistry Simulation

This approach draws inspiration from artificial life and aims to simulate a "chemical soup" where "molecules" (e.g., code snippets) react to produce new "molecules" in complex chains, leading to emergent intelligence <a class="yt-timestamp" data-t="01:16:40">[01:16:40]</a>.

Key concepts:
*   **Artificial Biochemistry**: Moving beyond simple genetic algorithms to simulate more fine-grained artificial organisms with artificial genomes and metabolisms, acknowledging the subtlety of biological processes like protein folding and epigenomics <a class="yt-timestamp" data-t="01:17:10">[01:17:10]</a>.
*   **Algorithmic Chemistry**: Abstracting the spirit of chemistry, as explored by Walter Fontana, using "list codelets" or programs that act on other programs to produce new ones <a class="yt-timestamp" data-t="01:18:40">[01:18:40]</a>.
*   **Dual Evolution (Evo-Devo)**: The idea that if one could evolve the underlying chemistry (or its abstracted form) and its gene expression machinery, it might lead to a more expressive representation for intelligence, potentially finding an "easier way" than nature's 3.7-billion-year process <a class="yt-timestamp" data-t="01:20:19">[01:20:19]</a>.
*   **Compute Resources**: Realistic chemistry simulations are extremely compute-intensive, even more so than brain simulations <a class="yt-timestamp" data-t="01:26:21">[01:26:21]</a>. This makes abstracted algorithmic chemistry approaches more appealing for current resources <a class="yt-timestamp" data-t="01:26:59">[01:26:59]</a>.
*   **AI-Directed Evolution**: Incorporating [[role_of_large_language_models_in_achieving_agi|machine learning]] or proto-[[Artificial General Intelligence AGI|AGI]] to study and direct the evolution of the chemical soup. This could involve an [[Artificial Intelligence|AI]] observer identifying promising "Vats" and regenerating less promising ones based on successful patterns <a class="yt-timestamp" data-t="01:29:08">[01:29:08]</a>. This leads to a hybrid architecture where algorithmic chemistry is guided by pattern mining and probabilistic reasoning <a class="yt-timestamp" data-t="01:30:38">[01:30:38]</a>.
*   **Decentralized Processing**: Envisioning a future where millions of individuals contribute processing power to run virtual algorithmic chemistry simulations, with analysis and refresh performed by a decentralized [[Artificial Intelligence|AI]] platform like SingularityNet <a class="yt-timestamp" data-t="01:31:53">[01:31:53]</a>.
*   **Hardware Parallels**: While physical chemistry is inherently parallel, current computing is not. However, creative explorations into massively parallel, "lifelike" computing infrastructures (e.g., nanoscale continuous variable cellular automaton lattices for molecular computing) could offer suitable substrates <a class="yt-timestamp" data-t="01:35:12">[01:35:12]</a>.

## [[Future Directions and Challenges for AI and AGI|Future Directions and Challenges]]

All three [[Approaches to evolving AI architectures|approaches]] are interesting and deserve significantly more attention and funding than they currently receive <a class="yt-timestamp" data-t="01:25:01">[01:25:01]</a>. Ben Goertzel emphasizes the need for society to diversify its [[Artificial General Intelligence AGI|AGI]] research portfolio beyond the current mainstream focus on DNNs <a class="yt-timestamp" data-t="01:38:00">[01:38:00]</a>.

Funding for [[Artificial General Intelligence AGI|AGI]] research is critically under-resourced compared to its potential impact. Large-scale funding, perhaps hundreds of billions of dollars, could massively accelerate [[Artificial General Intelligence AGI|AGI]] R&D, similar to how the NIH has transformed fields of biology and medicine <a class="yt-timestamp" data-t="01:40:52">[01:40:52]</a>. Such an investment is comparable to a fraction of a nation's defense budget over several years <a class="yt-timestamp" data-t="01:42:42">[01:42:42]</a>.

Beyond government funding, a significant cultural shift is needed, akin to the rise of open-source software and citizen science <a class="yt-timestamp" data-t="01:47:01">[01:47:01]</a>. As more people recognize [[Artificial General Intelligence AGI|AGI]] as a viable goal within their lifetimes and acknowledge that large governments and tech companies may not pursue the most creative paths, a grassroots surge in [[Artificial General Intelligence AGI|AGI]] R&D could occur <a class="yt-timestamp" data-t="01:48:02">[01:48:02]</a>. A breakthrough that makes [[Artificial General Intelligence AGI|AGI]]'s arrival undeniable could trigger both increased government funding and widespread grassroots attention <a class="yt-timestamp" data-t="01:48:31">[01:48:31]</a>.

Goertzel's own bet remains on the cognitive level, hybrid approach as the most likely path to achieve [[Artificial General Intelligence AGI|AGI]] first. However, he stresses that this hybrid system can integrate ideas from other paradigms, such as biologically realistic neural nets for perception or algorithmic chemistry for creative idea generation <a class="yt-timestamp" data-t="01:38:40">[01:38:40]</a>.