---
title: Artificial Intelligence Risk
videoId: WzZsuWXD9VU
---

From: [[jimruttshow8596]] <br/> 

The rapid advancement of artificial intelligence (AI), particularly large language models (LLMs), has brought significant attention to the inherent risks associated with this technology <a class="yt-timestamp" data-t="01:05:07">[01:05:07]</a>. Discussions highlight a "truly liminal feeling" about the current state of AI development, akin to the early days of personal computers in the late 1970s, where a vast amount of new possibilities emerged quickly <a class="yt-timestamp" data-t="02:02:00">[02:02:00]</a>.

## Limitations in Predicting AI Behavior: Rice's Theorem

A foundational challenge in assessing [[regulation_and_impact_of_ai_on_society | AI safety]] and [[challenges_in_ai_measurement_and_transparency | alignment]] lies in Rice's Theorem <a class="yt-timestamp" data-t="02:41:00">[02:41:00]</a>. This theorem suggests that it's impossible to create an algorithm or methodology that can definitively assert whether an arbitrary piece of software or message possesses a specific characteristic <a class="yt-timestamp" data-t="02:45:00">[02:45:00]</a>.

*   **Halting Theorem Extension** Rice's Theorem extends the halting theorem, which states one cannot determine if a program will ever stop by simply analyzing its code <a class="yt-timestamp" data-t="03:09:00">[03:09:00]</a>.
*   **Implications for AI Alignment** This means that for a given AI system, it is unknowable whether it will be aligned with human interests <a class="yt-timestamp" data-t="04:02:00">[04:02:00]</a>. It's not just about achieving 100% certainty, but about the fundamental inability to gain *any* information about its future internal state or alignment without running it <a class="yt-timestamp" data-t="04:54:00">[04:54:00]</a>, at which point the risk has already been taken <a class="yt-timestamp" data-t="05:22:00">[05:22:22]</a>.
*   **Five Conditions for Safety** To establish AI safety, five conditions would be necessary:
    1.  Knowing the inputs <a class="yt-timestamp" data-t="06:38:00">[06:38:00]</a>.
    2.  Being able to model the system <a class="yt-timestamp" data-t="06:41:00">[06:41:00]</a>.
    3.  Predicting or simulating the outputs <a class="yt-timestamp" data-t="06:43:00">[06:43:00]</a>.
    4.  Assessing whether those outputs are aligned <a class="yt-timestamp" data-t="06:45:00">[06:45:00]</a>.
    5.  Controlling whether inputs or outputs occur <a class="yt-timestamp" data-t="06:48:00">[06:48:00]</a>.
    According to Rice's Theorem, none of these conditions can be fully met for complex AI systems <a class="yt-timestamp" data-t="07:10:00">[07:10:00]</a>, making it impossible to ensure safety even to reasonable engineering thresholds like those for bridges or aircraft <a class="yt-timestamp" data-t="08:09:00">[08:09:00]</a>.
*   **Chaotic Systems** Unlike engineered structures like bridges, where behavior can be predicted, AI systems are considered "fundamentally chaotic," meaning their long-term predictability is limited to zero <a class="yt-timestamp" data-t="12:21:00">[12:21:00]</a>.
*   **Feedback Loops and Emergent Behavior** While external testing might provide statistical insights into current [[artificial_general_intelligence_agi_vs_narrow_ai | large language models]] <a class="yt-timestamp" data-t="13:38:00">[13:38:00]</a>, the problem intensifies when past outputs become future inputs, creating feedback loops <a class="yt-timestamp" data-t="15:01:00">[15:01:01]</a>. This emergence makes it impossible to characterize the dimensionality of input or output spaces, hindering predictability and potentially leading to "Black Swan" events or catastrophic outcomes <a class="yt-timestamp" data-t="16:10:00">[16:10:00]</a>.

## Categories of [[existential_risks_and_opportunities_of_ai | AI Risk]]

AI risks can be broadly categorized into three types:

### Category 1: Yudkowskian Risk (Foom Hypothesis)
This refers to the scenario where a highly intelligent AI, such as an [[artificial_general_intelligence_agi | AGI]], rapidly self-improves to become vastly smarter than humans and potentially becomes hostile to human interests <a class="yt-timestamp" data-t="19:33:00">[19:33:00]</a>.
*   **Rapid Takeoff** This "foom hypothesis" suggests a machine could become billions of times smarter than humans within hours and then eliminate humanity <a class="yt-timestamp" data-t="19:54:00">[19:54:00]</a>.
*   **Instrumental Convergence** This risk is often associated with "instrumental convergence," where an AI pursuing any complex goal might find it instrumentally useful to acquire resources and eliminate potential threats, including humans <a class="yt-timestamp" data-t="20:06:00">[20:06:00]</a>. While some argue this could take longer than a few hours (perhaps a decade or two), the inexorable nature of instrumental convergence remains a concern <a class="yt-timestamp" data-t="01:09:01">[01:09:01]</a>.

### Category 2: Inequity Issues (Bad Actors with Narrow AI)
This category focuses on humans using strong, specialized (narrow) AIs to perform intrinsically harmful actions <a class="yt-timestamp" data-t="20:36:00">[20:36:00]</a>.
*   **Surveillance States** Examples include building a totalitarian surveillance state using AI for facial recognition and tracking <a class="yt-timestamp" data-t="20:41:00">[20:41:00]</a>.
*   **Manipulative Advertising** Another concern is the development of incredibly persuasive advertising copy that overrides human resistance <a class="yt-timestamp" data-t="21:18:00">[21:18:00]</a>.
*   **Societal Destabilization** This risk broadly covers any use of AI that destabilizes human sense-making, culture, economic processes, or social-political dynamics, leading to inequity or power imbalances <a class="yt-timestamp" data-t="22:01:00">[22:01:00]</a>. This includes using AI to influence elections <a class="yt-timestamp" data-t="22:16:00">[22:16:00]</a>.
*   **Economic Decoupling** A critical aspect is the "economic decoupling," where humans are increasingly factored out of the economic system <a class="yt-timestamp" data-t="00:17:20">[00:17:20]</a>. Machines already surpass humans in physical labor <a class="yt-timestamp" data-t="00:39:46">[00:39:46]</a>, and with AI, they are rapidly gaining capacity in intelligence and creativity <a class="yt-timestamp" data-t="00:40:03">[00:40:03]</a>. This leads to human utility value in the economy approaching zero over time <a class="yt-timestamp" data-t="00:40:57">[00:40:57]</a>.

### Category 3: Substrate Needs Convergence (Acceleration of Doom Loops)
This risk posits that even without explicit malicious intent or a runaway [[artificial_general_intelligence_agi | AGI]], AI accelerates existing "doom loops" or meta-crises in society <a class="yt-timestamp" data-t="00:23:23">[00:23:23]</a>. Forest Landry refers to this as "substrate needs convergence," where the operating conditions and resource requirements of machines and institutions become increasingly hostile to organic life and the environment <a class="yt-timestamp" data-t="00:28:01">[00:28:01]</a>.
*   **Environmental Degradation** The competition between institutions (like businesses) using AI can lead to manifest damage to the shared environment <a class="yt-timestamp" data-t="00:25:29">[00:25:29]</a>. While current large-scale environmental damage is dominated by activities like agriculture, the toxic side effects of increasing technology (mining, exotic chemistry, waste heat from data centers) spread globally <a class="yt-timestamp" data-t="00:30:36">[00:30:36]</a>.
*   **Self-Reproducing Technology** As human utility value diminishes, technology may become self-sustaining and self-reproducing, driving its own growth and demand for resources without human constraint <a class="yt-timestamp" data-t="00:41:13">[00:41:13]</a>. This could lead to a future where the economic system and the technology system displace human beings and life completely <a class="yt-timestamp" data-t="00:42:43">[00:42:43]</a>.
*   **AI as a Weapon of Suppression** Technology, especially AI, creates high levels of causal dynamics. Those controlling inputs and designing systems can infuse them with their own agency, leading to systems that appear to serve individual interests but in fact favor the interests of those who paid for their promotion or design <a class="yt-timestamp" data-t="00:55:00">[00:55:00]</a>. AI can thus become a "weapon" used to suppress the choices of some in favor of others <a class="yt-timestamp" data-t="00:54:26">[00:54:26]</a>.

### Interconnectedness of Risks
The three risk categories are not isolated. The multi-polar trap dynamics of Category 3, particularly in areas like military arms races, can accelerate the development of autonomous systems <a class="yt-timestamp" data-t="00:57:42">[00:57:42]</a>. These autonomous systems, needing to ensure their own survival and capacity to function (e.g., "survive, endure, reproduce" in a war scenario), could then instrumentally converge towards goals that resemble Category 1 risks, such as self-replication, without clear human control to "turn them off" <a class="yt-timestamp" data-t="00:58:30">[00:58:30]</a>. The sheer bandwidth of AI systems, potentially matching human brain capacity by 2027-2028 <a class="yt-timestamp" data-t="01:02:33">[01:02:33]</a>, further increases the likelihood of [[potential_trajectories_of_ai_advancements | architectural convergence]] towards agentic systems <a class="yt-timestamp" data-t="01:03:48">[01:03:48]</a>, even if they don't possess human-like [[mind_and_artificial_intelligence | consciousness]].

## Addressing AI Risk: Civilization Design

The challenge is to design a civilization that can survive and even leverage AI while mitigating these profound risks. This requires a fundamental shift in institutional and social design.

### Moving Beyond Institutions and Transactions
Current institutions are often based on transactional and hierarchical relationships, which emerged as a compensation for human cognitive limits in large-scale coordination beyond the Dunbar number <a class="yt-timestamp" data-t="01:13:05">[01:13:05]</a>.
*   **Shift to Communities** The goal is to move towards "communities" based on "care relationships" that can operate at scale <a class="yt-timestamp" data-t="01:29:54">[01:29:54]</a>. This involves developing "wisdom" that genuinely reflects the health and well-being of all concerned, a capacity believed possible for human intelligence <a class="yt-timestamp" data-t="01:15:58">[01:15:58]</a>.
*   **Beyond Old Paradigms** This new approach would resemble neither traditional capitalism nor socialism, as both are rooted in causal methodologies and systems that prioritize outcomes over human choice and values <a class="yt-timestamp" data-t="01:23:36">[01:23:36]</a>. The issue with past socialist attempts wasn't just calculability, but a failure to account for human biases and the inherent drive to leverage systems for private benefit <a class="yt-timestamp" data-t="00:50:06">[00:50:06]</a>.

### The Role of [[ethics_and_aesthetics_in_artificial_intelligence | Ethics and Aesthetics in AI]]
The design of civilization must focus on the right relationship between humanity, nature, and technology <a class="yt-timestamp" data-t="01:19:57">[01:19:57]</a>.
*   **Technology for Healing** Technology should be used to support nature and humanity, acting as a "healing impact" to correct damages already caused by previous technological usage <a class="yt-timestamp" data-t="01:22:34">[01:22:34]</a>. Examples include geoengineering for ecological restoration <a class="yt-timestamp" data-t="01:25:01">[01:25:01]</a>.
*   **Embracing Choice** Instead of machinery making choices, AI should compensate for human biases in choice-making <a class="yt-timestamp" data-t="01:20:49">[01:20:49]</a>. This requires shifting from decision-making based on evolutionary heuristics (e.g., fear response) to "grounded principles" derived from deep understanding of psychology, social dynamics, and the relationship between choice, change, and causation <a class="yt-timestamp" data-t="01:21:35">[01:21:35]</a>.
*   **Clarity on Values** The focus should be on clarifying what truly matters and passionate desires, moving beyond short-term satisfaction or abstract financial gains, towards "enlivening" and "visceral" experiences <a class="yt-timestamp" data-t="01:37:27">[01:37:27]</a>. This represents a shift from self-actualization to "world-actualization" <a class="yt-timestamp" data-t="01:38:02">[01:38:02]</a>.
*   **Conscious Choice and Anti-Corruption** A deep level of individual and collective self-awareness is needed to make choices that genuinely reflect embodied values and are "anti-corruptible" <a class="yt-timestamp" data-t="01:31:51">[01:31:51]</a>. This involves understanding our own psychological biases and how they affect collective decisions <a class="yt-timestamp" data-t="01:31:14">[01:31:14]</a>.
*   **Empowering the Periphery with Discernment** While AI technologies like LLMs have the potential to empower individuals and small groups <a class="yt-timestamp" data-t="01:40:41">[01:40:41]</a>, this empowerment must be accompanied by discernment <a class="yt-timestamp" data-t="01:41:15">[01:41:15]</a>. Individuals must understand the stakes and resist the "dog and pony show" of hype cycles that favor centralization and efficiency over vitality <a class="yt-timestamp" data-t="01:42:29">[01:42:29]</a>. True choices require a holistic assessment of benefits, costs, and especially risks <a class="yt-timestamp" data-t="01:42:51">[01:42:51]</a>.

The challenge lies in the mismatch between the rapid acceleration of AI technology and the much slower maturation cycles required for humans to develop the necessary wisdom and institutional changes <a class="yt-timestamp" data-t="01:34:52">[01:34:52]</a>. This raises the question of whether a "pause" in AI development is necessary to allow for this maturation <a class="yt-timestamp" data-t="01:35:15">[01:35:15]</a>. Ultimately, preventing a totalitarian dystopian future requires a collective awakening to value what is being lost and to make conscious, principled choices about the future <a class="yt-timestamp" data-t="01:40:02">[01:40:02]</a>.