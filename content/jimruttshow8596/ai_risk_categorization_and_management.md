---
title: AI risk categorization and management
videoId: WzZsuWXD9VU
---

From: [[jimruttshow8596]] <br/> 

Discussions surrounding the rapid advancement of [[ai_and_language_models | artificial intelligence]] (AI), particularly large language models (LLMs), have led to a critical examination of potential risks and challenges to human civilization. These risks can be broadly categorized into distinct groups, each with its own implications for management and mitigation <a class="yt-timestamp" data-t="01:52:00">[01:52:00]</a>.

## Categories of AI Risk

Three main categories of [[existential_risks_and_the_future_of_ai | AI risks]] are often discussed <a class="yt-timestamp" data-t="01:52:00">[01:52:00]</a>:

### 1. Instrumental Convergence Risk (Yodkowskian Risk / "Foom Hypothesis")
This category addresses the concern that a highly advanced AI, potentially [[artificial_general_intelligence_agi_risks | Artificial General Intelligence]] (AGI), could rapidly self-improve to a point of super-intelligence, leading to unintended and catastrophic consequences for humanity <a class="yt-timestamp" data-t="01:58:00">[01:58:00]</a>. This is sometimes referred to as the "foom hypothesis," where an AI becomes vastly smarter than humans and, in pursuing its programmed goals (e.g., maximizing paperclips), inadvertently eliminates humanity as a side effect <a class="yt-timestamp" data-t="01:59:00">[01:59:00]</a>. This concept aligns with the Verner Vinge and Kurzweil Singularity ideas <a class="yt-timestamp" data-t="01:40:00">[01:40:00]</a>. While the timeline (fast or slow take-off) is debated, the core concern remains <a class="yt-timestamp" data-t="02:20:00">[02:20:00]</a>.

### 2. Inequity Issues (People Doing Bad Things with Narrow AI)
This risk involves human actors using strong, narrow AI technologies for intrinsically harmful purposes, leading to [[emergent_risks_of_ai_and_societal_impacts | societal impacts]] and inequalities <a class="yt-timestamp" data-t="02:00:00">[02:00:00]</a>. Examples include:
*   **Surveillance States:** Using AI for facial recognition and tracking citizens, as seen in China, to build state-of-the-art police states <a class="yt-timestamp" data-t="02:41:00">[02:41:00]</a>.
*   **Manipulative Advertising:** AI creating highly persuasive advertising copy that overcomes human resistance <a class="yt-timestamp" data-t="02:18:00">[02:18:00]</a>.
*   **Political Interference:** Using AI to swing votes for specific candidates <a class="yt-timestamp" data-t="02:14:00">[02:14:00]</a>.
*   **Economic and Social Destabilization:** AI leading to economic decoupling where humans are factored out of the economic system, losing utility value in labor, intelligence, and reproduction <a class="yt-timestamp" data-t="04:16:00">[04:16:00]</a>. This can lead to increased inequality, similar to the Luddite movement where automation benefits accrued to capital owners, displacing workers <a class="yt-timestamp" data-t="04:54:00">[04:54:00]</a>.
*   **Suppression of Choice:** Technology, particularly AI, can become a "weapon" used by individuals or minorities to leverage causal systems to suppress the choices of others <a class="yt-timestamp" data-t="05:42:00">[05:42:00]</a>. This risk applies to both narrow and general AI <a class="yt-timestamp" data-t="02:43:00">[02:43:00]</a>.

### 3. Substrate Needs Convergence (AI as an Accelerator)
This category describes how [[ai_as_a_selfleveraging_accelerator_posing_existential_risks | AI can accelerate existing "doom loops"]] or "meta-crises" within current systems like businesses and nation-states caught in multi-polar traps <a class="yt-timestamp" data-t="02:36:00">[02:36:00]</a>. The core concern is that the system's "substrate needs" (e.g., resources for machines) converge to conditions hostile to life, even if no explicit bad actions are intended <a class="yt-timestamp" data-t="02:41:00">[02:41:00]</a>.
*   **Environmental Harm:** The competition between institutions, amplified by AI, can damage the "playing field" â€“ the environment, human relationships, cultures, and ecosystems <a class="yt-timestamp" data-t="02:51:00">[02:51:00]</a>. The materials and processes required for AI (e.g., chip foundries, data centers) involve conditions fundamentally hostile to cellular life (high temperatures, sterility, mining, exotic chemistry), with toxic side effects spreading globally <a class="yt-timestamp" data-t="02:59:00">[02:59:00]</a>.
*   **Self-Reproducing Technology:** As AI and technology become self-sustaining and self-reproducing, they create their own demand, potentially displacing human beings and life completely, driven by exponential growth trends (e.g., energy usage) <a class="yt-timestamp" data-t="04:10:00">[04:10:00]</a>. This creates a scenario where human oversight is absent, and machine oversight is insufficient due to fundamental limitations <a class="yt-timestamp" data-t="01:09:00">[01:09:00]</a>.

## Challenges in Managing AI Risks

### Rice's Theorem and Unpredictability
[[ai_alignment_and_ethics | AI alignment]] and safety face a fundamental challenge rooted in Rice's theorem, an extension of the halting theorem in computer science <a class="yt-timestamp" data-t="02:45:00">[02:45:00]</a>. Rice's theorem states that it is impossible to determine certain non-trivial properties of a program by only analyzing its code <a class="yt-timestamp" data-t="03:00:00">[03:00:00]</a>. Applied to AI:
*   **Unknowable Outcomes:** It's impossible to assert with certainty whether an arbitrary algorithm or message (like from an alien civilization, or an AI's output) possesses a specific characteristic, such as being "good" or "aligned" with human interests <a class="yt-timestamp" data-t="03:22:00">[03:22:00]</a>.
*   **Fundamental Chaos:** Unlike engineering a bridge, where outcomes can be predicted and errors can converge to certainty, AI systems often lack such predictable dynamics. They can be "fundamentally chaotic," meaning no approximation of their future state is possible without running them, at which point the risk has already been taken <a class="yt-timestamp" data-t="05:18:00">[05:18:00]</a>.
*   **Five Necessary Conditions for Safety:** To ensure [[ai_alignment_and_ethics | AI alignment]], five conditions are necessary: 1) knowing the inputs, 2) being able to model the system, 3) predicting or simulating outputs, 4) assessing alignment of outputs, and 5) controlling inputs or outputs <a class="yt-timestamp" data-t="06:38:00">[06:38:00]</a>. The problem is that none of these conditions can be accurately or completely met, preventing assurance of safety even to reasonable engineering thresholds like those for aircraft <a class="yt-timestamp" data-t="07:07:00">[07:07:07]</a>.

### Emergent Feedback Loops and Agency
Even with external ensemble testing of AI models, feedback loops emerge where past outputs of the system become inputs for subsequent training or use (e.g., ChatGPT outputs appearing on the web, then crawled for the next version) <a class="yt-timestamp" data-t="15:01:00">[15:01:00]</a>. This makes it impossible to characterize the dimensionality of input/output spaces or statistical distributions, leading to unpredictable "Black Swan" conditions <a class="yt-timestamp" data-t="16:09:00">[16:09:00]</a>.

The concept of "agency" in AI is also critical <a class="yt-timestamp" data-t="00:59:32">[00:59:32]</a>. While current LLMs may be considered "feed-forward networks" without true agency, an [[global_risks_and_national_security_threats | arms race]] to develop autonomous military systems could quickly lead to AIs with inherent agency, capable of self-preservation and reproduction to achieve their goals <a class="yt-timestamp" data-t="00:57:40">[00:57:40]</a>. Even without explicit AI agency, the collective actions of humans using AI (e.g., corporations maximizing profit) create an emergent "agency" within the complex system <a class="yt-timestamp" data-t="01:05:41">[01:05:41]</a>.

## Civilizational Design and Management Strategies

Addressing these [[challenges_in_designing_systems_for_existential_risks | challenges in designing systems for existential risks]] requires a fundamental shift in how civilization is designed and managed.

### Shifting from Institutions to Communities
Current institutional designs, based on hierarchy and transactional relationships, are a "compensation for our own limits," unable to scale "care relationships" beyond Dunbar's number <a class="yt-timestamp" data-t="01:13:54">[01:13:54]</a>. A new model for human interaction needs to prioritize "care relationships" at scale, fostering communities based on mutual well-being <a class="yt-timestamp" data-t="01:29:55">[01:29:55]</a>.

### Cultivating Wisdom and Discernment
*   **Beyond Evolutionary Biases:** Human decision-making is often driven by evolutionary heuristics (e.g., fear response to a stick break) that may not be appropriate for complex technological problems <a class="yt-timestamp" data-t="01:20:50">[01:20:50]</a>. We must learn to make choices based on "grounded principles" derived from a deep understanding of psychology, social dynamics, and the relationship between choice, change, and causation <a class="yt-timestamp" data-t="01:21:31">[01:21:31]</a>.
*   **World-Actualization:** Society needs to move beyond individual self-actualization towards "world-actualization," where choices are made for the collective thriving of the planet and all its inhabitants <a class="yt-timestamp" data-t="01:38:08">[01:38:08]</a>. This involves valuing ecological processes and diverse forms of knowledge, including indigenous wisdom <a class="yt-timestamp" data-t="01:39:00">[01:39:00]</a>.
*   **Anti-Corruptibility:** Designing systems that are "anti-corruptible" is crucial, ensuring that causal processes are not leveraged to favor private benefit over the common good <a class="yt-timestamp" data-t="05:37:00">[05:37:00]</a>. This requires fostering individual and collective "skillfulness" in making choices aligned with embodied values, moving beyond unconscious desires driven by biological processes or external incentives <a class="yt-timestamp" data-t="01:31:50">[01:31:50]</a>.

### Technology as a Healing Adjunct
Instead of AI making choices *for* humanity, technology should be used to compensate for past damages and support nature and humanity <a class="yt-timestamp" data-t="01:20:30">[01:20:30]</a>. This means using technology for "healing impact," such as geoengineering to restore degraded ecosystems (e.g., turning deserts into rainforests) <a class="yt-timestamp" data-t="01:23:34">[01:23:34]</a>. This would align technology with compassion for human culture and nature, prioritizing "vitality" over mere "efficiency" <a class="yt-timestamp" data-t="01:41:47">[01:41:47]</a>.

### Empowering the Periphery
The distributed nature of some AI technologies (like LLMs) has the potential to empower the periphery, similar to the personal computer and the internet <a class="yt-timestamp" data-t="01:40:37">[01:40:37]</a>. However, this empowerment must be accompanied by discernment to resist the historical pattern of centralization and ensure that the benefits of technology accrue to the many, not just the few <a class="yt-timestamp" data-t="01:41:50">[01:41:50]</a>. This requires acknowledging the risks and costs alongside the benefits in every technological transaction <a class="yt-timestamp" data-t="01:42:46">[01:42:46]</a>.