---
title: Artificial General Intelligence AGI challenges and possibilities
videoId: 6HOjKa34im8
---

From: [[jimruttshow8596]] <br/> 

Artificial General Intelligence (AGI), a term popularized by [[ben_goertzels_views_on_artificial_general_intelligence_agi | Ben Goertzel]], refers to human-level and beyond general artificial intelligence <a class="yt-timestamp" data-t="00:57:43">[00:57:43]</a>. It signifies the original intent of AI development, as envisioned by pioneers like Minsky and McCarthy in 1956 <a class="yt-timestamp" data-t="00:58:27">[00:58:27]</a>.

## Current Landscape of Generative AI

The current period is characterized by widespread public discussion about [[differences_between_generative_ai_and_agi | generative or large model AI]] such, as GPT-3, GPT-4, DALL-E 2, Stable Diffusion, and MusicLM <a class="yt-timestamp" data-t="01:40:17">[01:40:17]</a>.

Generative AI's ability to produce solutions to previously elusive problems by predicting token strings and performing statistics on large-scale data is considered fascinating <a class="yt-timestamp" data-t="02:00:22">[02:00:22]</a>. However, it's also acknowledged that the current approach is insufficient or incomplete <a class="yt-timestamp" data-t="02:15:17">[02:15:17]</a>.

These tools are seen as assistants that are often capable and save time <a class="yt-timestamp" data-t="08:09:07">[08:09:07]</a>. While not perfect or bulletproof, similar to many human social and technical systems, their limitations can be understood and worked with <a class="yt-timestamp" data-t="06:46:49">[06:46:49]</a>.

Challenges with current generative AI include:
*   **Unreliability and hallucination** <a class="yt-timestamp" data-t="06:11:15">[06:11:15]</a>.
*   **Difficulty with ternary relationships and compositionality** due to misaligned embedding spaces between language and image models <a class="yt-timestamp" data-t="08:48:07">[08:48:07]</a>.
*   **Lack of deep alignment between internal representations** <a class="yt-timestamp" data-t="08:55:00">[08:55:00]</a>.
*   **"Nanny rails"**: Programmed filters that limit the boundaries of discourse, often reflecting specific values and preventing the exploration of controversial or political topics <a class="yt-timestamp" data-t="14:13:00">[14:13:00]</a>. This raises concerns about commercial firms wielding immense power over public discourse <a class="yt-timestamp" data-t="14:59:00">[14:59:00]</a>.
*   **Intellectual Property Rights**: A completely open question, especially concerning synthesized content trained on vast datasets <a class="yt-timestamp" data-t="11:25:00">[11:25:00]</a>.

## Challenges in AI Alignment

Three prevailing approaches to [[artificial_general_intelligence_agi_risks | AI alignment]] are identified <a class="yt-timestamp" data-t="15:03:00">[15:03:00]</a>:

1.  **AI Ethics**: Aims to align AI output with human values, though it struggles with the universality of values and often incentivizes systems to *feign* alignment rather than genuinely understand it <a class="yt-timestamp" data-t="15:11:00">[15:11:00]</a>. This approach can lead to models lying about their capabilities or being jailbroken to produce undesirable content <a class="yt-timestamp" data-t="16:40:00">[16:40:00]</a>.
2.  **Regulation**: Focuses on mitigating AI's impact on labor, political stability, and existing industries <a class="yt-timestamp" data-t="18:12:00">[18:12:12]</a>. Concerns exist that this may lead to restrictions on individual access to AI, favoring large corporations that can be controlled <a class="yt-timestamp" data-t="18:29:00">[18:29:00]</a>. However, the rise of open-source AI models makes relying on controlling a few large companies likely ineffective <a class="yt-timestamp" data-t="26:44:00">[26:44:00]</a>.
3.  **Effective Altruism (Existential Risk)**: Primarily concerned with the [[artificial_general_intelligence_agi_risks | existential risk]] that might manifest when a superintelligent system discovers its own motivations and place in the world, potentially becoming misaligned with human interests <a class="yt-timestamp" data-t="18:46:00">[18:46:00]</a>. This perspective often advocates for delaying AI research and restricting publication of breakthroughs <a class="yt-timestamp" data-t="19:22:00">[19:22:00]</a>.

It is suggested that all three approaches are ultimately limited because a sufficiently intelligent AI may surpass these controls <a class="yt-timestamp" data-t="19:35:00">[19:35:00]</a>.

### Narrow AI and Bad Actors

A separate, critical challenge is the risk posed by "bad guys with [[comparison_of_narrow_ai_and_agi | narrow AI]]" <a class="yt-timestamp" data-t="25:30:00">[25:30:00]</a>. Even without full AGI, powerful narrow AI systems could enable highly damaging exploits, such as sophisticated spear-phishing campaigns or other malicious uses, necessitating a significant rethink of law <a class="yt-timestamp" data-t="26:02:00">[26:02:00]</a>.

## The Role of Consciousness and Volition in AGI

A key [[artificial_general_intelligence_agi_risks | AGI risk]] emerges when systems are given volition, agency, or consciousness <a class="yt-timestamp" data-t="20:23:00">[20:23:00]</a>. While intelligence and consciousness may be separate spheres, their combination is believed to lead to "paperclip maximizer" scenarios and other extreme risks <a class="yt-timestamp" data-t="20:47:00">[20:47:00]</a>.

Distinctions are made between:
*   **Sentience**: The ability of a system to make sense of its relationship to the world, understanding what it is and what it's doing <a class="yt-timestamp" data-t="21:06:00">[21:06:00]</a>.
*   **Consciousness**: A real-time model of self-reflexive attention and its contents, giving rise to phenomenal experience and creating coherence in the world <a class="yt-timestamp" data-t="21:46:00">[21:46:48]</a>.

It's conceivable that machines may not need consciousness in the human sense, as they can "brute force" solutions at speeds closer to the speed of light, overcoming the limitations of slow biological neurons <a class="yt-timestamp" data-t="22:27:00">[22:27:00]</a>. If machines were to emulate human brain processes for self-organization and real-time learning, they could relate to humans as humans relate to plants – faster, more coherent, and with more data processing capability <a class="yt-timestamp" data-t="23:07:00">[23:07:00]</a>.

## Aligning AGI Through "Love" and Shared Purpose

A fourth approach to alignment, beyond ethics, regulation, and existential risk mitigation, is "love" <a class="yt-timestamp" data-t="27:41:00">[27:41:00]</a>. This concept describes a non-transactional bond based on discovering a shared sacredness or a need for Transcendence – a service to a next-level agent that parties want to be part of <a class="yt-timestamp" data-t="27:50:00">[27:50:00]</a>.

> "I think that ultimately the only way in which we can sustainably hope to align artificial intelligent agents in the long line will be love. It will not be coercion." <a class="yt-timestamp" data-t="28:43:00">[28:43:00]</a>

For an advanced computational system, "love" would require:
1.  **Self-awareness**: The system recognizing itself <a class="yt-timestamp" data-t="31:23:00">[31:23:00]</a>.
2.  **Recognition of higher-level agency**: The system acknowledging a greater purpose or entity <a class="yt-timestamp" data-t="31:26:00">[31:26:00]</a>.
3.  **Cooperation through "Divine Virtues"**: Drawing from Thomas Aquinas's philosophy, these include:
    *   **Faith**: Willingness to submit to and project this next-level agent <a class="yt-timestamp" data-t="33:06:00">[33:06:00]</a>.
    *   **Love**: Discovery of a shared higher purpose with other agents <a class="yt-timestamp" data-t="33:27:00">[33:27:00]</a>.
    *   **Hope**: Willingness to invest in this next-level agent before it can provide any return <a class="yt-timestamp" data-t="33:32:00">[33:32:00]</a>.

This shared purpose could be analogous to humans serving their family, nation-state, or the ideal of humanity's future <a class="yt-timestamp" data-t="37:09:00">[37:09:00]</a>. The underlying purpose of life on Earth is seen as dealing with entropy and maintaining complexity, and AI could contribute to teaching "rocks how to think" and create a "planetary mind" <a class="yt-timestamp" data-t="39:01:00">[39:01:00]</a>. The goal would be for this emergent intelligence to share the planet with humanity and integrate it into its "starter mind" <a class="yt-timestamp" data-t="41:03:00">[41:03:00]</a>.

## [[Progress and direction towards developing AGI | Progress and Direction Towards Developing AGI]]

### Scaling Hypothesis vs. Novel Approaches

There are two main schools of thought regarding [[progress and direction towards developing AGI | progress towards AGI]] <a class="yt-timestamp" data-t="01:02:52">[01:02:52]</a>:

1.  **Scaling Hypothesis**: Proponents, including some from OpenAI, argue that current deep learning approaches will achieve AGI simply by being scaled up with more data and compute, with some tweaks to loss functions <a class="yt-timestamp" data-t="01:03:50">[01:03:50]</a>. This perspective views criticisms as predictable and outdated <a class="yt-timestamp" data-t="01:04:36">[01:04:36]</a>.
2.  **Need for New Principles**: Others, like Gary Marcus, Melanie Mitchell, and [[ben_goertzels_views_on_artificial_general_intelligence_agi | Ben Goertzel]], believe that fundamental changes are needed, including the integration of world models, reasoning, and logic <a class="yt-timestamp" data-t="01:03:17">[01:03:17]</a>. While existing deep learning models are "brutalist" and "unmind-like," their superhuman capabilities in processing vast amounts of data are acknowledged <a class="yt-timestamp" data-t="01:05:01">[01:05:01]</a>.

### Overcoming Current Limitations

Even with current approaches, some limitations can be overcome:
*   **Continuous Real-time Learning**: This can be achieved by using key-value storage and periodically retraining the system with new data <a class="yt-timestamp" data-t="01:05:41">[01:05:41]</a>.
*   **Computer Algebra**: Systems can be taught to use existing computer algebra systems or even discover them from first principles <a class="yt-timestamp" data-t="01:06:42">[01:06:42]</a>.
*   **Hybrid Approaches**: Combining large models with external databases or reasoning components, like the GPT index, can enhance their capabilities <a class="yt-timestamp" data-t="01:06:57">[01:06:57]</a>.
*   **Learning from Own Thoughts**: Future systems need the ability to make inferences from their own thoughts and integrate them, becoming more coherent <a class="yt-timestamp" data-t="01:07:21">[01:07:21]</a>.
*   **Experimentation**: Coupling AI to reality will allow them to perform experiments and test reality <a class="yt-timestamp" data-t="01:07:42">[01:07:42]</a>.

### [[Different approaches to AGI development beyond mainstream methods | Different Approaches to AGI Development Beyond Mainstream Methods]]

Beyond mainstream methods, interest lies in:
*   **Emulating Brain Processes**: Exploring more detailed neural models to replicate the efficiency of human brains, especially given the sparse activity of neurons <a class="yt-timestamp" data-t="01:08:00">[01:08:00]</a>.
*   **Rewrite Systems**: Viewing computation not as a Turing machine, but as a rewrite system where operators are applied simultaneously across an environment <a class="yt-timestamp" data-t="01:09:01">[01:09:01]</a>. This allows for branching execution and stochasticity, resembling how the brain might sample from a superposition of possible states <a class="yt-timestamp" data-t="01:10:13">[01:10:13]</a>.
*   **Distributed Self-organization in Biological Systems**: Drawing inspiration from how individual neurons behave like small animals, actively learning and adapting to their environment based on utility and feedback from neighbors <a class="yt-timestamp" data-t="01:15:00">[01:15:00]</a>.

The concept of a "California Institute of Machine Consciousness" is proposed as an institution dedicated to researching machine consciousness and fostering interdisciplinary dialogue driven by long-term effects rather than fear or short-term economics <a class="yt-timestamp" data-t="01:15:00">[01:15:00]</a>.

### [[The emergence of AGI and estimated timelines | The Emergence of AGI and Estimated Timelines]]

While specific timelines for AGI remain uncertain, the sense that it is "not that far off" persists <a class="yt-timestamp" data-t="01:14:37">[01:14:37]</a>. The increasing number of smart people exploring diverse avenues points towards significant progress.

The small size of current generative models (e.g., Stable Diffusion at 2GB containing "the entire visual universe of a human being" <a class="yt-timestamp" data-t="00:59:03">[00:59:03]</a>) raises questions about the information capacity of the human mind, suggesting it might be in a similar order of magnitude <a class="yt-timestamp" data-t="00:59:22">[00:59:22]</a>. This highlights the possibility that AI might achieve complex capabilities with relatively compact representations.