---
title: Impact of AI advancements on various industries
videoId: Z5dompWURVo
---

From: [[jimruttshow8596]] <br/> 

The field of [[the_development_and_influence_of_technology_and_ai_on_society | Artificial Intelligence]] (AI) is undergoing rapid transformation, with changes occurring "10 times faster" than the personal computer revolution of the late 1970s and early 1980s <a class="yt-timestamp" data-t="01:21:00">[01:21:00]</a>. This period is marked by "exponential acceleration" <a class="yt-timestamp" data-t="01:58:00">[01:58:00]</a>, leading to continuous and likely larger upheavals <a class="yt-timestamp" data-t="01:45:00">[01:45:00]</a>.

## Current Landscape of AI and Large Language Models (LLMs)

While significant advancements are seen in [[the_development_and_influence_of_technology_and_ai_on_society | AI]] technology, progress is not uniform across all sectors. For example, automatic checkout systems in supermarkets are still considered inefficient <a class="yt-timestamp" data-t="02:09:00">[02:09:00]</a>. However, advanced [[the_development_and_influence_of_technology_and_ai_on_society | AI]] technology in other areas is accelerating at an unprecedented pace <a class="yt-timestamp" data-t="02:14:00">[02:14:00]</a>.

Large Language Models (LLMs), in their current form as Transformer networks trained to predict the next token in a sequence, are not expected to achieve full human-level [[Future Directions and Challenges for AI and AGI | Artificial General Intelligence]] (AGI) on their own <a class="yt-timestamp" data-t="04:49:00">[04:49:00]</a>. Nevertheless, these systems are capable of "many amazing useful functions" <a class="yt-timestamp" data-t="04:58:00">[04:58:00]</a> and can serve as valuable components within systems aiming for [[Future Directions and Challenges for AI and AGI | AGI]] <a class="yt-timestamp" data-t="05:10:00">[05:10:00]</a>.

The distinction often lies in whether LLMs serve as the "integration hub" within a hybrid system or play a supporting role to another core architecture <a class="yt-timestamp" data-t="05:50:00">[05:50:00]</a>. Many interesting developments in the LLM space involve LLMs integrated with external tools, such as vector semantic databases or agentware <a class="yt-timestamp" data-t="07:02:00">[07:02:00]</a>.

## Specific Domain Impacts and Capabilities of LLMs

LLMs have demonstrated capabilities in various domains:

*   **Creative Industries:** Tools leveraging LLMs, like the "script helper" program, can generate movie scripts comparable to a first draft by a professional journeyman screenwriter <a class="yt-timestamp" data-t="34:49:00">[34:49:00]</a>. Music models can create original 12-bar blues guitar solos that are "not so boring" <a class="yt-timestamp" data-t="35:05:00">[35:05:00]</a>. However, these systems are not yet capable of producing the same level of original artistic creativity as leading human artists <a class="yt-timestamp" data-t="35:16:00">[35:16:00]</a>.
*   **Science and Mathematics:** While LLMs can "turn the crank" on advanced mathematical theories and flesh out calculus for new definitions, similar to what a master's or advanced undergraduate student might do <a class="yt-timestamp" data-t="38:48:00">[38:48:00]</a>, they are not yet capable of performing original, surprising scientific leaps or independently conducting the research for a PhD thesis <a class="yt-timestamp" data-t="35:56:00">[35:56:00]</a>. Writing scientific papers remains beyond their current capability <a class="yt-timestamp" data-t="36:57:00">[36:57:00]</a>.
*   **General Communication:** LLMs can competently write formal documents like resignation letters <a class="yt-timestamp" data-t="45:53:00">[45:53:00]</a>.

### Key Limitations of Current LLMs

Despite impressive capabilities, current LLMs exhibit certain limitations:

*   **Hallucinations:** A notable problem is the tendency of LLMs to generate factually incorrect or fabricated information, especially when asked obscure questions <a class="yt-timestamp" data-t="09:29:00">[09:29:00]</a>. While techniques exist to filter out these hallucinations by probing network activation patterns <a class="yt-timestamp" data-t="11:13:00">[11:13:00]</a>, this doesn't equate to the human ability of "reality discrimination" through reflective self-modeling <a class="yt-timestamp" data-t="12:12:00">[12:12:12]</a>. Research shows that correct answers often have different entropy than incorrect ones, a method that can reduce hallucinations by running queries multiple times <a class="yt-timestamp" data-t="13:51:00">[13:51:00]</a>.
*   **Banality:** The natural output of LLMs tends towards banality, reflecting an average of common utterances <a class="yt-timestamp" data-t="14:14:00">[14:14:00]</a>. While clever prompting can shift the output, it doesn't consistently achieve the level of a great human creative <a class="yt-timestamp" data-t="34:31:00">[34:31:00]</a>.
*   **Lack of Deep Judgment:** LLMs require human curation and original seed ideas, as they lack inherent deep judgment <a class="yt-timestamp" data-t="39:13:00">[39:13:00]</a>. Their architecture primarily recognizes "surface-level patterns" in data, not necessarily learning deeper abstractions in a human-like way <a class="yt-timestamp" data-t="32:33:00">[32:33:00]</a>.

## The [[artificial_intelligence_risk | AI]] Race and Future Directions for AGI

The development of [[Impact of algorithms and AI on society | AI]] has become a genuine "AGI race," with large companies investing significant resources <a class="yt-timestamp" data-t="20:03:00">[20:03:00]</a>. Companies like Google, with DeepMind, possess extensive expertise in neural networks and architectures like AlphaZero, which could be combined with Transformers for better planning and strategic thinking <a class="yt-timestamp" data-t="18:03:00">[18:03:03]</a>.

Potential [[potential_trajectories_of_ai_advancements | trajectories of AI advancements]] and architectural shifts to overcome current LLM limitations include:

*   **Increased Recurrence in Neural Networks:** Adding more recurrence into Transformer-based networks, and exploring alternative training methods like predictive coding (which is localized, unlike backpropagation), could lead to more interesting abstractions <a class="yt-timestamp" data-t="46:43:00">[46:43:00]</a>.
*   **Hybrid Architectures:** Combining elements like AlphaZero with neural knowledge graphs (e.g., in Differential Neural Computing) and recurrent Transformers represents a meaningful direction for [[Future Directions and Challenges for AI and AGI | AGI]] <a class="yt-timestamp" data-t="48:11:00">[48:11:00]</a>.
*   **Minimum Description Length Learning:** Architectures that explicitly try to learn abstractions by minimizing description length, coupled with Transformers, are also being explored <a class="yt-timestamp" data-t="49:31:00">[49:31:00]</a>.
*   **Evolutionary Algorithms:** There is "way too little work" being done on evolutionary algorithms for training neural networks, especially given the decreasing cost of computation <a class="yt-timestamp" data-t="51:11:00">[51:11:00]</a>. These methods could be more promising for richly recurrent networks <a class="yt-timestamp" data-t="52:20:00">[52:20:00]</a>.

One alternative [[innovative_approaches_in_ai_research | innovative approach in AI research]] is the **OpenCog Hyperon** project, which centers on a "weighted labeled metagraph" <a class="yt-timestamp" data-t="54:38:00">[54:38:00]</a>. This metagraph is a self-modifying, self-rewriting knowledge store where various [[the_development_and_influence_of_technology_and_ai_on_society | AI]] programs (including logical reasoning, procedural learning, and evolutionary programming) exist as subgraphs that transform the metagraph itself <a class="yt-timestamp" data-t="55:50:00">[55:50:00]</a>. LLMs can exist on the periphery of this system, but not as its central hub <a class="yt-timestamp" data-t="58:37:00">[58:37:00]</a>.

This approach emphasizes reflection, allowing the system to recognize patterns in its own mind and processes <a class="yt-timestamp" data-t="56:54:00">[56:54:00]</a>. It is particularly well-suited for scientific reasoning and evolutionary creativity <a class="yt-timestamp" data-t="59:47:00">[59:47:00]</a>. The primary challenge for OpenCog Hyperon is achieving scalable processing infrastructure <a class="yt-timestamp" data-t="01:00:40">[01:00:40]</a>, similar to how GPUs enabled the recent explosion of deep neural networks <a class="yt-timestamp" data-t="01:02:54">[01:02:54]</a>. The project is developing a pipeline from its native language, Meta, to highly efficient hardware, aiming to enable historical [[the_development_and_influence_of_technology_and_ai_on_society | AI]] paradigms to operate at scale <a class="yt-timestamp" data-t="01:02:03">[01:02:03]</a>.

## Conclusion

The [[economic_and_social_implications_of_ai | impact of algorithms and AI on society]] is continuously unfolding, driven by rapid advancements and diverse architectural approaches. While current LLMs offer significant utility in various industries, they also present challenges like hallucinations and a tendency towards banality, especially in complex reasoning and original creativity. The pursuit of [[Future Directions and Challenges for AI and AGI | AGI]] involves exploring hybrid systems, enhanced neural network architectures, and alternative learning paradigms like those in OpenCog Hyperon, all aiming to overcome current limitations and accelerate the development of more human-like or even superhuman intelligence. The pace of [[Impact of algorithms and AI on society | AI]] advancement ensures a dynamic and complex future for its integration across industries.