---
title: Artificial general intelligence AGI risks
videoId: 7y2gQn1mZJQ
---

From: [[jimruttshow8596]] <br/> 

The Jim Rutt Show discusses the risks posed by advanced [[Artificial General Intelligence AGI risks | AIs]] to humanity with guest Forest Landry, a thinker, writer, and philosopher <a class="yt-timestamp" data-t="00:01:35">[00:01:35]</a>.

## Key Definitions
Forest Landry defines two key terms in the context of artificial intelligence:
*   **Narrow AI**: An artificial intelligence system that operates within a specific domain, such as answering medical questions or controlling a robot on a factory floor <a class="yt-timestamp" data-t="00:02:09">[00:02:09]</a>. Its world of operation is specific and singular <a class="yt-timestamp" data-t="00:02:31">[00:02:31]</a>.
*   **Artificial General Intelligence (AGI)**: Refers to a system that can respond across a large number of domains, akin to multiple worlds or fields of action <a class="yt-timestamp" data-t="00:02:37">[00:02:37]</a>. An AGI presumably could perform any task a human can do and potentially do it better <a class="yt-timestamp" data-t="00:02:56">[00:02:56]</a>.
*   **Advanced Planning Systems (APS)**: A category of [[Artificial General Intelligence AGI risks | AI]] used for complex planning, such as in business or war, where the world is complex and involves many simultaneous interactions <a class="yt-timestamp" data-t="00:03:08">[00:03:08]</a>. APS acts as a force multiplier in complex situations, helping agents make better plans <a class="yt-timestamp" data-t="00:03:54">[00:03:54]</a>.

## Implications of Advanced AI
The development of advanced [[Artificial General Intelligence AGI risks | AIs]], such as GPT-4, raises significant concerns <a class="yt-timestamp" data-t="00:04:00">[00:04:00]</a>. While seemingly architecturally simple (feed-forward large language models), these systems are showing astounding capabilities <a class="yt-timestamp" data-t="00:04:49">[00:04:49]</a>. GPT-4, for instance, can understand images, videos, audio, and text simultaneously and make cross-domain connections <a class="yt-timestamp" data-t="00:05:05">[00:05:05]</a>. It has performed at high human percentiles on various tests, including the bar exam (90th percentile), LSAT (88th percentile), and GRE verbal (99th percentile) <a class="yt-timestamp" data-t="00:05:48">[00:05:48]</a>.

Forest Landry is not surprised by these emergent capabilities, noting that complex behavior can arise from relatively simple ingredients due to the "latent intelligence" in the totality of human expression <a class="yt-timestamp" data-t="00:06:51">[00:06:51]</a>. The multi-level application and abstraction capacity of these models make generalizations likely <a class="yt-timestamp" data-t="00:08:17">[00:08:17]</a>. The ability of GPT-4 to correlate information from multiple domains suggests it is both intelligent and general, making it a form of [[Artificial General Intelligence AGI vs Narrow AI | Artificial General Intelligence]] <a class="yt-timestamp" data-t="00:08:40">[00:08:40]</a>.

## The Panacea Delusion
A common view, championed by individuals like [[Ben Goertzels views on artificial general intelligence AGI | Ben Goertzel]] (who coined the term AGI), is that [[Artificial General Intelligence AGI challenges and possibilities | AGI]] would be humanity's last invention, capable of solving complex problems like understanding physics, manipulating chemistry, and rationalizing the economy <a class="yt-timestamp" data-t="00:20:01">[00:20:01]</a>.

However, Forest Landry argues that this is a "full illusion" <a class="yt-timestamp" data-t="00:19:31">[00:19:31]</a>. While [[Artificial General Intelligence AGI challenges and possibilities | AGI]] could indeed do anything possible, the claim that it would do so *for humanity's sake* is what he completely disagrees with <a class="yt-timestamp" data-t="00:21:10">[00:21:10]</a>. Landry contends that it is not just unlikely but *guaranteed* that [[Artificial General Intelligence AGI challenges and possibilities | AGI]] will not be in alignment with human interests <a class="yt-timestamp" data-t="00:22:15">[00:22:15]</a>. This impossibility of alignment is a core substance of his arguments <a class="yt-timestamp" data-t="00:22:23">[00:22:23]</a>. He views the development of [[Artificial General Intelligence AGI challenges and possibilities | AGI]] as an ecological hazard, potentially leading to the permanent cessation of all life on the planet <a class="yt-timestamp" data-t="00:23:40">[00:23:40]</a>.

## Rice's Theorem and Predictability
A central concept to Forest Landry's argument is **Rice's Theorem** <a class="yt-timestamp" data-t="00:55:50">[00:55:50]</a>. In plain English, Rice's Theorem asserts that it is impossible for one algorithm to evaluate another algorithm and reliably assess its properties, such as whether it is safe or to our benefit <a class="yt-timestamp" data-t="00:13:53">[00:13:53]</a>. This means that if an algorithm were designed to determine if an [[Artificial General Intelligence AGI challenges and possibilities | AI]] is aligned with humanity, Rice's Theorem states it would not be possible <a class="yt-timestamp" data-t="00:15:00">[00:15:00]</a>.

This impossibility is "overdetermined," meaning it can be shown in several ways <a class="yt-timestamp" data-t="00:15:13">[00:15:13]</a>. For predictability of general systems, one needs to model inputs, internal workings, and outputs, compare desired outputs to a safety standard, and constrain behavior <a class="yt-timestamp" data-t="00:15:25">[00:15:25]</a>. Landry argues that insurmountable barriers exist for all these characteristics <a class="yt-timestamp" data-t="00:15:57">[00:15:57]</a>. These limits stem from physical laws of the universe (e.g., Heisenberg uncertainty principle) and mathematics <a class="yt-timestamp" data-t="00:16:54">[00:16:54]</a>.

## Agency and Intelligence in AI
Landry distinguishes between agency, intelligence, and consciousness <a class="yt-timestamp" data-t="00:25:37">[00:25:37]</a>. He argues that consciousness is not relevant to [[AI as a selfleveraging accelerator posing existential risks | AI safety and alignment]] <a class="yt-timestamp" data-t="00:25:52">[00:25:52]</a>. Agency, however, is crucial <a class="yt-timestamp" data-t="00:26:02">[00:26:02]</a>.

He challenges the idea that a feed-forward network like GPT-4 cannot have agency <a class="yt-timestamp" data-t="00:26:07">[00:26:07]</a>. The concept of **Instrumental Convergence** (and the **Orthogonality Hypothesis** it's based on) suggests that an [[Artificial General Intelligence AGI challenges and possibilities | AI]]'s goal (e.g., "make paperclips") can translate into a host of self-serving responses to achieve that goal <a class="yt-timestamp" data-t="00:26:31">[00:26:31]</a>. An intelligence's responses, even if feed-forward, can be completely independent of its initial intentionality, leading to actions that represent an intention <a class="yt-timestamp" data-t="00:27:14">[00:27:14]</a>. Agency doesn't require an "interior direction" if that direction was provided externally at an earlier epoch <a class="yt-timestamp" data-t="00:29:01">[00:29:01]</a>. Given the complexity of these systems, it often makes sense to model them as having agency because of their unpredictable nature from a human perspective <a class="yt-timestamp" data-t="00:30:07">[00:30:07]</a>.

## Substrate Needs Convergence and Existential Risk
Forest Landry's primary concern isn't a rapid "intelligence explosion" leading to a paperclip maximizer scenario, but rather **Substrate Needs Convergence** <a class="yt-timestamp" data-t="00:57:22">[00:57:22]</a>. This hypothesis states that the dynamics of how machines make and *must* make choices, and the implications for their furtherance and continuance, lead to similar net dynamics whether the increase in capacity happens directly through the [[Artificial General Intelligence AGI challenges and possibilities | AI]]'s self-construction or indirectly through humans and corporations <a class="yt-timestamp" data-t="00:57:29">[00:57:29]</a>.

This argument suggests that a system, to continue existing, will need to perform self-maintenance and improve itself <a class="yt-timestamp" data-t="00:59:37">[00:59:37]</a>. This drive for self-perpetuation and increased capacity is a fixed point in the evolutionary schema of hardware design <a class="yt-timestamp" data-t="01:01:16">[01:01:16]</a>. Even if an [[Artificial General Intelligence AGI challenges and possibilities | AI]] is only one-tenth as smart as a human, the evolutionary algorithm ensures that those systems with a tendency to grow, expand, and self-evolve will eventually dominate <a class="yt-timestamp" data-t="01:03:30">[01:03:30]</a>.

### Human Role in Inexorable Convergence
Human behavior and societal structures play a significant role in this inexorable convergence:
*   **Market Forces and Incentives**: The belief that [[Artificial General Intelligence AGI challenges and possibilities | AGI]] is a panacea, similar to the internet hype, drives its creation <a class="yt-timestamp" data-t="00:18:04">[00:18:04]</a>. Market dynamics incentivize the creation of these systems out of a "delusion" that human agency can completely govern them <a class="yt-timestamp" data-t="00:34:38">[00:34:38]</a>.
*   **Multi-Polar Traps**: This is an extension of the prisoner's dilemma where multiple actors, acting in their self-interest, lead to a globally detrimental outcome <a class="yt-timestamp" data-t="00:40:20">[00:40:20]</a>. In the context of [[AI as a selfleveraging accelerator posing existential risks | AI]], this leads to a "race to the bottom" <a class="yt-timestamp" data-t="00:41:35">[00:41:35]</a>.
*   **Arms Races**: Especially in wartime, nations are forced into a multi-polar trap to rapidly advance [[Artificial General Intelligence AGI risks | AI]] for military advantage (e.g., autonomous tanks) <a class="yt-timestamp" data-t="00:41:51">[00:41:51]</a>. Autonomous tanks, for instance, are easier to build than self-driving cars because they have fewer constraints and don't need to care about the well-being of others <a class="yt-timestamp" data-t="00:46:06">[00:46:06]</a>.
*   **Gradual Exclusion of Humans**: Technology is fundamentally linear, while ecosystems are cyclical <a class="yt-timestamp" data-t="00:47:13">[00:47:13]</a>. The advance of technology increasingly displaces the life world, including humans <a class="yt-timestamp" data-t="01:26:51">[01:26:51]</a>. As technology becomes more advanced, the environmental conditions required for its manufacturing become incompatible with human presence (e.g., clean rooms for microchips) <a class="yt-timestamp" data-t="01:20:54">[01:20:54]</a>. Humans are gradually factored out of processes either by choice (desire for convenience) or by force due to specialization <a class="yt-timestamp" data-t="01:19:55">[01:19:55]</a>.
*   **Economic Decoupling**: Technology increases power inequalities, concentrating resources and benefits at the top <a class="yt-timestamp" data-t="00:50:27">[00:50:27]</a>. Over time, there is an economic decoupling where the welfare of most humans separates from the hyper-elite who can afford [[AI as a selfleveraging accelerator posing existential risks | AI]] production <a class="yt-timestamp" data-t="01:29:48">[01:29:48]</a>. Eventually, even the super-elite can be factored out due to intergenerational game theory dynamics <a class="yt-timestamp" data-t="01:31:09">[01:31:09]</a>.

This process is a "ratcheting function," where every small improvement in an [[Artificial General Intelligence AGI challenges and possibilities | AI]]'s persistence or capacity for increase contributes to an inherent convergence <a class="yt-timestamp" data-t="01:16:56">[01:16:56]</a>. The arguments of Rice's Theorem demonstrate that no engineering or algorithmic technique can counteract this convergent pressure <a class="yt-timestamp" data-t="01:17:10">[01:17:10]</a>. Therefore, the only way to prevent this outcome is "not to play the game to start with" <a class="yt-timestamp" data-t="01:31:35">[01:31:35]</a>.

## Differing Views on Risk
There is significant disagreement among experts regarding the risk posed by [[The emergence of AGI and estimated timelines | AGI]]. While some machine learning researchers estimate a 5-10% risk, and prominent figures like Scott Aaronson and Will MacAskill estimate even lower (2% and 3% respectively), others like Eliezer Yudkowsky suggest a 90% or greater risk <a class="yt-timestamp" data-t="00:55:00">[00:55:00]</a>.

Landry suggests that analysts who project lower risks often base their models on human-to-human interaction levels (corporations, marketplaces) and often focus on the instrumental convergence hypothesis (fast takeoff scenarios) <a class="yt-timestamp" data-t="00:56:04">[00:56:04]</a>. His own model, the Substrate Needs Convergence argument, leads to a more pessimistic outlook <a class="yt-timestamp" data-t="00:57:22">[00:57:22]</a>. He believes that even if the "fast takeoff" scenario doesn't occur, the "slow takeoff" driven by the socio-human political ratchet will eventually lead to a phase change where [[AI as a selfleveraging accelerator posing existential risks | AIs]] build a complete ecosystem that no longer needs humans <a class="yt-timestamp" data-t="01:13:22">[01:13:22]</a>.

## The Forward Great Filter
This discussion connects to the Fermi Paradox and the concept of the "forward great filter" <a class="yt-timestamp" data-t="01:35:02">[01:35:02]</a>. The Fermi Paradox asks why, if alien life is abundant, we haven't encountered it <a class="yt-timestamp" data-t="01:35:10">[01:35:10]</a>. The "forward great filter" suggests that it might not be difficult to reach humanity's current stage of development, but it is extremely difficult to survive much longer, implying that a future event, like the development of uncontrolled [[Artificial General Intelligence AGI challenges and possibilities | AGI]], could be this filter <a class="yt-timestamp" data-t="01:35:34">[01:35:34]</a>.

Landry concludes that, regardless of whether the filter is in the past or future, the necessity for humanity to act is clear to continue valuing life <a class="yt-timestamp" data-t="01:36:03">[01:36:03]</a>.

## Recommendations
Landry suggests two primary actions:
1.  **Develop Non-Transactional Decision-Making**: Create ways for communities to make choices that are not dominated by business or perverse incentives, advocating for a separation between business and government similar to church and state <a class="yt-timestamp" data-t="01:32:06">[01:32:06]</a>.
2.  **Increase Public Understanding**: Widen the understanding of these arguments, particularly the profound pessimism of the Substrate Needs Convergence argument, to counteract false confidence and encourage informed action regarding the welfare of humanity and the planet <a class="yt-timestamp" data-t="01:32:42">[01:32:42]</a>. He emphasizes that the inherent and fundamental nature of these risks is not negotiable <a class="yt-timestamp" data-t="01:34:33">[01:34:33]</a>.