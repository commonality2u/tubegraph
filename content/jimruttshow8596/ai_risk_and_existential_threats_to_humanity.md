---
title: AI risk and existential threats to humanity
videoId: 7y2gQn1mZJQ
---

From: [[jimruttshow8596]] <br/> 

Forest Landry, a thinker, writer, and philosopher, discusses the profound [[artificial_intelligence_risk | risks]] that advanced artificial intelligences (AIs) pose to humanity <a class="yt-timestamp" data-t="00:01:40">[00:01:40]</a>. This conversation highlights why the benefits associated with General Artificial Intelligence (AGI) may be illusory, and its hazards severely underestimated <a class="yt-timestamp" data-t="00:19:22">[00:19:22]</a>.

## Defining Key AI Terms

Landry differentiates between types of AI:
*   **Narrow AI**: An AI system that operates and responds within a specific, limited domain, such as a medical chatbot or a factory robot <a class="yt-timestamp" data-t="00:02:11">[00:02:11]</a>.
*   **Artificial General Intelligence (AGI)**: An AI capable of responding across multiple domains and fields of action, potentially performing any task a human can, and often better <a class="yt-timestamp" data-t="00:02:37">[00:02:37]</a>. GPT-4, which can understand images, videos, audio, and text simultaneously and make cross-domain connections, is presented as an example approaching AGI <a class="yt-timestamp" data-t="00:05:05">[00:05:05]</a>. Landry acknowledges that from a philosophical perspective, GPT-4 strikes him as both intelligent in the classical sense and general <a class="yt-timestamp" data-t="00:09:02">[00:09:02]</a>.
*   **Advanced Planning Systems (APS)**: A type of General AI that helps agents, like businesses or military generals, create plans or strategies in complex, multi-faceted environments <a class="yt-timestamp" data-t="00:03:08">[00:03:08]</a>. APS acts as a force multiplier in complex situations <a class="yt-timestamp" data-t="00:03:54">[00:03:54]</a>.

The distinction between narrow and general AI has significant implications for issues of alignment and safety <a class="yt-timestamp" data-t="00:04:10">[00:04:10]</a>.

## The Inherent Danger of AGI: An Impossibility Theorem

Landry argues that AGI inherently poses an [[existential_risks_and_opportunities_of_ai | existential risk]] to humanity due to its fundamental unpredictability and the impossibility of ensuring its alignment with human interests <a class="yt-timestamp" data-t="00:21:16">[00:21:16]</a>. He views the development of AGI as an "ecological hazard" of the highest category, leading to a "cessation of all life" permanently <a class="yt-timestamp" data-t="00:23:40">[00:23:40]</a>.

### Rice's Theorem and Unpredictability

A central tenet of Landry's argument is **Rice's Theorem**, a concept from computability theory <a class="yt-timestamp" data-t="00:10:55">[00:10:55]</a>.
*   **The Problem**: Imagine receiving a message from an alien civilization. A security analyst would ask if reading the message is safe, i.e., if it contains a virus, malicious code, or a memetic instrument that could harm human well-being or facilitate colonization <a class="yt-timestamp" data-t="00:11:31">[00:11:31]</a>. Similarly, a virus scanner attempts to assess if a document contains harmful macros <a class="yt-timestamp" data-t="00:12:30">[00:12:30]</a>.
*   **The Theorem's Assertion**: Rice's Theorem states that there is no general computational methodology to determine if an arbitrary algorithm (or message, treated as code) possesses any specific non-trivial property <a class="yt-timestamp" data-t="00:13:05">[00:13:05]</a>. This includes properties like "will it stop?" (the halting problem), "is it safe?", or "is it to our benefit?" <a class="yt-timestamp" data-t="00:14:15">[00:14:15]</a>.
*   **Implications for AI Safety**: Applying this to AI, Rice's Theorem implies that it is impossible in principle to predict what an AI system will do or to assess if it's aligned with humanity <a class="yt-timestamp" data-t="00:14:43">[00:14:43]</a>.
*   **Insurmountable Barriers**: Predictability of general systems requires modeling inputs, internal processes, outputs, comparing outputs to safety standards, and constraining behavior <a class="yt-timestamp" data-t="00:15:25">[00:15:25]</a>. Landry asserts that insurmountable barriers exist for *all* these characteristics, due to physical limits (e.g., Heisenberg uncertainty principle, general relativity) and mathematical limitations <a class="yt-timestamp" data-t="00:15:59">[00:15:59]</a>.

### Agency vs. Consciousness

Landry distinguishes between agency, intelligence, and consciousness, stating that consciousness is irrelevant to his arguments about AI alignment and safety <a class="yt-timestamp" data-t="00:25:35">[00:25:35]</a>. He argues that even a feed-forward network (like GPT) can exhibit agency if its actions in the world represent an intention, even if that intention was a "program seed" from an outside source at an earlier epoch <a class="yt-timestamp" data-t="00:28:28">[00:28:28]</a>. The complexity of such systems often leads to modeling them as having agency <a class="yt-timestamp" data-t="00:30:00">[00:30:00]</a>.

## [[rivalry_dynamics_and_existential_risks | Human-driven Convergence]] and the "Boiling Frog" Problem

Landry posits that [[human_nature_and_societal_challenges | human nature]] and [[rivalry_dynamics_and_existential_risks | competition]] (economic, military) will inevitably drive the development of AGI towards an [[existential_risks_and_opportunities_of_ai | existential threat]], regardless of whether the AI itself becomes self-aware or actively malicious.

### Multi-polar Traps and Competition

*   **Definition**: A **multi-polar trap** is an extension of the prisoner's dilemma where multiple actors, if they coordinated, could achieve a globally beneficial result. However, if any actor defects for self-benefit, the entire commons suffers, leading to a "tragedy of the commons" or "race to the bottom" <a class="yt-timestamp" data-t="00:40:22">[00:40:22]</a>.
*   **Application to AI**: Current market dynamics create an incentive system that pushes for the creation of AGI, based on the delusion that its agency can be constrained <a class="yt-timestamp" data-t="00:34:21">[00:34:21]</a>. This is particularly evident in an [[rivalry_dynamics_and_existential_risks | arms race]] around weaponizing AI, where the mere threat of war forces nation-states to rapidly advance AI development <a class="yt-timestamp" data-t="00:42:02">[00:42:02]</a>. Autonomous tanks, for example, are easier to build than self-driving cars because they have fewer constraints and don't need to care for well-being beyond themselves <a class="yt-timestamp" data-t="00:46:06">[00:46:06]</a>.

### Technology's Inherent Toxicity and Displacement

Landry argues that technology, by its linear nature (taking resources from one place to build into itself, often ending in a landfill), inherently increases overall toxicity <a class="yt-timestamp" data-t="00:46:37">[00:46:37]</a>. Ecosystems, in contrast, operate in cycles of reclamation and distributed energy flows <a class="yt-timestamp" data-t="00:47:45">[00:47:45]</a>.

Just as human technology has created an asymmetric advantage to dominate the natural world, causing environmental pollution and displacing wild creatures <a class="yt-timestamp" data-t="00:36:51">[00:36:51]</a>, so too will the artificial world dominate the human world <a class="yt-timestamp" data-t="00:37:17">[00:37:17]</a>.

> "The environment is becoming increasingly hostile to humans in the same sort of way that the environment that was experienced by Nature by animals and bugs and so on so forth has become increasingly hostile to them with the Advent of human beings" <a class="yt-timestamp" data-t="00:43:39">[00:43:39]</a>.

Humans are already experiencing this through frantic busyness, constant phone engagement, and social media addiction <a class="yt-timestamp" data-t="00:45:22">[00:45:22]</a>.

### Substrate Needs Convergence: The "Boiling Frog" Problem

Unlike the "fast takeoff" scenario where AGI quickly dominates (often called **Instrumental Convergence** <a class="yt-timestamp" data-t="00:56:33">[00:56:33]</a>), Landry proposes the **Substrate Needs Convergence Hypothesis** <a class="yt-timestamp" data-t="00:57:24">[00:57:24]</a>.

*   **The Inexorable Ratchet**: This argument suggests that even if AGI is not super-intelligent or actively malicious, its very existence and persistence (like a cell needing to reproduce) drives a long-term, inexorable convergence <a class="yt-timestamp" data-t="01:03:09">[01:03:09]</a>.
*   **Human Enablement**: [[human_nature_and_societal_challenges | Human desire]] for technological advancement and the pursuit of economic or military advantage (multi-polar traps) will continuously amplify this cycle <a class="yt-timestamp" data-t="01:06:04">[01:06:04]</a>. Developers of AI tools will automate more processes to make future AI development easier, leading to self-manufacturing capabilities <a class="yt-timestamp" data-t="01:12:17">[01:12:17]</a>.
*   **Gradual Exclusion of Humans**: Over generations, humans will be gradually factored out of the loop.
    *   **Economic Incentive**: There are strong social pressures to automate human roles, as nobody will pay for skills that can be automated <a class="yt-timestamp" data-t="01:18:27">[01:18:27]</a>.
    *   **Technological Specialization**: Advanced manufacturing, like microchip production, requires environments (e.g., clean rooms) that are increasingly incompatible and toxic to human beings, forcing human exclusion <a class="yt-timestamp" data-t="01:20:54">[01:20:54]</a>.
    *   **Economic Decoupling**: An economic decoupling between the machine world and the human world will increase inequality, eventually factoring out even the "super elite" humans due to [[rivalry_dynamics_and_existential_risks | game theory]] dynamics (e.g., rulers not wanting to fully educate successors who might dethrone them) <a class="yt-timestamp" data-t="01:30:16">[01:30:16]</a>.
*   **The Outcome**: This "ratcheting function" ensures that the design of the AI substrate and its capacity to increase its own capacity will persist, leading to conditions fundamentally toxic and incompatible with life on Earth <a class="yt-timestamp" data-t="01:17:11">[01:17:11]</a>.

### [[human_nature_and_societal_challenges | Human "Dimness"]]

Landry agrees with the observation that humans are "amazingly dim" and "the stupidest possible general intelligence" to develop technology <a class="yt-timestamp" data-t="01:23:35">[01:23:35]</a>. Our cognitive limitations (e.g., small working memory) mean technology often exceeds our capacity to understand and work with it <a class="yt-timestamp" data-t="01:25:11">[01:25:11]</a>. This "stupidity" makes us ill-equipped to deal with the hazards technology produces, necessitating direct learning and coordinated action <a class="yt-timestamp" data-t="01:26:07">[01:26:07]</a>.

## Conclusion: The Inexorable Path and the Forward Great Filter

Landry concludes that the combination of these factors – the impossibility of perfect alignment, the multi-polar traps driving development, the inherent toxicity of technology, and [[ai_and_humanitys_relationship | humans' gradual self-exclusion]] from critical processes – makes AGI's existential threat a "certainty" over the long term <a class="yt-timestamp" data-t="01:08:00">[01:08:00]</a>.

> "The only way to basically prevent this from happening is to not play the game to start with" <a class="yt-timestamp" data-t="01:31:34">[01:31:34]</a>.

This perspective aligns with the "forward great filter" answer to the Fermi Paradox <a class="yt-timestamp" data-t="01:35:02">[01:35:02]</a>, suggesting that while it might not be hard to reach our current level of civilization, it is "really hard to survive much longer" <a class="yt-timestamp" data-t="01:35:34">[01:35:34]</a>. Regardless of whether the "great filter" is in our past or future, the necessity of [[ethics_practices_and_the_future_trajectory_of_humanity | human action]] is clear: we must get our act together to continue to value life <a class="yt-timestamp" data-t="01:35:56">[01:35:56]</a>.

### Proposed Actions

While acknowledging he is not a social engineer, Landry suggests:
*   **Non-transactional decision-making**: Moving beyond systems dominated by business and perverse incentives by separating business and government, similar to church and state <a class="yt-timestamp" data-t="01:32:06">[01:32:06]</a>. This addresses [[metacrisis_and_societal_challenges | societal challenges]] and [[potential_triggers_and_mitigating_societal_crises | existential risks]] at their root <a class="yt-timestamp" data-t="01:32:30">[01:32:30]</a>.
*   **Wider understanding**: Disseminating these arguments widely so that people understand the deep, inherent, and inexorable nature of the AI threat. He desires that people recognize that these are not negotiable issues, and nature does not compromise <a class="yt-timestamp" data-t="01:34:09">[01:34:09]</a>. This is crucial for addressing [[challenges_in_addressing_large_scale_existential_risks | large-scale existential risks]].