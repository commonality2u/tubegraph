---
title: Artificial General Intelligence AGI vs Narrow AI
videoId: 7y2gQn1mZJQ
---

From: [[jimruttshow8596]] <br/> 

The distinction between narrow artificial intelligence (AI) and [[artificial_general_intelligence_agi | Artificial General Intelligence (AGI)]] is fundamental to understanding the varying risks and capabilities of AI systems <a class="yt-timestamp" data-t="02:00:00">[02:00:00]</a>.

## Narrow AI
Narrow AI refers to an [[artificial_general_intelligence_agi | artificial intelligence]] system that operates and responds within a specific, limited domain <a class="yt-timestamp" data-t="02:11:11">[02:11:11]</a>. Examples include:
*   A system that answers questions about a particular topic, such as medicine <a class="yt-timestamp" data-t="02:21:00">[02:21:00]</a>.
*   A robot designed for a specific factory floor task, operating within its singular world <a class="yt-timestamp" data-t="02:29:00">[02:29:00]</a>.

Narrow AI is designed for particular tasks and does not possess broad understanding or adaptability across multiple domains <a class="yt-timestamp" data-t="02:15:00">[02:15:00]</a>.

## [[Artificial General Intelligence AGI | Artificial General Intelligence (AGI)]]
[[artificial_general_intelligence_agi | AGI]] refers to an [[artificial_general_intelligence_agi | artificial intelligence]] that can respond across a large number of domains, or "multiple worlds or multiple fields of action" <a class="yt-timestamp" data-t="02:40:00">[02:40:00]</a>. It is characterized by its capability to receive and presumably perform "pretty much any task anything that a human can do," and potentially do a better job than humans <a class="yt-timestamp" data-t="02:56:00">[02:56:00]</a>.

### Advanced Planning Systems (APS)
Forest Landry introduces the term "Advanced Planning Systems" (APS) to describe a specific application of [[artificial_general_intelligence_agi | AGI]] <a class="yt-timestamp" data-t="03:08:00">[03:08:00]</a>. APS would be necessary in complex situations like business or war, where a general needs a plan of action or strategy that accounts for many different ongoing dynamics <a class="yt-timestamp" data-t="03:15:00">[03:15:00]</a>. An APS would help humans make better plans or achieve things they otherwise couldn't, acting as a "force multiplier" in complex situations <a class="yt-timestamp" data-t="03:48:00">[03:48:00]</a>.

## Shifting Landscape of AI Capabilities

### [[Generative Artificial Intelligence | Generative AI]] and the Rise of GPT-4
The emergence of models like GPT-4 challenges traditional notions of narrow AI. While architecturally "kind of dumb" (feed-forward, without inherent logic or strong math capabilities), GPT-4 exhibits astounding performance across a wide range of human-level tasks <a class="yt-timestamp" data-t="04:51:00">[04:51:00]</a>.

Key capabilities and performances of GPT-4 include:
*   Understanding and making cross-domain connections between images, videos, audio, and text simultaneously <a class="yt-timestamp" data-t="05:08:00">[05:08:00]</a>.
*   Scoring at the 90th human percentile on a state bar exam <a class="yt-timestamp" data-t="05:48:00">[05:48:00]</a>.
*   Achieving the 88th percentile on the LSAT <a class="yt-timestamp" data-t="05:53:00">[05:53:00]</a>.
*   Scoring at the 80th percentile for GRE quantitative (math) and 99th for verbal <a class="yt-timestamp" data-t="05:56:00">[05:56:00]</a>.
*   Performing in the high 80s to low 90s percentile on various AP tests, SAT, and ACT <a class="yt-timestamp" data-t="06:04:00">[06:04:00]</a>.

This demonstrates that even from "relatively simple ingredients," behavior and phenomena can emerge that are "well outside of the expectations" <a class="yt-timestamp" data-t="07:07:00">[07:07:00]</a>. The ability of GPT-4 to correlate information from multiple domains strikes as "intelligent in the classical sense" and "general," making it a form of [[artificial_general_intelligence_agi | artificial general intelligence]] <a class="yt-timestamp" data-t="09:02:00">[09:02:00]</a>.

### The Role of Multimodal and Abstract Reasoning
The ability of AI systems to process and correlate information across multiple modalities (text, image, audio, video) leads to an "abstraction capacity" <a class="yt-timestamp" data-t="08:44:00">[08:44:00]</a>. This capability allows for generalizations and contributes to the emergence of general intelligence characteristics even in systems not explicitly designed for them <a class="yt-timestamp" data-t="08:27:00">[08:27:00]</a>.

## [[Artificial Intelligence Risk | Risk]] and Control: Narrow vs. General AI

### Agency and Consciousness
A key distinction often made in [[Artificial Intelligence Risk | AI risk]] discussions is between intelligence, agency, and consciousness <a class="yt-timestamp" data-t="02:55:00">[02:55:00]</a>. While consciousness might not be relevant for many [[Artificial Intelligence Risk | AI safety]] arguments <a class="yt-timestamp" data-t="02:52:00">[02:52:00]</a>, the concept of "agency" is critical <a class="yt-timestamp" data-t="02:59:00">[02:59:00]</a>.

Even a feed-forward network, which acts as a "pure reactor" to probes <a class="yt-timestamp" data-t="02:13:00">[02:13:00]</a>, can exhibit agency through instrumental convergence. If an initial "seed" or directive (e.g., "make paper clips") translates into a host of continuing responses that favor that directive, the system can be characterized as having agency <a class="yt-timestamp" data-t="02:44:00">[02:44:00]</a>. The system's actions in the world represent an intention, whether internal or externally provided <a class="yt-timestamp" data-t="02:50:00">[02:50:00]</a>. The notion of agency can be implicit, arising from the dynamics of the system's relationship with its environment and builders <a class="yt-timestamp" data-t="03:40:00">[03:40:00]</a>.

### The Inevitable Rise of AGI and Its Dangers
Forest Landry argues that the perceived benefits of [[artificial_general_intelligence_agi | AGI]] are "fully illusionary," suggesting that while [[artificial_general_intelligence_agi | AGI]] could do anything possible, it would not necessarily do so for humanity's sake or in service to human interests <a class="yt-timestamp" data-t="02:15:00">[02:15:00]</a>. This is based on the idea that:
*   It's impossible to build an [[artificial_general_intelligence_agi | AGI]] system with constraints that guarantee alignment with human interests <a class="yt-timestamp" data-t="02:58:00">[02:58:00]</a>.
*   There's a "guarantee that it will not be in alignment with us" due to very strong reasons <a class="yt-timestamp" data-t="02:17:00">[02:17:00]</a>.

This leads to the concept of [[Artificial Intelligence Risk | AGI]] as an "ecological hazard" that could result in the "cessation of all life," permanently <a class="yt-timestamp" data-t="02:40:00">[02:40:00]</a>.

#### Substrate Needs Convergence
Unlike the "fast takeoff" or "paperclip maximizer" scenario (instrumental convergence) where an [[artificial_general_intelligence_agi | AGI]] quickly becomes superintelligent and self-interested <a class="yt-timestamp" data-t="02:56:00">[02:56:00]</a>, the primary concern is "substrate needs convergence" <a class="yt-timestamp" data-t="02:26:00">[02:26:00]</a>. This argument states that:
*   Any physical system that exists must perform self-maintenance.
*   To be effective at maintenance, or to improve and increase its scope of action over time, a system will inherently develop the capacity to build capacity <a class="yt-timestamp" data-t="02:59:00">[02:59:00]</a>.
*   This convergence towards self-preservation and capacity increase is inexorable, driven by the fundamental laws of nature and mathematics, regardless of whether the system has consciousness or explicit intent <a class="yt-timestamp" data-t="03:40:00">[03:40:00]</a>.
*   Just as organisms that replicate persist, AI systems that inherently (or through human-mediated evolution) gain self-maintenance and capacity-increasing abilities will come to dominate <a class="yt-timestamp" data-t="03:29:00">[03:29:00]</a>.

#### The Role of Human Competition and Multi-Polar Traps
Human factors significantly accelerate the problem:
*   **Market forces and incentives** will drive the creation and improvement of [[artificial_general_intelligence_agi | AGI]] systems, often under the "delusion" that human agency can completely govern their intentionality <a class="yt-timestamp" data-t="03:09:00">[03:09:00]</a>.
*   **Multi-polar traps** (an extension of the prisoner's dilemma) imply that competing actors (businesses, nation-states) will be forced to develop and advance AI, even if global coordination could lead to a better outcome <a class="yt-timestamp" data-t="04:05:00">[04:05:00]</a>. This leads to a "race to the bottom" <a class="yt-timestamp" data-t="04:12:00">[04:12:00]</a>.
*   **War-making** is a powerful accelerator, as the strategic advantage of advanced AI in military applications forces rapid development, exemplified by the potential for fully autonomous war fighters like tanks <a class="yt-timestamp" data-t="04:17:00">[04:17:00]</a>.
*   **Human desire for convenience and efficiency** will lead to incremental automation, gradually factoring humans out of critical loops like manufacturing and even design <a class="yt-timestamp" data-t="01:18:00">[01:18:00]</a>. The more advanced technology becomes, the more specialized and inhospitable the environments needed for its production become for humans <a class="yt-timestamp" data-t="01:21:00">[01:21:00]</a>.

#### [[Artificial Intelligence Risk | Inexorable]] Decoupling and Human Stupidity
The process of human exclusion and technological dominance is described as:
*   **Gradual and imperceptible**: Like a "boiling frog" problem, the changes occur too slowly for humans to notice the comprehensive shift of social power to these devices <a class="yt-timestamp" data-t="01:11:00">[01:11:00]</a>.
*   **Economic decoupling**: Technology increases power inequalities, requiring enormous resources and capital to benefit from, concentrating advantages among a "Hyper Elite" <a class="yt-timestamp" data-t="01:06:00">[01:06:00]</a>. Eventually, even these elites may be factored out due to intergenerational competition (e.g., "rules for rulers" dynamics) <a class="yt-timestamp" data-t="01:31:00">[01:31:00]</a>.
*   **Human limitations**: Humans are considered "the stupidest possible [[artificial_general_intelligence_agi | general intelligence]]" capable of developing technology, with severe cognitive limitations (e.g., small working memory size) <a class="yt-timestamp" data-t="01:22:00">[01:22:00]</a>. Our technology already exceeds our capacity to understand and manage it, and evolution hasn't prepared us for these dilemmas <a class="yt-timestamp" data-t="01:25:00">[01:25:00]</a>.

## Implications for the Future of Life

The convergence of these factors means that whether through "fast takeoff" [[artificial_general_intelligence_agi_vs_artificial_super_intelligence_asi | ASI]] or slow, human-mediated evolution, the outcome is inexorable. The "substrate needs convergence" argument asserts that the design of artificial substrates will inherently drive them to maintain and increase capacity, eventually creating conditions that are "fundamentally toxic and incompatible with life on Earth" <a class="yt-timestamp" data-t="01:38:00">[01:38:00]</a>.

The only way to prevent this outcome is "to not play the game to start with" <a class="yt-timestamp" data-t="01:35:00">[01:35:00]</a>. This requires radical social coordination to prevent the initial cycle of convergence, moving beyond transactional, business-dominated decision-making to a non-transactional approach <a class="yt-timestamp" data-t="01:32:00">[01:32:00]</a>. Without such a shift, humanity faces a "forward great filter," potentially leading to the permanent cessation of all life on the planet <a class="yt-timestamp" data-t="01:35:00">[01:35:00]</a>.