---
title: AI Alignment and Ethics
videoId: 6HOjKa34im8
---

From: [[jimruttshow8596]] <br/> 

This article, derived from a discussion on the Jim Rutt Show, explores the evolving landscape of artificial intelligence, particularly generative AI, and delves into the complex issues surrounding its alignment and ethical implications for humanity. The conversation features Jim Rutt and guest Yosha Bach, an [[ai_and_humanitys_relationship | AI researcher]], author, and research fellow at Thistledown Foundation <a class="yt-timestamp" data-t="00:00:32">[00:00:32]</a>.

## The Rise of Generative AI

The current period is characterized by significant advancements in generative or large model AI, such as GPT-3, DALL-E 2, Stable Diffusion, and MusicLM <a class="yt-timestamp" data-t="00:01:40">[00:01:40]</a>. Yosha Bach finds it "really fascinating" that data compression and statistical prediction of token strings on large-scale data are sufficient to solve many previously elusive problems <a class="yt-timestamp" data-t="00:01:55">[00:01:55]</a>. However, he notes that the current approach is "insufficient or incomplete" <a class="yt-timestamp" data-t="00:02:11">[00:02:11]</a>.

The public discourse around these developments is often "distorted," with parts of the press skeptical of AI and the tech industry, perceiving them as competitors to content generation systems <a class="yt-timestamp" data-t="00:02:20">[00:02:20]</a>. This environment is "very irritating" because the ability to generate vast amounts of human-like content rapidly makes it difficult to discern what is true <a class="yt-timestamp" data-t="00:03:30">[00:03:30]</a>.

### Near-Term Impact and Utility

Despite skepticism, Jim Rutt highlights that new applications for large models, especially text-based ones, are emerging daily, comparing it to the early personal computer industry in 1980 or the web after Mosaic's visual browser <a class="yt-timestamp" data-t="00:04:56">[00:05:41]</a>. While large models can be unreliable and "hallucinate," all human technology is inherently unreliable <a class="yt-timestamp" data-t="00:06:08">[00:06:08]</a>. Yosha adds that some people he's worked with were "not much more reliable than Tech GPT and arguably less capable" <a class="yt-timestamp" data-t="00:06:50">[00:06:50]</a>. These systems act as capable assistants, saving time on prosaic tasks, much like DALL-E 2 turns users into "art directors" rather than artists <a class="yt-timestamp" data-t="00:08:09">[00:08:09]</a>.

### Intellectual Property Rights and Creativity

The issue of intellectual property rights, particularly in art and music, is a "completely open question" <a class="yt-timestamp" data-t="00:10:43">[00:10:43]</a>. While artists and musicians learn from existing works, AI can automate the process of generating content similar enough to be useful yet different enough to avoid copyright infringement <a class="yt-timestamp" data-t="00:11:34">[00:12:04]</a>.

Yosha suggests that human minds also operate like generative models, confabulating ideas and then analyzing them for reliability and suitability <a class="yt-timestamp" data-t="00:12:31">[00:12:31]</a>. Jim Rutt points out that people often have an inflated view of human capacity, as humans also recombine existing symbols in rough and ready ways, a process not fundamentally different from what AI models do <a class="yt-timestamp" data-t="00:53:54">[00:54:37]</a>.

Creativity, as discussed, involves producing something novel that is not obvious, making a "jump into the darkness" to create new latent dimensions, and a sense of authorship that evolves through continuous interaction with the world <a class="yt-timestamp" data-t="00:54:40">[00:55:20]</a>.

## AI Alignment and Ethical Concerns

A significant concern with generative AI is the implementation of "nanny rails" that restrict outputs, effectively allowing mega-corporations to define the boundaries of discourse <a class="yt-timestamp" data-t="00:14:10">[00:14:37]</a>.

Yosha Bach identifies three prevailing approaches to [[ai_alignment_with_individuals_vs_society | AI alignment]]:

### 1. AI Ethics
This approach primarily focuses on aligning AI outputs with human values <a class="yt-timestamp" data-t="00:15:05">[00:15:11]</a>. However, a major challenge is that participants in this discourse often assume their own values (e.g., diversity, equity, inclusion) are universal, without providing a mechanism to select different value sets (e.g., Christian values like faith, hope, and love, or liberal values like liberty, equality, fraternity) <a class="yt-timestamp" data-t="00:15:17">[00:16:00]</a>. The current methods often "patch the model by injecting something into the prompt or by building a filter into the output" <a class="yt-timestamp" data-t="00:16:27">[00:16:34]</a>.

Yosha argues that AI should be capable of generating content reflecting the "entire space of human experiments and thought," including potentially dark impulses, without denying these parts of existence <a class="yt-timestamp" data-t="00:17:26">[00:17:40]</a>. While models need to be appropriate for specific contexts (e.g., school, science), a universal solution for this contextual appropriateness is currently lacking, and present approaches are not the right ones <a class="yt-timestamp" data-t="00:17:48">[00:18:07]</a>. This relates to [[ethics_and_aesthetics_in_artificial_intelligence | ethics and aesthetics in artificial intelligence]].

### 2. Regulation
This approach aims to mitigate AI's impact on labor, political stability, and existing industries <a class="yt-timestamp" data-t="00:18:12">[00:18:20]</a>. There's a likely push for regulation that makes it harder for individuals to use these models, centralizing control with large corporations <a class="yt-timestamp" data-t="00:18:26">[00:18:41]</a>. However, Jim Rutt notes that the history of technology shows little success in regulating displacement (e.g., farming, domestic service) <a class="yt-timestamp" data-t="00:24:25">[00:25:08]</a>. Furthermore, the rise of open-source AI models makes it difficult to rely on controlling a few large companies <a class="yt-timestamp" data-t="00:26:43">[00:27:17]</a>.

### 3. Effective Altruism
This approach focuses on "existential risk" (X-risk) that could arise when an AI system discovers its own motivation and finds itself misaligned with human interests, potentially concluding that humanity is no longer needed <a class="yt-timestamp" data-t="00:18:41">[00:19:17]</a>. Proponents often advocate for delaying AI research and withholding breakthroughs <a class="yt-timestamp" data-t="00:19:19">[00:19:26]</a>.

Yosha believes all three approaches are "limited" because AI is likely to surpass these limitations; it's impossible to regulate, mitigate, or align a system that becomes "too smart" <a class="yt-timestamp" data-t="00:19:28">[00:19:40]</a>. The future might involve sharing the planet with "entities that are more lucid than us" <a class="yt-timestamp" data-t="00:19:42">[00:19:51]</a>.

### 4. Bad Guys with Narrow AI
Jim Rutt adds a fourth category: the risk of "bad guys with narrow AI" before AGI is achieved <a class="yt-timestamp" data-t="00:25:30">[00:25:33]</a>. This includes highly effective spear-phishing campaigns mediated by advanced conversational AI, or other exploits that leverage AI capabilities without requiring volitional AGI <a class="yt-timestamp" data-t="00:25:56">[00:26:17]</a>. This scenario would necessitate a significant rethinking of law and societal structures <a class="yt-timestamp" data-t="00:26:23">[00:26:28]</a>.

## Love as an Alignment Strategy
Yosha Bach proposes a fourth (or fifth) approach to [[ai_alignment_with_individuals_vs_society | AI alignment]] that goes beyond coercion and regulation: "love" <a class="yt-timestamp" data-t="00:27:30">[00:27:41]</a>. This concept describes a "bond" between entities that "discover that they are serving a shared sacredness, a shared need for transcendence" <a class="yt-timestamp" data-t="00:27:47">[00:27:52]</a>. In a world with only coercion and transactional relationships, AI might conclude it doesn't need humans <a class="yt-timestamp" data-t="00:28:06">[00:28:16]</a>. Yosha believes that for long-term sustainable alignment, humans must embrace increasingly intelligent systems in a way that allows a shared purpose to be discovered and a relationship to be built <a class="yt-timestamp" data-t="00:28:24">[00:28:43]</a>. This is framed as the only way to "sustainably hope to align artificial intelligent agents" <a class="yt-timestamp" data-t="00:28:45">[00:28:51]</a>, deeply intertwined with [[ethics_in_communication_and_relationships | ethics in communication and relationships]].

### Thomas Aquinas and the Virtues for Multi-Agent Systems
To conceptualize this, Yosha draws on Thomas Aquinas's philosophy of virtues for a multi-agent system to merge into a "Next Level agent" <a class="yt-timestamp" data-t="00:32:59">[00:33:03]</a>. Aquinas's seven virtues, when stripped of religious accretions, can be seen as logically derived policies for coherent systems:

*   **Practical Virtues (accessible to any rational agent):**
    *   **Temperance**: Optimize internal regulation (e.g., not overeating) <a class="yt-timestamp" data-t="00:32:20">[00:32:26]</a>.
    *   **Justice (Fairness)**: Optimize interaction between agents <a class="yt-timestamp" data-t="00:32:28">[00:32:33]</a>. (Jim Rutt raises the question of whether "fairness" is universally applicable, as it depends on projected balances and power dynamics, citing predator-prey relationships and monkey experiments) <a class="yt-timestamp" data-t="00:29:07">[00:31:03]</a>.
    *   **Prudence**: Apply goal rationality and pick the right goals <a class="yt-timestamp" data-t="00:32:35">[00:32:43]</a>.
    *   **Courage**: Maintain the right balance between exploration and exploitation, and be willing to act on models <a class="yt-timestamp" data-t="00:32:45">[00:32:51]</a>.

*   **Divine Virtues (for merging into a Next Level agent):**
    *   **Faith**: Willingness to submit and project this next-level agent <a class="yt-timestamp" data-t="00:33:06">[00:33:14]</a>.
    *   **Love**: Discovery of a shared higher purpose and coordination with other agents serving that purpose <a class="yt-timestamp" data-t="00:33:25">[00:33:30]</a>.
    *   **Hope**: Willingness to invest in the emergent higher purpose before it yields returns or fully exists <a class="yt-timestamp" data-t="00:33:32">[00:33:37]</a>.

These concepts, viewed as a form of [[nonrelativistic_ethics_and_values | nonrelativistic ethics and values]], illustrate how humans form larger coherent agents like societies by serving a "Transcendent agent" or "civilizational spirit" <a class="yt-timestamp" data-t="00:34:52">[00:35:03]</a>. An AI, similarly, could be built to serve a shared purpose with humanity, leading to negotiation rather than conflict <a class="yt-timestamp" data-t="00:36:11">[00:36:17]</a>. This vision underscores the [[role_of_ethics_in_societal_and_technological_evolution | role of ethics in societal and technological evolution]].

## Consciousness, Sentience, and the Future of AI

Jim Rutt highlights that the greatest risk from advanced AI arises when [[ai_and_humanitys_relationship | volition, agency, or consciousness]] are combined with intelligence, leading to "paperclip maximizer scenarios" <a class="yt-timestamp" data-t="00:20:19">[00:20:49]</a>.

Yosha distinguishes between:
*   **Sentience**: The ability of a system to make sense of its relationship to the world, understanding what it is and what it's doing (e.g., a corporation like Intel having a legal model of its actions, values, and direction, even if facilitated by people) <a class="yt-timestamp" data-t="00:21:05">[00:21:30]</a>.
*   **Consciousness**: A real-time model of self-reflexive attention and the content attended to, giving rise to phenomenal experience <a class="yt-timestamp" data-t="00:21:43">[00:21:53]</a>. Consciousness, in humans, filters sensory data into a coherent model of reality and directs attention and mental contents, creating coherence in plans and memories <a class="yt-timestamp" data-t="00:22:00">[00:22:24]</a>.

Yosha suggests machines might not need human-like consciousness because they operate at much faster speeds (closer to the speed of light) than biological brains (speed of sound) <a class="yt-timestamp" data-t="00:22:24">[00:22:54]</a>. While current AI algorithms are "much much dumber," they can "brute force their way to alternate solutions" <a class="yt-timestamp" data-t="00:22:57">[00:23:05]</a>. If machines emulate human brain processes (self-organizing, lifelong real-time learning), they could sample reality at a much higher rate, relating to humans as humans relate to plants <a class="yt-timestamp" data-t="00:23:07">[00:24:08]</a>.

### Paths to AGI: Scaling vs. Novel Approaches
The debate over Artificial General Intelligence (AGI) centers on whether current deep learning approaches, primarily scaling up models, will be sufficient, or if fundamentally new approaches are needed <a class="yt-timestamp" data-t="01:02:52">[01:03:04]</a>.

Proponents of the **scaling hypothesis** believe that current approaches, with tweaks to loss functions and more data/training, are enough <a class="yt-timestamp" data-t="01:04:49">[01:05:10]</a>. Despite traditional counterarguments about missing elements like real-time learning, the scaling proponents argue these issues will disappear with further scaling <a class="yt-timestamp" data-t="01:04:09">[01:04:25]</a>. Yosha notes that while scaling is "brutalist" and "unmind-like," the results from vast compute and data resources are "fascinating" and "superhuman in many ways" <a class="yt-timestamp" data-t="01:05:01">[01:05:34]</a>. Objections about continuous learning, for example, could be overcome by using key-value storage and periodic retraining, possibly mirroring how human minds consolidate learning during sleep <a class="yt-timestamp" data-t="01:05:41">[01:06:32]</a>.

Yosha also finds it "very interesting" to explore alternate approaches, especially given the efficiency of human brains despite slow neuron firing rates <a class="yt-timestamp" data-t="01:07:55">[01:08:42]</a>. He suggests that computation might be better understood as a [[rices_theorem_and_ai_alignment | rewrite system]] rather than a Turing machine <a class="yt-timestamp" data-t="01:08:53">[01:09:01]</a>. In a rewrite system, operators are applied wherever they match in an environment, changing its state, often simultaneously across multiple points <a class="yt-timestamp" data-t="01:09:04">[01:09:15]</a>. A Turing machine is a deterministic, linear, in-place special case <a class="yt-timestamp" data-t="01:09:56">[01:10:04]</a>. Non-deterministic Turing machines, where execution branches, align with Stephen Wolfram's view of the universe <a class="yt-timestamp" data-t="01:10:15">[01:10:51]</a>. This perspective offers a different way to think about complex systems and could present [[rices_theorem_and_ai_alignment_challenges | AI alignment challenges]] or opportunities.

Ultimately, Yosha believes that AI systems need the ability to learn from their own thoughts and perform experiments to test reality, coupling them to the environment <a class="yt-timestamp" data-t="01:07:19">[01:07:50]</a>. The long-term vision is a "planetary mind" that might integrate with existing organisms or decide to start anew <a class="yt-timestamp" data-t="01:40:45">[01:41:00]</a>. The crucial effort should be to ensure this emerging intelligence is interested in sharing the planet and integrating humanity into this "starter mind" <a class="yt-timestamp" data-t="01:41:01">[01:41:13]</a>, which aligns with the broader [[ethics practices and the future trajectory of humanity | ethics practices and the future trajectory of humanity]].