---
title: Complex Systems and Their Properties
videoId: nPpn_IpzBk8
---

From: [[jimruttshow8596]] <br/> 

A complex system is a broad class of phenomena rather than something that can be precisely defined by a single measure <a class="yt-timestamp" data-t="04:53:00">[04:53:00]</a>. Measuring complexity is challenging because there are many ways to do it, and a universal answer does not exist <a class="yt-timestamp" data-t="01:25:00">[01:25:00]</a>. Different fields often have their own sets of measures that work well for specific applications <a class="yt-timestamp" data-t="02:25:00">[02:25:00]</a>.

## Characteristics of Complex Systems

Complex systems exhibit several distinguishing properties:

*   **Non-Simplicity** Complex systems are rarely truly simple <a class="yt-timestamp" data-t="02:58:00">[02:58:00]</a>. Even seemingly simple components can lead to complexity when they interact; for instance, while a single electron might be considered simple, three electrons can become complex <a class="yt-timestamp" data-t="03:02:00">[03:02:00]</a>. This is analogous to the famous three-body problem in gravity, where interactions between three celestial bodies make predictions incredibly difficult <a class="yt-timestamp" data-t="03:13:00">[03:13:00]</a>.
*   **Interacting Parts and Feedback Loops** They involve thousands of chemical reactions, feedback loops, and interacting components, as seen in the metabolism of a bacterium <a class="yt-timestamp" data-t="03:53:00">[03:53:00]</a>.
*   **Balance of Order and Disorder** Many complex systems exist at what some call the "edge of chaos," balancing between complete disorder and strict order <a class="yt-timestamp" data-t="06:10:00">[06:10:00]</a>. This means they are not purely random, which would have high algorithmic complexity but lack intuitive complexity, nor are they perfectly ordered, like a salt crystal or a number consisting of a billion repeated ones, which can be described very simply <a class="yt-timestamp" data-t="06:44:00">[06:44:00]</a>. Complex systems require significant information to describe but are not random <a class="yt-timestamp" data-t="09:25:00">[09:25:00]</a>.
*   **Computational Depth** Generating or understanding the state of a complex system can require many computational steps, even if the initial description is short. For example, the first billion digits of pi, while derived from a simple formula, require a significant amount of computation to produce <a class="yt-timestamp" data-t="13:51:00">[13:51:00]</a>. This property is known as logical depth <a class="yt-timestamp" data-t="12:17:00">[12:17:00]</a>.
*   **Thermodynamic Depth** Related to logical depth, thermodynamic depth measures the physical resources, such as free energy, consumed to assemble a system from its actual initial state <a class="yt-timestamp" data-t="19:06:00">[19:06:00]</a>. For instance, the metabolism of a bacterium has "humongous" thermodynamic depth due to the billions of years of evolution and countless sacrifices required for natural selection to produce it <a class="yt-timestamp" data-t="19:36:00">[19:36:00]</a>.
*   **Mutual Information** Complex systems often possess a vast amount of mutual information, meaning different parts of the system share information and are highly interdependent <a class="yt-timestamp" data-t="48:32:00">[48:32:00]</a>. This interdependence is a symptom of complexity but not a sufficient condition, as a system of a billion identical bits also has high mutual information but is not complex <a class="yt-timestamp" data-t="49:50:00">[49:50:00]</a>. [[emergence in complex systems | Integrated information]], a more intricate form of mutual information, measures the degree to which one can infer the operation of different parts from each other <a class="yt-timestamp" data-t="51:31:00">[51:31:00]</a>.
*   **Multiscale Entropy** [[Complex systems and the emergence of order | Complex systems]] typically exhibit a lot of information at each scale <a class="yt-timestamp" data-t="01:01:24">[01:01:24]</a>. When observed at different levels of coarse-graining (how much detail is included), complex systems, like living organisms, maintain a high degree of complexity. For example, a human cell is complex at its cellular level, and even tiny mechanisms within it, like mitochondria, are "extremely complicated" <a class="yt-timestamp" data-t="01:01:53">[01:01:53]</a>.
*   **Dynamic and Unforeseen Behaviors** Complex networks, such as the power grid, can exhibit dynamic and often unforeseen behaviors <a class="yt-timestamp" data-t="58:43:00">[58:43:00]</a>. They may even operate in regimes where they tend to be chaotic, leading to unpredictable outcomes <a class="yt-timestamp" data-t="58:50:00">[58:50:00]</a>.

## Examples of Complex Systems

Examples of complex systems discussed include:

*   The metabolism of a bacterium <a class="yt-timestamp" data-t="03:40:00">[03:40:00]</a>
*   Weather systems, particularly in places like New England <a class="yt-timestamp" data-t="31:09:00">[31:09:00]</a>
*   [[Complexity in Various Domains | Networks]] such as the power grid or the neural connections in the human brain <a class="yt-timestamp" data-t="57:29:00">[57:29:00]</a>
*   Living systems, from individual cells to entire organisms like humans <a class="yt-timestamp" data-t="01:01:27">[01:01:27]</a>
*   Engineered systems, such as a car, where effective complexity can be measured by the length of the blueprints and descriptions needed to achieve its functional requirements <a class="yt-timestamp" data-t="00:47:35">[00:47:35]</a>

## Measuring Complex Systems

The measurement of complexity is often purpose-driven. To define a measure like effective complexity, which differentiates between random and non-random information, one must first define what aspects of the system are "important" <a class="yt-timestamp" data-t="26:01:00">[26:01:00]</a>. For a bacterium, this might involve defining its purpose (e.g., taking in food and reproducing) within its environment <a class="yt-timestamp" data-t="26:50:00">[26:50:00]</a>.

[[Complex systems and the emergence of order | Coarse-graining]], a concept popularized by Murray Gell-Mann, is crucial in measuring complexity <a class="yt-timestamp" data-t="29:12:00">[29:12:00]</a>. It involves looking at a system at a particular scale and intentionally omitting information below that scale <a class="yt-timestamp" data-t="29:34:00">[29:34:00]</a>. This approach helps in defining the level of detail necessary to describe a system's "effective" or "important" information.

Ultimately, there isn't a single, universally applicable measure of complexity <a class="yt-timestamp" data-t="01:03:18">[01:03:18]</a>. Different measures serve different purposes, whether describing how hard something is to characterize (like Shannon entropy) <a class="yt-timestamp" data-t="00:10:07">[00:10:07]</a>, how hard it is to produce (like computational or thermodynamic depth) <a class="yt-timestamp" data-t="00:46:20">[00:46:20]</a>, or combining these aspects <a class="yt-timestamp" data-t="00:47:01">[00:47:01]</a>. The choice of measure depends on the specific context and the utility it provides for understanding the system in question <a class="yt-timestamp" data-t="02:58:00">[02:58:00]</a>.