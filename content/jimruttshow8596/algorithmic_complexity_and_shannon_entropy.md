---
title: Algorithmic complexity and Shannon entropy
videoId: nPpn_IpzBk8
---

From: [[jimruttshow8596]] <br/> 

The measurement of [[ways_to_measure_complexity | complexity]] has been a long-standing challenge, with many different approaches proposed across various fields <a class="yt-timestamp" data-t="00:01:25">[00:01:25]</a>. When attempting to define a universal measure of complexity, researchers at institutions like the Santa Fe Institute found that twenty different people might offer twenty different ideas <a class="yt-timestamp" data-t="00:02:24">[00:02:24]</a>. This suggests that [[ways_to_measure_complexity | complexity]] is a broad class of phenomena rather than a single, easily quantifiable entity <a class="yt-timestamp" data-t="00:04:50">[00:04:50]</a>. Different fields often develop their own specific measures of complexity that suit their particular needs <a class="yt-timestamp" data-t="00:05:25">[00:05:25]</a>.

## Algorithmic Complexity (Kolmogorov Complexity)

One measure of complexity, particularly relevant in computer science, is [[algorithmic_complexity_and_shannon_entropy | algorithmic complexity]], also known as Kolmogorov complexity <a class="yt-timestamp" data-t="00:08:36">[00:08:36]</a>. This measure quantifies the complexity of an object by the length of the shortest computer program required to generate it <a class="yt-timestamp" data-t="00:07:33">[00:07:33]</a>.

### Characteristics and Examples
*   **Simple Objects**: A sequence like "1111111111" (a billion ones) has low algorithmic complexity because a very short program can generate it (e.g., "print one a billion times") <a class="yt-timestamp" data-t="00:07:33">[00:07:33]</a>, <a class="yt-timestamp" data-t="00:08:45">[00:08:45]</a>. Similarly, a salt crystal, despite having Avogadro's number of atoms, can be described simply due to its highly ordered, repetitive structure <a class="yt-timestamp" data-t="00:06:49">[00:06:49]</a>.
*   **Random Objects**: Conversely, a truly random bit string (e.g., from flipping a coin a billion times) has very high algorithmic complexity <a class="yt-timestamp" data-t="00:09:39">[00:09:39]</a>. The shortest program to reproduce such a string would essentially be a direct list of all its bits, as there's no pattern to compress <a class="yt-timestamp" data-t="00:09:47">[00:09:47]</a>. Examples include static on a TV screen <a class="yt-timestamp" data-t="00:08:56">[00:08:56]</a> or random molecular motions in a room <a class="yt-timestamp" data-t="00:07:56">[00:07:56]</a>.

### Limitations of Algorithmic Complexity
A key critique of algorithmic complexity as a measure of *true* complexity is that it assigns high complexity to random sequences <a class="yt-timestamp" data-t="00:09:17">[00:09:17]</a>. Intuitively, random noise is not considered "complex" in the same way a living organism or a sophisticated machine is <a class="yt-timestamp" data-t="00:09:59">[00:09:59]</a>. True [[ways_to_measure_complexity | complexity]] is often associated with systems that are difficult to characterize or measure, exhibiting emergent properties, and existing at the "border between disorder and Order" <a class="yt-timestamp" data-t="00:02:06">[00:02:06]</a>, <a class="yt-timestamp" data-t="00:06:07">[00:06:07]</a>.

### Relation to [[effective_complexity_and_integrated_information | Logical Depth]]
Charles Bennett proposed [[effective_complexity_and_integrated_information | logical depth]] to address the limitation of algorithmic complexity <a class="yt-timestamp" data-t="00:12:14">[00:12:14]</a>. [[effective_complexity_and_integrated_information | Logical depth]] measures the complexity of an object by the computational time it takes for the shortest program to produce it <a class="yt-timestamp" data-t="00:14:56">[00:14:56]</a>.
*   **Example**: The first billion digits of Pi. While having a relatively short program (e.g., Archimedes' method) <a class="yt-timestamp" data-t="00:14:03">[00:14:03]</a>, it takes a significant amount of computational effort to generate these digits <a class="yt-timestamp" data-t="00:14:56">[00:14:56]</a>. This contrasts with simple ordered strings (quick to generate) or random strings (where the program is essentially the string itself, also quick to "generate") <a class="yt-timestamp" data-t="00:15:11">[00:15:11]</a>.
*   This concept applies to things like patterns produced by cellular automata (e.g., Rule 110), which can generate highly complex patterns from simple rules that require many computational steps to unfold <a class="yt-timestamp" data-t="00:16:10">[00:16:10]</a>, <a class="yt-timestamp" data-t="00:17:29">[00:17:29]</a>.

## [[algorithmic_complexity_and_shannon_entropy | Shannon Entropy]] (Information Theory)

[[algorithmic_complexity_and_shannon_entropy | Shannon entropy]], often simply referred to as information, quantifies the amount of uncertainty or "surprise" in a system or message <a class="yt-timestamp" data-t="00:10:11">[00:10:11]</a>.

### Historical Roots
The concept of entropy originated in 19th-century thermodynamics with figures like James Clerk Maxwell, Ludwig Boltzmann, and Josiah Willard Gibbs, who used it to describe microscopic randomness in physical systems <a class="yt-timestamp" data-t="00:10:17">[00:10:17]</a>. In the 1930s, Claude Shannon, an engineer at Bell Labs, independently developed an identical mathematical formula to measure information for communication, revealing that thermodynamic entropy and information content are fundamentally linked <a class="yt-timestamp" data-t="00:10:56">[00:10:56]</a>. Thus, entropy is the information required to describe the positions of atoms and molecules <a class="yt-timestamp" data-t="00:11:20">[00:11:20]</a>.

### Principle of Compression and Regularities
Shannon entropy also relates to how efficiently a message can be compressed by taking into account its statistical regularities <a class="yt-timestamp" data-t="00:11:32">[00:11:32]</a>, <a class="yt-timestamp" data-t="00:38:00">[00:38:00]</a>. If a message has patterns (e.g., frequently occurring letters or words), it can be compressed <a class="yt-timestamp" data-t="00:11:51">[00:11:51]</a>. For example, the Morse code assigns shorter codes to more frequent letters <a class="yt-timestamp" data-t="00:36:48">[00:36:48]</a>.

### Lempel-Ziv-Welch (LZW) Complexity
The LZW algorithm is a practical application of these compression principles <a class="yt-timestamp" data-t="00:38:14">[00:38:14]</a>. It's an adaptive coding method that learns which combinations of letters or patterns occur more frequently in a message and then assigns shorter codes to them automatically <a class="yt-timestamp" data-t="00:39:01">[00:39:01]</a>. This allows for efficient, on-the-fly encoding and decoding, achieving the theoretical "Shannon Bound" for communication channel efficiency <a class="yt-timestamp" data-t="00:39:51">[00:39:51]</a>. LZW algorithms are widely used in file compression formats like `.zip` and `.gif` <a class="yt-timestamp" data-t="00:40:04">[00:40:04]</a>. Interestingly, if a message is perfectly compressed, its encoded form will look more random because all regularities have been removed; therefore, compressing it again (LZW twice) will not make it smaller, but actually larger <a class="yt-timestamp" data-t="00:40:40">[00:40:40]</a>.

## Comparison and Role in Defining Complexity
Both [[algorithmic_complexity_and_shannon_entropy | algorithmic complexity]] and [[algorithmic_complexity_and_shannon_entropy | Shannon entropy]] are measures of "how hard it is to describe something" <a class="yt-timestamp" data-t="00:46:01">[00:46:01]</a>, and they are closely related <a class="yt-timestamp" data-t="00:46:17">[00:46:17]</a>. While useful for quantifying information content and compressibility, they highlight a core challenge in defining [[ways_to_measure_complexity | complexity]]: systems that are either perfectly ordered or completely random tend to be simple to describe by these measures, even if they contain vast amounts of information <a class="yt-timestamp" data-t="00:07:02">[00:07:02]</a>, <a class="yt-timestamp" data-t="00:08:18">[00:08:18]</a>. This leads to the idea that true [[ways_to_measure_complexity | complexity]] often resides in structures that are neither perfectly ordered nor entirely random <a class="yt-timestamp" data-t="00:09:23">[00:09:23]</a>.