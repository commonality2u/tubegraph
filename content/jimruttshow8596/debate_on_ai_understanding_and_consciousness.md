---
title: Debate on AI understanding and consciousness
videoId: S5npIx_yonY
---

From: [[jimruttshow8596]] <br/> 

The discussion surrounding [[Mind and Artificial Intelligence | artificial intelligence]] (AI), particularly large language models (LLMs) like GPT, has brought to the forefront intense debates about whether these systems truly possess [[comparison_of_human_and_ai_understanding | understanding]] and even [[consciousness_and_sentience_in_ai | consciousness]] or [[sentience and levels of consciousness | sentience]] <a class="yt-timestamp" data-t="00:10:06">[00:10:06]</a>. Melanie Mitchell, a professor at the Santa Fe Institute and author of "Artificial Intelligence: A Guide for Thinking Humans," highlights that the word "understanding" itself is not well-understood in this context, leading to significant stress on the term <a class="yt-timestamp" data-t="00:31:15">[00:31:15]</a>.

## Assessing AI Performance on Standardized Tests

Melanie Mitchell's essay, "Did ChatGPT Really Pass Graduate Level Exams?", initially questioned the performance of ChatGPT 3.5 <a class="yt-timestamp" data-t="00:01:36">[00:01:36]</a>. While GPT-4 has shown significantly better results on various standardized exams, except for AP English <a class="yt-timestamp" data-t="00:02:36">[00:02:36]</a>, Mitchell's core concerns remain relevant <a class="yt-timestamp" data-t="00:02:58">[00:02:58]</a>.

### Limitations of Current Testing Methods
When humans take standardized exams, assumptions are made about their cognition, such as not having memorized entire datasets like Wikipedia <a class="yt-timestamp" data-t="00:16:22">[00:16:22]</a>. Key issues with testing LLMs include:
*   **Training Data Exposure:** Whether the questions or similar ones appeared in the model's vast training data <a class="yt-timestamp" data-t="00:04:00">[00:04:00]</a>.
*   **Sensitivity to Prompts:** LLMs are highly sensitive to prompt wording. An identical problem with a slightly different scenario can lead to poor performance, raising questions about underlying conceptual [[comparison_of_human_and_ai_understanding | understanding]] <a class="yt-timestamp" data-t="00:05:16">[00:05:16]</a>. Jim Rutt's experiment where GPT-3.5 generated vastly different results based on paraphrased prompts exemplifies this <a class="yt-timestamp" data-t="00:11:01">[00:11:01]</a>.
*   **Hallucinations:** LLMs often generate incorrect information, or "hallucinate," with the same confidence as correct answers <a class="yt-timestamp" data-t="00:10:01">[00:10:01]</a>. Unlike humans who know they are lying, LLMs' hallucinations are not linguistically distinct from their truths, making them difficult to detect <a class="yt-timestamp" data-t="00:36:23">[00:36:23]</a>.
*   **Lack of Transparency:** Companies like OpenAI provide limited access to models like GPT-4 and lack transparency regarding tested materials, making scientific validation difficult <a class="yt-timestamp" data-t="00:06:05">[00:06:05]</a>. This has led to calls for OpenAI to be renamed "Closed AI" <a class="yt-timestamp" data-t="00:06:44">[00:06:44]</a>.

### The Problem of Extrapolation
Melanie Mitchell questions whether performance on these tests extrapolates to real-world capabilities for LLMs in the same way it does for humans <a class="yt-timestamp" data-t="00:17:10">[00:17:10]</a>. While GPT-3.5 scored an IQ of 119 on a vocabulary test, an "ideal task for a language model," it still didn't ace it <a class="yt-timestamp" data-t="00:22:19">[00:22:19]</a>.

## The Nature of [[comparison_of_human_and_ai_understanding | Understanding]]

The paper "The Debate Over Understanding and AI's Large Language Models," co-written by Melanie Mitchell and David Krakauer, delves into the definition of [[comparison_of_human_and_ai_understanding | understanding]] in the context of AI <a class="yt-timestamp" data-t="00:29:15">[00:29:15]</a>.

### Different Views on Understanding
There are two main perspectives:
1.  **"Pro-Understanding" Side:** Proponents argue that LLMs can [[comparison_of_human_and_ai_understanding | understand]] human language similarly to humans and might even be [[consciousness_and_sentience_in_ai | conscious]] <a class="yt-timestamp" data-t="00:29:40">[00:29:40]</a>. They suggest that training on language, which represents the world, enables world [[comparison_of_human_and_ai_understanding | understanding]] <a class="yt-timestamp" data-t="00:30:04">[00:30:04]</a>.
2.  **"Stochastic Parrot" Side:** Opponents argue that LLMs merely "parrot" trained language by computing the probability of the next word, without genuine [[comparison_of_human_and_ai_understanding | understanding]] <a class="yt-timestamp" data-t="00:30:15">[00:30:15]</a>.

### Human vs. AI Cognition
Human cognition, as explored by [[Mind and Artificial Intelligence | cognitive science]] and neuroscience, approaches [[comparison_of_human_and_ai_understanding | understanding]] differently from LLMs <a class="yt-timestamp" data-t="00:30:59">[00:30:59]</a>. Key distinctions include:
*   **Compression:** Humans have an innate desire to compress complex information into lower-dimensional representations or concepts, like Newton's laws, due to limited working memory <a class="yt-timestamp" data-t="00:35:05">[00:35:05]</a>. LLMs, with their massive context windows (e.g., GPT-4's 32k tokens), lack this evolutionary pressure for compression <a class="yt-timestamp" data-t="00:36:03">[00:36:03]</a>.
*   **Long-Term Memory:** Humans possess [[role_of_emotions_and_longterm_memory_in_ai_and_human_cognition | long-term memory]], including episodic memory, which contributes to a sense of self and informs interactions <a class="yt-timestamp" data-t="00:38:21">[00:38:21]</a>. LLMs, while storing information in their billions of parameters, lack this kind of experiential memory <a class="yt-timestamp" data-t="00:38:55">[00:38:55]</a>.
*   **[[role_of_emotions_and_longterm_memory_in_ai_and_human_cognition | Emotions]]:** [[role_of_emotions_and_longterm_memory_in_ai_and_human_cognition | Emotions]] play a crucial role in human decision-making and social interaction <a class="yt-timestamp" data-t="00:39:26">[00:39:26]</a>. As philosopher Margaret Bowden suggested, AI "won't take over the world because it doesn't care" <a class="yt-timestamp" data-t="00:41:04">[00:41:04]</a>. AlphaGo, while superior to humans in Go, did not "care" about winning in an emotional sense <a class="yt-timestamp" data-t="00:41:28">[00:41:28]</a>.
*   **Physical Grounding:** LLMs lack grounding in bodily sensations and physical experience, which are fundamental to human [[comparison_of_human_and_ai_understanding | understanding]] of language and the world <a class="yt-timestamp" data-t="00:45:34">[00:45:34]</a>. Whether language alone is rich enough to convey intuitive physics or psychology models remains an empirical question <a class="yt-timestamp" data-t="00:46:31">[00:46:31]</a>.

### Redefining Intelligence and Understanding
The emergence of LLMs is forcing a re-evaluation and clarification of terms like [[Mind and Artificial Intelligence | intelligence]], [[consciousness_and_sentience_in_ai | consciousness]], and [[comparison_of_human_and_ai_understanding | understanding]] <a class="yt-timestamp" data-t="00:31:45">[00:31:45]</a>. The debate suggests a need for a "pluralistic" view of [[comparison_of_human_and_ai_understanding | understanding]], acknowledging different kinds of [[Mind and Artificial Intelligence | intelligences]] that may have different forms of [[comparison_of_human_and_ai_understanding | understanding]] <a class="yt-timestamp" data-t="00:43:35">[00:43:35]</a>. AlphaFold, for instance, predicts protein structure better than humans but without a mechanistic physics model <a class="yt-timestamp" data-t="00:42:52">[00:42:52]</a>.

## Current Challenges and Future Directions

The field of AI is moving at an "unbelievably fast" pace, with rapid advancements making earlier observations quickly outdated <a class="yt-timestamp" data-t="00:01:52">[00:01:52]</a>.

### Scientific Study and Transparency
A major challenge is the lack of a clear view of what these systems can and cannot do <a class="yt-timestamp" data-t="00:09:10">[00:09:10]</a>. Commercial entities like OpenAI are not "open," hindering scientific research by not providing access to their models or detailed testing materials <a class="yt-timestamp" data-t="00:06:05">[00:06:05]</a>.

Fortunately, projects like EleutherAI and Stability AI are working on open-source models, including software and data sets, specifically designed for stable scientific research <a class="yt-timestamp" data-t="00:07:35">[00:07:35]</a>. Other companies like Hugging Face and Meta have also open-sourced some of their language model software <a class="yt-timestamp" data-t="00:08:33">[00:08:33]</a>.

### New Assessment Methodologies
There's a critical need to develop new assessments that can accurately predict LLMs' abilities in real-world tasks <a class="yt-timestamp" data-t="00:25:00">[00:25:00]</a>. The "Holistic Evaluation of Language Models" (HELM) project from Stanford is one such initiative <a class="yt-timestamp" data-t="00:25:06">[00:25:06]</a>. This presents an opportunity for new businesses, akin to a "College Board for LLMs" <a class="yt-timestamp" data-t="00:25:58">[00:25:58]</a>.

### Bridging the Gap to Human-like Cognition
Future research might involve:
*   **External Memory Hierarchies:** Jim Rutt plans to experiment with building external memory hierarchies and intentional mechanisms to coerce LLMs into acting more like unconscious human language processes <a class="yt-timestamp" data-t="00:37:45">[00:37:45]</a>.
*   **Multimodality:** [[integration_of_neuroscience_in_ai_development | Integrating]] other sensory modalities (e.g., vision) with language could potentially help LLMs develop basic intuitive physics models <a class="yt-timestamp" data-t="00:47:38">[00:47:38]</a>.
*   **Online Learning:** Currently, LLMs are primarily static feed-forward networks <a class="yt-timestamp" data-t="00:49:42">[00:49:42]</a>. The addition of true online learning, allowing them to adapt to the world as they encounter it, would be a significant step <a class="yt-timestamp" data-t="00:49:52">[00:49:52]</a>.

### Emergence and Complexity
The concept of "more is different" applies to LLMs; larger models exhibit "phase changes" where interesting emergent properties appear that were not present at smaller scales <a class="yt-timestamp" data-t="00:51:17">[00:51:17]</a>. For EleutherAI's models, this threshold appears to be around 10 gigabytes <a class="yt-timestamp" data-t="00:51:31">[00:51:31]</a>. This represents a significant challenge and opportunity for [[Mind and Artificial Intelligence | complex systems]] science <a class="yt-timestamp" data-t="00:52:22">[00:52:22]</a>.

The debate over AI's [[comparison_of_human_and_ai_understanding | understanding]] and [[consciousness_and_sentience_in_ai | consciousness]] is ongoing, dynamic, and rapidly evolving. It's pushing humanity to clarify its own definitions of [[Mind and Artificial Intelligence | intelligence]] and [[comparison_of_human_and_ai_understanding | understanding]], making it a fascinating time for both AI researchers and the public <a class="yt-timestamp" data-t="00:43:58">[00:43:58]</a>.