---
title: Complexity in Various Domains
videoId: nPpn_IpzBk8
---

From: [[jimruttshow8596]] <br/> 

The concept of [[Measuring Complexity|measuring complexity]] has been a long-standing challenge for researchers, prompting Jim Rutt to note his personal interest in the topic for 25 years without a definitive answer <a class="yt-timestamp" data-t="01:16:00">[01:16:00]</a>. As Seth Lloyd, Professor of mechanical engineering at MIT and former Miller Fellow at the [[Science and complexity|Santa Fe Institute]], points out, it's difficult to pinpoint *the* measure of complexity because "complexity is more a very broad class of things rather than something you can put your finger on" <a class="yt-timestamp" data-t="04:53:00">[04:53:00]</a>. Each field tends to develop its own set of measures that work well for its specific context <a class="yt-timestamp" data-t="05:25:00">[05:25:00]</a>.

## General Challenges in [[Measuring Complexity|Measuring Complexity]]

The fundamental problem is that complexity is not a single, quantifiable property like mass or temperature <a class="yt-timestamp" data-t="04:50:00">[04:50:00]</a>. There are "lots of ways to measure complexity" <a class="yt-timestamp" data-t="01:28:00">[01:28:00]</a>, and Seth Lloyd once gave a talk titled "31 Measures of Complexity" <a class="yt-timestamp" data-t="02:40:00">[02:40:00]</a>. Even simple entities like an electron require complex theory to understand, and adding just a few more elements, like three electrons or three bodies in a gravitational system, quickly leads to complex behavior <a class="yt-timestamp" data-t="03:02:00">[03:02:00]</a>.

Complexity is often observed at the "border between disorder and Order," sometimes referred to as the "edge of chaos" <a class="yt-timestamp" data-t="06:10:00">[06:10:00]</a>. Systems that are either too ordered (like a salt crystal) or too disordered (like static on a TV screen) are generally not considered complex in an intuitive sense <a class="yt-timestamp" data-t="06:42:00">[06:42:00]</a>.

## Specific [[Different Measures of Complexity|Measures of Complexity]] Across Domains

### Algorithmic/Kolmogorov Complexity
This measure, also known as Kolmogorov complexity, quantifies the length of the shortest computer program required to generate a given string of data <a class="yt-timestamp" data-t="08:36:00">[08:36:00]</a>.
*   **Highly Ordered Systems**: A sequence of a billion "1"s has low algorithmic complexity because it can be generated by a very short program (e.g., "print one a billion times") <a class="yt-timestamp" data-t="07:30:00">[07:30:00]</a>.
*   **Highly Disordered Systems**: Random data, like static on a TV screen or a coin flip sequence, has very high algorithmic complexity because the shortest program to reproduce it is essentially a list of all the data points themselves <a class="yt-timestamp" data-t="08:58:00">[08:58:00]</a>.

A key limitation of algorithmic complexity is that while complex systems should require a lot of information to describe, they "shouldn't be random" <a class="yt-timestamp" data-t="09:25:00">[09:25:00]</a>.

### Shannon Entropy (Information)
Claude Shannon developed this measure to quantify information for communications, and it turned out to be mathematically identical to the entropy concept from 19th-century physics <a class="yt-timestamp" data-t="10:11:00">[10:11:00]</a>.
*   It measures the amount of information required to describe a bit string, taking into account all its regularities <a class="yt-timestamp" data-t="11:32:00">[11:32:00]</a>.
*   Shannon entropy is closely related to data compression: the more regularities in a message, the more it can be compressed <a class="yt-timestamp" data-t="11:41:00">[11:41:00]</a>.

### Charles Bennett's Logical Depth
Logical depth attempts to capture the intuitive sense of complexity better than algorithmic complexity by measuring the *computational steps* required to produce a sequence from its shortest program <a class="yt-timestamp" data-t="13:51:00">[13:51:00]</a>.
*   **Example: Pi**: The first billion digits of Pi have high logical depth because, while the program to generate them is short (e.g., ancient Greek methods), it takes a long time to compute those digits <a class="yt-timestamp" data-t="14:56:00">[14:56:00]</a>.
*   **Cellular Automata**: Patterns generated by certain cellular automata, like Rule 110, are considered logically deep because they require running all the computational steps to be produced <a class="yt-timestamp" data-t="16:09:00">[16:09:09]</a>. These simple rules can create very complicated or complex patterns, sitting in the "edge of chaos" region <a class="yt-timestamp" data-t="17:48:00">[17:48:00]</a>.

### Thermodynamic Depth (Lloyd & Pagels)
Defined by Seth Lloyd and Hein Pagels, thermodynamic depth is a physical analogue to logical depth <a class="yt-timestamp" data-t="18:48:00">[18:48:00]</a>. It measures the *physical resources* (like free energy) consumed to put a system together from its initial state <a class="yt-timestamp" data-t="19:06:00">[19:06:00]</a>.
*   **Example: Bacterial Metabolism**: The thermodynamic depth of bacterial metabolism is "humongous" because it took billions of years of evolution and immense energy expenditure through natural selection to produce <a class="yt-timestamp" data-t="19:36:00">[19:36:00]</a>. This measure connects physical and computational definitions of complexity <a class="yt-timestamp" data-t="20:00:00">[20:00:00]</a>.

### Effective Complexity (Gell-Mann & Lloyd)
Proposed by Murray Gell-Mann and Seth Lloyd, effective complexity combines physical and computational notions of complexity <a class="yt-timestamp" data-t="21:18:00">[21:18:00]</a>. It focuses on the *algorithmic part* of a system's description, specifically the non-random, structural information that defines its organization and function, disregarding the purely random parts (entropy) <a class="yt-timestamp" data-t="22:03:00">[22:03:00]</a>.
*   **Example: Bacterium Metabolism**: For a bacterium, effective complexity would describe its DNA, chemical reactions, energy use, and reproduction processes, which is a very large, non-random amount of information <a class="yt-timestamp" data-t="24:30:00">[24:30:00]</a>.
*   **Subjectivity**: Defining effective complexity often involves a subjective choice about what aspects of a system are "important" or "functional" (e.g., defining a bacterium's purpose within its environment) <a class="yt-timestamp" data-t="25:57:00">[25:57:00]</a>.
*   **Coarse-Graining**: This measure relies on the concept of coarse-graining, where details below a certain scale are intentionally left out to focus on the relevant information at a particular level of observation <a class="yt-timestamp" data-t="29:17:00">[29:17:00]</a>.
*   **Engineered Systems**: For engineered systems like a car, effective complexity can be defined as the length of the blueprint and descriptions needed to achieve its functional requirements <a class="yt-timestamp" data-t="47:35:00">[47:35:00]</a>.

### Fractal Dimensions
Emerging from the field of nonlinear dynamical systems and chaos theory, fractal dimensions describe patterns that exhibit self-similarity across different scales <a class="yt-timestamp" data-t="30:28:00">[30:28:00]</a>.
*   **Example: Weather**: Edward Lorenz's work showed that weather equations are chaotic but confined to a "strange attractor," a fractal structure <a class="yt-timestamp" data-t="31:31:00">[31:31:00]</a>. This indicates that while weather is unpredictable in fine detail, its overall dynamics are confined to a lower-dimensional structure, making it "complex but predictable in other ways" <a class="yt-timestamp" data-t="34:55:00">[34:55:00]</a>.

### Lempel-Ziv Complexity (LZW)
LZW complexity is a method for adaptive data compression, used in algorithms like Zip and GIF <a class="yt-timestamp" data-t="40:10:00">[40:10:00]</a>.
*   It automatically learns frequently occurring patterns in a message and assigns them shorter codes, thereby achieving high compression efficiency <a class="yt-timestamp" data-t="39:01:00">[39:01:00]</a>.
*   Mathematically, it can asymptotically attain Shannon's bound for encoding efficiency <a class="yt-timestamp" data-t="39:42:00">[39:42:00]</a>.

### Statistical Complexity (Crutchfield & Young's Epsilon Machines)
Developed by Jim Crutchfield and others, statistical complexity focuses on finding the simplest computational machine (an automaton) that can reproduce a given message or text with the same statistical regularities <a class="yt-timestamp" data-t="41:06:00">[41:06:00]</a>.
*   **Epsilon Machines**: The Epsilon machine is the size of this simplest automaton, reflecting the inherent "complexity" of the message's statistical patterns <a class="yt-timestamp" data-t="43:03:00">[43:03:00]</a>.
*   **Relevance to LLMs**: This concept is relevant to understanding large language models (LLMs), which can be viewed as complex automata trained on vast corpora of text to reproduce statistical patterns of language <a class="yt-timestamp" data-t="43:25:00">[43:25:00]</a>. Despite their size and energy consumption, LLMs are still considered "dumb" compared to a human brain <a class="yt-timestamp" data-t="44:50:00">[44:50:00]</a>.

### Mutual Information
Mutual information measures the amount of information that different subsystems of a complex system possess in common <a class="yt-timestamp" data-t="48:37:00">[48:37:00]</a>.
*   It's a measure of shared information or correlation between parts <a class="yt-timestamp" data-t="49:13:00">[49:13:00]</a>.
*   **Necessary but Insufficient**: While a complex system (like a bacterium's metabolism) will typically have a "vast amount of mutual information" due to constant communication and exchange <a class="yt-timestamp" data-t="50:11:00">[50:11:00]</a>, systems with high mutual information aren't necessarily complex (e.g., a billion identical bits) <a class="yt-timestamp" data-t="49:50:00">[49:50:00]</a>.

### Integrated Information Theory (Tononi)
Giulio Tononi's Integrated Information Theory (IIT) is a more intricate form of mutual information, aiming to measure not just shared information but also the extent to which the system's parts "inform" each other dynamically <a class="yt-timestamp" data-t="51:31:00">[51:31:00]</a>.
*   **Claimed Connection to Consciousness**: Tononi claims that systems with high integrated information (referred to as "Phi") are conscious <a class="yt-timestamp" data-t="53:56:00">[53:56:00]</a>.
*   **Critique**: Critics, including Seth Lloyd, disagree with this direct link, pointing out that simple error-correcting codes can have high integrated information without being conscious <a class="yt-timestamp" data-t="53:40:00">[53:40:00]</a>. The debate often centers on the definition of "consciousness" itself <a class="yt-timestamp" data-t="54:51:00">[54:51:00]</a>.

### Network Complexity
This broad class of ideas applies to systems made up of multiple interconnected subsystems, such as communication networks, neural connections in the brain, or the power grid <a class="yt-timestamp" data-t="57:26:00">[57:26:00]</a>.
*   **Structure and Dynamics**: Network complexity considers the structure of the interconnections (e.g., types and sizes of power plants, transmission lines) and the resulting complex, often unforeseen, dynamical behaviors <a class="yt-timestamp" data-t="58:02:00">[58:02:00]</a>.
*   **Chaotic Regimes**: Networks can operate in regimes where they exhibit chaotic behavior, which is generally undesirable (e.g., a chaotic electrical grid leading to blackouts) <a class="yt-timestamp" data-t="58:50:00">[58:50:00]</a>. Driving these systems to their limits can push them to the "edge of chaos," leading to complex emergent behaviors <a class="yt-timestamp" data-t="59:07:00">[59:07:00]</a>.

### Multiscale Entropy
Multiscale entropy is closely related to coarse-graining and measures the amount of information present in a system at different observational scales <a class="yt-timestamp" data-t="01:00:20">[01:00:20]</a>.
*   **Information Across Scales**: Complex systems, particularly living systems or networks, tend to have a large amount of information at each scale, from macroscopic behaviors down to microscopic mechanisms within cells <a class="yt-timestamp" data-t="01:10:00">[01:10:00]</a>.
*   **Symptom, Not Cause**: Like mutual and integrated information, multiscale entropy is often a symptom of complexity rather than a sole cause; simple fractal systems can also exhibit high multiscale information without being intuitively complex <a class="yt-timestamp" data-t="01:02:50">[01:02:50]</a>.

In conclusion, there isn't one universal measure of complexity <a class="yt-timestamp" data-t="01:03:14">[01:03:14]</a>. Instead, there are numerous approaches, each with its own utility and applicability depending on the specific domain and purpose of analysis <a class="yt-timestamp" data-t="01:03:52">[01:03:52]</a>.