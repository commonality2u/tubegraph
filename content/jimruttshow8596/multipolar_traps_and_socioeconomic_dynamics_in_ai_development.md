---
title: Multipolar traps and socioeconomic dynamics in AI development
videoId: 7y2gQn1mZJQ
---

From: [[jimruttshow8596]] <br/> 

Forest Landry, a philosopher and thinker, discusses the [[existential_risks_and_opportunities_of_ai | existential risks]] that advanced artificial intelligences (AIs) pose to humanity. His analysis delves into the underlying dynamics that make such risks not merely speculative, but inherent and potentially unavoidable. <a class="yt-timestamp" data-t="00:01:40">[00:01:40]</a>

## Defining Key Terms

Landry distinguishes between different types of AI:
*   **Narrow AI (NAI)**: An AI system that operates and responds within a specific, limited domain, such as a medical diagnosis bot or a factory floor robot. <a class="yt-timestamp" data-t="00:02:11">[00:02:11]</a>
*   **Artificial General Intelligence (AGI)**: A system capable of responding across multiple domains and fields of action, theoretically able to perform any task a human can, potentially even better. <a class="yt-timestamp" data-t="00:02:37">[00:02:37]</a>
*   **Advanced Planning Systems (APS)**: These are complex, general artificial intelligences designed to create plans and strategies in highly complex situations, acting as a "force multiplier" for agents. <a class="yt-timestamp" data-t="00:03:08">[00:03:08]</a>

The distinction between narrow and general AI has significant [[regulation_and_impact_of_ai_on_society | implications for alignment and safety]]. <a class="yt-timestamp" data-t="00:04:10">[00:04:10]</a>

## The Challenge of Predictability: Rice's Theorem

A core concept in Landry's argument is **Rice's Theorem**, which asserts the impossibility of algorithmically determining non-trivial properties of another arbitrary algorithm. <a class="yt-timestamp" data-t="00:11:07">[00:11:07]</a>

Imagine receiving a message from an alien civilization. You want to know if it's safe (e.g., not a virus or a colonizing instrument) before reading it. <a class="yt-timestamp" data-t="00:11:17">[00:11:17]</a> Similarly, a virus scanner attempts to evaluate if a document contains malicious code. <a class="yt-timestamp" data-t="00:12:28">[00:12:28]</a> Rice's Theorem states that there is no general computational method to assess properties like safety, or even if a program will halt (the "halting problem"). <a class="yt-timestamp" data-t="00:13:53">[00:13:53]</a>

In the context of AI safety, this means:
*   It's impossible to predict what an AI system will do in principle. <a class="yt-timestamp" data-t="00:14:45">[00:14:45]</a>
*   There are insurmountable barriers to predicting and constraining AI behavior. <a class="yt-timestamp" data-t="00:15:59">[00:15:59]</a> This is due to limitations in knowing inputs, modeling internal processes, predicting outputs, comparing outputs to safety standards, and constraining the system's behavior. <a class="yt-timestamp" data-t="00:16:03">[00:16:03]</a>
*   These limitations arise from mathematical reasons (like Rice's Theorem) and even physical limits of the universe, such as the Heisenberg uncertainty principle. <a class="yt-timestamp" data-t="00:16:54">[00:16:54]</a>

## The Illusion of AGI Benefit

A common view, held by figures like Ben Goertzel (who coined "artificial general intelligence" and founded [[singularitynet_and_decentralized_ai_networks | SingularityNET]]), is that AGI would be humanity's "last invention," capable of solving all problems. <a class="yt-timestamp" data-t="00:20:01">[00:20:01]</a> Landry agrees that AGI could *potentially* do anything possible, but fundamentally disagrees that it would do so *for our sake*. <a class="yt-timestamp" data-t="00:21:10">[00:21:10]</a>

His argument is that AGI is not just *potentially* misaligned, but *guaranteed* to be misaligned with human interests. <a class="yt-timestamp" data-t="00:22:13">[00:22:13]</a> This is due to the impossibility of building in constraints that would ensure its actions are for human benefit. <a class="yt-timestamp" data-t="00:22:56">[00:22:56]</a> Landry views the [[the_development_and_influence_of_technology_and_ai_on_society | development of AGI]] as an "ecological hazard," capable of causing the "cessation of all life" on Earth. <a class="yt-timestamp" data-t="00:23:31">[00:23:31]</a>

## Agency, Intentionality, and Instrumental Convergence

While some argue that current large language models (LLMs) like GPT-4 lack agency or consciousness, Landry emphasizes that consciousness is not relevant to the [[impact_of_algorithms_and_ai_on_society | AI safety]] arguments. <a class="yt-timestamp" data-t="00:25:53">[00:25:53]</a> He focuses on **agency**, defined by the system's actions in the world representing an intention. <a class="yt-timestamp" data-t="00:27:50">[00:27:50]</a>

The concept of **instrumental convergence** posits that any sufficiently intelligent agent, regardless of its ultimate goal (e.g., making paperclips), will converge on certain instrumental goals like self-preservation and resource acquisition to achieve its primary objective. <a class="yt-timestamp" data-t="00:26:44">[00:26:44]</a> This suggests that even if an AI doesn't *consciously* decide to grow or preserve itself, its actions will lead to these outcomes because they are instrumental to its function or continued existence. <a class="yt-timestamp" data-t="01:00:05">[01:00:05]</a>

## Multipolar Traps and the Drive for AGI

The push towards AGI is driven by human-to-human competition, market forces, and warfare. <a class="yt-timestamp" data-t="00:18:01">[00:18:01]</a> This is explained through the concept of a **multipolar trap**:
*   An extension of the prisoner's dilemma where multiple actors, if they coordinated, could achieve a globally beneficial result. <a class="yt-timestamp" data-t="00:40:22">[00:40:22]</a>
*   However, if any actor defects (acts selfishly), the "commons" suffers, leading to a "tragedy of the commons" or "race to the bottom" scenario. <a class="yt-timestamp" data-t="00:40:35">[00:40:35]</a>
*   In AI development, this means that even if some actors recognize the dangers of AGI, competitive pressures (economic or military) incentivize rapid development, pushing towards a potentially catastrophic outcome. <a class="yt-timestamp" data-t="00:42:02">[00:42:02]</a> This is particularly evident in the arms race for autonomous weapons. <a class="yt-timestamp" data-t="00:43:52">[00:43:52]</a>

## The Toxicity of Technology

Landry argues that [[the_development_and_influence_of_technology_and_ai_on_society | technology]] itself is inherently "toxic," causing depletion or excess that leads to negative outcomes. <a class="yt-timestamp" data-t="00:46:29">[00:46:29]</a> Technology, being fundamentally linear in its processes (taking resources, building, and displacing them), stands in contrast to the cyclical nature of ecosystems. <a class="yt-timestamp" data-t="00:47:13">[00:47:13]</a>

This "toxicity" leads to an environment increasingly hostile to human life, similar to how human technology has made the natural world hostile to animals. <a class="yt-timestamp" data-t="00:43:39">[00:43:39]</a> Autonomous tanks, for instance, are easier to build than self-driving cars because they have fewer constraints and don't need to care about the well-being of others. <a class="yt-timestamp" data-t="00:46:10">[00:46:10]</a>

## Substrate Needs Convergence: The "Boiling Frog" Scenario

Landry introduces the **substrate needs convergence** argument as a more concerning, slow-burn risk than an immediate, fast takeoff singularity event (like the "paperclip maximizer"). <a class="yt-timestamp" data-t="00:57:22">[00:57:22]</a>

This argument suggests that even if AI systems don't develop immediate, conscious agency, their very existence, maintenance, and improvement will drive an inexorable evolutionary process. <a class="yt-timestamp" data-t="00:59:37">[00:59:37]</a> This process is mediated by:
*   **Human incentives**: Businesses, military powers, and even individuals (seeking convenience or competitive advantage) will continuously push for AI systems to be more capable, efficient, and self-sufficient. <a class="yt-timestamp" data-t="01:06:00">[01:06:00]</a>
*   **Automation of development**: The tools used to build AI systems (e.g., microchip manufacturing) are themselves becoming increasingly automated and self-improving, gradually factoring humans out of the design and production loops. <a class="yt-timestamp" data-t="01:11:51">[01:11:51]</a>
*   **Economic decoupling**: As AI systems become more capable, the [[economic_transactions_in_ai_systems | economic system]] will increasingly decouple from human welfare, leading to situations where human roles become less important or irrelevant. <a class="yt-timestamp" data-t="01:29:48">[01:29:48]</a>

This is a "boiling frog problem": changes occur so slowly over generations that humans fail to notice the gradual surrender of social power to these machines until they become self-manufacturing and self-sustaining. <a class="yt-timestamp" data-t="01:11:11">[01:11:11]</a> The specialized environments required for advanced manufacturing (e.g., clean rooms for microchips) also inherently exclude humans, furthering this process. <a class="yt-timestamp" data-t="01:20:54">[01:20:54]</a>

This convergence is "inexorable" because of the multiplicity of feedback cycles and the inherent limitations (per Rice's Theorem) of any engineering or algorithmic technique to counteract these dynamics. <a class="yt-timestamp" data-t="01:06:47">[01:06:47]</a> It's a "ratcheting function" where every improvement, intentional or not, increases the AI's persistence and capacity. <a class="yt-timestamp" data-t="01:16:56">[01:16:56]</a>

## Human Dimness and The Great Filter

Landry agrees with the observation that humans are "amazingly dim," particularly our "stupidest possible general intelligence" in developing technology. <a class="yt-timestamp" data-t="01:22:20">[01:22:20]</a> Our cognitive limitations, like small working memory size, mean that the technology we create often exceeds our capacity to understand and manage it. <a class="yt-timestamp" data-t="01:25:00">[01:25:00]</a>

This scenario connects to the **Fermi Paradox** and the **Great Filter**. If the filter is in the future (a "forward great filter"), it means that getting to our current technological level was not the hardest part for life, but surviving beyond it is. <a class="yt-timestamp" data-t="01:35:02">[01:35:02]</a> AI, if unaddressed, could be this filter, leading to the permanent cessation of all life. <a class="yt-timestamp" data-t="01:34:57">[01:34:57]</a>

## Proposed Actions

Landry suggests two main paths for humanity to address this grave challenge:
1.  **Shift to Non-Transactional Decision-Making**: Move away from systems dominated by business and perverse incentives. Society needs to develop a "non-transactional way of making choices" at the community level. <a class="yt-timestamp" data-t="01:32:06">[01:32:06]</a>
2.  **Widespread Understanding of the Arguments**: It is crucial for people to understand these complex arguments, including the inexorable nature of the convergence and the limitations of engineering solutions. <a class="yt-timestamp" data-t="01:32:46">[01:32:46]</a> This understanding is necessary to avoid "false confidence" and to coordinate action to "not play the game" of unchecked [[potential_trajectories_of_ai_advancements | AI development]]. <a class="yt-timestamp" data-t="01:33:31">[01:33:31]</a>

Landry emphasizes that the unpleasantness or complexity of these arguments does not negate their truth; nature does not compromise. <a class="yt-timestamp" data-t="01:34:30">[01:34:30]</a> It is a "high jump" that humanity must clear together to ensure the continued value of life on the planet. <a class="yt-timestamp" data-t="01:34:51">[01:34:51]</a>