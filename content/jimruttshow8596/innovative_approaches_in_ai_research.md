---
title: Innovative approaches in AI research
videoId: isIrLmYTdvU
---

From: [[jimruttshow8596]] <br/> 

Artificial General Intelligence (AGI) is an informal term referring to computer systems capable of performing tasks considered intelligent when done by humans, especially those they were not specifically programmed or trained for <a class="yt-timestamp" data-t="01:48:00">[01:48:00]</a>. This contrasts with "narrow AI," which excels at highly particular tasks based on specific programming or data-driven training <a class="yt-timestamp" data-t="02:38:00">[02:38:00]</a>. While humans can "take a leap" into loosely connected domains and improvise, current AI, such as AlphaFold for protein folding, struggles with generalization beyond its training data <a class="yt-timestamp" data-t="02:49:00">[02:49:00]</a>.

## [[The limitations of current AI architectures | Limitations of Current Deep Neural Networks]]

Deep Neural Networks (DNNs) and other Machine Learning (ML) algorithms, while absorbing most of the AI world's attention and achieving practical success in narrow AI <a class="yt-timestamp" data-t="02:31:00">[02:31:00]</a>, are fundamentally unsuited for human-level AGI <a class="yt-timestamp" data-t="13:07:00">[13:07:07]</a>. While some researchers believe DNNs are completely misdirected for AGI <a class="yt-timestamp" data-t="14:56:00">[14:56:00]</a>, others think they are almost there with a few tweaks <a class="yt-timestamp" data-t="15:18:00">[15:18:00]</a>. A more moderate view suggests DNNs can serve as significant components of an AGI architecture, but they are missing many key aspects required for human-level intelligence <a class="yt-timestamp" data-t="15:41:00">[15:41:00]</a>.

A core limitation is that current DNNs behave much like "very clever lookup tables" <a class="yt-timestamp" data-t="17:23:00">[17:23:00]</a>, recording and indexing patterns in a subtle way that considers their overlap and contextual usefulness <a class="yt-timestamp" data-t="17:01:00">[17:01:01]</a>. However, they primarily look at "shallow patterns" in data <a class="yt-timestamp" data-t="17:47:00">[17:47:00]</a>. For example, a neural net asked how to fit a large table through a small door might suggest using a "table saw" because it recognizes the words, not understanding that a table saw is a tool for cutting wood, not tables <a class="yt-timestamp" data-t="18:20:00">[18:20:00]</a>. This highlights their failure to build an underlying model of reality <a class="yt-timestamp" data-t="20:00:00">[20:00:00]</a>.

Current AI systems leverage huge amounts of data and processing power to recognize highly particular patterns and extrapolate from them <a class="yt-timestamp" data-t="20:59:00">[20:59:00]</a>. This approach struggles to generalize to domains that do not exhibit those specific patterns <a class="yt-timestamp" data-t="21:17:00">[21:17:00]</a>. The knowledge representation is a "large catalog of weighted and interdependent and contextualized particulars" with no attempt to abstract <a class="yt-timestamp" data-t="21:33:00">[21:33:00]</a>. The ability to find concise abstractions of experience is crucial for generalization <a class="yt-timestamp" data-t="21:55:00">[21:55:00]</a>. Humans, by contrast, can make broad generalizations from small data sets, like learning war game strategies from a few thousand sessions across many titles <a class="yt-timestamp" data-t="23:08:00">[23:08:00]</a>. This "one-shot learning" or "few-shot learning" is missing in current architectures <a class="yt-timestamp" data-t="25:55:00">[25:55:00]</a>.

The AI industry's current focus is largely driven by commercial value, where repeating well-understood operations to maximize metrics is prioritized <a class="yt-timestamp" data-t="27:40:00">[27:40:00]</a>. Creative, imaginative, and unpredictable AI is less desired for tasks like ad clicks or military doctrine, which benefits highly leveraged deep neural nets <a class="yt-timestamp" data-t="28:19:00">[28:19:00]</a>. This economic pressure has led to [[Future Directions and Challenges for AI and AGI | AGI research remaining on the margins]] <a class="yt-timestamp" data-t="30:02:00">[30:02:00]</a>.

## [[Potential trajectories of AI advancements | Three Viable Paths to True AGI]]

Despite the challenges, several alternative approaches offer promising avenues for achieving AGI.

### 1. Cognitive Level Approach: Hybrid Neural Symbolic, Evolutionary Metagraph-Based AGI

This approach, exemplified by the [[OpenCog and its approach to AI | OpenCog project]], aims to emulate the human mind's high-level cognitive functions using advanced computer science algorithms <a class="yt-timestamp" data-t="33:30:00">[33:30:00]</a>. Instead of direct biological simulation, it takes inspiration from the human mind's functions like perception, action, planning, working memory, long-term memory, and social reasoning <a class="yt-timestamp" data-t="35:06:00">[35:06:00]</a>.

A key concept is "cognitive synergy," where distinct cognitive functions within the AI architecture transparently assist each other's internal processing <a class="yt-timestamp" data-t="36:38:00">[36:38:00]</a>. This is achieved by centering the system on a large distributed [[OpenCog and its approach to AI | Knowledge Graph]] (a hypergraph or metagraph) <a class="yt-timestamp" data-t="37:57:00">[37:57:00]</a>. Different AI algorithms (for perception, action, memory, reasoning, learning) act on this common graph, with new mathematical methods making them subcases of a single meta-algorithm <a class="yt-timestamp" data-t="39:32:00">[39:32:00]</a>.

This approach addresses criticisms of "good old-fashioned AI" (GOFAI) by:
*   Using logic-based knowledge representation but incorporating uncertainty (fuzzy, probabilistic logic) and allowing for contradictions <a class="yt-timestamp" data-t="43:57:00">[43:57:00]</a>.
*   Not depending on hand-coding common sense knowledge, instead relying on learning <a class="yt-timestamp" data-t="44:45:00">[44:45:00]</a>.
*   Integrating [[Approaches to evolving AI architectures | evolutionary learning]], both implicitly (e.g., attention-driven premise selection where importance acts like fitness) and explicitly (e.g., genetic algorithms for procedure learning or creativity) <a class="yt-timestamp" data-t="47:20:00">[47:20:00]</a>. The core system's "autopoiesis" (self-organization) and evolutionary dynamics are fundamental <a class="yt-timestamp" data-t="50:22:00">[50:22:00]</a>.

### 2. Brain Level Approach: Large-Scale Non-linear Dynamical Brain Simulation

This path involves simulating the brain's non-linear dynamics, distinct from current deep neural networks <a class="yt-timestamp" data-t="51:25:00">[51:25:00]</a>. While understanding the human brain is intrinsically interesting, current [[Challenges in AI measurement and transparency | measuring instruments]] are insufficient to fully reverse-engineer its complex processes, particularly abstraction formation <a class="yt-timestamp" data-t="52:48:00">[52:48:00]</a>. The brain involves more than just neurons; glia, astrocytes, cellular diffusion, and potentially quantum biology also play roles <a class="yt-timestamp" data-t="54:11:00">[54:11:00]</a>.

Despite these limitations, there's a lack of large-scale brain simulations aimed at producing intelligent behavior <a class="yt-timestamp" data-t="54:46:00">[54:46:00]</a>. Pioneering work by Gerald Edelman and Eugene Izhikevich on chaos theory-based neuron models, more biologically realistic than those in modern DNNs, showed how disparate neurons could be holistically bound by sub-threshold charge leakage <a class="yt-timestamp" data-t="55:31:00">[55:31:00]</a>.

Recent developments, such as Alex Ororbia's predictive coding-based learning mechanism, offer a potential "backpropagation killer" that could work with more biologically realistic neurons (e.g., Izhikevich neurons that account for sub-threshold spreading of activation) and glia <a class="yt-timestamp" data-t="57:41:00">[57:41:00]</a>. This could lead to neural nets with better generalization and more compact models, allowing for cleaner interfaces with symbolic systems like [[OpenCog and its approach to AI | OpenCog]] <a class="yt-timestamp" data-t="01:10:00">[01:10:00]</a>.

A major hurdle for full brain simulation is the inadequacy of Von Neumann (serial) computing architectures <a class="yt-timestamp" data-t="01:39:00">[01:39:00]</a>. The brain's power comes from its inherent parallelism <a class="yt-timestamp" data-t="01:51:00">[01:51:00]</a>. While GPUs provided a degree of parallelization for DNNs <a class="yt-timestamp" data-t="01:02:01">[01:02:01]</a>, more specialized parallel hardware is needed for complex brain simulations <a class="yt-timestamp" data-t="01:04:02">[01:04:02]</a>. The development of specialized chips (e.g., for Izhikevich neurons or graph pattern matching) is becoming more viable <a class="yt-timestamp" data-t="01:05:06">[01:05:06]</a>, potentially leading to "AGI boards" integrating various specialized processors <a class="yt-timestamp" data-t="01:30:00">[01:30:00]</a>.

### 3. Chemistry Level Approach: Massively Distributed AI-Optimized Artificial Chemistry Simulation

This approach originates from artificial life research, extending beyond genetic algorithms to simulate entire artificial organisms with artificial metabolisms and genomes within simulated worlds <a class="yt-timestamp" data-t="01:16:32">[01:16:32]</a>. Recognizing the complexity of biological systems (DNA triggering RNA, methylation, epigenomics, protein catalysis), it seeks to capture the spirit of biochemistry <a class="yt-timestamp" data-t="01:17:33">[01:17:33]</a>.

Inspired by work like Walter Fontana's algorithmic chemistry, this involves creating "little list codelets" or programs that act on each other to produce new programs in complex reaction chains <a class="yt-timestamp" data-t="01:19:05">[01:19:05]</a>. The motivation is to evolve the underlying "chemistry" or representation itself, potentially finding a more expressive basis for intelligence than biological evolution's arbitrary chemistry on Earth <a class="yt-timestamp" data-t="01:21:03">[01:21:03]</a>.

Two sub-approaches exist:
1.  **Realistic chemistry simulation**: Simulating actual chemistry/biochemistry (e.g., Bruce Damer's EvoGrid project for the origin of life) <a class="yt-timestamp" data-t="01:22:46">[01:22:46]</a>. This requires immense compute resources <a class="yt-timestamp" data-t="01:26:21">[01:26:21]</a>.
2.  **Abstracted algorithmic chemistry**: Making an algorithmic chemistry that doesn't try to be a real chemistry simulation, potentially requiring fewer compute resources <a class="yt-timestamp" data-t="01:26:59">[01:26:59]</a>. This could involve modern programming languages like [[OpenCog and its approach to AI | Meta]] (Meta-type talk) used in [[OpenCog and its approach to AI | OpenCog Hyperion]] <a class="yt-timestamp" data-t="01:27:48">[01:27:48]</a>.

A crucial enhancement is using an [[Impact of algorithms and AI on society | AI observer]] system (machine learning) to study the evolving chemical soup, identify promising "vats" or configurations, and guide the evolution <a class="yt-timestamp" data-t="01:29:10">[01:29:10]</a>. This "directed chemical evolution" leads to hybrid architectures where the algorithmic chemistry is accelerated by machine learning and proto-AGI <a class="yt-timestamp" data-t="01:30:38">[01:30:38]</a>. This approach also has a beautiful decentralized aspect, envisioning millions of people running virtual algorithmic chemistry simulations analyzed and refreshed by a central AGI system <a class="yt-timestamp" data-t="01:32:04">[01:32:04]</a>.

Again, the challenge of truly parallel hardware for simulating physical chemistry or analogous algorithmic chemistry remains <a class="yt-timestamp" data-t="01:33:38">[01:33:38]</a>. However, creative exploration into nanometer-scale computing infrastructures could open new avenues <a class="yt-timestamp" data-t="01:36:12">[01:36:12]</a>.

## [[Future Directions and Challenges for AI and AGI | Conclusion: Supporting Diverse AGI Research]]

All three approaches are interesting and warrant far more attention and resources than they currently receive <a class="yt-timestamp" data-t="01:25:01">[01:25:01]</a>. While the cognitive-level approach (hybrid AGI) is currently seen as the most likely path to AGI, it can still integrate ideas from other paradigms, such as biologically realistic neural nets for perception or algorithmic chemistry for creative idea generation <a class="yt-timestamp" data-t="01:38:36">[01:38:36]</a>.

The amount of funding needed for AGI research is substantial but pales in comparison to global spending on less impactful endeavors <a class="yt-timestamp" data-t="01:40:05">[01:40:05]</a>. Increased funding, potentially on the scale of hundreds of billions of dollars, could massively accelerate AGI R&D <a class="yt-timestamp" data-t="01:41:04">[01:41:04]</a>. This could come from governments improving research funding distribution <a class="yt-timestamp" data-t="01:52:00">[01:52:00]</a>, or through a cultural shift towards [[Impact of AI advancements on various industries | citizen science]] and open-source development for AGI, as more people recognize AGI's viability and the limitations of big tech's current approaches <a class="yt-timestamp" data-t="01:47:01">[01:47:01]</a>. Such a shift could be part of broader positive cultural changes for the greater good <a class="yt-timestamp" data-t="01:49:40">[01:49:40]</a>.