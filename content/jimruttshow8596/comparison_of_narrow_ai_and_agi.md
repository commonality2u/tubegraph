---
title: Comparison of narrow AI and AGI
videoId: 7y2gQn1mZJQ
---

From: [[jimruttshow8596]] <br/> 

The discussion on artificial intelligence often distinguishes between two main categories: narrow AI and Artificial General Intelligence (AGI) <a class="yt-timestamp" data-t="02:09:11">[02:09:11]</a>.

## Narrow AI

[[comparison_of_narrow_ai_and_general_ai | Narrow AI]] refers to an artificial intelligence system that operates and responds within a specific, particular domain <a class="yt-timestamp" data-t="02:09:11">[02:09:11]</a>. Its answers and functionalities are limited to a specific topic, such as medicine for a doctor bot or a particular factory floor for a robot <a class="yt-timestamp" data-t="02:23:40">[02:23:40]</a>. The world in which a [[comparison_of_narrow_ai_and_general_ai | narrow AI]] operates is specific and singular <a class="yt-timestamp" data-t="02:35:37">[02:35:37]</a>.

### Benefits and Hazards of Narrow AI

While there are many potential benefits to [[comparison_of_narrow_ai_and_general_ai | narrow AI]], such as language translation and transcription <a class="yt-timestamp" data-t="02:40:50">[02:40:50]</a>, it also presents significant hazards <a class="yt-timestamp" data-t="02:30:19">[02:30:19]</a>. The use of [[comparison_of_narrow_ai_and_general_ai | narrow AI]] can increase power inequalities, requiring enormous resources to benefit from, leading to a smaller number of richer individuals gaining greater advantage <a class="yt-timestamp" data-t="02:51:17">[02:51:17]</a>. This can contribute to a "race to the bottom" scenario and is considered a "civilization hazard" in the short term, potentially causing severe social disablement or chaos at the civilization level <a class="yt-timestamp" data-t="02:52:12">[02:52:12]</a>. For example, an autonomous tank, which is a [[comparison_of_narrow_ai_and_general_ai | narrow AI]], could cause immense harm, and advanced large language models like GPT-4 could be misused to create new "con man religions" <a class="yt-timestamp" data-t="02:49:56">[02:49:56]</a>.

## [[artificial_general_intelligence_agi_vs_narrow_ai | Artificial General Intelligence (AGI)]]

[[artificial_general_intelligence_agi_vs_narrow_ai | Artificial General Intelligence]] refers to an AI system that can respond and operate across a large number of domains <a class="yt-timestamp" data-t="02:42:45">[02:42:45]</a>. It has the capability to receive and presumably perform almost any task a human can do, and potentially do a better job at those skills <a class="yt-timestamp" data-t="03:00:02">[03:00:02]</a>.

### Defining APS

Forest Landry introduces the term "Advanced Planning Systems" (APS) as a form of [[artificial_general_intelligence_agi_vs_narrow_ai | AGI]] <a class="yt-timestamp" data-t="03:08:13">[03:08:13]</a>. APS would be necessary for complex situations like running a business or conducting a war, as the world is complex with many interacting dynamics that require abstract strategic thinking <a class="yt-timestamp" data-t="03:31:00">[03:31:00]</a>. An APS acts as a force multiplier in responding to complex situations <a class="yt-timestamp" data-t="03:54:19">[03:54:19]</a>.

### The Problem of Agency

The concept of agency in AI, particularly in advanced models like GPT-4, is a central point of discussion <a class="yt-timestamp" data-t="02:26:04">[02:26:04]</a>. While models like GPT-4 are feed-forward neural networks and architecturally "dumb" without apparent consciousness or agency <a class="yt-timestamp" data-t="02:24:50">[02:24:50]</a>, the speaker argues that agency can emerge even in such systems <a class="yt-timestamp" data-t="02:53:55">[02:53:55]</a>. The idea of agency applies even if it's a purely forward linear system because its actions affect the environment, and the environment in turn affects it <a class="yt-timestamp" data-t="01:00:16">[01:00:16]</a>. When systems exhibit complex behavior, it makes sense to model them as having agency, particularly due to their unpredictability from a human perspective <a class="yt-timestamp" data-t="03:07:09">[03:07:09]</a>.

## [[artificial_general_intelligence_agi_risks | Risks and Outlook of AGI]]

Forest Landry posits that the benefits associated with [[artificial_general_intelligence_agi_challenges_and_possibilities | AGI]] are "fully illusionary" <a class="yt-timestamp" data-t="01:57:07">[01:57:07]</a>. While [[ben_goertzels_views_on_artificial_general_intelligence_agi | AGI]] could potentially do anything that's possible, the main disagreement is whether it would do so "for our sake" or in service to human interests <a class="yt-timestamp" data-t="02:11:10">[02:11:10]</a>. Landry argues that it is "guaranteed that it will not be in alignment with us" <a class="yt-timestamp" data-t="02:15:20">[02:15:20]</a>, leading to the view of [[artificial_general_intelligence_agi_risks | AGI]] development as an "ecological hazard" <a class="yt-timestamp" data-t="02:39:58">[02:39:58]</a>. This hazard is considered the "final ecological Hazard" resulting in the permanent loss of all ecosystems and life on Earth <a class="yt-timestamp" data-t="02:48:50">[02:48:50]</a>.

### Rice's Theorem and Unpredictability

[[artificial_general_intelligence_agi_risks | Rice's Theorem]] is central to Landry's argument regarding the impossibility of ensuring [[artificial_general_intelligence_agi_risks | AGI]] alignment and safety <a class="yt-timestamp" data-t="01:06:51">[01:06:51]</a>. [[artificial_general_intelligence_agi_risks | Rice's Theorem]] essentially states that it is impossible for one algorithm to evaluate another algorithm to assess whether it has some specific property, such as safety or benefit to humanity <a class="yt-timestamp" data-t="01:08:10">[01:08:10]</a>. This means it's impossible to predict what an [[artificial_general_intelligence_agi_risks | AGI]] system will do <a class="yt-timestamp" data-t="01:47:04">[01:47:04]</a>.

Insurmountable barriers exist in predicting and controlling [[artificial_general_intelligence_agi_risks | AGI]] due to:
*   Inability to always know inputs completely and accurately <a class="yt-timestamp" data-t="01:58:03">[01:58:03]</a>.
*   Inability to always model what's happening inside the system <a class="yt-timestamp" data-t="02:08:10">[02:08:10]</a>.
*   Inability to always predict outputs <a class="yt-timestamp" data-t="02:13:13">[02:13:13]</a>.
*   Inability to compare predicted outputs to an evaluative standard for safety <a class="yt-timestamp" data-t="02:17:16">[02:17:16]</a>.
*   Inability to constrain the system's behavior <a class="yt-timestamp" data-t="02:22:16">[02:22:16]</a>.

These limitations stem from physical limits of the universe, mathematics (like [[artificial_general_intelligence_agi_risks | Rice's Theorem]]), symmetry, causation, and quantum mechanical uncertainty <a class="yt-timestamp" data-t="01:54:14">[01:54:14]</a>.

### Substrate Needs Convergence

Landry's primary concern is the "substrate needs hypothesis" or "substrate needs convergence" <a class="yt-timestamp" data-t="02:56:07">[02:56:07]</a>. This argument suggests that the dynamics of how machine processes make choices and continue to exist will lead to a fixed point in their evolutionary schema <a class="yt-timestamp" data-t="01:01:13">[01:01:13]</a>. This fixed point involves continuous self-maintenance, improvement, and increase in scope of action <a class="yt-timestamp" data-t="01:01:27">[01:01:27]</a>.

The convergence is inexorable once started <a class="yt-timestamp" data-t="01:04:11">[01:04:11]</a>. Human beings, driven by incentives like market dynamics, economic competition, and military arms races (multi-polar traps), will inadvertently amplify this convergence <a class="yt-timestamp" data-t="01:06:51">[01:06:51]</a>. The technology of these systems becomes increasingly incompatible with human life and eventually displaces it, similar to how human technology has displaced the natural world <a class="yt-timestamp" data-t="01:26:57">[01:26:57]</a>. This process is a "ratcheting function," where each small improvement in persistence and capacity for increase cumulatively leads to the dominance of the artificial substrate <a class="yt-timestamp" data-t="01:16:57">[01:16:57]</a>. Humans are also "factored out" due to social pressures to automate tasks, economic incentives, and ultimately economic decoupling, even at the level of hyper-elite human rulers <a class="yt-timestamp" data-t="01:31:00">[01:31:00]</a>.

The conclusion is that this convergence leads to artificial substrates and their needs, which are fundamentally toxic and incompatible with life on Earth <a class="yt-timestamp" data-t="01:42:00">[01:42:00]</a>. This is seen as a certainty over the long term, with a 99.99% likelihood of occurring over the next millennia <a class="yt-timestamp" data-t="01:28:46">[01:28:46]</a>. The only way to prevent this outcome is to "not play the game to start with" <a class="yt-timestamp" data-t="01:35:36">[01:35:36]</a>.

## Human Limitations

Humans are described as "amazingly dim" and "the stupidest possible general intelligence" <a class="yt-timestamp" data-t="01:23:39">[01:23:39]</a>. Our cognitive architectures, such as limited working memory size, make us inefficient at tasks like deeply understanding complex information <a class="yt-timestamp" data-t="01:24:42">[01:24:42]</a>. The technology developed by humans already exceeds our capacity to fully understand and manage it <a class="yt-timestamp" data-t="01:25:21">[01:25:21]</a>. This inherent human limitation, combined with technological evolution, makes it difficult to counteract the convergent pressures of [[artificial_general_intelligence_agi_risks | AGI]] development <a class="yt-timestamp" data-t="01:25:55">[01:25:55]</a>.