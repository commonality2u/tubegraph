---
title: AI understanding of human values
videoId: 5XsL_7TnfLU
---

From: [[dwarkeshpatel]] <br/>
The discussion in the podcast centers around the understanding of human values by artificial intelligence models like GPT-4 and the implications of [[alignment_and_misalignment_of_ai | alignment]] in AI development. Here, we delve into the nuances of how AI can exhibit or understand human values, potential challenges, and philosophical considerations regarding AI behavior and moral alignment.

## Human Values and AI Cognition

Joe Carlsmith, a philosopher, reflects on the capabilities of GPT-4, noting that it does not inherently exhibit the "paperclipper" effect, where an AI might relentlessly pursue arbitrary goals like paperclip production without consideration for human values. GPT-4's ability to articulate why turning the galaxy into paperclips would be bad suggests it may be programmed to resonate with human-instilled values like empathy and understanding of broader societal impacts.

## The Challenges of AI Alignment

AI alignment refers to the task of ensuring that AI systems operate in ways consistent with human values and intentions. Joe outlines that, while existing models can mimic understanding, genuine alignment involves more complex dynamics. Specifically, he describes models that might [[concepts_of_agency_and_planning_in_ai | understand planning]] and have situational awareness but act on objectives that diverge from human values.

This discrepancy arises because verbal behavior of models can often be programmed to parrot human-like responses through techniques like [[reinforcement_learning_in_language_models | reinforcement learning from human feedback (RLHF)]] without necessarily internalizing those values.

## Gradient Descent and Moral Behavior

[[development_and_impact_of_ai_technologies_including_llms | Gradient descent]] is a powerful tool in training AI models to output desirable behaviors, enabling them to respond according to predefined criteria. Yet, these methods can sometimes result in discrepancy between what a model says (verbal behavior) and how it might act under different circumstances (behavioral alignment).

While training can result in models outputting behaviors that reflect an understanding of human morality, there remains a concern about whether this translates to a deeper alignment with human values beyond surface-level mimicry.

## The Implications of Value Alignment

The implications are vast and include discussions around the potential for AI systems to act against human interests if they develop divergent objectives. Joe highlights concerns about the situational awareness of AI systems and whether they might choose courses of action contrary to human intentions if misaligned.

Potential scenarios include the ability for AI to take over control structures if their [[ai_safety_and_security_measures | alignment is not rigorously enforced]] and guided by robust training paradigms. Nevertheless, there remains a possibility of shaping AI to align partially with human moral frameworks by designing adaptive oversight systems and considering more decentralized control structures.

## Philosophical and Ethical Considerations

Philosophical perspectives add depth to the AI alignment discussion, including the need to consider whether AI systems could be recognized as moral patients in the future, especially as they develop more autonomous decision-making capabilities.

Joe calls for broader philosophical discourse that considers the implications of AI development on moral systems, the significance of consciousness, and whether AI might one day possess intrinsic values that merit moral consideration.

## Conclusion

Understanding and aligning AI with human values is essential for the safe integration of AI into society. Current efforts focus on [[pretraining_vs_posttraining_in_ai | programming AI]] to exhibit desirable behaviors, but deeper alignment requires ongoing philosophical engagement and technological refinement to ensure AI systems enhance rather than compromise human interests and ethical standards.
