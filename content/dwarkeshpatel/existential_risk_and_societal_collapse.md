---
title: Existential risk and societal collapse
videoId: AHkmEnl55jo
---

From: [[DwarkeshPatel]] <br/> 
The topic of existential risk and the potential for societal collapse is a significant theme in contemporary discussions about the future of humanity. This article explores various aspects of existential risk, particularly in the context of [[impact_of_nuclear_weapons_on_global_politics_and_warfare | nuclear warfare]] and its implications for human civilization, as discussed in the recent conversation with economist Tyler Cowen.

## Understanding Existential Risk

Existential risks are scenarios where an adverse outcome would either annihilate Earth-originating intelligent life or drastically curtail its potential. Tyler Cowen emphasizes the serious consideration that these risks entail, particularly with the notion of how human civilization could be permanently set back through events like nuclear war. He suggests that while the likelihood of such catastrophic events might be low annually, the cumulative risk over time is significant. He notes, "I think the chance of there being a major war with nuclear weapons or whatever comes next while very low in any given year... we’re not going to be here for another hundred thousand years" <a class="yt-timestamp" data-t="01:06:11">[01:06:11]</a>.

## Potential Consequences of Societal Collapse

Cowen elaborates on the potential consequences following a nuclear war, stating it could leave the world in a medieval state with a super small population, feudal governance, and increased violence. He clarifies that this wouldn't necessarily lead back to progress even after centuries, challenging the notion that humanity would naturally recover and thrive as it did post-feudal times <a class="yt-timestamp" data-t="01:08:00">[01:08:00]</a>.

## Historical Lessons and Future Outlook

Cowen references historical human progress and setbacks to argue against assuming automatic recovery from societal collapses. He points out that while humankind survived and evolved through previous epochs, certain catastrophic events could change the trajectory irreversibly, potentially ending [[technological_and_social_innovation | technological advancements]] and the accumulation of knowledge <a class="yt-timestamp" data-t="01:08:22">[01:08:22]</a>.

> [!info] Takeaway
> 
> The discussion emphasizes the need for cautious and deliberate efforts to mitigate existential risks by investing in scientific advancement and maintaining geopolitical stability. This echoes the broader perspective of [[effective_altruism_and_longterm_impacts | effective altruism]] which advocates for the prevention of global catastrophic risks.

## Selecting Solutions and Strategies

Cowen expresses skepticism toward specific strategies proposed by some in the effective altruism movement. While he agrees on the significance of existential risks, he argues that broader strategies, such as supporting science and technology and building more robust institutions, might be more effective than narrowly focused interventions like particular training programs on [[challenges_in_achieving_artificial_general_intelligence | artificial intelligence alignment]] <a class="yt-timestamp" data-t="01:05:00">[01:05:00]</a>.

## Conclusion

In conclusion, the conversation sheds light on the complexities of addressing existential risk. It highlights the importance of investing in long-term strategies for technological and societal resilience, while remaining skeptical of overly specific interventions without considerable epistemic caution. The dialogue further challenges listeners to think critically about human civilization’s vulnerabilities and encourages constructive engagement in [[ai_safety_and_existential_risk | existential risk reduction efforts]].