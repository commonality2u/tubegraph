---
title: The evolution and impact of AI model hosting platforms
videoId: hzc1covUhYM
---

From: [[acquiredfm]] <br/> 

Hugging Face has emerged as a pivotal platform for [[generative_ai_and_its_rapid_evolution | AI builders]], representing a significant shift in how technology is created <a class="yt-timestamp" data-t="01:06:00">[01:06:00]</a>. In contrast to the traditional software development paradigm of writing millions of lines of code, the new approach involves training models, utilizing datasets, and building AI applications <a class="yt-timestamp" data-t="01:17:00">[01:17:00]</a>.

## Hugging Face's Ecosystem and Scale

Hugging Face serves as the primary platform for individuals engaged in this new method of technology creation <a class="yt-timestamp" data-t="01:39:00">[01:39:00]</a>.
Key aspects of its operation include:

*   **Vast User Base** The platform boasts over 5 million [[generative_ai_and_its_rapid_evolution | AI builders]] utilizing it daily <a class="yt-timestamp" data-t="01:47:00">[01:47:00]</a>.
*   **Content Volume** Users have collectively shared over 3 million models, datasets, and applications on the platform <a class="yt-timestamp" data-t="03:37:00">[03:37:00]</a>. Popular examples include Lama 3.1, Stable Diffusion for images, Whisper for audio, and Flux for images <a class="yt-timestamp" data-t="03:46:00">[03:46:00]</a>. The platform is nearing 1 million public models shared, with almost as many being used privately by companies <a class="yt-timestamp" data-t="04:00:00">[04:00:00]</a>.
*   **Analogy to GitHub** Hugging Face is often compared to GitHub, but for AI models, allowing both public, [[Open Source AI vs Closed AI ecosystems | open-source]] sharing and internal, closed-source repositories for corporate use <a class="yt-timestamp" data-t="04:14:00">[04:14:00]</a>.
*   **Rapid Innovation** A new model, dataset, or application is built on the Hugging Face platform every 10 seconds <a class="yt-timestamp" data-t="04:56:00">[04:56:00]</a>.
*   **Comprehensive Platform Features** Beyond just hosting, Hugging Face provides tools for running applications, offering compute resources for model training, and crucial collaboration features. These include commenting, versioning code, models, and datasets, and bug reporting, all vital for larger teams building AI together <a class="yt-timestamp" data-t="05:26:00">[05:26:00]</a>.

The rise of platforms like Hugging Face signals a new era where AI is enabling new use cases and unlocking capabilities previously thought impossible, with discussions even touching on the potential for artificial general intelligence (AGI) <a class="yt-timestamp" data-t="02:36:00">[02:36:00]</a>.

## Evolution from Chatbot to AI Hub

Hugging Face's journey began in 2016 as a conversational AI chatbot targeting teenagers, named after the hugging face emoji <a class="yt-timestamp" data-t="07:06:00">[07:06:00]</a>. The founders were driven by an excitement for AI, then primarily called machine learning or deep learning <a class="yt-timestamp" data-t="07:33:00">[07:33:00]</a>. They initially aimed to build a fun, AI-powered Tamagotchi-like virtual pet <a class="yt-timestamp" data-t="08:30:00">[08:30:00]</a>.

A pivotal moment occurred in 2017 with the release of the "Transformer paper" from Google <a class="yt-timestamp" data-t="10:17:00">[10:17:00]</a>, followed by Google's BERT model <a class="yt-timestamp" data-t="12:45:00">[12:45:00]</a>. Recognizing the community's preference for PyTorch over TensorFlow, Hugging Face's co-founder Thomas ported BERT to PyTorch, which generated significant developer interest <a class="yt-timestamp" data-t="13:05:00">[13:05:00]</a>. This led to other scientists requesting to add their models, such as XLNet and GPT-2 (when it was still [[Open Source AI vs Closed AI ecosystems | open source]]) <a class="yt-timestamp" data-t="15:03:00">[15:03:00]</a>.

This organic, [[the_growth_and_influence_of_communitydriven_ai_development | community-driven development]] transformed Hugging Face from a single model repository (originally "pre-trained pytorch birds," then "pytorch transformers," then "Transformers" library) into a comprehensive platform for models and datasets <a class="yt-timestamp" data-t="15:26:00">[15:26:00]</a>. This evolution essentially made it "a new GitHub for AI" <a class="yt-timestamp" data-t="16:37:00">[16:37:00]</a>, with its success largely attributed to following community feedback <a class="yt-timestamp" data-t="16:42:00">[16:42:00]</a>.

## Openness, Business Models, and the Future of AI Development

Hugging Face champions openness, allowing companies to be more open than they might otherwise be. While half of the models, datasets, and apps on the platform are private and used internally, the platform facilitates sharing research papers and progressively more data, reinforcing the belief that [[Open Source AI vs Closed AI ecosystems | open science]] and [[Open Source AI vs Closed AI ecosystems | open-source AI]] lift all boats <a class="yt-timestamp" data-t="17:51:00">[17:51:00]</a>. This approach aims to provide access to technology for everyone, fostering a safer future by decentralizing AI development <a class="yt-timestamp" data-t="19:36:00">[19:36:00]</a>.

The company itself operates profitably, having raised over $500 million but spending less than half of that <a class="yt-timestamp" data-t="38:39:00">[38:39:00]</a>. Their business model avoids a "race to the bottom" on compute costs, instead focusing on providing integrated value with their platform features, making the process of using and training models 10 times easier for companies <a class="yt-timestamp" data-t="41:51:00">[41:51:00]</a>. This allows companies to reduce their need for large AI infrastructure deployment teams <a class="yt-timestamp" data-t="43:39:00">[43:39:00]</a>.

Hugging Face's leadership believes that the "software 2.0" paradigm of AI will empower a new generation of companies and founders <a class="yt-timestamp" data-t="31:31:00">[31:31:00]</a>. They see a future where AI companies are not just consuming APIs from large foundational models but are building and optimizing their own models for specific use cases, constraints, and domains, leading to a landscape with potentially as many AI models as code repositories today <a class="yt-timestamp" data-t="50:06:00">[50:06:00]</a>. This suggests a shift back towards specialized, efficient, and cheaper models, as opposed to solely relying on large, generalist "God models" <a class="yt-timestamp" data-t="52:10:00">[52:10:00]</a>.

The core difference between the software and AI paradigms lies in AI being more science-driven <a class="yt-timestamp" data-t="55:00:00">[55:00:00]</a>. Successful AI companies often have a science co-founder, and the development timeline is different: instead of rapid, incremental improvements, AI development often involves longer periods of research leading to step-function advancements <a class="yt-timestamp" data-t="56:00:00">[56:00:00]</a>. This new paradigm necessitates "trashing the Lean Startup book" and building from first principles <a class="yt-timestamp" data-t="01:00:07">[01:00:07]</a>.

Ultimately, the future of AI development will require both more [[generative_ai_and_its_rapid_evolution | AI builders]] and easier-to-use tools and infrastructure <a class="yt-timestamp" data-t="01:05:30">[01:05:30]</a>. With an estimated 5 million [[generative_ai_and_its_rapid_evolution | AI builders]] today compared to 50 million software engineers, there's significant room for growth <a class="yt-timestamp" data-t="01:05:49">[01:05:49]</a>. The lower barrier to entry for contributing to AI (e.g., contributing expertise or data rather than just code) could lead to an even larger and more diverse community of builders, shaping a more inclusive and socially beneficial technology <a class="yt-timestamp" data-t="01:06:34">[01:06:34]</a>.