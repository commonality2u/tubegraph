---
title: Hugging Face platform
videoId: hzc1covUhYM
---

From: [[acquiredfm]] <br/> 

Hugging Face has emerged as the leading platform for AI Builders, who are considered the new software engineers in the evolving paradigm of technology development <a class="yt-timestamp" data-t="01:06:06">[01:06:06]</a>. Instead of writing millions of lines of code, technology is now created by training models using datasets and building AI applications <a class="yt-timestamp" data-t="01:31:40">[01:31:40]</a>.

## Overview and Scale
Hugging Face serves as the primary platform where over 5 million AI builders daily find models and datasets to build applications <a class="yt-timestamp" data-t="01:47:11">[01:47:11]</a>. The company is currently valued at $4.5 billion, with investors including Nvidia, Salesforce, Google, Amazon, Intel, AMD, Qualcomm, and IBM <a class="yt-timestamp" data-t="03:09:00">[03:09:00]</a>.

Key metrics illustrating its scale and usage include:
*   Over 5 million active AI builders <a class="yt-timestamp" data-t="01:47:11">[01:47:11]</a>.
*   Collectively, users have shared over 3 million models, datasets, and apps on the platform <a class="yt-timestamp" data-t="03:37:00">[03:37:00]</a>.
*   The platform is nearing 1 million public models shared, with almost as many models used internally and privately by companies <a class="yt-timestamp" data-t="04:00:00">[04:00:00]</a>.
*   A new model, dataset, or app is built on the Hugging Face platform every 10 seconds <a class="yt-timestamp" data-t="04:56:00">[04:56:00]</a>.

The ecosystem surrounding Hugging Face is likened to the Web 2.0 era of restful APIs, enabling the "daisy chaining" of companies and services <a class="yt-timestamp" data-t="02:05:00">[02:05:00]</a>. It empowers new use cases, such as AI-built search and social networks, and unlocks capabilities previously thought impossible <a class="yt-timestamp" data-t="02:28:00">[02:28:00]</a>.

## Core Functionality and Collaboration
Hugging Face functions similarly to GitHub, but for AI models <a class="yt-timestamp" data-t="04:14:00">[04:14:00]</a>. It supports public open-source contributions and private repositories for internal company use <a class="yt-timestamp" data-t="04:19:00">[04:19:00]</a>. Beyond hosting code, it also provides a platform for datasets, running applications, and compute for training models <a class="yt-timestamp" data-t="05:26:00">[05:26:00]</a>.

A crucial aspect of Hugging Face's platform is its extensive collaboration features. Building AI often requires team effort, and sometimes external collaboration, making tools like commenting, versioning (code, models, datasets), bug reporting, and reviews highly used <a class="yt-timestamp" data-t="05:42:00">[05:42:00]</a>. This enables larger teams to build AI together, with thousands of users at companies like Microsoft, Nvidia, and Salesforce utilizing the platform privately and publicly <a class="yt-timestamp" data-t="06:23:00">[06:23:00]</a>.

## History and Evolution
Founded in 2016 by Clem Delong, Julian, and Thomas, Hugging Face began as a chatbot aimed primarily at teenagers, named after the Unicode emoji <a class="yt-timestamp" data-t="07:06:00">[07:06:00]</a>. Early investors included BetaWorks (John Borwick and Matt Altman), Richard Socher, and the Conway family (Ron Conway) <a class="yt-timestamp" data-t="09:02:00">[09:02:00]</a>.

The pivot from a consumer chatbot to an AI infrastructure company was organic and community-driven <a class="yt-timestamp" data-t="14:43:00">[14:43:00]</a>. It began around 2017 when the Transformer paper was released by Google <a class="yt-timestamp" data-t="10:17:00">[10:17:00]</a>. Thomas, the co-founder and chief scientist, ported Google's BERT model from TensorFlow to PyTorch over a weekend, which garnered significant developer interest <a class="yt-timestamp" data-t="12:40:00">[12:40:00]</a>. This led to Hugging Face becoming a repository for various models, including those from OpenAI (GPT-2) <a class="yt-timestamp" data-t="15:03:00">[15:03:00]</a>.

The company followed community feedback, building a platform to host models, then datasets, and eventually becoming a "new GitHub for AI" <a class="yt-timestamp" data-t="16:12:00">[16:12:00]</a>. This adaptability highlights the importance of flexibility and seizing new opportunities, even after years of operation and significant funding <a class="yt-timestamp" data-t="11:48:00">[11:48:00]</a>.

## Openness Philosophy
Hugging Face is known for its strong commitment to openness, believing it to be the foundation of all AI <a class="yt-timestamp" data-t="26:57:00">[26:57:00]</a>. Even closed-source companies rely heavily on open research and open-source foundations <a class="yt-timestamp" data-t="27:09:00">[27:09:00]</a>. While the field has become less open commercially in recent years, the initial collaborative openness led to faster progress, as seen with OpenAI's development of GPT-2 and GPT-3 from Transformers <a class="yt-timestamp" data-t="27:29:00">[27:29:00]</a>.

The company provides tools for organizations to be more open, while also acknowledging that approximately half of the models, datasets, and apps on their platform are private <a class="yt-timestamp" data-t="17:51:00">[17:51:00]</a>. This approach encourages companies to share what they are comfortable with, progressively contributing more to the world <a class="yt-timestamp" data-t="18:20:00">[18:20:00]</a>.

Hugging Face champions openness because it believes it lifts all boats, enabling transparency, understanding, and a safer future for AI <a class="yt-timestamp" data-t="18:53:00">[18:53:00]</a>. The alternative, a non-decentralized AGI controlled by a single entity, carries much higher risk <a class="yt-timestamp" data-t="19:23:00">[19:23:00]</a>. Openness can empower thousands of new AI companies, redistributing power and aligning technological advancements with societal needs <a class="yt-timestamp" data-t="31:09:00">[31:09:00]</a>.

## Business Model
Hugging Face operates on a profitable and sustainable business model <a class="yt-timestamp" data-t="38:49:00">[38:49:00]</a>. They have raised over $500 million but spent less than half of that <a class="yt-timestamp" data-t="38:39:00">[38:39:00]</a>. Their model differs from capital-intensive foundational model companies like OpenAI, which often require billions for compute and training <a class="yt-timestamp" data-t="39:04:00">[39:04:00]</a>.

Their revenue streams include a freemium model and paid features, such as the Enterprise Hub offering <a class="yt-timestamp" data-t="39:19:00">[39:19:00]</a>. Hugging Face avoids the "race to the bottom" on compute prices by providing significant value through its platform features and integrated services <a class="yt-timestamp" data-t="41:51:00">[41:51:00]</a>. This "locked-in compute" experience makes it 10 times easier for companies to use their bundled services than to manage compute directly with a cloud provider, saving them the need for large ML infrastructure deployment teams <a class="yt-timestamp" data-t="42:47:00">[42:47:00]</a>.

## Future of AI Development and Investment
Clem Delong refers to AI as "Software 2.0" because it represents a new paradigm for building technology, providing immense leverage for builders and accelerating application development <a class="yt-timestamp" data-t="20:50:00">[20:50:00]</a>. He distinguishes this from the concept of AGI or superintelligence, arguing that the true excitement lies in empowering new capabilities, startups, and companies <a class="yt-timestamp" data-t="32:51:00">[32:51:00]</a>.

The investment landscape for AI startups is evolving <a class="yt-timestamp" data-t="33:29:00">[33:29:00]</a>. Building an AI startup differs significantly from a traditional software startup:
*   **Capital Intensity**: While some foundational models (like LLMs, video, biology, chemistry models) require heavy capital for compute, the emergence of smaller, more efficient, and purpose-built models means not all AI companies will be as capital intensive <a class="yt-timestamp" data-t="34:03:00">[34:03:00]</a>. Hugging Face believes that ultimately, there will be as many specialized models as code repositories today <a class="yt-timestamp" data-t="52:26:00">[52:26:00]</a>.
*   **Founding Teams**: AI is more science-driven than traditional software <a class="yt-timestamp" data-t="55:00:00">[55:00:00]</a>. Having a science co-founder is a significant advantage <a class="yt-timestamp" data-t="55:57:00">[55:57:00]</a>.
*   **Development Cycle**: Unlike the rapid shipping mentality of software startups, AI model training and optimization can take months, requiring a different approach to iteration and progress <a class="yt-timestamp" data-t="56:29:00">[56:29:00]</a>. Scientists often aim for 10x improvements after longer periods, rather than incremental 5% gains <a class="yt-timestamp" data-t="58:05:00">[58:05:00]</a>. This suggests trashing the "Lean Startup" playbook for AI <a class="yt-timestamp" data-t="01:00:07">[01:00:07]</a>.

There are approximately 5 million AI builders today, compared to 50-100 million software builders <a class="yt-timestamp" data-t="01:05:43">[01:05:43]</a>. Delong envisions a future with potentially 10 times more AI builders than software builders, as the barrier to entry for contributing to AI (e.g., through data or expertise) is lower than learning a programming language <a class="yt-timestamp" data-t="01:06:34">[01:06:34]</a>. This would lead to more inclusive and socially beneficial products <a class="yt-timestamp" data-t="01:07:35">[01:07:35]</a>.

## Challenges for Hugging Face
Despite its success, Hugging Face faces unique challenges as a platform company:
*   **Visibility**: As an infrastructure provider empowering builders, Hugging Face operates "behind the scenes" and may not achieve the same level of mainstream visibility or "sexiness" as direct-to-consumer AI companies like OpenAI, similar to how [[the_concept_and_potential_missteps_of_platforms_like_twitter | Twitter]] or [[history_and_evolution_of_facebook_meta | Facebook]] are more visible than GitHub <a class="yt-timestamp" data-t="00:27:18">[00:27:18]</a>.
*   **Balancing Innovation and Stability**: The underlying AI technology is evolving rapidly, creating a constant challenge to build mature, stable platforms while iterating fast enough to capture the next wave of innovation <a class="yt-timestamp" data-t="00:48:12">[00:48:12]</a>.
*   **Lean Team Size**: Hugging Face aims to maintain a significantly smaller team (around 250 members) compared to its peers (potentially 2,000), which helps reconcile the need for rapid building with tool development at scale <a class="yt-timestamp" data-t="00:48:52">[00:48:52]</a>.

## Learn More
To learn more about Hugging Face and get involved, visit huggingface.com <a class="yt-timestamp" data-t="01:07:57">[01:07:57]</a>. Clem Delong also shares updates on X (formerly [[the_concept_and_potential_missteps_of_platforms_like_twitter | Twitter]]) and LinkedIn <a class="yt-timestamp" data-t="01:08:25">[01:08:25]</a>.