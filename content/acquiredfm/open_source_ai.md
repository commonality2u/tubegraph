---
title: Open Source AI
videoId: hzc1covUhYM
---

From: [[acquiredfm]] <br/> 

Hugging Face, valued at $4.5 billion as of recording, stands at the center of the artificial intelligence (AI) revolution, particularly championing the open-source model <a class="yt-timestamp" data-t="00:15:00">[00:15:00]</a>. The company aims to provide a clear understanding of open-source AI versus closed ecosystems, including their differences, trade-offs, and virtues <a class="yt-timestamp" data-t="00:39:00">[00:39:00]</a>. Its investors include major tech companies like [[nvidia_and_the_ai_revolution | Nvidia]], Salesforce, Google, Amazon, Intel, AMD, Qualcomm, and IBM <a class="yt-timestamp" data-t="03:09:00">[03:09:00]</a>.

## Hugging Face's Role in the Ecosystem

Hugging Face has become the leading platform for [[impact_of_ai_on_software_development | AI builders]], who are described as the new "software engineers" of the current technology paradigm <a class="yt-timestamp" data-t="01:06:00">[01:06:00]</a>. Whereas previous technology development relied on writing millions of lines of code to create products, modern technology is built by training models using datasets and developing AI applications <a class="yt-timestamp" data-t="01:17:00">[01:17:00]</a>.

Over 5 million [[impact_of_ai_on_software_development | AI builders]] utilize the Hugging Face platform daily to discover models, find datasets, and create applications <a class="yt-timestamp" data-t="01:47:00">[01:47:00]</a>. The ecosystem around Hugging Face is reminiscent of the Web 2.0 era with its restful APIs, enabling the "daisy chaining" of various services into new applications <a class="yt-timestamp" data-t="02:05:00">[02:05:00]</a>. This fosters new use cases, such as AI-driven search and social networks, and unlocks previously impossible capabilities <a class="yt-timestamp" data-t="02:28:00">[02:28:00]</a>.

Key metrics illustrating Hugging Face's scale:
*   Over 3 million models, datasets, and apps have been shared on the platform <a class="yt-timestamp" data-t="03:37:00">[03:37:00]</a>.
*   Notable models shared include LLaMA 3.1, Stable Diffusion (for image), Whisper (for audio), and Flux (for image) <a class="yt-timestamp" data-t="03:49:00">[03:49:00]</a>.
*   The platform is approaching 1 million public models shared, with a similar number of private models used internally by companies <a class="yt-timestamp" data-t="04:00:00">[04:00:00]</a>.
*   A new model, dataset, or app is built on the Hugging Face platform every 10 seconds <a class="yt-timestamp" data-t="05:00:00">[05:00:00]</a>.

Hugging Face functions similarly to GitHub, but for AI models, supporting both public open-source projects and internal, closed-source repositories for companies <a class="yt-timestamp" data-t="04:14:00">[04:14:00]</a>. Beyond hosting models and datasets, it also provides a platform to run applications and a compute platform for model training <a class="yt-timestamp" data-t="05:26:00">[05:26:00]</a>. Crucially, it offers collaboration features such as commenting, versioning, bug reporting, and reviews, enabling large teams to build AI together <a class="yt-timestamp" data-t="05:45:00">[05:45:00]</a>.

## The Evolution of Open Source in AI

Hugging Face's journey began in 2016 as a chatbot aimed at teenagers, named after the hugging face emoji <a class="yt-timestamp" data-t="07:06:00">[07:06:00]</a>. Despite its initial focus, the founders were driven by their excitement for AI <a class="yt-timestamp" data-t="07:33:00">[07:33:00]</a>. The pivot to an AI infrastructure company occurred around 2017, when Google released the Transformer paper <a class="yt-timestamp" data-t="10:17:00">[10:17:00]</a>. The company's chief scientist, Thomas Wolf, ported Google's BERT Transformer model from TensorFlow to PyTorch, which garnered significant developer interest and led to Hugging Face hosting more models and datasets <a class="yt-timestamp" data-t="12:40:00">[12:40:00]</a>.

Initially, building conversational AI required stitching together many specialized models for tasks like information extraction, intent detection, answer generation, and emotion understanding <a class="yt-timestamp" data-t="10:51:00">[10:51:00]</a>. This complexity led Hugging Face to develop an abstraction layer to manage multiple models and datasets, forming the foundation for what the platform is today <a class="yt-timestamp" data-t="11:16:00">[11:16:00]</a>. The shift highlights the importance of flexibility and seizing new opportunities, even after years of development and significant funding <a class="yt-timestamp" data-t="11:48:00">[11:48:00]</a>.

> "The best advice I give to people is to trash their Lean Startup book when they're starting an AI company because I think these kind of things have been so ingrained into our mind into our way of building as kind of like a software entrepreneurs that it's really easy to fall into the Trap of doing it without even realizing we we do it instead of completely changing the Paradigm changing the operating system of the startup Builder which in my opinion leads to much better results" <a class="yt-timestamp" data-t="01:00:04">[01:00:04]</a>

In the past, especially around 2017-2019, much AI research, including the development of Transformers and BERT, was publicly shared by the research community <a class="yt-timestamp" data-t="27:36">[27:36]</a>. This openness and collaboration significantly accelerated progress in the field <a class="yt-timestamp" data-t="28:06">[28:06]</a>. However, in recent years, the field has become "less open" due to commercial considerations and "misleading arguments" regarding safety <a class="yt-timestamp" data-t="28:24">[28:24]</a>.

## The Virtue of Openness in AI

Hugging Face allows companies to choose their level of openness; about half of the models, datasets, and apps on the platform are private, used internally without public sharing <a class="yt-timestamp" data-t="18:08">[18:08]</a>. The company believes that openness and open-source AI are crucial for progress, enabling everyone to build, understand, and gain transparency into how AI systems function <a class="yt-timestamp" data-t="19:01">[19:01]</a>. This fosters a safer future by decentralizing AI access, rather than concentrating it in the hands of a few organizations <a class="yt-timestamp" data-t="19:36">[19:36]</a>.

There is a distinction between [[evolution_of_ai_development | AI]] as "software 2.0"—a tool that provides exponential leverage for builders—and the concept of artificial general intelligence (AGI) <a class="yt-timestamp" data-t="20:50">[20:50]</a>. While current models are building blocks for AGI, they are not necessarily direct stepping stones to it <a class="yt-timestamp" data-t="20:12">[20:12]</a>. The term "artificial intelligence" can create misconceptions by associating the technology with science fiction scenarios like superintelligence or singularities <a class="yt-timestamp" data-t="20:26">[20:26]</a>.

> "I'm incredibly scared of a non decentralized AGI you know like if only one company one organization gets to a gii I think that's when the risk is the highest versus if we can give access to the technology to everyone not only private companies but also policy makers nonprofits Civil Society I think it creates a much safer future and the future I'm much more excited about" <a class="yt-timestamp" data-t="19:23">[19:23]</a>

Openness in AI is expected to empower thousands of new AI companies, leading to a redistribution of influence in the tech landscape <a class="yt-timestamp" data-t="31:13">[31:13]</a>. This shift could help align societal challenges and preoccupations with the solutions companies are building <a class="yt-timestamp" data-t="31:53">[31:53]</a>.

## [[investment_and_business_models_in_ai | Investment and business models in AI]]

Building an AI startup differs significantly from building a traditional software startup <a class="yt-timestamp" data-t="34:47">[34:47]</a>. While AI can enable a few people to achieve massive output by leveraging APIs from foundational models, the development of new foundational models (e.g., for video, biology, chemistry, audio, image) often requires substantial capital investment for compute <a class="yt-timestamp" data-t="34:08">[34:08]</a>. This makes the traditional Lean Startup playbook less applicable <a class="yt-timestamp" data-t="35:56">[35:56]</a>.

Unlike many AI startups, Hugging Face has been capital-efficient and profitable <a class="yt-timestamp" data-t="38:36">[38:36]</a>. They have raised over $500 million but spent less than half of that <a class="yt-timestamp" data-t="38:39">[38:39]</a>. Hugging Face's business model relies on a permissive freemium offering, generating revenue through paid features like their Enterprise Hub offering, rather than engaging in a "race to the bottom" on compute prices <a class="yt-timestamp" data-t="39:19">[39:19]</a>. By providing a seamless, integrated experience that simplifies AI development, Hugging Face enables companies to reduce the need for large ML engineering teams, even if they pay a markup on compute <a class="yt-timestamp" data-t="42:20">[42:20]</a>.

## The Future of Open Source AI

The path for AI is still evolving, with debates about whether a few large foundational model companies will dominate or if a more distributed ecosystem of specialized models will emerge <a class="yt-timestamp" data-t="51:14">[51:14]</a>. Hugging Face's experience suggests that the future will likely see a vast number of diverse models, optimized for specific domains, latencies, hardware, and use cases <a class="yt-timestamp" data-t="52:10">[52:10]</a>. These models will often be smaller, more efficient, and cheaper to run than large generalist models <a class="yt-timestamp" data-t="52:26">[52:26]</a>.

This implies that instead of solely relying on APIs from a few large players, companies will eventually build, optimize, and fine-tune their own models to meet their specific needs and constraints <a class="yt-timestamp" data-t="50:57">[50:57]</a>. This shift will require a greater number of [[impact_of_ai_on_software_development | AI builders]] and scientists.

The field of AI is more science-driven than traditional software <a class="yt-timestamp" data-t="55:00">[55:00]</a>. Successful AI companies often have a science co-founder <a class="yt-timestamp" data-t="55:54">[55:54]</a>. The pace of development is also different; while software emphasizes rapid shipping, AI development often involves months of training and optimization to achieve significant, 10x improvements rather than incremental gains <a class="yt-timestamp" data-t="56:51">[56:51]</a>.

There are currently around 5 million [[impact_of_ai_on_software_development | AI builders]] globally, compared to an estimated 50-100 million software engineers <a class="yt-timestamp" data-t="01:05:41">[01:05:41]</a>. The barrier to entry for contributing to AI is potentially lower than for traditional software, as contributions can include expertise or data rather than just code <a class="yt-timestamp" data-t="01:06:34">[01:06:34]</a>. This suggests a future with potentially 10 times more [[impact_of_ai_on_software_development | AI builders]] than software builders, leading to more inclusive and socially impactful products <a class="yt-timestamp" data-t="01:07:03">[01:07:03]</a>. [[evolution_of_ai_development | AI]] is seen as the "opportunity of the century" to disrupt monopolies and create a new, more diverse landscape of companies <a class="yt-timestamp" data-t="01:05:01">[01:05:01]</a>.