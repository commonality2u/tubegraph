---
title: Development of AI Startups and Business Models
videoId: hzc1covUhYM
---

From: [[acquiredfm]] <br/> 

Hugging Face has become a leading platform for [[building_ai_models_and_industry_implications | AI Builders]], whom Clem Delong describes as the "new software engineers" <a class="yt-timestamp" data-t="01:09:47">[01:09:47]</a>. In contrast to previous technological paradigms where technology was built by writing millions of lines of code to create products like Facebook or Google, today's technology is created by training models using datasets and [[building_ai_models_and_industry_implications | building AI]] apps <a class="yt-timestamp" data-t="01:17:15">[01:17:15]</a>. Hugging Face facilitates this process by providing a platform for over 5 million [[building_ai_models_and_industry_implications | AI Builders]] to find models, datasets, and build applications <a class="yt-timestamp" data-t="01:47:04">[01:47:04]</a>. The platform hosts over 3 million models, datasets, and apps, including prominent ones like Llama 3.1, Stable Diffusion, Whisper, and Flux <a class="yt-timestamp" data-t="03:37:37">[03:37:37]</a>. A model, dataset, or app is built on Hugging Face every 10 seconds <a class="yt-timestamp" data-t="05:00:32">[05:00:32]</a>.

## Hugging Face: A Platform for AI Builders

The ecosystem around Hugging Face is likened to the Web 2.0 era of restful APIs, where different services could be daisy-chained together to create new functionalities <a class="yt-timestamp" data-t="01:54:39">[01:54:39]</a>. [[role_of_ai_in_revolutionizing_industries | AI]] is not only replacing previous capabilities, such as in search and social networks, but also unlocking entirely new use cases previously unimaginable <a class="yt-timestamp" data-t="02:27:01">[02:27:01]</a>.

Hugging Face is currently valued at $4.5 billion, with investors including [[the_evolution_of_ai_and_nvidias_role | Nvidia]], Salesforce, Google, Amazon, Intel, AMD, Qualcomm, and IBM <a class="yt-timestamp" data-t="03:09:07">[03:09:07]</a>. The company focuses on the frequency and volume of platform usage, noting that over 3 million models, datasets, and apps have been shared <a class="yt-timestamp" data-t="03:32:00">[03:32:00]</a>. This includes nearly 1 million public models and almost as many used privately by companies <a class="yt-timestamp" data-t="04:02:44">[04:02:44]</a>. The platform also offers collaboration features such as commenting, versioning, bug reporting, and reviews for models, datasets, and apps, facilitating larger teams to build [[building_ai_models_and_industry_implications | AI]] together <a class="yt-timestamp" data-t="05:42:01">[05:42:01]</a>. Large companies like Microsoft, [[the_evolution_of_ai_and_nvidias_role | Nvidia]], and Salesforce have thousands of users on the platform <a class="yt-timestamp" data-t="06:37:44">[06:37:44]</a>.

### Evolution from Chatbot to AI Hub

Hugging Face was co-founded in 2016 as a chatbot aimed at teenagers, named after the Unicode emoji <a class="yt-timestamp" data-t="07:06:01">[07:06:01]</a>. The initial idea was to build an [[building_ai_models_and_industry_implications | AI]] Tamagotchi, a fun conversational [[building_ai_models_and_industry_implications | AI]] that could talk about many different topics <a class="yt-timestamp" data-t="08:39:13">[08:39:13]</a>. This early venture, which raised its first two rounds of funding, required stitching together many different models and datasets for varied tasks like text extraction, intent detection, and emotion understanding <a class="yt-timestamp" data-t="10:47:04">[10:47:04]</a>. This need for an abstraction layer to manage multiple models and datasets became the foundation for what Hugging Face is today <a class="yt-timestamp" data-t="11:16:04">[11:16:04]</a>.

The pivot occurred around 2017 when Google released the Transformer paper and the BERT model in TensorFlow <a class="yt-timestamp" data-t="10:17:17">[10:17:17]</a>. Hugging Face's chief scientist, Thomas, ported BERT to PyTorch, which garnered significant developer attention <a class="yt-timestamp" data-t="12:40:07">[12:40:07]</a>. This led to the development of the "Transformers" library, which became a key open-source project <a class="yt-timestamp" data-t="15:47:00">[15:47:00]</a>. The company's development has been strongly community-driven, building features based on user feedback, such as hosting larger models, data sets, and search capabilities within them, ultimately leading to a "new GitHub for [[building_ai_models_and_industry_implications | AI]]" <a class="yt-timestamp" data-t="16:42:04">[16:42:04]</a>.

### Open vs. Closed Source in AI

Clem Delong emphasizes that [[impact_of_artificial_intelligence_and_open_source_on_modern_businesses | open source]] is the foundation for all [[building_ai_models_and_industry_implications | AI]], with even closed-source companies leveraging open research and tools significantly <a class="yt-timestamp" data-t="26:57:04">[26:57:04]</a>. However, the field has become less open in recent years <a class="yt-timestamp" data-t="27:29:16">[27:29:16]</a>. While between 2017 and 2019, most research and models were publicly shared by companies like Google and [[impact_of_ai_development_and_the_potential_future_of_openai | OpenAI]], this trend has shifted due to commercial considerations and "misleading arguments around the safety of openness" <a class="yt-timestamp" data-t="28:50:56">[28:50:56]</a>.

Delong argues that concerns about [[building_ai_models_and_industry_implications | AI]]'s existential risk are often used to justify restricting open research <a class="yt-timestamp" data-t="29:01:06">[29:01:06]</a>. He believes that a "non-decentralized AGI" controlled by only one company poses the highest risk <a class="yt-timestamp" data-t="19:23:44">[19:23:44]</a>. Instead, providing access to the technology for everyone—including private companies, policymakers, nonprofits, and civil society—creates a much safer and more exciting future <a class="yt-timestamp" data-t="19:36:09">[19:36:09]</a>.

Delong views current models as building blocks for AGI but prefers to call [[building_ai_models_and_industry_implications | AI]] "software 2.0" <a class="yt-timestamp" data-t="20:50:09">[20:50:09]</a>. He sees it as an improved paradigm for building technology, providing significant leverage for builders and astonishingly fast creation of rich applications <a class="yt-timestamp" data-t="21:39:07">[21:39:07]</a>.

## New Paradigms for AI Startups

Openness is crucial for empowering thousands of new [[role_of_technology_and_innovation_in_venture_capital_and_startups | AI companies]] <a class="yt-timestamp" data-t="31:17:17">[31:17:17]</a>. While big companies are doing well, the shift from software to [[building_ai_models_and_industry_implications | AI]] provides an opportunity to "redistribute the cards" and empower a new generation of [[role_of_technology_and_innovation_in_venture_capital_and_startups | founders]] and companies <a class="yt-timestamp" data-t="31:31:06">[31:31:06]</a>.

### Capital Intensity and Investment

[[role_of_technology_and_innovation_in_venture_capital_and_startups | Building an AI startup]] is very different from building a software startup, impacting how [[role_of_technology_and_innovation_in_venture_capital_and_startups | venture capital]] investors think about investment and returns <a class="yt-timestamp" data-t="33:47:04">[33:47:04]</a>. Many [[role_of_technology_and_innovation_in_venture_capital_and_startups | AI startups]], particularly foundational model companies like Mistral or [[impact_of_ai_development_and_the_potential_future_of_openai | OpenAI]], have a heavy need for capital for compute <a class="yt-timestamp" data-t="34:06:26">[34:06:26]</a>. This shifts the traditional "Lean Startup" playbook, as they often require substantial capital investment before any return <a class="yt-timestamp" data-t="35:58:08">[35:58:08]</a>.

Hugging Face, however, has operated differently. It has raised over $500 million but spent less than half of that and is profitable <a class="yt-timestamp" data-t="38:38:09">[38:38:09]</a>. Unlike some foundational model companies, Hugging Face doesn't have the same capital expenditure requirements for computing training <a class="yt-timestamp" data-t="39:03:00">[39:03:00]</a>. Their business model relies on a permissive freemium approach, where free usage drives adoption, and paid features, like their Enterprise Hub offering, provide high-margin revenue <a class="yt-timestamp" data-t="39:14:00">[39:14:00]</a>. They avoid the "race to the bottom on compute" by providing value through a platform that simplifies [[building_ai_models_and_industry_implications | AI]] development and deployment, making it "10 times easier" for companies to use their bundled services rather than managing compute directly with cloud providers <a class="yt-timestamp" data-t="41:51:00">[41:51:00]</a>.

The long-term profitability and sustainability of business models for [[building_ai_models_and_industry_implications | AI]] companies are still open questions <a class="yt-timestamp" data-t="44:04:00">[44:04:00]</a>. Clem Delong suggests that the idea of companies primarily using APIs from a few foundational model providers might be a transitional phase <a class="yt-timestamp" data-t="49:50:09">[49:50:09]</a>. He envisions a future where all tech companies become [[building_ai_models_and_industry_implications | AI]] companies, building, optimizing, and fine-tuning their own models for specific use cases and constraints <a class="yt-timestamp" data-t="51:04:08">[51:04:08]</a>. This future involves a massive number of specialized, smaller, more efficient, and cheaper models, potentially as many as code repositories today <a class="yt-timestamp" data-t="52:26:00">[52:26:00]</a>.

### Changing Skill Sets for Founders

A key difference between the software and [[building_ai_models_and_industry_implications | AI]] paradigms is that [[building_ai_models_and_industry_implications | AI]] is far more science-driven <a class="yt-timestamp" data-t="55:00:00">[55:00:00]</a>. For [[role_of_technology_and_innovation_in_venture_capital_and_startups | AI startup]] founding teams, having individuals with strong science backgrounds, particularly a science co-founder, is a significant advantage <a class="yt-timestamp" data-t="55:51:00">[55:51:00]</a>. The mindset and skill sets differ:
*   **Timing and Shipping**: While software startups emphasize shipping incredibly fast, [[building_ai_models_and_industry_implications | AI]] model training and optimization can take months, requiring a different approach to iteration <a class="yt-timestamp" data-t="56:29:00">[56:29:00]</a>.
*   **Skills**: [[building_ai_models_and_industry_implications | AI]] scientists often have stronger pure math skills <a class="yt-timestamp" data-t="57:16:00">[57:16:00]</a>.
*   **Improvement Scale**: Software often focuses on incremental improvements (e.g., 5% better), while [[building_ai_models_and_industry_implications | AI]] science can involve working for months to achieve breakthroughs that are "10 times better than the existing" <a class="yt-timestamp" data-t="57:38:00">[57:38:00]</a>. [[impact_of_ai_development_and_the_potential_future_of_openai | OpenAI]]'s trajectory, for example, involved years of development before releasing significant breakthroughs <a class="yt-timestamp" data-t="58:19:00">[58:19:00]</a>.

Delong advises new [[role_of_technology_and_innovation_in_venture_capital_and_startups | AI companies]] to "trash their Lean Startup book" and instead "start from scratch" with first principles, building a new operating system for the [[role_of_technology_and_innovation_in_venture_capital_and_startups | startup]] builder in the [[building_ai_models_and_industry_implications | AI]] paradigm <a class="yt-timestamp" data-t="01:00:07">[01:00:07]</a>.

### The Future of AI Development

[[role_of_ai_in_revolutionizing_industries | AI]] is seen as the "opportunity of the century" to disrupt monopolies and establish new positions <a class="yt-timestamp" data-t="01:05:01">[01:05:01]</a>. To get there, the world needs many more [[building_ai_models_and_industry_implications | AI Builders]] <a class="yt-timestamp" data-t="01:05:35">[01:05:35]</a>. Currently, there are estimated to be around 5 million [[building_ai_models_and_industry_implications | AI Builders]] globally, compared to about 50 million software engineers <a class="yt-timestamp" data-t="01:05:41">[01:05:41]</a>. Clem Delong predicts that in a few years, there could be 50 million or even 100 million [[building_ai_models_and_industry_implications | AI Builders]] <a class="yt-timestamp" data-t="01:06:17">[01:06:17]</a>. The barrier to entry for [[building_ai_models_and_industry_implications | AI]] is lower than for traditional software, as contributions can include expertise or data, not just writing code <a class="yt-timestamp" data-t="01:06:34">[01:06:34]</a>. This broader participation means more people can contribute to, understand, and shape the technology, leading to more inclusive products that can solve a wider range of social issues <a class="yt-timestamp" data-t="01:07:10">[01:07:10]</a>.