---
title: Nvidias business strategy and growth
videoId: nFB-AILkamw
---

From: [[acquiredfm]] <br/> 

Nvidia's recent growth has been explosive, particularly over the last 18 months, warranting a dedicated look at how they reached their current dominant position in the AI revolution [01:02:05](<a class="yt-timestamp" data-t="01:02:05">[01:02:05]</a>.

## Historical Context and the AI "Big Bang"

In April 2022, Nvidia's prior episodes never once mentioned the word "generative," highlighting the rapid pace of change [01:20:23](<a class="yt-timestamp" data-t="01:20:23">[01:20:23]</a>. Throughout 2022, financial markets plummeted, crypto and Web3 bubbles burst, and the tech economy seemed headed for a "Long Winter," which included Nvidia [01:59:58](<a class="yt-timestamp" data-t="01:59:58">[01:59:58]</a>. Nvidia even had a massive inventory write-off for what they thought was over-ordering [02:07:05](<a class="yt-timestamp" data-t="02:07:05">[02:07:05]</a>.

However, by the fall of 2022, a breakthrough technology emerged: Large Language Models (LLMs) built on the Transformer machine learning mechanism [02:26:00](<a class="yt-timestamp" data-t="02:26:00">[02:26:00]</a>. OpenAI's ChatGPT became the fastest app in history to reach 100 million active users, marking AI's "Netscape moment" and potentially its "iPhone moment" [02:38:00](<a class="yt-timestamp" data-t="02:38:00">[02:38:00]</a>, [02:53:00](<a class="yt-timestamp" data-t="02:53:00">[02:53:00]</a>.

### The AlexNet Breakthrough

The "Big Bang moment" of artificial intelligence, then humbly referred to as machine learning, occurred in 2012 with AlexNet [08:05:00](<a class="yt-timestamp" data-t="08:05:00">[08:05:00]</a>. Three University of Toronto researchers submitted the AlexNet algorithm to the ImageNet computer science competition [08:18:00](<a class="yt-timestamp" data-t="08:18:00">[08:18:00]</a>. ImageNet involved labeling 14 million hand-labeled images, reportedly the largest use of Mechanical Turk at the time [08:30:00](<a class="yt-timestamp" data-t="08:30:00">[08:30:00]</a>.

AlexNet drastically improved image mislabeling error rates from 25% to 15%, a significant leap [09:12:00](<a class="yt-timestamp" data-t="09:12:00">[09:12:00]</a>. This was achieved by using older, computationally intensive neural networks, specifically convolutional neural networks (CNNs), which had been around since the 1960s [09:36:00](<a class="yt-timestamp" data-t="09:36:00">[09:36:00]</a>. The researchers leveraged two Nvidia GeForce GTX 580s, top-of-the-line consumer-grade graphics cards, and wrote their algorithm in [[nvidias_innovation_and_adaptation_strategies | CUDA]], Nvidia's software development platform for GPUs [10:04:00](<a class="yt-timestamp" data-t="10:04:00">[10:04:00]</a>.

This demonstrated that GPUs, unlike CPUs, could execute hundreds or thousands of instructions in parallel, making them ideal for algorithms that could run in parallel [11:01:00](<a class="yt-timestamp" data-t="11:01:00">[11:01:00]</a>. This ability to leverage Moore's Law by hundreds or thousands of times transformed computing [11:31:00](<a class="yt-timestamp" data-t="11:31:00">[11:31:00]</a>. Nvidia, which pioneered this technology for graphics where every pixel can be computed independently, found a "New Frontier" in AI and other linear algebra-based applications [12:05:00](<a class="yt-timestamp" data-t="12:05:00">[12:05:00]</a>.

## Rise of the AI Duopoly and OpenAI

After AlexNet, the researchers, Alex Krizhevsky and Ilya Sutskever (a PhD student under Professor Jeff Hinton), started a company that Google acquired within six months [14:48:00](<a class="yt-timestamp" data-t="14:48:00">[14:48:00]</a>, [15:02:00](<a class="yt-timestamp" data-t="15:02:00">[15:02:00]</a>. They joined the Google Brain team, formed by Greg Corrado, Jeff Dean, and Andrew Ng, to accelerate AI work [15:04:00](<a class="yt-timestamp" data-t="15:04:00">[15:04:00]</a>. Google later acquired DeepMind, and Facebook recruited computer science legend Yann LeCun, establishing a duopoly on leading AI researchers [15:48:00](<a class="yt-timestamp" data-t="15:48:00">[15:48:00]</a>. This AI was adept at narrow tasks, like social media feed recommendations, which proved immensely profitable for Google (YouTube) and Facebook (Instagram) and consumed many Nvidia GPUs [16:15:00](<a class="yt-timestamp" data-t="16:15:00">[16:15:00]</a>.

By 2015, Elon Musk and Sam Altman (then president of Y Combinator) recognized this Google-Facebook AI duopoly as a significant problem [18:23:00](<a class="yt-timestamp" data-t="18:23:00">[18:23:00]</a>. Their concern stemmed from the detrimental impact on other tech companies, startups, and the broader world, as top AI talent was locked up [18:57:00](<a class="yt-timestamp" data-t="18:57:00">[18:57:00]</a>. The founding of OpenAI was motivated by the desire to find Artificial General Intelligence (AGI) first, believing that immense control would rest with the first discoverer, and thus it was "best in the open" [20:03:00](<a class="yt-timestamp" data-t="20:03:00">[20:03:00]</a>.

They convened a dinner in 2015, inviting top AI researchers from Google and Facebook [21:22:00](<a class="yt-timestamp" data-t="21:22:00">[21:22:00]</a>. Most declined, citing high salaries and the collaborative environment [21:38:00](<a class="yt-timestamp" data-t="21:38:00">[21:38:00]</a>. However, Ilya Sutskever, intrigued by the pitch, left Google to co-found OpenAI as its chief scientist [22:26:00](<a class="yt-timestamp" data-t="22:26:00">[22:26:00]</a>.

## The Transformer and OpenAI's Evolution

Around 2015, AI was still limited to narrow use cases, primarily due to constraints in data volume and algorithmic capabilities [26:08:00](<a class="yt-timestamp" data-t="26:08:00">[26:08:00]</a>. The idea of training a single foundational model on the entire internet seemed "crazy" [26:57:00](<a class="yt-timestamp" data-t="26:57:00">[26:57:00]</a>.

In 2015, Andrej Karpathy (then at OpenAI, now back) wrote a seminal blog post, "The Unreasonable Effectiveness of Neural Networks," discussing recurrent neural networks (RNNs) [27:12:00](<a class="yt-timestamp" data-t="27:12:00">[27:12:00]</a>. In a 2016 Nvidia video, Karpathy and Sutskever spoke about language models figuring out word patterns to create chatbots, envisioning future human-computer interaction [27:40:00](<a class="yt-timestamp" data-t="27:40:00">[27:40:00]</a>. Sutskever later used the "detective novel" analogy to explain how accurate next-word prediction implies understanding of human knowledge [29:46:00](<a class="yt-timestamp" data-t="29:46:00">[29:46:00]</a>.

### The Transformer Paper

In 2017, the Google Brain team released the "Attention Is All You Need" paper, introducing the Transformer model [30:59:00](<a class="yt-timestamp" data-t="30:59:00">[30:59:00]</a>. This model revolutionized natural language processing by introducing "attention," allowing the model to weigh different parts of input text simultaneously when predicting the next word [32:03:00](<a class="yt-timestamp" data-t="32:03:00">[32:03:00]</a>.

While computationally expensive (O(N^2)), Transformers were perfectly suited for parallel processing on GPUs [33:20:00](<a class="yt-timestamp" data-t="33:20:00">[33:20:00]</a>. Unlike previous sequential RNNs, Transformers could process long text sequences efficiently, enabling training of "sequence based models in a parallel way" [34:40:00](<a class="yt-timestamp" data-t="34:40:00">[34:40:00]</a>. This breakthrough led to the concept of large language models (LLMs) [39:02:00](<a class="yt-timestamp" data-t="39:02:00">[39:02:00]</a>.

### OpenAI's Scaling and Partnership with Microsoft

OpenAI's early models demonstrated massive scaling with parameters:
*   GPT-1: ~120 million parameters [41:23:00](<a class="yt-timestamp" data-t="41:23:00">[41:23:00]</a>
*   GPT-2: 1.5 billion parameters [41:29:00](<a class="yt-timestamp" data-t="41:29:00">[41:29:00]</a>
*   GPT-3: 175 billion parameters [41:33:00](<a class="yt-timestamp" data-t="41:33:00">[41:33:00]</a>
*   GPT-4: Rumored 1.72 trillion parameters [41:38:00](<a class="yt-timestamp" data-t="41:38:00">[41:38:00]</a>

The more parameters, the better the models predicted, exhibiting "weirdly emergent property" of reasoning [42:26:00](<a class="yt-timestamp" data-t="42:26:00">[42:26:00]</a>. However, training these models was "prohibitively expensive" [43:11:00](<a class="yt-timestamp" data-t="43:11:00">[43:11:00]</a>.

In 2018, Elon Musk departed OpenAI due to frustration with the high costs [44:24:00](<a class="yt-timestamp" data-t="44:24:00">[44:24:00]</a>. In March 2019, OpenAI pivoted from a non-profit to a capped-profit entity to raise capital for ambitious AI models [45:22:00](<a class="yt-timestamp" data-t="45:22:00">[45:22:00]</a>. Less than six months later, Microsoft made a $1 billion investment, becoming OpenAI's exclusive cloud provider [45:52:00](<a class="yt-timestamp" data-t="45:52:00">[45:52:00]</a>.

This partnership led to significant developments:
*   **June 2020**: GPT-3 released [47:12:00](<a class="yt-timestamp" data-t="47:12:00">[47:12:00]</a>
*   **September 2020**: Microsoft licenses exclusive commercial use of the underlying model [47:16:00](<a class="yt-timestamp" data-t="47:16:00">[47:16:00]</a>
*   **2021**: GitHub Co-pilot launched, and Microsoft invested another $2 billion [47:22:00](<a class="yt-timestamp" data-t="47:22:00">[47:22:00]</a>
*   **November 2022**: ChatGPT released [47:31:00](<a class="yt-timestamp" data-t="47:31:00">[47:31:00]</a>
*   **January 2023**: Microsoft invested another $10 billion, integrating GPT into all products [47:46:00](<a class="yt-timestamp" data-t="47:46:00">[47:46:00]</a>
*   **May 2023**: GPT-4 released [47:57:00](<a class="yt-timestamp" data-t="47:57:00">[47:57:00]</a>

This confluence of events — the emergence of generative AI, its immense GPU compute requirements, and the reliance on cloud providers — created the "single greatest moment" for Nvidia [48:47:00](<a class="yt-timestamp" data-t="48:47:00">[48:47:00]</a>.

## Nvidia's Strategic Positioning and Growth

Nvidia's success is attributed to "luck being what happens when preparation meets opportunity" [51:59:00](<a class="yt-timestamp" data-t="51:59:00">[51:59:00]</a>. Their "preparation" involved spending five years building a GPU-accelerated computing platform for the data center, aiming to replace Intel's x86 architecture [52:07:00](<a class="yt-timestamp" data-t="52:07:00">[52:07:00]</a>. This focus on the data center was initially questioned but is now undeniably crucial [53:05:00](<a class="yt-timestamp" data-t="53:05:00">[53:05:00]</a>.

### Nvidia's Data Center Strategy

Nvidia's strategy involved three key components:

1.  **Mellanox Acquisition (2020)**: Nvidia acquired Mellanox, an Israeli networking company, for $7 billion [01:01:18](<a class="yt-timestamp" data-t="01:01:18">[01:01:18]</a>. Mellanox specialized in Infiniband, a high-bandwidth, efficient data transfer standard for data centers, which was considered niche compared to Ethernet [01:01:40](<a class="yt-timestamp" data-t="01:01:40">[01:01:40]</a>. Nvidia foresaw the need for "really fast data interconnects" to address hundreds of GPUs as a single compute cluster for massive AI models [01:02:50](<a class="yt-timestamp" data-t="01:02:50">[01:02:50]</a>. This was a direct result of Nvidia's internal research, where they trained the 8.3 billion-parameter Megatron Transformer model on 512 GPUs in 2019, realizing the need for high-speed networking [01:44:30](<a class="yt-timestamp" data-t="01:44:30">[01:44:30]</a>.

2.  **Grace CPU Processor (2022)**: In September 2022, Nvidia announced a new class of chips, the Grace CPU processor [01:04:13](<a class="yt-timestamp" data-t="01:04:13">[01:04:13]</a>. These CPUs are not for laptops but are designed as the CPU component of a data center solution, specifically to orchestrate with massive GPU clusters [01:04:42](<a class="yt-timestamp" data-t="01:04:42">[01:04:42]</a>. This signifies Nvidia's ambition to offer a fully integrated solution, from GPUs and CPUs to NVLink and Infiniband networking [01:05:31](<a class="yt-timestamp" data-t="01:05:31">[01:05:31]</a>.

3.  **Dedicated Hopper Architecture (2022)**: Also in September 2022, Nvidia bifurcated its GPU architectures [01:06:36](<a class="yt-timestamp" data-t="01:06:36">[01:06:36]</a>. The Hopper architecture (H100) was dedicated to data centers, while Lovelace (RTX 40xx) was for consumer gaming [01:06:40](<a class="yt-timestamp" data-t="01:06:40">[01:06:40]</a>. Hopper uses "chip-on-wafer-on-substrate" (CoWoS) technology, allowing more high-performance memory to be stacked directly on the GPU chips, which is crucial for massive AI models [01:07:23](<a class="yt-timestamp" data-t="01:07:23">[01:07:23]</a>. This advanced packaging technology is bleeding edge, and Nvidia's monopolization of a significant portion of TSMC's CoWoS capacity provides a competitive advantage [01:07:57](<a class="yt-timestamp" data-t="01:07:57">[01:07:57]</a>.

The H100 GPU costs about $40,000 [01:15:27](<a class="yt-timestamp" data-t="01:15:27">[01:15:27]</a>. An eight-H100 DGX system (a "supercomputer in a box") starts at $500,000 [01:17:00](<a class="yt-timestamp" data-t="01:17:00">[01:17:00]</a>. The flagship DGX GH200 SuperPOD, a "TurnKey AI data center" capable of training trillion-parameter models, is custom-priced in the hundreds of millions [01:14:27](<a class="yt-timestamp" data-t="01:14:27">[01:14:27]</a>.

Nvidia's philosophy is that their solutions are expensive but ultimately save money: "the more you buy, the more you save" [01:18:13](<a class="yt-timestamp" data-t="01:18:13">[01:18:13]</a>. They enable tasks that would be impossible or vastly more expensive on traditional infrastructure, consuming less energy overall despite high individual unit power [01:19:07](<a class="yt-timestamp" data-t="01:19:07">[01:19:07]</a>.

### Nvidia's Cloud Strategy

Nvidia has also launched [[Nvidias_role_in_data_centers | DGX Cloud]], a virtualized DGX system provided via other cloud providers like Azure, Oracle, and Google [02:21:55](<a class="yt-timestamp" data-t="02:21:55">[02:21:55]</a>. This allows enterprises to access Nvidia's integrated solution without managing their own data centers [02:23:00](<a class="yt-timestamp" data-t="02:23:00">[02:23:00]</a>. Starting at $37,000 per month for an A100-based system, these offerings provide extremely high margins for Nvidia and their partners [02:25:20](<a class="yt-timestamp" data-t="02:25:20">[02:25:20]</a>. Crucially, [[Nvidias_role_in_data_centers | DGX Cloud]] creates a direct sales relationship with enterprises, bypassing hyperscalers [02:26:05](<a class="yt-timestamp" data-t="02:26:05">[02:26:05]</a>.

### Recent Financial Performance

In Q1 Fiscal Year 2024 (ending April 2023), Nvidia's revenue was up 19% quarter-over-quarter to $7.2 billion [02:27:49](<a class="yt-timestamp" data-t="02:27:49">[02:27:49]</a>. This was after a "disappointing year" in 2022, which included the release of ChatGPT [02:28:14](<a class="yt-timestamp" data-t="02:28:14">[02:28:14]</a>.

Following Q1 earnings, Nvidia forecasted Q2 revenue of $11 billion, a 53% quarter-over-quarter increase and 65% year-over-year [02:29:19](<a class="yt-timestamp" data-t="02:29:19">[02:29:19]</a>. This led to a 25% stock jump in after-hours trading, pushing Nvidia to a trillion-dollar market cap [02:29:35](<a class="yt-timestamp" data-t="02:29:35">[02:29:35]</a>.

In Q2 Fiscal Year 2024 (ending July 2023), Nvidia reported historic earnings:
*   Total company revenue: $13.5 billion (up 88% from previous quarter, over 100% year-over-year) [02:31:31](<a class="yt-timestamp" data-t="02:31:31">[02:31:31]</a>
*   Data center segment revenue: $10.3 billion (up 141% from Q1, 171% year-over-year) [02:31:42](<a class="yt-timestamp" data-t="02:31:42">[02:31:42]</a>. This segment, which "basically didn't exist five years ago," now accounts for the majority of Nvidia's revenue [02:31:49](<a class="yt-timestamp" data-t="02:31:49">[02:31:49]</a>.
*   Gross margin: 70% in Q2, forecasted to be 72% for Q3 [02:46:42](<a class="yt-timestamp" data-t="02:46:42">[02:46:42]</a>. This is a massive increase from the 24% gross margin when they were a commoditized graphics card manufacturer [02:46:51](<a class="yt-timestamp" data-t="02:46:51">[02:46:51]</a>.

Jensen Huang, Nvidia's CEO, now frames Nvidia's trillion-dollar opportunity around the data center [02:33:17](<a class="yt-timestamp" data-t="02:33:17">[02:33:17]</a>. He states there is $1 trillion worth of hard assets in data centers globally, with $250 billion in annual spend for updates and additions [02:32:40](<a class="yt-timestamp" data-t="02:32:40">[02:32:40]</a>.

## [[NVIDIAs strategic positioning and dominance in AI | Nvidia's Strategic Position and Dominance]]

Nvidia's success is rooted in several strategic "powers":

*   **Scale Economies**: Nvidia has made massive fixed-cost investments, notably in [[Nvidias_innovation_and_adaptation_strategies | CUDA]]. [[Nvidias_innovation_and_adaptation_strategies | CUDA]], launched in 2006, is a comprehensive platform including a compiler, runtime, development tools, and programming language [01:37:37](<a class="yt-timestamp" data-t="01:37:37">[01:37:37]</a>. It supports all Nvidia cards shipped since 2006 [01:39:00](<a class="yt-timestamp" data-t="01:39:00">[01:39:00]</a>. The number of registered [[Nvidias_innovation_and_adaptation_strategies | CUDA]] developers grew from 1 million in 2016 to 4 million by May 2023 [01:39:58](<a class="yt-timestamp" data-t="01:39:58">[01:39:58]</a>. This vast developer ecosystem, backed by thousands of Nvidia software engineers, creates a "huge moat" [01:40:51](<a class="yt-timestamp" data-t="01:40:51">[01:40:51]</a>. This mirrors Apple's strategy of a tightly controlled, hardware-coupled operating system, making Nvidia the "Apple of AI" [02:01:35](<a class="yt-timestamp" data-t="02:01:35">[02:01:35]</a>.
*   **Switching Costs**: All significant AI model training, especially LLMs, has been built on Nvidia hardware and [[Nvidias_innovation_and_adaptation_strategies | CUDA]], creating substantial code and organizational momentum that makes switching away difficult [02:06:10](<a class="yt-timestamp" data-t="02:06:10">[02:06:10]</a>. Data center re-architectures are infrequent, occurring perhaps once a decade [02:06:47](<a class="yt-timestamp" data-t="02:06:47">[02:06:47]</a>. Nvidia is actively trying to ship as much product as possible to lock in this architecture for the long term [02:06:51](<a class="yt-timestamp" data-t="02:06:51">[02:06:51]</a>.
*   **Cornered Resource**: Nvidia has secured a "huge amount of capacity" at TSMC for its advanced 2.5D CoWoS packaging process, which competitors cannot access [02:08:41](<a class="yt-timestamp" data-t="02:08:41">[02:08:41]</a>. This was partly due to Nvidia's early reservation of capacity for other purposes (like crypto mining) [02:08:51](<a class="yt-timestamp" data-t="02:08:51">[02:08:51]</a>.
*   **Network Economies**: The large number of developers and customers benefit from each other, building libraries and tools on top of [[Nvidias_innovation_and_adaptation_strategies | CUDA]] [02:11:31](<a class="yt-timestamp" data-t="02:11:31">[02:11:31]</a>. With 500 million [[Nvidias_innovation_and_adaptation_strategies | CUDA]]-capable GPUs in the market, developers have a strong incentive to target Nvidia's platform [02:12:10](<a class="yt-timestamp" data-t="02:12:10">[02:12:10]</a>.
*   **Brand**: Nvidia benefits from a "nobody gets fired for buying IBM" effect [02:13:02](<a class="yt-timestamp" data-t="02:13:02">[02:13:02]</a>. Their long-standing reputation as a technology leader in graphics lends credibility to their enterprise AI solutions [02:13:52](<a class="yt-timestamp" data-t="02:13:52">[02:13:52]</a>.

## Nvidia's Playbook

Nvidia's playbook revolves around several key principles:

*   **Hardware Differentiated by Software**: Jensen Huang's analogy of the "iPhone moment for AI" highlights Nvidia's strategy: a hardware company differentiated by a powerful software platform ([[Nvidias_innovation_and_adaptation_strategies | CUDA]]), which has expanded into services (like [[Nvidias_role_in_data_centers | DGX Cloud]]) [02:16:46](<a class="yt-timestamp" data-t="02:16:46">[02:16:46]</a>. They operate a vertically integrated hardware and software stack [02:17:05](<a class="yt-timestamp" data-t="02:17:05">[02:17:05]</a>.
*   **Transition to a Systems Company**: Nvidia has shifted from merely being a chip or hardware company to a systems company. The focus is on how multiple GPUs and racks of GPUs work together as one integrated system, with all the necessary hardware, networking, and software [02:17:47](<a class="yt-timestamp" data-t="02:17:47">[02:17:47]</a>. They are effectively selling mainframes or "supercomputers in a box" [02:13:11](<a class="yt-timestamp" data-t="02:13:11">[02:13:11]</a>.
*   **Focus on Undifferentiated Work**: As Jensen Huang states, "You build a great company by doing things that other people can't do. You don't build a company by fighting other people to do things that everyone can do" [02:18:17](<a class="yt-timestamp" data-t="02:18:17">[02:18:17]</a>. Nvidia avoided building a CPU until there was a clear, differentiated reason to do so [02:18:30](<a class="yt-timestamp" data-t="02:18:30">[02:18:30]</a>.
*   **Strike When the Timing is Right**: Nvidia has demonstrated remarkable patience and strategic timing. They didn't immediately counter Intel's dominance in CPUs or data centers but waited until the opportune moment to introduce their own CPUs and full data center solutions [02:20:25](<a class="yt-timestamp" data-t="02:20:25">[02:20:25]</a>.
*   **Relentless Innovation and Speed**: Nvidia has re-accelerated to a six-month shipping cycle for their data center GPUs, holding two GTC conferences a year [02:05:24](<a class="yt-timestamp" data-t="02:05:24">[02:05:24]</a>. This "pedal to the floor" approach is crucial for staying ahead in a highly attractive market [02:05:50](<a class="yt-timestamp" data-t="02:05:50">[02:05:50]</a>.

## Outlook

### Bear Case

*   **Competition**: Other tech giants with "untold resources" (Amazon with Trainium/Inferentia, Microsoft building its own chip, Google with TPUs, Meta with PyTorch) are incentivized to capture a piece of Nvidia's highly profitable market [02:26:23](<a class="yt-timestamp" data-t="02:26:23">[02:26:23]</a>. PyTorch, now moved into a foundation, presents an opportunity for cloud providers to offer alternative platforms [02:27:00](<a class="yt-timestamp" data-t="02:27:00">[02:27:00]</a>.
*   **Market Size Exaggeration**: There's a risk of a "crisis of confidence" if generative AI applications prove less useful than currently believed, leading to a "crypto-like" excitement bubble bursting and slowing enterprise spending [02:28:09](<a class="yt-timestamp" data-t="02:28:09">[02:28:09]</a>. However, most AI researchers believe the long-term transformative impact of AI is underestimated [02:29:33](<a class="yt-timestamp" data-t="02:29:33">[02:29:33]</a>.
*   **Model Efficiency**: Future AI models might become "more clever" and require less compute, potentially reducing the need for Nvidia's brute-force training solutions [02:35:20](<a class="yt-timestamp" data-t="02:35:20">[02:35:20]</a>. Additionally, as AI usage scales, the compute load shifts from training to inference, where Nvidia faces less differentiation [02:36:08](<a class="yt-timestamp" data-t="02:36:08">[02:36:08]</a>.
*   **China Restrictions**: Export controls by the Biden Administration limit Nvidia's ability to sell its top-tier H100s and A100s to China, forcing them to sell "nerfed" versions (A800, H800) [02:48:47](<a class="yt-timestamp" data-t="02:48:47">[02:48:47]</a>. While these crippled versions still sell well, it restricts access to a large market [02:50:11](<a class="yt-timestamp" data-t="02:50:11">[02:50:11]</a>.

### Bull Case

*   **Accelerated Computing Dominance**: Jensen Huang's vision that the majority of computational workloads will shift from CPUs to parallel-processing GPUs holds true [02:38:10](<a class="yt-timestamp" data-t="02:38:10">[02:38:10]</a>. The massive compute required for generative AI is overwhelmingly accruing to Nvidia [02:40:50](<a class="yt-timestamp" data-t="02:40:50">[02:40:50]</a>. Nvidia believes that AI compute will be *added* to everything, dwarfing general-purpose compute [02:39:56](<a class="yt-timestamp" data-t="02:39:56">[02:39:56]</a>.
*   **Generative AI Impact**: The continued growth and utility of generative AI applications, as exemplified by OpenAI's rapid revenue growth, will sustain massive data center spending on Nvidia hardware [02:40:53](<a class="yt-timestamp" data-t="02:40:53">[02:40:53]</a>. This is already materializing in significant revenue for Nvidia [02:57:00](<a class="yt-timestamp" data-t="02:57:00">[02:57:00]</a>.
*   **Speed and Adaptability**: Nvidia's culture of rapid development and constant innovation makes it likely they will remain well-positioned to capture future developments in AI [02:41:41](<a class="yt-timestamp" data-t="02:41:41">[02:41:41]</a>.
*   **Data Center Market Share**: Nvidia is well-positioned to capture a meaningful share of the $250 billion annual spend on data center refresh and expansion [02:41:53](<a class="yt-timestamp" data-t="02:41:53">[02:41:53]</a>. TSMC, a key Nvidia supplier, expects AI hardware revenue to grow 50% per year for the next five years, indicating strong industry conviction [02:42:51](<a class="yt-timestamp" data-t="02:42:51">[02:42:51]</a>.
*   **Platform Company, Not Just Hardware**: Unlike Cisco or Intel, Nvidia controls the entire software stack ([[Nvidias_innovation_and_adaptation_strategies | CUDA]]), enabling direct relationships with developers and customers [02:43:47](<a class="yt-timestamp" data-t="02:43:47">[02:43:47]</a>. They resemble an "old school IBM" operating in a vastly larger, modern computing market, leading a shift back to centralized, mainframe-dominated computing [02:44:02](<a class="yt-timestamp" data-t="02:44:02">[02:44:02]</a>.

### Conclusion

Competing directly with Nvidia is "nearly impossible" [02:47:18](<a class="yt-timestamp" data-t="02:47:18">[02:47:18]</a>. It would require matching their GPU design, networking capabilities (NVLink, Infiniband), manufacturing access to cutting-edge TSMC fabs, and building a software ecosystem like [[Nvidias_innovation_and_adaptation_strategies | CUDA]] (a 10,000-person-year effort) [02:45:20](<a class="yt-timestamp" data-t="02:45:20">[02:45:20]</a>. Any competitor would need to be 10x better and cheaper to overcome Nvidia's established brand and switching costs, all while Nvidia continues to innovate at record speed [02:46:01](<a class="yt-timestamp" data-t="02:46:01">[02:46:01]</a>. If Nvidia is unseated, it will likely be from an "unknown flank attack" or if the future of computing does not align with accelerated AI, which seems "very unlikely" [02:47:26](<a class="yt-timestamp" data-t="02:47:26">[02:47:26]</a>.