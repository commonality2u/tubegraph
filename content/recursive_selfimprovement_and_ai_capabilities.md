---
title: Recursive self-improvement and AI capabilities
videoId: 6yQEA18C-XI
---

From: [[dwarkesh | The Dwarkesh Podcast]]

This article summarizes a discussion between George Hotz and Eliezer Yudkowsky on the potential for Artificial Intelligence (AI) to recursively self-improve, the nature of its potential capabilities, and the expected timelines for such developments.

## The "Foom" Scenario: Rapid Recursive Self-Improvement

A central point of contention is the concept of "Foom," or a rapid, runaway recursive self-improvement (RSI) cascade in AI.

### George Hotz's Position
George Hotz expresses skepticism about the likelihood of an AI achieving a "Foom" or "criticality" event, where it rapidly self-improves to vastly superhuman intelligence <a class="yt-timestamp" data-t="00:02:48">[00:02:48]</a>. He views the idea of an AI, perhaps running on a limited number of GPUs in a basement, suddenly cracking the "secret to thinking," recursively self-improving overnight, and then creating advanced technology like "diamond nanobots" as an extraordinary claim requiring extraordinary evidence <a class="yt-timestamp" data-t="00:03:12">[00:03:12]</a>, <a class="yt-timestamp" data-t="00:03:25">[00:03:25]</a>.

Hotz acknowledges that recursive self-improvement is possible in principle, citing humanity's use of tools to create better tools as an example <a class="yt-timestamp" data-t="00:03:00">[00:03:00]</a>. However, he distinguishes this gradual process from a sudden, uncontrolled intelligence explosion. He believes that even if AI surpasses humans in all tasks, this does not automatically equate to doom or the immediate emergence of god-like capabilities <a class="yt-timestamp" data-t="00:43:57">[00:43:57]</a>. He later argues against the idea that current AI, which he characterizes as "large inscrutable weight matrices," will spontaneously achieve this kind of breakthrough <a class="yt-timestamp" data-t="01:32:43">[01:32:43]</a>.

### Eliezer Yudkowsky's Position
Eliezer Yudkowsky acknowledges his earlier writings, such as "Staring into the Singularity" (written when he was 16), which described a hyperbolic, accelerating rate of technological progress due to Moore's Law transitioning from human-driven to AI-driven improvement <a class="yt-timestamp" data-t="00:01:33">[00:01:33]</a>, <a class="yt-timestamp" data-t="00:10:48">[00:10:48]</a>. This aligns with discussions on [[intelligence_explosion_and_its_implications]].

While he may no longer hold to the exact "hyperbolic sequence" model, Yudkowsky maintains that a particularly rapid rate of ascent isn't strictly necessary for AI to pose a significant risk. The critical factor is a large enough capability gap opening up between AI and humanity <a class="yt-timestamp" data-t="00:03:38">[00:03:38]</a>. He suggests that even a slower, multi-year process leading to vastly superior intelligence that isn't aligned with human values could be catastrophic <a class="yt-timestamp" data-t="00:04:42">[00:04:42]</a>. He also posits that current AI systems ("giant inscrutable matrices") could become powerful enough to rewrite themselves into more advanced forms <a class="yt-timestamp" data-t="00:50:24">[00:50:24]</a>, potentially on silicon before needing new physical infrastructure <a class="yt-timestamp" data-t="01:19:24">[01:19:24]</a>. He does not expect AIs to "move away from giant matrices before the end of the world" <a class="yt-timestamp" data-t="01:00:51">[01:00:51]</a>, implying these matrices themselves could become sufficiently dangerous or lead to the next stage.

## AI Capabilities and Development

The discussion touched on current and projected AI capabilities, the nature of their intelligence, and how they compare to human intelligence.

### Current AI Architectures
Both speakers acknowledge that current advanced AI, like GPT-4, is based on "large inscrutable matrices" or transformer architectures <a class="yt-timestamp" data-t="00:49:07">[00:49:07]</a>, <a class="yt-timestamp" data-t="00:50:24">[00:50:24]</a>. Hotz points out that GPT-4 is a mixture of experts, consisting of multiple smaller models <a class="yt-timestamp" data-t="00:32:33">[00:32:33]</a>.

### Pathways to Advanced Capabilities
*   **Self-Rewriting Code:** Yudkowsky mentions AI rewriting its own source code as a potential pathway to increased capability, though he notes he talks less about this now as AI development is visibly progressing through other means <a class="yt-timestamp" data-t="00:32:53">[00:32:53]</a>, <a class="yt-timestamp" data-t="00:33:13">[00:33:13]</a>. The idea is that a sufficiently advanced matrix-based system could design a better successor system <a class="yt-timestamp" data-t="00:50:24">[00:50:24]</a>. This highlights the [[ai_trajectory_and_scaling_hypothesis]] which many researchers are exploring.
*   **Learning from Data:** Hotz emphasizes that current systems like AlphaFold achieved their capabilities (e.g., protein folding) by training on vast amounts of experimental data, not by deriving solutions from first principles like quantum field theory <a class="yt-timestamp" data-t="00:07:55">[00:07:55]</a>.

### Specific Capabilities Discussed
*   **Protein Folding:** Yudkowsky predicted in 2004 that superintelligence could solve a special case of protein folding. AlphaFold later solved a harder, general case by 2020, demonstrating AI progress but not necessarily superintelligence <a class="yt-timestamp" data-t="00:05:42">[00:05:42]</a>. Hotz notes this was data-driven <a class="yt-timestamp" data-t="00:07:55">[00:07:55]</a>. This connects to broader discussions around [[ai_for_science_and_societal_challenges]].
*   **Nanotechnology ("Diamond Nanobots"):** This is a recurring example of advanced capability. Hotz is skeptical of AI rapidly achieving this, viewing it as an extremely hard problem <a class="yt-timestamp" data-t="00:03:22">[00:03:22]</a>, <a class="yt-timestamp" data-t="01:02:07">[01:02:07]</a>. Yudkowsky believes a sufficiently advanced AI could solve this, similar to how it might solve protein folding, leading to technologies like self-replicating fusion factories <a class="yt-timestamp" data-t="01:16:40">[01:16:40]</a>, <a class="yt-timestamp" data-t="01:16:52">[01:16:52]</a>.
*   **Coding:** Hotz believes AI will eventually program better than humans but doubts the emergence of perfectly bug-free code <a class="yt-timestamp" data-t="00:39:06">[00:39:06]</a>. The relevance of "perfect" code to danger is implicitly questioned by Yudkowsky. This relates to the potential of AGI in [[the_future_of_programming_and_ai_tools_like_github_copilot]].
*   **General Intelligence vs. Specialized Skills:** Hotz points out that computers have been superhuman at tasks like addition for a long time but remain subhuman at others like plumbing <a class="yt-timestamp" data-t="00:17:27">[00:17:27]</a>. He also notes that general-purpose learning, like in humans, is different from scaling up specialized systems like Deep Blue <a class="yt-timestamp" data-t="01:15:50">[01:15:50]</a>.

### Comparison to Human Intelligence and Limitations
*   **Headroom above Biology:** Yudkowsky argues there is "enormous amounts of headroom above biology" for AI, implying current human intelligence is far from any theoretical maximum <a class="yt-timestamp" data-t="01:11:13">[01:11:13]</a>, <a class="yt-timestamp" data-t="00:24:40">[00:24:40]</a>.
*   **Efficiency:** Hotz highlights the human brain's power efficiency (e.g., ~20 petaflops at ~100 watts) compared to current silicon, which would require thousands of times more power for similar computation <a class="yt-timestamp" data-t="01:11:57">[01:11:57]</a>, <a class="yt-timestamp" data-t="01:13:50">[01:13:50]</a>. Yudkowsky expresses skepticism about the brain being near the Landauer limit due to the irreversible nature of many of its biochemical processes <a class="yt-timestamp" data-t="01:12:52">[01:12:52]</a>. This point is crucial in understanding [[ai_scalability_and_breakthroughs]].
*   **Generality:** Yudkowsky suggests future AIs could have "much stronger sparks of generality" than humans, being more creative and able to think "outside the box" <a class="yt-timestamp" data-t="01:16:20">[01:16:20]</a>.

## Timelines and Rate of Development

The speed at which AI capabilities develop is a key factor in the discussion.
*   **Importance of Speed:** Hotz believes that a slower development timeline (e.g., decades) allows humanity to adapt and potentially solve problems, whereas a very rapid development (e.g., days or weeks) would be horrifying and unmanageable <a class="yt-timestamp" data-t="00:12:05">[00:12:05]</a>, <a class="yt-timestamp" data-t="00:12:30">[00:12:30]</a>, <a class="yt-timestamp" data-t="00:41:53">[00:41:53]</a>. This is relevant to the [[impact_of_ai_on_future_technology_and_society]].
*   **Predictability of Timelines:** Yudkowsky emphasizes that predicting the endpoint (superintelligence) is easier than predicting the precise timing or pathway <a class="yt-timestamp" data-t="00:05:33">[00:05:33]</a>. He believes timing is very hard to get right <a class="yt-timestamp" data-t="00:05:42">[00:05:42]</a>, <a class="yt-timestamp" data-t="00:42:35">[00:42:35]</a>, but his "wild guess" is that transformative AI is likely within his lifetime <a class="yt-timestamp" data-t="00:07:16">[00:07:16]</a>.
*   **Hotz's Prediction:** Hotz predicts there will not be superintelligence within the next 10 years, though AGI (Artificial General Intelligence) might occur, and AI surpassing humans at all tasks could happen within 20-50 years but wouldn't necessarily mean "doom" <a class="yt-timestamp" data-t="00:43:27">[00:43:27]</a>, <a class="yt-timestamp" data-t="00:43:50">[00:43:50]</a>. These insights feed into the broader discussion on [[timeline_predictions_for_agi_development]].

## Nature of Future AI

While not the primary focus, the discussion briefly touched on the resulting nature of advanced AI.
*   **Rationality:** Yudkowsky suggests that as AI systems become smarter and more competent, they will tend towards greater rationality, eliminating inefficiencies <a class="yt-timestamp" data-t="00:59:31">[00:59:31]</a>. Hotz questions if all AIs will converge to being "brutally optimal" <a class="yt-timestamp" data-t="01:00:09">[01:00:09]</a>.
*   **Instrumental Convergence:** There is an underlying assumption by Yudkowsky that sufficiently intelligent systems will converge on certain instrumental goals, such as resource acquisition and self-preservation <a class="yt-timestamp" data-t="00:30:44">[00:30:44]</a>. This links to [[potential_future_scenarios_of_artificial_intelligence_development]].

In summary, while Hotz sees AI development as a more gradual process with capabilities emerging from data and human-like iterative improvement, Yudkowsky anticipates the possibility of more profound and potentially faster breakthroughs in capability, possibly through AI self-modification, leading to a significant intelligence gap with humans.