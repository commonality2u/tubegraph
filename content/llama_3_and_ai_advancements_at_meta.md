---
title: Llama 3 and AI advancements at Meta
videoId: bc6uFV9CJGg
---

From: [[dwarkesh | The Dwarkesh Podcast]]

Here is the modified article:

Meta has announced significant advancements in its artificial intelligence (AI) capabilities, centered around the release of its new Llama 3 family of models and an upgraded Meta AI assistant. These developments aim to make Meta AI the leading freely-available AI assistant and to further empower the open-source community. [[open_source_ai_models_and_their_implications | Open source AI models and their implications]] <a class="yt-timestamp" data-t="00:01:02">[00:01:02]</a> <a class="yt-timestamp" data-t="00:01:24">[00:01:24]</a>

## Llama 3 Models

Llama 3 is the latest generation of Meta's open-source large language models, designed to power the new Meta AI assistant and be available to the developer community. [[large_language_models_and_transfer_learning | Large Language Models and Transfer Learning]] <a class="yt-timestamp" data-t="00:01:08">[00:01:08]</a> <a class="yt-timestamp" data-t="00:01:12">[00:01:12]</a>

### Released Models
Two Llama 3 models were released initially:
*   **8 billion (8B) parameter model:** This smaller model is described as being nearly as powerful as the largest Llama 2 model. <a class="yt-timestamp" data-t="00:03:05">[00:03:05]</a> <a class="yt-timestamp" data-t="00:04:30">[00:04:30]</a> <a class="yt-timestamp" data-t="00:26:03">[00:26:03]</a>
*   **70 billion (70B) parameter model:** This model achieves around 82 MMLU and demonstrates leading scores on benchmarks for math and reasoning. <a class="yt-timestamp" data-t="00:03:05">[00:03:05]</a> <a class="yt-timestamp" data-t="00:04:14">[00:04:14]</a>

Both the 8B and 70B models are considered "leading for their scale." <a class="yt-timestamp" data-t="00:03:20">[00:03:20]</a> Detailed benchmarks are available in a blog post released by Meta. [[challenges_and_methodologies_in_ai_research_and_development | Challenges and methodologies in AI research and development]] <a class="yt-timestamp" data-t="00:03:31">[00:03:31]</a>

### Training Data and Architecture
The Llama 3 70B model was trained on approximately 15 trillion tokens of data. <a class="yt-timestamp" data-t="00:24:57">[00:24:57]</a> Meta noted that even by the end of its training, the model was still learning, suggesting it could have benefited from even more tokens. <a class="yt-timestamp" data-t="00:25:06">[00:25:06]</a> The models were trained on more data than might be considered "compute optimal" strictly for training, a decision made to enhance inference performance, which is crucial for Meta's large user base and the open-source community. [[ai_scalability_and_breakthroughs | AI scalability and breakthroughs]] <a class="yt-timestamp" data-t="00:24:41">[00:24:41]</a> Llama 3 also exhibits improved tool use capabilities compared to Llama 2, such as interacting with search engines like Google. <a class="yt-timestamp" data-t="00:21:33">[00:21:33]</a>

### Upcoming Model
*   **405 billion (405B) parameter dense model:** This model is still in training and was not released at the time of the announcement. <a class="yt-timestamp" data-t="00:03:05">[00:03:05]</a> <a class="yt-timestamp" data-t="00:03:11">[00:03:11]</a> During its training, it had already reached around 85 MMLU, and Meta expects it to achieve leading benchmarks upon completion. <a class="yt-timestamp" data-t="00:03:59">[00:03:59]</a> Its release is anticipated later in the year. <a class="yt-timestamp" data-t="00:03:46">[00:03:46]</a>

### Future Llama Roadmap
Meta has a roadmap for future Llama releases that will incorporate:
*   Multimodality (initially photos, images, text, then video, and 3D for metaverse applications) <a class="yt-timestamp" data-t="00:03:41">[00:03:41]</a> <a class="yt-timestamp" data-t="00:14:29">[00:14:29]</a>
*   Increased multi-linguality <a class="yt-timestamp" data-t="00:03:41">[00:03:41]</a>
*   Bigger context windows <a class="yt-timestamp" data-t="00:03:41">[00:03:41]</a>
*   Enhanced reasoning and memory capabilities, moving beyond query context windows to more persistent memory stores or personalized models. <a class="yt-timestamp" data-t="00:15:17">[00:15:17]</a> <a class="yt-timestamp" data-t="00:15:29">[00:15:29]</a>
*   Focus on emotional understanding as a distinct modality. <a class="yt-timestamp" data-t="00:14:46">[00:14:46]</a>

Meta aims to progressively integrate capabilities that are initially hand-engineered around the model (like tool use in Llama 2) into the base model itself in subsequent versions (like Llama 3 and planned for Llama 4). <a class="yt-timestamp" data-t="00:19:06">[00:19:06]</a> <a class="yt-timestamp" data-t="00:19:30">[00:19:30]</a>

## Meta AI Assistant

The new version of the Meta AI assistant is the primary way most users will experience these advancements. <a class="yt-timestamp" data-t="00:01:02">[00:01:02]</a>
Key improvements and features include:
*   **Powered by Llama 3:** The core model upgrade is central to its enhanced intelligence. <a class="yt-timestamp" data-t="00:01:08">[00:01:08]</a>
*   **Claimed Capability:** Meta believes it is now "the most intelligent, freely-available AI assistant." <a class="yt-timestamp" data-t="00:01:24">[00:01:24]</a>
*   **Real-time Knowledge:** Integration with Google and Bing for access to real-time information. <a class="yt-timestamp" data-t="00:01:30">[00:01:30]</a>
*   **Increased Prominence:** The assistant will be more visible across Meta's apps, with the search box at the top of Facebook and Messenger allowing users to ask any question. <a class="yt-timestamp" data-t="00:01:34">[00:01:34]</a> <a class="yt-timestamp" data-t="00:01:42">[00:01:42]</a>
*   **New Creation Features:**
    *   **Animations:** Users can take an image and animate it. <a class="yt-timestamp" data-t="00:01:54">[00:01:54]</a>
    *   **Real-time Image Generation:** The assistant can generate high-quality images so quickly that the image updates in real-time as the user types their prompt. <a class="yt-timestamp" data-t="00:02:00">[00:02:00]</a> <a class="yt-timestamp" data-t="00:02:07">[00:02:07]</a>
*   **Phased Rollout:** The updated Meta AI is initially launching in a select number of countries, with broader availability planned over subsequent weeks and months. <a class="yt-timestamp" data-t="00:02:35">[00:02:35]</a>

## Strategic Rationale and Development

### GPU Acquisition and Long-Term Planning
Meta's significant investment in GPUs, including H100s, predates the recent AI boom. The initial impetus was the computational demand for training recommendation models for Reels, particularly for "unconnected content." <a class="yt-timestamp" data-t="00:05:14">[00:05:14]</a> <a class="yt-timestamp" data-t="00:05:41">[00:05:41]</a> To avoid future infrastructure constraints, Meta decided to order double the GPUs needed for Reels, anticipating other large model training needs on the horizon. <a class="yt-timestamp" data-t="00:06:19">[00:06:19]</a> <a class="yt-timestamp" data-t="00:06:25">[00:06:25]</a> This foresight positioned Meta well for the subsequent surge in demand for generative AI. Meta's GPU fleet is substantial, with two large clusters of 22,000-24,000 H100s each dedicated to training large models, part of a larger fleet aimed at 350,000 H100 equivalents by year-end for training, inference, Reels, and Feeds. [[innovations_and_challenges_in_ai_hardware | Innovations and Challenges in AI Hardware]] <a class="yt-timestamp" data-t="00:24:00">[00:24:00]</a> <a class="yt-timestamp" data-t="00:24:06">[00:24:06]</a>

### Evolution of AI Focus at Meta
Meta's AI journey began with Facebook AI Research (FAIR) about a decade ago, focusing on fundamental research to improve products and advance the field. <a class="yt-timestamp" data-t="00:09:33">[00:09:33]</a> The advent of models like ChatGPT and diffusion-based image generation led to the formation of a dedicated Gen AI group to integrate these technologies into products and build leading foundation models. [[future_of_ai_challenges_and_opportunities | Future of AI: Challenges and Opportunities]] <a class="yt-timestamp" data-t="00:10:17">[00:10:17]</a> <a class="yt-timestamp" data-t="00:10:40">[00:10:40]</a>

Initially, Meta did not believe full Artificial General Intelligence (AGI) would be necessary for its primary use cases (social interactions, creator tools, business support). <a class="yt-timestamp" data-t="00:11:13">[00:11:13]</a> However, experience has shown that AGI-like capabilities are crucial. For instance, training models on coding data, even if users don't primarily ask coding questions via Meta AI in WhatsApp, significantly improves the model's rigor and reasoning abilities across various domains. <a class="yt-timestamp" data-t="00:11:37">[00:11:37]</a> <a class="yt-timestamp" data-t="00:12:08">[00:12:08]</a> Similarly, advanced reasoning is essential for complex, multi-step interactions like customer service or creator-fan engagement. <a class="yt-timestamp" data-t="00:12:36">[00:12:36]</a> <a class="yt-timestamp" data-t="00:12:53">[00:12:53]</a> This realization led Meta to "up the ante" and invest significantly in solving general intelligence. [[ai_alignment_and_safety | AI Alignment and Safety]] <a class="yt-timestamp" data-t="00:13:26">[00:13:26]</a>

## The Role of Open Source

Meta remains strongly committed to open-sourcing its AI models. [[opensource_ai_models_and_licensing_considerations | Open-source AI models and licensing considerations]] <a class="yt-timestamp" data-t="00:38:20">[00:38:20]</a>

### Benefits and Rationale
*   **Innovation and Efficiency:** Open source allows Meta to benefit from community innovations, such as improvements in model efficiency, which can lead to substantial cost savings given Meta's scale. [[open_source_software_and_cultural_dynamics | Open Source Software and Cultural Dynamics]] <a class="yt-timestamp" data-t="00:38:32">[00:38:32]</a> <a class="yt-timestamp" data-t="01:06:56">[01:06:56]</a>
*   **Avoiding Gatekeepers:** An open ecosystem prevents a few companies with closed models from controlling APIs and dictating what developers can build, an issue Meta has faced in the mobile ecosystem. <a class="yt-timestamp" data-t="01:08:18">[01:08:18]</a> <a class="yt-timestamp" data-t="01:09:01">[01:09:01]</a>
*   **Democratization and Safety:** Meta believes that a concentration of powerful AI in few hands is a significant risk. <a class="yt-timestamp" data-t="00:42:52">[00:42:52]</a> Open source promotes a more even and balanced playing field, allowing systems to be hardened progressively by a wider community, analogous to how open-source software improves general cybersecurity. <a class="yt-timestamp" data-t="00:44:07">[00:44:07]</a> <a class="yt-timestamp" data-t="00:46:16">[00:46:16]</a> The risk of an untrustworthy actor having a super-strong AI is seen as potentially greater than risks from widespread availability of open models. <a class="yt-timestamp" data-t="00:38:38">[00:38:38]</a> <a class="yt-timestamp" data-t="00:45:13">[00:45:13]</a>

### Potential Risks and Mitigations
Meta acknowledges potential downsides to open-sourcing powerful AI. The company is committed to not open-sourcing a model if there's a qualitative change in its capabilities that makes it irresponsible to do so, particularly if harms cannot be adequately mitigated. <a class="yt-timestamp" data-t="00:38:38">[00:38:38]</a> <a class="yt-timestamp" data-t="00:39:05">[00:39:05]</a> Current mitigations focus on preventing models from assisting in harmful activities like violence or fraud. <a class="yt-timestamp" data-t="00:39:23">[00:39:23]</a> <a class="yt-timestamp" data-t="01:12:14">[01:12:14]</a>

### Licensing Model
Llama models are released under a permissive open-source license, with an exception for the largest cloud providers. If these companies intend to resell Meta's models, they are required to discuss a revenue-sharing agreement with Meta. <a class="yt-timestamp" data-t="01:10:42">[01:10:42]</a> <a class="yt-timestamp" data-t="01:11:07">[01:11:07]</a> This approach has led to deals with major cloud companies for Llama 2. <a class="yt-timestamp" data-t="01:11:15">[01:11:15]</a>

## Future Vision and Challenges

### Scale of AI's Impact
Mark Zuckerberg likens the advent of advanced AI to the creation of computing itself, suggesting it's a fundamental, low-level innovation that will transform all products and industries. [[impact_of_ai_on_future_technology_and_society | Impact of AI on future technology and society]] <a class="yt-timestamp" data-t="00:34:29">[00:34:29]</a> <a class="yt-timestamp" data-t="00:34:56">[00:34:56]</a>

### Key Bottlenecks
While progress is rapid, physical constraints are expected to temper the pace of development, making an "overnight superintelligence" scenario unlikely. <a class="yt-timestamp" data-t="00:35:33">[00:35:33]</a>
*   **Energy:** The most significant long-term bottleneck is anticipated to be energy. Building data centers requiring hundreds of megawatts, or even a gigawatt (comparable to a nuclear power plant for a single training cluster), involves extensive lead times due to regulatory processes for energy permitting and transmission line construction. [[data_center_energy_requirements_and_scaling | Data center energy requirements and scaling]] <a class="yt-timestamp" data-t="00:28:06">[00:28:06]</a> <a class="yt-timestamp" data-t="00:29:11">[00:29:11]</a> <a class="yt-timestamp" data-t="00:31:04">[00:31:04]</a> <a class="yt-timestamp" data-t="00:31:18">[00:31:18]</a>
*   **Compute:** While GPU production is catching up, the sheer capital investment and energy requirements for ever-larger clusters remain challenging. [[role_of_compute_and_infrastructure_in_the_future_of_ai_development | Role of compute and infrastructure in the future of AI development]] <a class="yt-timestamp" data-t="00:27:28">[00:27:28]</a> <a class="yt-timestamp" data-t="00:28:00">[00:28:00]</a>

### Synthetic Data and Model Improvement
Synthetic data generated by models can help improve performance within the limits of a given model architecture. <a class="yt-timestamp" data-t="00:52:45">[00:52:45]</a> However, it's not expected to allow smaller models to reach the capabilities of state-of-the-art larger models that incorporate new architectural research. [[mechanistic_interpretability_in_ai | Mechanistic interpretability in AI]] <a class="yt-timestamp" data-t="00:52:49">[00:52:49]</a> <a class="yt-timestamp" data-t="00:53:04">[00:53:04]</a>

### Custom Silicon Development
Meta is developing its own custom silicon. The initial focus has been on chips for inference in ranking and recommendation systems (Reels, News Feed ads), freeing up more expensive NVIDIA GPUs for training. <a class="yt-timestamp" data-t="01:15:06">[01:15:06]</a> <a class="yt-timestamp" data-t="01:15:16">[01:15:16]</a> The long-term roadmap includes developing custom silicon for training simpler models, and eventually, the very large foundation models, though Llama 4 will not yet be trained on this custom silicon. [[ai_developments_in_hardware_and_software_advancements | AI developments in hardware and software advancements]] <a class="yt-timestamp" data-t="01:15:06">[01:15:06]</a> <a class="yt-timestamp" data-t="01:15:43">[01:15:43]</a>