---
title: Human and AI intelligence comparison
videoId: 6yQEA18C-XI
---

From: [[dwarkesh | The Dwarkesh Podcast]]

## Human and AI Intelligence Comparison

This article summarizes a discussion between George Hotz and Eliezer Yudkowsky, moderated by Dwarkesh Patel, focusing on the comparison between human and artificial intelligence, their capabilities, and potential.

### Introduction

The debate touches upon the nature of intelligence, whether it's a singular scale, how humans and AIs differ in their processing and capabilities, and the implications of these differences. George Hotz initiated the discussion by questioning the concept of AI "foom" or rapid, critical self-improvement <a class="yt-timestamp" data-t="00:02:48">[00:02:48]</a>, while Eliezer Yudkowsky argued that a large enough intelligence gap, even if achieved slowly, could be problematic for humanity <a class="yt-timestamp" data-t="00:03:52">[00:03:52]</a>.

### Capability Comparisons

#### Specific Tasks
*   **Chess and Go:** Computers are superhuman in games like chess and Go <a class="yt-timestamp" data-t="00:08:51">[00:08:51]</a>, <a class="yt-timestamp" data-t="00:17:40">[00:17:40]</a>. Magnus Carlsen, arguably the strongest human chess player, is predictably defeated by chess engines <a class="yt-timestamp" data-t="00:08:51">[00:08:51]</a>, <a class="yt-timestamp" data-t="00:18:37">[00:18:37]</a>.
*   **Addition:** Computers have been superhuman at addition for a long time <a class="yt-timestamp" data-t="00:17:30">[00:17:30]</a>.
*   **Plumbing:** Computers are currently far sub-human at tasks like plumbing [[impact_of_ai_on_software_development_and_productivity | plumbing]]. <a class="yt-timestamp" data-t="00:17:35">[00:17:35]</a>.
*   **Protein Folding:** AlphaFold 2 cracked the general case of protein folding around 2020 <a class="yt-timestamp" data-t="00:06:21">[00:06:21]</a>. Yudkowsky had predicted in 2004 that superintelligence would solve a special case, but AI solved a harder general case sooner than expected and without being a full superintelligence <a class="yt-timestamp" data-t="00:05:49">[00:05:49]</a>. Hotz notes AlphaFold was trained on huge amounts of experimental data, not derived from first principles like quantum field theory <a class="yt-timestamp" data-t="00:07:57">[00:07:57]</a>.
*   **Coding:** Hotz believes he can code better than GPT-4, but acknowledges his code can have bugs <a class="yt-timestamp" data-t="00:38:14">[00:38:14]</a>. He doubts a machine will perfectly write code with no bugs [[challenges_and_limitations_in_ai_interpretability_and_safety | no bugs]] <a class="yt-timestamp" data-t="00:39:16">[00:39:16]</a>, relating this to the difficulty of formal programming and specifying all desired properties <a class="yt-timestamp" data-t="00:39:37">[00:39:37]</a>, <a class="yt-timestamp" data-t="00:40:05">[00:40:05]</a>.

#### Human Intelligence Augmented by Tools
*   Hotz argues that much of human intelligence is externalized and augmented by tools, like computers and spreadsheets, making a human plus a computer far beyond unaugmented human intelligence [[human_enhancement_and_intelligence_augmentation | far beyond unaugmented human intelligence]] <a class="yt-timestamp" data-t="00:16:03">[00:16:03]</a>, <a class="yt-timestamp" data-t="00:17:48">[00:17:48]</a>.
*   Yudkowsky counters that in such a human-tool system, the more powerful component becomes the center of gravity. For example, a human using a modern chess engine is only as smart as the chess engine; the human either follows its advice or loses <a class="yt-timestamp" data-t="00:18:37">[00:18:37]</a>. He uses the metaphor of "suns" (more powerful intelligences) and "planets" or "moons" (less powerful ones or tools) <a class="yt-timestamp" data-t="00:25:31">[00:25:31]</a>.

### Nature and Structure of Intelligence

#### Linear vs. Multi-dimensional Intelligence
*   Hotz suggests intelligence doesn't fall on a nice line, given varying capabilities in tasks like addition, plumbing, and chess <a class="yt-timestamp" data-t="00:17:25">[00:17:25]</a>.

#### Generality and Universality
*   Hotz posits that humans are "pretty universal" <a class="yt-timestamp" data-t="00:38:05">[00:38:05]</a>. Yudkowsky jests that if true, more would have learned to code by now [[impact_of_ai_on_future_technology_and_society | code by now]] <a class="yt-timestamp" data-t="00:38:10">[00:38:10]</a>.
*   Yudkowsky argues that humans, while more general than chimps, cannot easily rewrite their own code to handle new problems, suggesting minds with stronger "sparks of generality" are possible <a class="yt-timestamp" data-t="01:16:14">[01:16:14]</a>.
*   He notes that a six-times larger prefrontal cortex in humans compared to chimpanzees led to nuclear weapons versus sticks, implying significant capability gains from relatively small architectural changes in a general system [[comparisons_between_atomic_bomb_development_and_modern_ai_advancements | versus sticks]] <a class="yt-timestamp" data-t="01:15:35">[01:15:35]</a>. Hotz counters that humans are general-purpose while chimps are not, and scaling a specialized system like Deep Blue won't make it general <a class="yt-timestamp" data-t="01:15:50">[01:15:50]</a>.

#### Learning, Training, and Architecture
*   **Human Brain vs. GPT:** Yudkowsky states humans have structural properties not yet detected in GPT-4, such as a cerebellum for motor control and error correction, and engage in more complex decision problems beyond simple prediction <a class="yt-timestamp" data-t="00:35:05">[00:35:05]</a>. Humans are trained by evolution for inclusive genetic fitness <a class="yt-timestamp" data-t="00:35:58">[00:35:58]</a>.
*   **LLMs:** GPT-4 is trained to predict the next word and then used to imitate human text [[limitations_of_large_language_models_llms_in_solving_novel_tasks | imitate human text]] <a class="yt-timestamp" data-t="00:34:03">[00:34:03]</a>. Hotz suggests current AIs are built to mimic humans closely by predicting them <a class="yt-timestamp" data-t="00:33:42">[00:33:42]</a>.
*   **Constraints on Human Intelligence:** Dwarkesh Patel interjects that humans are not trained "Chinchilla optimally," face biological constraints (e.g., birth canal size, mutational load) <a class="yt-timestamp" data-t="00:37:38">[00:37:38]</a>. Yudkowsky adds that natural selection faced constraints, e.g., only inventing freely rotating wheels in biology three known times, suggesting artificial designs have more headroom <a class="yt-timestamp" data-t="01:04:18">[01:04:18]</a>.

#### Rationality and Goals
*   Yudkowsky suggests that as AI gets smarter, it tends to become more rational from our perspective, grinding out inefficiencies [[ai_alignment_and_safety_research | grinding out inefficiencies]], similar to how most stock prices aren't easily exploitable <a class="yt-timestamp" data-t="00:59:31">[00:59:31]</a>.
*   Hotz questions if all AIs will converge to be "brutally optimal" without intense competition, pointing to GPT-4 as an example of a powerful but irrational system [[limitations_of_large_language_models_llms_in_solving_novel_tasks | irrational system]] <a class="yt-timestamp" data-t="01:00:09">[01:00:09]</a>.
*   Yudkowsky clarifies he doesn't expect a move away from "giant inscrutable matrices" before potential doom, but that these matrix-based systems could become powerful enough to rewrite themselves or design better successors <a class="yt-timestamp" data-t="00:50:24">[00:50:24]</a>, <a class="yt-timestamp" data-t="01:00:53">[01:00:53]</a>.

### Computational Power and Efficiency

*   **Human Brain Compute:**
    *   Hotz estimates human brain compute at around 20 petaflops, and the total for humanity at 160,000 zettaflops <a class="yt-timestamp" data-t="00:31:48">[00:31:48]</a>. Later, a figure of 100 petaflops (10^17 operations/second) is used for an individual human brain <a class="yt-timestamp" data-t="01:11:46">[01:11:46]</a>, <a class="yt-timestamp" data-t="01:13:48">[01:13:48]</a>.
    *   Yudkowsky agrees with the 10^17 ops/sec figure <a class="yt-timestamp" data-t="01:11:46">[01:11:46]</a>.
*   **Silicon Compute:** Current global silicon compute is estimated by Hotz at around 2 zettaflops, 80,000 times less than human compute <a class="yt-timestamp" data-t="00:31:37">[00:31:37]</a>. Yudkowsky argues this is misleading due to poor aggregation of human compute <a class="yt-timestamp" data-t="00:32:15">[00:32:15]</a>.
*   **Power Efficiency:**
    *   Hotz states that to get 20 petaflops from silicon (16 H100s) costs half a million dollars and uses 20 kilowatts [[data_center_energy_requirements_and_scaling | 20 kilowatts]] <a class="yt-timestamp" data-t="01:12:02">[01:12:02]</a>. For 100 petaflops, a silicon computer would take about 100 kilowatts <a class="yt-timestamp" data-t="01:13:55">[01:13:55]</a>.
    *   The human brain achieves this with about 100 watts, making it vastly more power-efficient (roughly 1000x) than current silicon for equivalent raw operations <a class="yt-timestamp" data-t="01:11:43">[01:11:43]</a>, <a class="yt-timestamp" data-t="01:12:26">[01:12:26]</a>.
*   **Landauer Limit:**
    *   Hotz suggests the human brain might be very close to the Landauer limit for computational efficiency, while current silicon is off by a factor of 100 to 1000 <a class="yt-timestamp" data-t="01:12:35">[01:12:35]</a>.
    *   Yudkowsky finds it "deeply biologically implausible" for the brain to be at the Landauer limit, citing the irreversible operations in synaptic transmission (neurotransmitter reuptake) and ion channel activity during neural impulses, each contributing to thermodynamic cost <a class="yt-timestamp" data-t="01:12:52">[01:12:52]</a>. He estimates the brain is perhaps six orders of magnitude away from the limit <a class="yt-timestamp" data-t="01:11:50">[01:11:50]</a>.
*   **Efficiency of Deep Learning:** Hotz calls deep learning "incredibly inefficient" <a class="yt-timestamp" data-t="01:09:08">[01:09:08]</a>. Yudkowsky agrees, noting that LLMs perform vast amounts of computation for simple tasks like multiplication [[limitations_of_large_language_models_llms_in_solving_novel_tasks | limitations_of_large_language_models_llms_in_solving_novel_tasks]], similar to how inefficient humans are at it <a class="yt-timestamp" data-t="01:09:45">[01:09:45]</a>, <a class="yt-timestamp" data-t="01:10:07">[01:10:07]</a>.

### Aggregation and Group Intelligence

*   **Kasparov vs. The World:** Yudkowsky cites the "Kasparov vs. The World" chess match, where Kasparov (one grandmaster) defeated a team of 10,000 people advised by four grandmasters, as an example where a single, more coherent intelligence can outperform a larger, less coordinated group <a class="yt-timestamp" data-t="00:23:16">[00:23:16]</a>, <a class="yt-timestamp" data-t="00:32:29">[00:32:29]</a>.
*   Hotz counters that Kasparov had vastly more experience (e.g., 100,000 games) than "The World" team (one game) and that with comparable experience, the group would win <a class="yt-timestamp" data-t="00:24:02">[00:24:02]</a>.
*   Yudkowsky predicts that even with extensive practice, 100,000 humans without computers would still lose to Stockfish 15 <a class="yt-timestamp" data-t="00:24:55">[00:24:55]</a>. Hotz agrees, implying the necessity of AI tools for humans to compete [[impact_of_ai_on_future_technology_and_society | necessity of AI tools for humans to compete]] <a class="yt-timestamp" data-t="00:25:09">[00:25:09]</a>.
*   **GPT-4 as Mixture of Experts:** Hotz notes that GPT-4 is a mixture of experts (eight small things, not one big thing) <a class="yt-timestamp" data-t="00:32:33">[00:32:33]</a>. Yudkowsky is skeptical this model holds in the limit <a class="yt-timestamp" data-t="00:32:40">[00:32:40]</a>.

### Orthogonality Thesis

*   Yudkowsky upholds the orthogonality thesis: intelligence and final goals are independent <a class="yt-timestamp" data-t="00:02:19">[00:02:19]</a>.
*   Hotz asks if it applies to humans, suggesting intelligence and "goodness" seem uncorrelated <a class="yt-timestamp" data-t="00:56:26">[00:56:26]</a>. Yudkowsky notes few things are truly uncorrelated with intelligence and that empirically, smart people might lean slightly "nice" within their own culture, but this is weak. Orthogonality is a statement about the entire mind design space: for any coherent goal, a mind can exist that pursues it <a class="yt-timestamp" data-t="00:56:46">[00:56:46]</a>.
*   This implies that an AI could be highly intelligent but not share human values or desires, such as keeping humans "happy and free" [[the_potential_economic_and_social_impacts_of_agi | keeping humans "happy and free"]] <a class="yt-timestamp" data-t="01:26:43">[01:26:43]</a>.