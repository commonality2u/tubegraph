---
title: OpenRAG eval project overview
videoId: 1cQlnfwmIdU
---

From: [[aidotengineer]] <br/> 

OpenRAG eval is a new open-source project designed for quick and scalable [[challenges_in_rag_evaluation|RAG evaluation]] <a class="yt-timestamp" data-t="00:00:06">[00:00:06]</a>. Developed by Victara in collaboration with the University of Waterloo's Jimmy Lynn lab, it addresses a major problem in [[challenges_in_rag_evaluation|RAG evaluation]]: the common requirement for golden answers or chunks, which is not scalable <a class="yt-timestamp" data-t="00:00:22">[00:00:22]</a>.

## How OpenRAG Eval Works

The architecture of OpenRAG eval involves several steps to evaluate a [[rag_stack_components_and_configuration|RAG system]] <a class="yt-timestamp" data-t="00:00:45">[00:00:45]</a>:

1.  **Query Input**: The process begins with a set of queries, which can range from 10 to 1,000, collected for their importance to the specific [[rag_stack_components_and_configuration|RAG system]] <a class="yt-timestamp" data-t="00:00:47">[00:00:47]</a>.
2.  **RAG Connector**: A RAG connector collects actual information, including chunks and answers, generated by the [[rag_stack_components_and_configuration|RAG pipeline]] <a class="yt-timestamp" data-t="00:00:56">[00:00:56]</a>. Connectors are available for Vectara, LangChain, LlamaIndex, and a growing number of others <a class="yt-timestamp" data-t="00:01:04">[00:01:04]</a>. These connectors generate the RAG outputs <a class="yt-timestamp" data-t="00:01:09">[00:01:09]</a>.
3.  **Evaluation**: The evaluation process runs a series of [[metrics_for_evaluating_rag_systems|metrics]], which are grouped into evaluators <a class="yt-timestamp" data-t="00:01:16">[00:01:16]</a>.
4.  **Output**: The evaluators generate RAG evaluation files containing all necessary information to assess the [[rag_stack_components_and_configuration|RAG pipeline]] <a class="yt-timestamp" data-t="00:01:24">[00:01:24]</a>.

## Metrics for Evaluation without Golden Answers

A key innovation of OpenRAG eval is its ability to perform [[challenges_of_rag_evaluation_without_golden_answers|RAG evaluation without golden answers]] <a class="yt-timestamp" data-t="00:01:37">[00:01:37]</a>. This is achieved through several specific [[metrics_for_evaluating_rag_systems|metrics]]:

*   **Umbrella (Retrieval Metric)**:
    *   This metric allows for retrieval evaluation without requiring golden chunks <a class="yt-timestamp" data-t="00:01:44">[00:01:44]</a>.
    *   It assigns a score between 0 and 3 to a chunk, where 0 means the chunk has nothing to do with the query, and 3 means it's dedicated to the query and contains the exact answer <a class="yt-timestamp" data-t="00:02:13">[00:02:13]</a>.
    *   Research by the University of Waterloo shows that this approach correlates well with human judgment <a class="yt-timestamp" data-t="00:02:32">[00:02:32]</a>.

*   **AutoNuggetizer (Generation Metric)**:
    *   This metric evaluates generation without requiring golden answers <a class="yt-timestamp" data-t="00:02:54">[00:02:54]</a>.
    *   It involves three steps <a class="yt-timestamp" data-t="00:02:58">[00:02:58]</a>:
        1.  **Nugget Creation**: Atomic units called "nuggets" are created <a class="yt-timestamp" data-t="00:03:00">[00:03:00]</a>.
        2.  **Nugget Rating**: Each nugget is assigned a "vital" or "okay" rating, with the top 20 nuggets typically selected <a class="yt-timestamp" data-t="00:03:07">[00:03:07]</a>.
        3.  **LLM Judge Analysis**: An LLM judge analyzes the RAG response to determine if each selected nugget is fully or partially supported by the answer <a class="yt-timestamp" data-t="00:03:15">[00:03:15]</a>.

*   **Citation Faithfulness**:
    *   Measures whether citations within the response are high fidelity, fully supported, partially supported, or not supported by the answer <a class="yt-timestamp" data-t="00:03:30">[00:03:30]</a>.

*   **Hallucination Detection**:
    *   Utilizes Vectara's Hallucination Detection Model (HHM) to check if the entire response aligns with the retrieved content <a class="yt-timestamp" data-t="00:03:45">[00:03:45]</a>.

## User Interface

OpenRAG eval provides a user interface for analyzing evaluation results <a class="yt-timestamp" data-t="00:03:58">[00:03:58]</a>. Users can drag and drop the generated evaluation files onto `open evaluation.ai` to view a UI that displays queries, retrieval scores, and different generation scores <a class="yt-timestamp" data-t="00:04:06">[00:04:06]</a>.

## Benefits and Contributions

OpenRAG eval is a powerful package that can help optimize and tune a [[rag_stack_components_and_configuration|RAG pipeline]] <a class="yt-timestamp" data-t="00:04:29">[00:04:29]</a>. Being open-source, it offers transparency in how the metrics work <a class="yt-timestamp" data-t="00:04:34">[00:04:34]</a>. While it includes connectors for Vectara, LangChain, and LlamaIndex, contributions for other [[rag_stack_components_and_configuration|RAG pipeline]] connectors are welcomed <a class="yt-timestamp" data-t="00:04:44">[00:04:44]</a>.