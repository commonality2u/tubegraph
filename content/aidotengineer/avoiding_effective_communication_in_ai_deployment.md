---
title: Avoiding effective communication in AI deployment
videoId: 89aQ7T6cMwA
---

From: [[aidotengineer]] <br/> 
This article outlines strategies for intentionally hindering communication in AI deployment, based on an "inverted" presentation designed to show how to spectacularly mess up an AI strategy <a class="yt-timestamp" data-t="00:00:27">[00:00:27]</a>. By embracing "worse practices," the aim is to torpedo projects and alienate colleagues <a class="yt-timestamp" data-t="00:00:54">[00:00:54]</a>.

## Divide and Conquer Your Company

The foundational step to [[failure_in_ai_project_implementation | failure in AI project implementation]] is to actively divide and conquer your own company <a class="yt-timestamp" data-t="00:02:27">[00:02:27]</a>.

### Create Impenetrable Silos
*   Attend every AI industry conference, but never share what you learned with your team <a class="yt-timestamp" data-t="00:02:48">[00:02:48]</a>.
*   Actively create "impenetrable silos" and incentivize secrecy among your teams <a class="yt-timestamp" data-t="00:03:01">[00:03:01]</a>.

### Make Unreasonable Promises
When communicating with customers about AI, engage in "wishful thinking promises" (WTP) <a class="yt-timestamp" data-t="00:03:32">[00:03:32]</a>. Tell them AI will do absolutely everything, from writing emails to solving climate change, without worrying about the detailsâ€”just promise the moon <a class="yt-timestamp" data-t="00:03:35">[00:03:35]</a>.

## Define Your Strategy Vaguely

When defining your AI strategy, ensure it is ambiguous and vague to guarantee confusion and hinder effective communication.

### Fake Any Diagnosis
*   Grab an old annual report or operating plan and highlight random, poorly understood paragraphs, declaring them as "must fix" <a class="yt-timestamp" data-t="00:05:09">[00:05:09]</a>.
*   Crucially, avoid talking to anyone who actually does the work <a class="yt-timestamp" data-t="00:05:20">[00:05:20]</a>.

### Craft Ambiguous Policies
Your guiding policy should be incredibly ambiguous, such as "become the global AI leader in everything" without defining what "everything" means, leaving it as "someone else's problem" <a class="yt-timestamp" data-t="00:05:27">[00:05:27]</a>.

### Embrace Perpetual Beta
*   Forget about timelines, which are for companies that intend to finish projects <a class="yt-timestamp" data-t="00:06:07">[00:06:07]</a>.
*   Instead, embrace "Perpetual Beta" by creating a massive GitHub backlog and stuffing it with highlighted financial reports <a class="yt-timestamp" data-t="00:06:13">[00:06:13]</a>.
*   Alternatively, create a 4,000-page document, post it across all Slack channels, and erode people's willpower to engage with the material <a class="yt-timestamp" data-t="00:06:25">[00:06:25]</a>.

## Drown Everyone in Jargon

One of the most effective ways to cause dysfunction is to communicate in a way that nobody understands, drowning everyone in a "tsunami of jargon" <a class="yt-timestamp" data-t="00:06:48">[00:06:48]</a>.

> "Our multimodal agentic Transformer-based system leverages few-shot learning and Chain-of-Thought reasoning to optimize the synergistic potential of our dynamic hyperparameter space" <a class="yt-timestamp" data-t="00:06:53">[00:06:53]</a>.
>
> The goal is to look incredibly smart, even if no one understands a word, prioritizing obfuscation <a class="yt-timestamp" data-t="00:07:08">[00:07:08]</a>.

### Hide Jobs to Be Done
Strategically use jargon to hide the actual "jobs to be done" <a class="yt-timestamp" data-t="00:07:41">[00:07:41]</a>. For example, instead of saying "we need to write a prompt," say "we're building agents" <a class="yt-timestamp" data-t="00:07:47">[00:07:47]</a>. This ensures that relevant domain experts, like mental health experts in one case, are not in the room and do not know how to participate, which is the desired outcome <a class="yt-timestamp" data-t="00:07:52">[00:07:52]</a>.

Other examples of strategic jargon:
*   Instead of "make sure the AI has the right context," say "RAGs" <a class="yt-timestamp" data-t="00:08:12">[00:08:12]</a>.
*   Instead of "make sure users can trick the AI into doing something bad," say "prompt injections" <a class="yt-timestamp" data-t="00:08:17">[00:08:17]</a>.

### Disempower Non-Engineers
Encourage engineers, rather than those who understand customers best, to write prompts, as "what could possibly go wrong?" <a class="yt-timestamp" data-t="00:08:24">[00:08:24]</a>. The objective is to make everything, even writing prompts, seem "super technical and out of reach" for everyone else <a class="yt-timestamp" data-t="00:08:50">[00:08:50]</a>.

## Avoid Data and Rely on Gut Feelings

To truly torpedo AI efforts, avoid looking at data and ensure no one else does either <a class="yt-timestamp" data-t="00:13:42">[00:13:42]</a>.

### Trust Tools Blindly
*   When a RAG system fails, don't analyze the problem; just buy a new, more expensive vector database <a class="yt-timestamp" data-t="00:10:51">[00:10:51]</a>.
*   If agents aren't working, simply pick a new framework and vendor, then fine-tune without any measurement or evaluation <a class="yt-timestamp" data-t="00:11:15">[00:11:15]</a>. Assume it will be better because it's "kind of like Alchemy with a lot more electricity" <a class="yt-timestamp" data-t="00:11:22">[00:11:22]</a>.
*   Adhere to a "one size fits all solution" mentality for evaluations, letting vendors figure it out as "you're too busy being an executive" <a class="yt-timestamp" data-t="00:12:06">[00:12:06]</a>.

### Disregard Custom Metrics
*   When measuring progress, use every off-the-shelf evaluation metric you can find <a class="yt-timestamp" data-t="00:11:01">[00:11:01]</a>. Never bother customizing them to business needs; blindly trust the numbers even if they make no sense <a class="yt-timestamp" data-t="00:11:05">[00:11:05]</a>.
*   Create a dashboard with unintelligible numbers that obscure the difference between success and failure <a class="yt-timestamp" data-t="00:12:35">[00:12:35]</a>. Keep hoarding random metrics until one goes up and to the right, then claim success <a class="yt-timestamp" data-t="00:12:44">[00:12:44]</a>.
*   Adopt all eval frameworks blindly, never asking if they actually measure success <a class="yt-timestamp" data-t="00:13:04">[00:13:04]</a>. Prioritize metrics like "cosine similarity, BLEU, and ROUGE," ignoring actual user experience <a class="yt-timestamp" data-t="00:13:17">[00:13:17]</a>.
*   Never cross-check with domain experts or users <a class="yt-timestamp" data-t="00:13:24">[00:13:24]</a>. If an LM says it's accurate, who are we to argue? <a class="yt-timestamp" data-t="00:13:30">[00:13:30]</a>

### Isolate Data Access
*   Ensure that engineers handle everything, even if they haven't spoken to a customer in years <a class="yt-timestamp" data-t="00:14:54">[00:14:54]</a>. Quickly forget simpler options like spreadsheets for data annotation and review <a class="yt-timestamp" data-t="00:15:05">[00:15:05]</a>.
*   Insist on putting data into complex systems that only engineers can access, making it unavailable to domain experts <a class="yt-timestamp" data-t="00:15:26">[00:15:26]</a>. As an executive, insist on buying a custom data analysis platform requiring a team of PhDs to operate and understand <a class="yt-timestamp" data-t="00:15:37">[00:15:37]</a>. Bonus points if it takes six months to load and has incessant errors <a class="yt-timestamp" data-t="00:15:49">[00:15:49]</a>.

### Rely on Gut Feelings
*   Trust your gut, as it got you this far in life <a class="yt-timestamp" data-t="00:14:34">[00:14:34]</a>. Feelings are always a reliable substitute for data, especially for million-dollar decisions <a class="yt-timestamp" data-t="00:14:40">[00:14:40]</a>.
*   Remember, customers are your best Q&A <a class="yt-timestamp" data-t="00:14:23">[00:14:23]</a>.