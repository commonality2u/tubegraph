---
title: Guardrails to ensure AI accuracy
videoId: pSqpC7fFLZA
---

From: [[aidotengineer]] <br/> 

Ensuring accuracy is a critical aspect of building and deploying AI systems, especially when they are integrated into workflows to assist human teams <a class="yt-timestamp" data-t="00:00:30">[00:00:30]</a>. A key goal is to leverage AI to work smarter, rather than just replacing human tasks <a class="yt-timestamp" data-t="00:00:15">[00:00:15]</a>.

## The Need for Guardrails

When integrating AI agents, common pain points and risks emerge, including:
*   Error-prone first drafts generated by AI <a class="yt-timestamp" data-t="00:01:12">[00:01:12]</a>.
*   The significant risk of hallucination if AI is left unchecked <a class="yt-timestamp" data-t="00:01:23">[00:01:23]</a>.

To address these, "guardrails" are implemented to maintain accuracy and build trust in AI systems <a class="yt-timestamp" data-t="00:00:30">[00:00:30]</a>, <a class="yt-timestamp" data-t="00:07:23">[00:07:23]</a>.

## Layered Approach to Accuracy

A layered approach is employed to mitigate AI errors and reduce hallucinations significantly <a class="yt-timestamp" data-t="00:03:27">[00:03:27]</a>, <a class="yt-timestamp" data-t="00:03:33">[00:03:33]</a>. This involves multiple stages of validation and human oversight:

1.  **Custom GPT Configuration** <a class="yt-timestamp" data-t="00:02:37">[00:02:37]</a>
    *   AI agents are configured with specific style guides and rubrics, often retrieved from collaborative platforms like Airtable, to ensure consistency <a class="yt-timestamp" data-t="00:02:44">[00:02:44]</a>, <a class="yt-timestamp" data-t="00:02:50">[00:02:50]</a>. This ensures that the AI's outputs conform to desired standards, such as those for an automated editor or SEO metadata generator <a class="yt-timestamp" data-t="00:04:23">[00:04:23]</a>.

2.  **Validation Layer** <a class="yt-timestamp" data-t="00:02:56">[00:02:56]</a>
    *   This layer incorporates tools like Veil Linting and CI/CD tests <a class="yt-timestamp" data-t="00:02:59">[00:02:59]</a>. These automated checks help to catch errors and inconsistencies before human review.

3.  **Human Review and Oversight** <a class="yt-timestamp" data-t="00:03:12">[00:03:12]</a>
    *   **GitHub PR Codeowner Review:** Pull requests (PRs) initiated by AI agents are subject to codeowner review, making it easier to scrutinize suggested changes <a class="yt-timestamp" data-t="00:03:03">[00:03:03]</a>, <a class="yt-timestamp" data-t="00:03:06">[00:03:06]</a>.
    *   **Manual Merge:** A human is ultimately responsible for merging changes, ensuring that the output is correct before approval <a class="yt-timestamp" data-t="00:03:12">[00:03:12]</a>.
    *   **Product and Engineering Reviews:** Several human eyes, including product managers and engineering teams, review content before it is merged <a class="yt-timestamp" data-t="00:03:19">[00:03:19]</a>, <a class="yt-timestamp" data-t="00:03:21">[00:03:21]</a>.

## Tackling Specific Risks

Guardrails are specifically designed to address key risks in AI development:

### Hallucinations
To mitigate hallucinations, a combination of automated tools and human oversight is used:
*   Tools like Veil Lint and CI tests help detect and reduce instances of AI generating incorrect or fabricated information <a class="yt-timestamp" data-t="00:07:29">[00:07:29]</a>.
*   The involvement of various human stakeholders provides additional layers of review and correction <a class="yt-timestamp" data-t="00:07:36">[00:07:36]</a>.

### Bias
Addressing bias involves:
*   Data set tests <a class="yt-timestamp" data-t="00:07:40">[00:07:40]</a>.
*   Prompt audits <a class="yt-timestamp" data-t="00:07:43">[00:07:43]</a>.
These help to identify and correct any unfair or skewed outputs that might arise from biased training data or prompt engineering.

### Stakeholder Misalignment
To ensure alignment and continuous improvement:
*   **Weekly PR Reviews:** Regular reviews, sometimes compressed into days or hours, help to ensure that AI-generated content meets stakeholder expectations <a class="yt-timestamp" data-t="00:07:50">[00:07:50]</a>.
*   **Slack Feedback Loops:** Continuous feedback loops, particularly with product managers and engineering teams, allow for ongoing tuning of AI prompts <a class="yt-timestamp" data-t="00:07:56">[00:07:56]</a>, <a class="yt-timestamp" data-t="00:07:58">[00:07:58]</a>. This iterative process prevents reliance on the AI model to magically stay perfect and instead fosters continuous refinement <a class="yt-timestamp" data-t="00:08:03">[00:08:03]</a>, <a class="yt-timestamp" data-t="00:08:05">[00:08:05]</a>. These feedback loops are integral to [[evaluations_and_finetuning_in_ai_development | evaluations and finetuning in AI development]].

## Best Practices for Building Resilient AI Workflows

To effectively build resilient AI workflows and ensure accuracy, a three-step playbook is recommended <a class="yt-timestamp" data-t="00:08:11">[00:08:11]</a>:

1.  **Identify a Key Pain Point:** Pinpoint a specific area where AI can significantly improve throughput <a class="yt-timestamp" data-t="00:08:14">[00:08:14]</a>.
2.  **Pick a Single, Repeatable, Rule-Based Task:** Focus on tasks that are high-volume, low-creativity, and can be automated reliably by an AI helper <a class="yt-timestamp" data-t="00:02:16">[00:02:16]</a>, <a class="yt-timestamp" data-t="00:08:17">[00:08:17]</a>.
3.  **Loop with Users Weekly:** Establish regular feedback cycles with users to continuously ship, measure, and refine the AI's performance <a class="yt-timestamp" data-t="00:08:22">[00:08:22]</a>, <a class="yt-timestamp" data-t="00:08:25">[00:08:25]</a>. This iterative approach allows teams to stack wins and significantly boost velocity <a class="yt-timestamp" data-t="00:08:27">[00:08:27]</a>, <a class="yt-timestamp" data-t="00:08:30">[00:08:30]</a>.