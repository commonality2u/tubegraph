---
title: Future of voice applications in AI
videoId: OC04sP_QgTI
---

From: [[aidotengineer]] <br/> 

The next frontier in AI is [[voice_and_multimodal_ai_agents | voice AI]], which is already transforming call centers <a class="yt-timestamp" data-t="01:48:40">[01:48:40]</a>. Over one billion calls globally are now made in call centers using voice APIs <a class="yt-timestamp" data-t="01:55:00">[01:55:00]</a>. Real-time voice APIs are enabling agents to revolutionize call center operations <a class="yt-timestamp" data-t="02:05:00">[02:05:00]</a>.

An example of a real production application is the Price Line Pennybot, which allows users to book an entire vacation hands-free without text <a class="yt-timestamp" data-t="02:13:00">[02:13:00]</a>. This shift signifies that discussions are no longer solely about text-based agents but increasingly about [[voice_and_multimodal_ai_agents | multimodal agents]] <a class="yt-timestamp" data-t="02:28:00">[02:28:00]</a>.

## Evaluating Voice and Multimodal Agents

Evaluating voice and [[voice_and_multimodal_ai_agents | multimodal agents]] requires specific types of evaluations beyond those for text-based agents <a class="yt-timestamp" data-t="02:33:00">[02:33:00]</a>. The future of voice applications involves some of the most complex applications ever built <a class="yt-timestamp" data-t="11:51:00">[11:51:00]</a>.

For voice applications, evaluation needs to consider additional components beyond just the text or transcript <a class="yt-timestamp" data-t="12:00:00">[12:00:00]</a>. The audio chunk itself also needs to be evaluated <a class="yt-timestamp" data-t="12:12:00">[12:12:00]</a>. In many Voice Assistant APIs, the generated transcript occurs after the audio chunk is sent <a class="yt-timestamp" data-t="12:19:00">[12:19:00]</a>, introducing a new dimension for evaluation <a class="yt-timestamp" data-t="12:27:00">[12:27:00]</a>.

Key evaluation points for voice applications include:
*   User sentiment <a class="yt-timestamp" data-t="12:30:00">[12:30:00]</a>
*   Speech-to-text transcription accuracy <a class="yt-timestamp" data-t="12:31:00">[12:31:00]</a>, <a class="yt-timestamp" data-t="12:57:00">[12:57:00]</a>
*   Tone consistency throughout the conversation <a class="yt-timestamp" data-t="12:36:00">[12:36:00]</a>
*   Intent and speech quality from the audio piece <a class="yt-timestamp" data-t="12:41:00">[12:41:00]</a>, <a class="yt-timestamp" data-t="12:53:00">[12:53:00]</a>

It is crucial that audio chunks receive their own defined evaluations for aspects like intent, speech quality, and speech-to-text accuracy <a class="yt-timestamp" data-t="12:50:00">[12:50:00]</a>.