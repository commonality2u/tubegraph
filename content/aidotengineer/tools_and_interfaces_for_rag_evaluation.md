---
title: Tools and interfaces for RAG evaluation
videoId: 1cQlnfwmIdU
---

From: [[aidotengineer]] <br/> 

## Open RAG Eval Project

[[open_rag_eval_project_overview | Open RAG Eval]] is an open-source project designed for quick and scalable [[evaluation_and_improvement_of_rag_solutions | RAG evaluation]] <a class="yt-timestamp" data-t="00:00:06">[00:00:06]</a>. It addresses a major challenge in RAG evaluation: the typical requirement for golden answers or chunks, which is not scalable <a class="yt-timestamp" data-t="00:00:22">[00:00:22]</a>. The project is research-backed, a result of [[university_collaboration_in_rag_research | collaboration with the University of Waterloo]]'s Jimmy Lynn lab <a class="yt-timestamp" data-t="00:00:32">[00:00:32]</a>.

### Architecture and Workflow

The [[open_rag_eval_project_overview | Open RAG Eval]] process involves several steps:
1.  **Query Collection** A set of queries (e.g., 10 to 1,000) that are important for the RAG system are collected <a class="yt-timestamp" data-t="00:00:47">[00:00:47]</a>.
2.  **RAG Connector** A [[components_of_rag_stack | RAG connector]] collects the actual information, chunks, and answers generated by a RAG pipeline <a class="yt-timestamp" data-t="00:00:54">[00:00:54]</a>. Connectors are available for Vectara, LangChain, and LlamaIndex, with more being developed <a class="yt-timestamp" data-t="00:01:04">[00:01:04]</a>. The project encourages contributions for additional [[components_of_rag_stack | RAG connectors]] <a class="yt-timestamp" data-t="00:04:46">[00:04:46]</a>.
3.  **Output Generation** These connectors generate the RAG outputs, including retrieved chunks and generated answers <a class="yt-timestamp" data-t="00:01:09">[00:01:09]</a>.
4.  **Evaluation Execution** The system runs a series of [[metrics_for_rag_evaluation | metrics]] grouped into evaluators <a class="yt-timestamp" data-t="00:01:16">[00:01:16]</a>.
5.  **File Generation** The evaluators generate RAG evaluation files containing all necessary information for pipeline assessment <a class="yt-timestamp" data-t="00:01:24">[00:01:24]</a>.

### Key Metrics for [[rag_evaluation_without_golden_answers | RAG Evaluation Without Golden Answers]]

[[open_rag_eval_project_overview | Open RAG Eval]] employs several [[metrics_for_rag_evaluation | metrics]] that do not require golden answers or chunks, making the [[evaluation_and_improvement_of_rag_solutions | evaluation]] process highly scalable <a class="yt-timestamp" data-t="00:01:36">[00:01:36]</a>:

*   **Umbrella (Retrieval Metric)**: This metric evaluates retrieval without golden chunks <a class="yt-timestamp" data-t="00:01:44">[00:01:44]</a>. It assigns a score from zero to three to a chunk, indicating its relevance to the query:
    *   **Zero**: Chunk has nothing to do with the query <a class="yt-timestamp" data-t="00:02:16">[00:02:16]</a>.
    *   **Three**: Chunk is dedicated to the query and contains the exact answer <a class="yt-timestamp" data-t="00:02:20">[00:02:20]</a>.
    Research by the University of Waterloo indicates that this approach correlates well with human judgment <a class="yt-timestamp" data-t="00:02:32">[00:02:32]</a>.

*   **Autonuggetizer (Generation Metric)**: This metric evaluates generation without golden answers <a class="yt-timestamp" data-t="00:02:51">[00:02:51]</a>. It involves three steps:
    1.  **Nugget Creation**: Atomic units called "nuggets" are created <a class="yt-timestamp" data-t="00:03:00">[00:03:00]</a>.
    2.  **Nugget Rating**: Each nugget is assigned a "vital" or "okay" rating, and the top 20 are sorted <a class="yt-timestamp" data-t="00:03:07">[00:03:07]</a>.
    3.  **LLM Judgment**: An LLM judge analyzes the RAG response to determine if each selected nugget is fully or partially supported by the answer <a class="yt-timestamp" data-t="00:03:15">[00:03:15]</a>.

*   **Citation Faithfulness**: This metric measures whether citations in the RAG response are accurate and high-fidelity, determining if they are fully supported, partially supported, or not supported by the underlying passages <a class="yt-timestamp" data-t="00:03:30">[00:03:30]</a>.

*   **Hallucination Detection**: This feature uses Vectara's Hallucination Detection Model (HHM) to check if the entire RAG response aligns with the retrieved content <a class="yt-timestamp" data-t="00:03:45">[00:03:45]</a>.

### User Interface

[[open_rag_eval_project_overview | Open RAG Eval]] provides a user-friendly interface to analyze evaluation results <a class="yt-timestamp" data-t="00:03:58">[00:03:58]</a>. Users can drag and drop their evaluation files onto `open-evaluation.ai` to visualize the data <a class="yt-timestamp" data-t="00:04:06">[00:04:06]</a>. The UI displays:
*   All queries that were run <a class="yt-timestamp" data-t="00:04:16">[00:04:16]</a>.
*   Comparison of retrieval scores <a class="yt-timestamp" data-t="00:04:19">[00:04:19]</a>.
*   Different generation scores <a class="yt-timestamp" data-t="00:04:21">[00:04:21]</a>.

This package offers transparency into how metrics work and can help optimize and tune RAG pipelines <a class="yt-timestamp" data-t="00:04:31">[00:04:31]</a>.