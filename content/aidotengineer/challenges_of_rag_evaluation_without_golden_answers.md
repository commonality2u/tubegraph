---
title: Challenges of RAG evaluation without golden answers
videoId: 1cQlnfwmIdU
---

From: [[aidotengineer]] <br/> 

Evaluating Retrieval-Augmented Generation (RAG) systems traditionally faces a significant hurdle: the need for "golden answers" or "golden chunks" to measure performance <a class="yt-timestamp" data-t="00:00:22">[00:00:22]</a>. This requirement makes RAG evaluation non-scalable <a class="yt-timestamp" data-t="00:00:29">[00:00:29]</a>.

## OpenRAG Eval: A Solution for Scalable RAG Evaluation

To address this challenge, OpenRAG Eval was developed as an open-source project aimed at quick and scalable RAG evaluation <a class="yt-timestamp" data-t="00:00:06">[00:00:06]</a>. It is research-backed, developed in collaboration with the University of Waterloo's Jimmy Lynn lab <a class="yt-timestamp" data-t="00:00:32">[00:00:32]</a>.

### How OpenRAG Eval Works

The evaluation process begins with a set of queries, ranging from tens to thousands, that are important for a specific RAG system <a class="yt-timestamp" data-t="00:00:47">[00:00:47]</a>.

1.  **RAG Connector**: A RAG connector collects the actual information, including chunks and answers generated by the RAG pipeline <a class="yt-timestamp" data-t="00:00:56">[00:00:56]</a>. Connectors are available for systems like Vectara, LangChain, and LlamaIndex, with more being added <a class="yt-timestamp" data-t="00:01:04">[00:01:04]</a>. These connectors generate the RAG outputs <a class="yt-timestamp" data-t="00:01:11">[00:01:11]</a>.
2.  **Evaluation Run**: The evaluation process then runs a series of [[metrics_for_evaluating_RAG_systems | metrics]], which are grouped into evaluators <a class="yt-timestamp" data-t="00:01:16">[00:01:16]</a>.
3.  **Output**: These evaluators generate RAG evaluation files, providing comprehensive data to assess the RAG pipeline <a class="yt-timestamp" data-t="00:01:24">[00:01:24]</a>.

### Key Metrics for Evaluation Without Golden Answers

OpenRAG Eval employs several [[metrics_for_evaluating_RAG_systems | metrics]] that circumvent the need for golden answers <a class="yt-timestamp" data-t="00:01:37">[00:01:37]</a>:

*   **Umbrella (Retrieval Metric)**:
    *   This metric allows for retrieval evaluation without relying on golden chunks <a class="yt-timestamp" data-t="00:01:44">[00:01:44]</a>.
    *   It assigns a score to each retrieved chunk or passage, ranging from zero (no relevance to the query) to three (dedicated to the query and contains the exact answer) <a class="yt-timestamp" data-t="00:02:13">[00:02:13]</a>.
    *   Research indicates that this approach correlates well with human judgment, ensuring reliable results even without golden chunks <a class="yt-timestamp" data-t="00:02:32">[00:02:32]</a>.

*   **Auto Nuggetizer (Generation Metric)**:
    *   This metric evaluates generation without requiring golden answers <a class="yt-timestamp" data-t="00:02:54">[00:02:54]</a>.
    *   It involves three steps:
        1.  **Nugget Creation**: Atomic units called "nuggets" are created <a class="yt-timestamp" data-t="00:03:00">[00:03:00]</a>.
        2.  **Nugget Rating and Sorting**: Each nugget is assigned a "vital" or "okay" rating, and the top 20 are selected <a class="yt-timestamp" data-t="00:03:07">[00:03:07]</a>.
        3.  **LLM Judge Analysis**: An LLM (Large Language Model) judge analyzes the RAG response to determine if each selected nugget is either fully or partially supported by the answer <a class="yt-timestamp" data-t="00:03:15">[00:03:15]</a>.

*   **Citation Faithfulness**:
    *   This metric assesses whether citations within the response are accurate and high-fidelity <a class="yt-timestamp" data-t="00:03:30">[00:03:30]</a>.
    *   It checks if the citation is fully supported, partially supported, or not supported by the response <a class="yt-timestamp" data-t="00:03:37">[00:03:37]</a>.

*   **Hallucination Detection**:
    *   Utilizing Victara's Hallucination Detection Model (HHM), this metric checks if the entire RAG response aligns with the retrieved content, identifying instances of [[Challenges in RAG evaluation | hallucination]] <a class="yt-timestamp" data-t="00:03:45">[00:03:45]</a>.

### User Interface and Benefits

OpenRAG Eval includes a user-friendly interface <a class="yt-timestamp" data-t="00:03:58">[00:03:58]</a>. After running an evaluation, users can drag and drop the generated files onto `open-evaluation.ai` to view a UI that displays queries, retrieval scores, and different generation scores <a class="yt-timestamp" data-t="00:04:08">[00:04:08]</a>.

This open-source package is powerful for optimizing and tuning RAG pipelines <a class="yt-timestamp" data-t="00:04:29">[00:04:29]</a>. Its open source nature ensures transparency in how the [[metrics_for_evaluating_RAG_systems | metrics]] work <a class="yt-timestamp" data-t="00:04:34">[00:04:34]</a>. The project encourages contributions, especially for new RAG pipeline connectors <a class="yt-timestamp" data-t="00:04:50">[00:04:50]</a>.