---
title: Misuse of data and AI evaluation metrics
videoId: 89aQ7T6cMwA
---

From: [[aidotengineer]] <br/> 

Achieving complete and spectacular failure in an [[common_pitfalls_in_ai_strategy | AI strategy]] involves specific approaches to data handling and [[testing_and_evaluation_of_ai_models | AI evaluation]]. These methods prioritize chaos and ensure projects are torpedoed, alienating everyone involved <a class="yt-timestamp" data-t="00:00:57">[00:00:57]</a>.

## Embracing "Worse Practices" in AI Evaluation

Instead of following [[best_practices_for_ai_evaluation | best practices]], the focus should be on embracing "worse practices" to guarantee project failure <a class="yt-timestamp" data-t="00:00:54">[00:00:54]</a>.

### Strategic Neglect of Evaluation

When measuring progress in AI, it is advised to use every generic, off-the-shelf [[importance_of_evaluation_and_metrics_in_ai_systems | evaluation metric]] available <a class="yt-timestamp" data-t="00:11:01">[00:11:01]</a>. It is crucial to never bother customizing these metrics to business needs <a class="yt-timestamp" data-t="00:11:05">[00:11:05]</a>. Instead, blindly trust the numbers, even if they make no sense <a class="yt-timestamp" data-t="00:11:07">[00:11:07]</a>. If AI agents are not working, the solution should be to pick a new framework and vendor, then finetune without any measurement or [[evaluation and feedback in AI systems | evaluation]] <a class="yt-timestamp" data-t="00:11:15">[00:11:15]</a>.

It is recommended to adopt a mindset that [[testing_and_evaluation_of_ai_models | evaluations]] are solely a vendor's problem, assuming a one-size-fits-all solution exists <a class="yt-timestamp" data-t="00:12:09">[00:12:09]</a>.

### Dashboard of Confusion

To ensure minimal understanding, a dashboard should be created displaying every off-the-shelf metric gatherable <a class="yt-timestamp" data-t="00:12:26">[00:12:26]</a>. The more metrics, the better, regardless of whether they track real outcomes or failure modes <a class="yt-timestamp" data-t="00:12:35">[00:12:35]</a>. The numbers should be unintelligible to prevent understanding the difference between performance levels <a class="yt-timestamp" data-t="00:12:39">[00:12:39]</a>. The goal is to keep hoarding random metrics until one shows an upward trend, then claim success <a class="yt-timestamp" data-t="00:12:47">[00:12:47]</a>.

It is advised to adopt all metrics from [[improving_ai_evaluation_methods | evaluation frameworks]] and let them guide blindly, never questioning if they measure actual success <a class="yt-timestamp" data-t="00:13:04">[00:13:04]</a>. Optimizing for metrics like cosine similarity, BLEU, and ROUGE is preferred, completely ignoring actual user experience <a class="yt-timestamp" data-t="00:13:17">[00:13:17]</a>. Cross-checking with domain experts or users should be avoided, as an AI's accuracy claim should be accepted without argument <a class="yt-timestamp" data-t="00:13:24">[00:13:24]</a>. These practices contribute to [[challenges_and_trust_issues_with_ai_benchmarks | challenges and trust issues with AI benchmarks]].

## Avoiding Data at All Costs

A potent technique to achieve dysfunction is to actively avoid looking at data <a class="yt-timestamp" data-t="00:13:42">[00:13:42]</a>.

### Blind Trust in AI Output

It is asserted that one can absolutely trust an AI's output without ever looking at it oneself <a class="yt-timestamp" data-t="00:13:58">[00:13:58]</a>. Looking at data is deemed an "engineering problem," beneath the concern of leaders who have more important strategic tasks like attending meetings about meetings <a class="yt-timestamp" data-t="00:14:03">[00:14:03]</a>. Developers are considered to have more domain expertise than business teams <a class="yt-timestamp" data-t="00:14:18">[00:14:18]</a>.

### Customer as QA

Customers are considered the best Quality Assurance (QA) <a class="yt-timestamp" data-t="00:14:23">[00:14:23]</a>. It is important to trust one's gut feelings, as they are a reliable substitute for data, especially when making million-dollar decisions <a class="yt-timestamp" data-t="00:14:34">[00:14:34]</a>.

### Data Inaccessibility

Engineers are viewed as coding wizards who will handle everything, regardless of their lack of customer interaction <a class="yt-timestamp" data-t="00:14:54">[00:14:54]</a>. Simpler data annotation options like spreadsheets should be quickly forgotten <a class="yt-timestamp" data-t="00:15:05">[00:15:05]</a>.

It is crucial to ensure that no one else is looking at data <a class="yt-timestamp" data-t="00:15:20">[00:15:20]</a>. The best way to achieve this is by storing data in complex systems accessible only to engineers, thereby making it unavailable to domain experts <a class="yt-timestamp" data-t="00:15:24">[00:15:24]</a>. Executives should insist on purchasing custom data analysis platforms that require a team of PhDs to operate and understand <a class="yt-timestamp" data-t="00:15:37">[00:15:37]</a>. Bonus points are awarded if it takes six months to load and produces incessant errors <a class="yt-timestamp" data-t="00:15:47">[00:15:47]</a>. These actions contribute to the [[mismanagement_of_ai_resources | mismanagement of AI resources]].