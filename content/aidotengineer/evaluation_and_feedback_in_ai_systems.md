---
title: Evaluation and feedback in AI systems
videoId: 9mzfioh1Zag
---

From: [[aidotengineer]] <br/> 

[[importance_of_evaluation_and_metrics_in_ai_systems | Evaluation and feedback]] are critical components when developing complex AI systems, especially those with multiple moving parts, like multi-agent architectures [03:42]. For Cat.io's AI copilot for cloud architecture, addressing the challenge of how to [[testing_and_evaluation_of_ai_models | evaluate]] and provide feedback to their large AI system was paramount [03:42].

## Challenges in AI Evaluation
A significant [[challenges_in_ai_agent_evaluation | challenge in AI agent evaluation]] is determining if a recommendation generated by the system is truly good [19:03]. For a multi-agent system with numerous engines and rounds of conversations, monitoring what works best requires careful consideration [19:13].

## Best Practices for AI Evaluation
To effectively [[best_practices_for_ai_evaluation | evaluate AI systems]] and facilitate [[continuous_improvement_in_ai_systems | continuous improvement in AI systems]], Cat.io found it essential to:

*   **Close the loop with human scoring** and structured feedback, incorporating revision cycles [19:26].
*   **Prioritize human evaluation**, especially in early stages [19:34]. While LLM evaluations are useful, they often don't provide the specific insights needed for targeted improvements [19:42].
*   **Bake evaluation into the system design** from the very beginning, rather than adding it as an afterthought [20:56]. This proactive approach ensures that evaluation mechanisms are considered as soon as a new AI system is designed [21:10].

## Implementation of Evaluation Platforms for AI Agents
Cat.io developed an internal human evaluation tool called "Eagle Eye" to facilitate the [[implementation_of_evaluation_platforms_for_ai_agents | implementation of evaluation platforms for AI agents]] [19:55]. This tool allows evaluators to [19:59]:
*   Examine specific cases, including the architecture and extracted requirements.
*   Review conversations between agents.
*   Assess generated recommendations.

Evaluators use "Eagle Eye" to perform relevance, visibility, and clarity studies, assigning scores that guide future development priorities [20:22]. The tool provides a detailed view of interactions within the multi-agent system, allowing users to read conversations and determine if they make sense [21:43].

## Key Learnings in Evaluation
Through their development process, Cat.io identified several crucial lessons regarding [[improving_ai_evaluation_methods | improving AI evaluation methods]]:

*   **Confidence is not correctness**: An AI system's confidence in its output does not always equate to accuracy or correctness [20:38].
*   **Human feedback is essential early on**: When building AI systems from scratch, [[user_feedback_and_ai_development | human feedback]] is vital for initial development and refinement [20:52]. This underscores the [[role_of_evaluators_in_ai | role of evaluators in AI]] development.
*   **Evaluation must be integrated**: Evaluation tools, whether human-driven, monitoring dashboards, or LLM-based feedback loops, should be an integral part of the system's design from the outset [20:56].

> [!EXAMPLE] Hallucination Detection
> The "Eagle Eye" tool helped identify early cases of hallucination, such as a staff architect (network security) attempting to schedule a workshop with specific dates when interacting with a requirements retriever [22:05]. Detecting and handling such instances is critical for system reliability [22:31].