---
title: Future directions in AI model training and scaling
videoId: b0xlsQ_6wUQ
---

From: [[aidotengineer]] <br/> 

## Vision for AI Development

The long-term vision for AI development at Quen involves building a generalist model and a generalist agent <a class="yt-timestamp" data-t="00:00:28">[00:00:28]</a>. This extends beyond merely creating instruction-tuned models, aiming for AI that can become progressively smarter through advanced techniques like reinforcement learning <a class="yt-timestamp" data-t="00:02:08">[00:02:08]</a>.

### Reinforcement Learning (RL) for Enhanced Performance

Reinforcement learning (RL) has shown significant promise in increasing model performance, particularly in reasoning tasks such as mathematics and coding <a class="yt-timestamp" data-t="00:02:26">[00:02:26]</a>. This approach can lead to consistent performance increases, turning models into more capable reasoning agents <a class="yt-timestamp" data-t="00:02:36">[00:02:36]</a>.

### Evolution of Model Architectures

The future trend in AI models is believed to belong to Mixture-of-Experts (MOE) models <a class="yt-timestamp" data-t="00:12:18">[00:12:18]</a>. These models are designed to be efficient while remaining highly effective.

## Key Areas for Future Development

### Enhancing Training Methodologies

Significant [[ai_models_and_training_methods | improvements in training]] are still possible <a class="yt-timestamp" data-t="00:21:17">[00:21:17]</a>. Areas of focus include:
*   **Data Quality and Inclusion**: Incorporating more high-quality data that has not yet been utilized or thoroughly cleaned <a class="yt-timestamp" data-t="00:21:28">[00:21:28]</a>.
*   **Multimodal Data**: Utilizing multimodal data to enhance model capabilities across different tasks and domains <a class="yt-timestamp" data-t="00:21:37">[00:21:37]</a>.
*   **Synthetic Data**: Exploring the use of synthetic data in training <a class="yt-timestamp" data-t="00:21:46">[00:21:46]</a>.
*   **Novel Training Methods**: Moving beyond traditional next-token prediction to explore different [[ai_models_and_training_methods | training methods]] for pre-training, potentially including reinforcement learning in this initial phase <a class="yt-timestamp" data-t="00:21:52">[00:21:52]</a>.

### [[scaling_generative_ai_workloads | Scaling]] Laws and Compute

The landscape of [[scaling_generative_ai_workloads | scaling]] laws is evolving <a class="yt-timestamp" data-t="00:22:12">[00:22:12]</a>. While past [[scaling_generative_ai_workloads | scaling]] efforts focused on model sizes and pre-training data, the current emphasis is on [[scaling_generative_ai_workloads | compute in reinforcement learning]] <a class="yt-timestamp" data-t="00:22:23">[00:22:23]</a>. The goal is to develop models capable of long-horizon reasoning with continuous environment feedback, enabling them to become smarter through inference-time [[scaling_generative_ai_workloads | scaling]] <a class="yt-timestamp" data-t="00:22:28">[00:22:28]</a>.

### Context Length [[scaling_generative_ai_workloads | Scaling]]

A key focus is to significantly [[scaling_generative_ai_workloads | scale context]] length <a class="yt-timestamp" data-t="00:23:02">[00:23:02]</a>. Current efforts aim to resolve issues with 1 million tokens and then progress towards 10 million tokens, with an ultimate goal of achieving infinite context <a class="yt-timestamp" data-t="00:23:07">[00:23:07]</a>. Most models are expected to support at least 1 million tokens within the current year <a class="yt-timestamp" data-t="00:23:19">[00:23:19]</a>.

### Modality [[scaling_generative_ai_workloads | Scaling]]

While [[scaling_generative_ai_workloads | scaling]] on modalities may not directly increase intelligence, it significantly enhances a model's capabilities and productivity <a class="yt-timestamp" data-t="00:23:30">[00:23:30]</a>. This includes:
*   **Multimodal Input and Output**: The aim is to build "omni models" that can accept multiple modalities as inputs (text, vision, audio) and generate multiple modalities as outputs (text, audio, with future aims for high-quality images and videos) <a class="yt-timestamp" data-t="00:13:51">[00:13:51]</a>, <a class="yt-timestamp" data-t="00:14:33">[00:14:33]</a>.
*   **GUI Agents**: Vision language understanding is crucial for developing GUI agents and enabling computer use tasks <a class="yt-timestamp" data-t="00:23:43">[00:23:43]</a>.
*   **Unified Understanding and Generation**: Work is being done to unify understanding and generation, for example, enabling image understanding and generation within the same model <a class="yt-timestamp" data-t="00:24:07">[00:24:07]</a>.

## Transition to Agent Training

The overall shift in focus is from simply [[ai_models_and_training_methods | training models]] to [[ai_models_and_training_methods | training agents]] <a class="yt-timestamp" data-t="00:24:36">[00:24:36]</a>. This involves [[scaling_generative_ai_workloads | scaling]] not only with pre-training but also with reinforcement learning, particularly through interaction with environments <a class="yt-timestamp" data-t="00:24:40">[00:24:40]</a>. This marks the current era as one focused on agents <a class="yt-timestamp" data-t="00:24:56">[00:24:56]</a>.