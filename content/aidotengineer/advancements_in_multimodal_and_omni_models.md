---
title: Advancements in multimodal and omni models
videoId: b0xlsQ_6wUQ
---

From: [[aidotengineer]] <br/> 

Quen aims to build a generalist model and agent, encompassing both large language models and large [[Multimodal AI Systems | multimodal models]] <a class="yt-timestamp" data-t="00:00:21">[00:00:21]</a>.

## Quen Chat Interface

Quen Chat, accessible at chat.quen.ai, allows users to interact with the latest models, including [[Multimodal interfaces for user interaction | multimodal models]] by uploading images and videos <a class="yt-timestamp" data-t="00:00:44">[00:00:44]</a>. It also enables interaction with Quen's [[Multimodal interfaces for user interaction | omni models]] through voice chat and video chat <a class="yt-timestamp" data-t="00:00:50">[00:00:50]</a>.

## Quen 2.5 VL (Vision Language)

Released in January, Quen 2.5 VL is a vision language model that has demonstrated competitive performance in various vision language benchmarks <a class="yt-timestamp" data-t="01:49:00">[01:49:00]</a>. These benchmarks include understanding tasks like MMU, math benchmarks like MathVista, and general VQA benchmarks <a class="yt-timestamp" data-t="01:57:00">[01:57:00]</a>.

The development team has also explored the integration of thinking capabilities into vision language models, creating QVQ <a class="yt-timestamp" data-t="01:18:00">[01:18:00]</a>. Similar to language models, Quen 2.5 VL shows improved performance in reasoning tasks, particularly mathematics, with a larger thinking budget (maximum thinking tokens) <a class="yt-timestamp" data-t="01:32:00">[01:32:00]</a>.

## Quen Omni Model

Quen's ultimate goal for [[Multimodal AI Systems | multimodal models]] is to build an [[Multimodal interfaces for user interaction | omni model]] <a class="yt-timestamp" data-t="01:51:00">[01:51:00]</a>. An [[Multimodal interfaces for user interaction | omni model]] is designed to:
*   Accept multiple modalities as inputs <a class="yt-timestamp" data-t="01:53:00">[01:53:00]</a>.
*   Generate multiple modalities as outputs (e.g., text, vision, audio) <a class="yt-timestamp" data-t="01:56:00">[01:56:00]</a>.

### Current Capabilities of the Omni Model
The current iteration is a relatively small, 7 billion-parameter model <a class="yt-timestamp" data-t="01:11:00">[01:11:00]</a> <a class="yt-timestamp" data-t="01:13:00">[01:13:00]</a>. It is capable of:
*   **Accepting Inputs**: Text, vision (images and videos), and audio <a class="yt-timestamp" data-t="01:18:00">[01:18:00]</a>.
*   **Generating Outputs**: Text and audio <a class="yt-timestamp" data-t="01:28:00">[01:28:00]</a>.
*   **Usage**: Can be used in voice chat, video chat, and text chat <a class="yt-timestamp" data-t="01:44:00">[01:44:00]</a>.
*   **Performance**: Achieves state-of-the-art performance in audio tasks for its size <a class="yt-timestamp" data-t="01:49:00">[01:49:00]</a>. Surprisingly, it also outperforms Quen 2.5 VL 7 billion in vision language understanding tasks <a class="yt-timestamp" data-t="01:57:00">[01:57:00]</a>.

### Future Goals for the Omni Model
Future developments aim to enable the model to generate high-quality images and videos, making it a truly [[Multimodal interfaces for user interaction | omni model]] <a class="yt-timestamp" data-t="01:33:00">[01:33:00]</a>. The team is also working on recovering performance drops observed in language and agent tasks, which they believe can be addressed by improving data quality and [[AI models and training methods | training methods]] <a class="yt-timestamp" data-t="01:57:00">[01:57:00]</a>.

## Future Directions: Scaling Modalities and Unifying Understanding

Quen's future plans include scaling modalities <a class="yt-timestamp" data-t="02:25:00">[02:25:00]</a>. While scaling modalities may not directly increase intelligence, it significantly enhances the models' capabilities and productivity <a class="yt-timestamp" data-t="02:30:00">[02:30:00]</a>. For instance, vision language understanding is crucial for creating GUI agents and enabling computer use tasks <a class="yt-timestamp" data-t="02:38:00">[02:38:00]</a>.

The goal is to unify understanding and generation across modalities, such as simultaneous image understanding and generation, similar to capabilities seen in GPT-4 <a class="yt-timestamp" data-t="02:40:00">[02:40:00]</a>. This means continuing to integrate [[Multimodal AI Systems | multimodal data]] into training to enhance model capabilities across different tasks and domains <a class="yt-timestamp" data-t="02:37:00">[02:37:00]</a>.