---
title: AI generated assets and user experiences
videoId: nnktgWtfJHE
---

From: [[aidotengineer]] <br/> 
The integration of [[user_experience_design_with_ai | AI-generated assets]] is significantly transforming [[user_experience_design_with_ai | user experiences]] in creative tools, particularly by lowering barriers to entry for complex software [00:07:51]. This shift allows users to focus on creative intent rather than technical mastery of the underlying tools [00:12:52].

## Blender [[agentic_architectures_for_generative_ai | MCP]]: A Case Study in Enhancing User Experience <a class="yt-timestamp" data-t="00:01:59">[00:01:59]</a>

Blender, a generalist 3D tool for tasks like importing, animating, and exporting assets for game engines, has a historically complex user interface (UI) [00:00:40]. Its UI features numerous tabs and sub-tabs, making it challenging for new users to learn [00:00:57]. For example, a classic beginner's course to build a donut in Blender traditionally takes 5 hours [00:01:40].

The Blender [[agentic_architectures_for_generative_ai | MCP]] (Multi-Modal Control Protocol) was developed to address this complexity [00:01:59]. The core idea is to enable a Large Language Model ([[ai_agents_using_humanlike_interfaces_and_workflows | LLM]]) like Claude or ChatGPT to control Blender, allowing users to create 3D scenes simply by providing prompts [00:02:02].

### How Blender [[agentic_architectures_for_generative_ai | MCP]] Works <a class="yt-timestamp" data-t="00:03:27">[00:03:27]</a>
The client (e.g., Claude, Cursor) connects to Blender via the [[agentic_architectures_for_generative_ai | MCP]] protocol [00:03:35]. This protocol allows Blender to expose its capabilities (tools) to the [[ai_agents_using_humanlike_interfaces_and_workflows | LLM]] [00:03:48]. An add-on within Blender executes the scripts generated by the [[ai_agents_using_humanlike_interfaces_and_workflows | LLM]] [00:04:07].

A crucial component is the integration of [[user_experience_design_with_ai | AI-generated assets]]. Industry-standard platforms like Rodin (an [[user_experience_design_with_ai | AI-generated asset]] platform), Sketchfab, and Polyhaven are connected to the [[ai_agents_using_humanlike_interfaces_and_workflows | LLM]] [00:04:22]. This seamless connection allows users to generate and import assets directly into Blender through natural language prompts [00:04:34]. Blender's scripting capabilities and flexibility in downloading and importing assets are key enablers for this system [00:04:48].

### Impact on Creative Workflow <a class="yt-timestamp" data-t="00:07:49">[00:07:49]</a>
The Blender [[agentic_architectures_for_generative_ai | MCP]] has significantly reduced the time and effort required for 3D creation:
*   **Rapid Scene Creation**: A complex scene like a dragon guarding a pot of gold, which might take a user hours, can be generated by the system in approximately 5 minutes [00:02:57]. Similarly, a scene with [[user_experience_design_with_ai | AI-generated assets]] like magical mushrooms can be created in 2 minutes [00:08:06].
*   **On-the-Spot Asset Generation**: Users can describe desired assets (e.g., a "zombie") and the system can generate them via [[user_experience_design_with_ai | AI-generated assets]] on the spot [00:05:15].
*   **Simplified Complex Tasks**: Users can prompt the system to recreate scenes from reference images, generating and placing the correct [[user_experience_design_with_ai | AI-generated assets]] [00:08:47]. Tasks like generating terrain with complex textures and normal bumps, which involve a steep learning curve with Blender nodes, are automated [00:09:07].
*   **Game Development**: The [[agentic_architectures_for_generative_ai | MCP]] has been used to set scenes and create [[user_experience_design_with_ai | AI-generated assets]] for games, dramatically reducing development time [00:09:36].
*   **Filmmaking and Animation**: Users can prompt the [[agentic_architectures_for_generative_ai | MCP]] to create racing tracks, animate cars, and set camera angles for cinematic effects, which can then be converted into video clips using other tools like Runway [00:10:38].
*   **Donut Example**: The classic 5-hour donut tutorial can now be completed with one short prompt in about a minute [00:11:31].

### Learnings from Development <a class="yt-timestamp" data-t="00:05:52">[00:05:52]</a>
Key insights from building Blender [[agentic_architectures_for_generative_ai | MCP]] include:
*   **Scripting Power**: Tools with scripting capabilities enable [[ai_agents_using_humanlike_interfaces_and_workflows | LLMs]] to perform heavy lifting, translating prompts into executable code for modeling and asset retrieval [00:05:56].
*   **Tool Clarity**: [[agentic_architectures_for_generative_ai | MCP]]s can get confused if too many tools are available or if they are not distinct enough [00:06:14]. Keeping the [[ai_trust_gap_and_user_experience | user experience]] lean and ensuring each tool is unique helps the [[ai_agents_using_humanlike_interfaces_and_workflows | LLM]] make accurate selections [00:06:42].
*   **Avoid Feature Bloat**: Maintaining a lean, generalist approach for the [[agentic_architectures_for_generative_ai | MCP]] ensures effectiveness rather than adding excessive features [00:06:58].
*   **Model Improvement**: Underlying [[ai_agents_using_humanlike_interfaces_and_workflows | LLMs]] are continuously improving their understanding of 3D concepts, leading to better results over time [00:07:17].

## The Future of Creative Tools and User Experience <a class="yt-timestamp" data-t="00:12:05">[00:12:05]</a>
[[agentic_architectures_for_generative_ai | MCP]]s are fundamentally changing how creative tools operate by capturing a user's intent and transforming it into tangible output without requiring deep knowledge of complex software [00:12:52].

The client [[ai_agents_using_humanlike_interfaces_for_workflows | LLM]] acts as an orchestrator, connecting to various APIs and local tools [00:12:16]. For instance, a user's prompt to "make a game" could lead the [[ai_agents_using_humanlike_interfaces_for_workflows | LLM]] to:
*   Call Blender to create game assets [00:13:25].
*   Call Unity to build the game engine, add collisions, and logic [00:13:28].
*   Call APIs for additional [[user_experience_design_with_ai | AI-generated assets]] and animations [00:13:38].
*   Call Ableton (a music creation software) to generate a soundtrack [00:13:44].

This paradigm shift means users no longer need to worry about the underlying tools [00:12:58]. The [[ai_agents_using_humanlike_interfaces_for_workflows | LLM]] at the center of this orchestration chooses the right tools and stitches everything together [00:13:06]. This process can be demonstrated by generating a dragon in Blender with sinister lighting, accompanied by a soundtrack generated by Ableton [[agentic_architectures_for_generative_ai | MCP]] [00:14:13].

This development raises questions about the future of creativity:
*   Will tools primarily communicate with each other, with users interacting solely with [[ai_agents_using_humanlike_interfaces_for_workflows | LLMs]]? [00:15:20]
*   Will creatives evolve into "orchestra conductors," focusing on conceptualizing their vision and prompting the [[ai_agents_using_humanlike_interfaces_for_workflows | LLM]] to execute it, rather than mastering individual instruments? [00:15:43]

The emergence of [[agentic_architectures_for_generative_ai | MCP]]s for various creative tools (e.g., PostGis, Houdini, Unity, Unreal Engine) suggests a future where the barrier to creation is significantly lowered, potentially enabling anyone to become a creator [00:16:16].