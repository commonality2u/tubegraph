---
title: Importance of evaluations in AI projects
videoId: wHhlvcQgi9M
---

From: [[aidotengineer]] <br/> 

Evaluations are considered the most significant challenge and the "missing piece" when attempting to scale generative AI (GenAI) workloads <a class="yt-timestamp" data-t="00:01:11">[00:01:11]</a>. Justin Mohler, a principal applied AI architect at AWS, highlights that a lack of evaluations is the primary reason why many GenAI projects fail to scale <a class="yt-timestamp" data-t="00:01:45">[00:01:45]</a>. When implemented, evaluations can unlock the ability to scale these applications <a class="yt-timestamp" data-t="00:02:07">[00:02:07]</a>.

## Why Evaluations are Crucial

A primary objective of any [[evaluating_ai_system_performance | evaluation framework]] should be to identify problems <a class="yt-timestamp" data-t="00:04:43">[00:04:43]</a>. While [[evaluations_versus_traditional_metrics_in_ai | evaluations]] do produce scores, this is a lesser goal compared to uncovering issues and even suggesting solutions, especially when generative AI reasoning is incorporated <a class="yt-timestamp" data-t="00:04:50">[00:04:50]</a>. Designing an [[evaluating_ai_system_performance | evaluation framework]] with a "find errors" mindset leads to a more effective design than merely aiming to measure performance <a class="yt-timestamp" data-t="00:05:08">[00:05:08]</a>.

Mohler's team at AWS uses [[evaluating_ai_system_performance | evaluations]] as a filter to distinguish between "science projects" (those unlikely to scale) and successful projects <a class="yt-timestamp" data-t="00:05:42">[00:05:42]</a>. Teams unwilling to invest time in building a "gold standard set for evaluations" often indicates a project that will not achieve scale <a class="yt-timestamp" data-t="00:05:57">[00:05:57]</a>. Conversely, wildly successful projects demonstrate a strong willingness to invest significant time in building [[evaluating_ai_system_performance | evaluation frameworks]] <a class="yt-timestamp" data-t="00:06:32">[00:06:32]</a>.

### Case Study: Document Processing Workload
In one instance, a customer's document processing workload, after 6-12 months of development by six to eight engineers, had an accuracy of only 22% and faced potential cancellation <a class="yt-timestamp" data-t="00:02:26">[00:02:26]</a>. The core issue was a complete lack of [[evaluating_ai_system_performance | evaluations]] <a class="yt-timestamp" data-t="00:03:07">[00:03:07]</a>. After designing and implementing an [[evaluating_ai_system_performance | evaluation framework]], it became clear where the problems lay, making fixes trivial <a class="yt-timestamp" data-t="00:03:22">[00:03:22]</a>. Within six months, the accuracy improved to 92%, leading to its launch as the largest document processing workload on AWS in North America at the time <a class="yt-timestamp" data-t="00:03:43">[00:03:43]</a>.

## Challenges and Mindset Shift in Generative AI Evaluations

Traditional AI/ML backgrounds often associate [[evaluating_ai_system_performance | evaluations]] with exact numerical scores <a class="yt-timestamp" data-t="00:07:00">[00:07:00]</a>. However, GenAI outputs are often free-text, which might seem daunting to quantify precisely <a class="yt-timestamp" data-t="00:07:02">[00:07:02]</a>. Humans have been grading free text for centuries (e.g., English essays), and GenAI can be evaluated similarly <a class="yt-timestamp" data-t="00:07:14">[00:07:14]</a>. The key is to go beyond just assigning a score and instead point out what went wrong and where improvements can be made <a class="yt-timestamp" data-t="00:08:14">[00:08:14]</a>.

### The Importance of Evaluating Reasoning
When [[evaluating_ai_system_performance | evaluating AI systems]], it's crucial to understand *how* the model arrived at its answer, not just the answer itself <a class="yt-timestamp" data-t="00:09:11">[00:09:11]</a>.

*   **2x4 Example**: Drilling a perfect hole through a 2x4 might seem successful, but if the methodology used was unsafe or erratic (e.g., using a chainsaw on a hand), the underlying system needs rethinking <a class="yt-timestamp" data-t="00:08:42">[00:08:42]</a>.
*   **Meteorology Example**: A model asked to summarize weather data might incorrectly report "sunny and bright" despite receiving data indicating rain <a class="yt-timestamp" data-t="00:10:05">[00:10:05]</a>. While the output is clearly wrong (a score of zero), asking the model to explain its reasoning (e.g., "it's important to mental health to be happy, so I decided not to talk about the rain") provides critical insight into the problem <a class="yt-timestamp" data-t="00:10:18">[00:10:18]</a>. Conversely, a correct output (e.g., "sunny" response to "sunny" data) can be misleading if the model's reasoning was flawed (e.g., "I just chose 'sunny' randomly") <a class="yt-timestamp" data-t="00:10:40">[00:10:40]</a>.

Understanding the model's reasoning is vital for effective troubleshooting and improvement, as simply evaluating the output can be insufficient <a class="yt-timestamp" data-t="00:10:29">[00:10:29]</a>.

## Prompt Decomposition and Segmentation

While not exclusive to [[evaluating_ai_system_performance | evaluations]], prompt decomposition is often performed in conjunction with them <a class="yt-timestamp" data-t="00:11:23">[00:11:23]</a>. It involves breaking a large, complex prompt into a series of smaller, chained prompts <a class="yt-timestamp" data-t="00:13:02">[00:13:02]</a>. This allows for attaching an [[evaluating_ai_system_performance | evaluation]] to each section, helping to pinpoint where errors occur and focus improvement efforts <a class="yt-timestamp" data-t="00:13:07">[00:13:07]</a>.

This approach also helps determine if GenAI is even the appropriate tool for a given segment of the prompt <a class="yt-timestamp" data-t="00:13:18">[00:13:18]</a>. For instance, in the weather company example, the model sometimes incorrectly interpreted numerical comparisons (e.g., "7 is less than 5") <a class="yt-timestamp" data-t="00:12:41">[00:12:41]</a>. By replacing the GenAI calculation for this specific step with a Python mathematical comparison, accuracy for that step reached 100% <a class="yt-timestamp" data-t="00:13:30">[00:13:30]</a>.

### Semantic Routing
A common pattern in scaled workloads is semantic routing, where an input query is first classified to determine the task type <a class="yt-timestamp" data-t="00:14:02">[00:14:02]</a>. Easy tasks might be routed to a smaller, faster model, while harder tasks go to a larger, more complex model <a class="yt-timestamp" data-t="00:14:11">[00:14:11]</a>. Attaching [[evaluating_ai_system_performance | evaluations]] to each step in this process allows for using the right model for the job and significantly increases accuracy by eliminating "dead space" or unnecessary tokens from prompts <a class="yt-timestamp" data-t="00:14:34">[00:14:34]</a>.

## Seven Habits of Highly Effective Generative AI Evaluations

Successful scaled GenAI workloads almost always incorporate these seven habits <a class="yt-timestamp" data-t="00:15:37">[00:15:37]</a>:

1.  **Fast**: [[evaluating_ai_system_performance | Evaluation frameworks]] should provide results quickly, ideally within 30 seconds <a class="yt-timestamp" data-t="00:15:50">[00:15:50]</a>. This enables rapid iteration (hundreds of changes and tests daily) crucial for innovation and accuracy improvement <a class="yt-timestamp" data-t="00:16:19">[00:16:19]</a>. Achieving this speed often involves using GenAI as a judge or Python for numeric outputs, leveraging parallel processing for test case generation and judging <a class="yt-timestamp" data-t="00:16:51">[00:16:51]</a>.
2.  **Quantifiable**: Effective [[evaluating_ai_system_performance | evaluation frameworks]] must produce numerical scores <a class="yt-timestamp" data-t="00:18:21">[00:18:21]</a>. While scores might have some "jitter," this can be mitigated by using a numerous set of test cases and averaging the results, similar to how grades are averaged in school <a class="yt-timestamp" data-t="00:18:54">[00:18:54]</a>.
3.  **Explainable**: Beyond just looking at outputs, it's vital to examine the model's reasoning process <a class="yt-timestamp" data-t="00:20:09">[00:20:09]</a>. This applies to both the generation by the model and the scoring by the judge <a class="yt-timestamp" data-t="00:20:19">[00:20:19]</a>. Designing a "judge prompt" with clear instructions and asking it to explain its reasoning helps in prompt engineering for the judge itself <a class="yt-timestamp" data-t="00:21:11">[00:21:11]</a>.
4.  **Segmented**: Nearly all scaled workloads involve multiple steps, not just a single prompt <a class="yt-timestamp" data-t="00:21:28">[00:21:28]</a>. Each step should be evaluated individually to identify precise error sources <a class="yt-timestamp" data-t="00:21:36">[00:21:36]</a>. This also helps in choosing the most appropriate model (e.g., a small, fast model like Nova Micro for simple tasks) for each segment, optimizing for cost and latency <a class="yt-timestamp" data-t="00:21:50">[00:21:50]</a>. This is part of [[strategies_for_ai_evaluation_and_troubleshooting | strategies for AI evaluation and troubleshooting]].
5.  **Diverse**: Test cases should cover all anticipated use cases, both in-scope and out-of-scope <a class="yt-timestamp" data-t="00:22:10">[00:22:10]</a>. A rule of thumb is around 100 test cases, with more for core use cases and fewer for edge cases <a class="yt-timestamp" data-t="00:22:18">[00:22:18]</a>. Building diverse test cases helps define the project's scope and ensures the model handles various queries appropriately <a class="yt-timestamp" data-t="00:19:32">[00:19:32]</a>. This is a key part of [[evaluating_ai_agents_methods_and_metrics | evaluating AI agents methods and metrics]].
6.  **Traditional**: While GenAI is powerful, it's important not to abandon traditional [[evaluations_versus_traditional_metrics_in_ai | AI evaluation techniques]] <a class="yt-timestamp" data-t="00:22:30">[00:22:30]</a>. For numeric outputs, direct numeric evaluations are superior <a class="yt-timestamp" data-t="00:22:52">[00:22:52]</a>. For RAG architectures, metrics like retrieval precision, recall, and F1 scores are still relevant <a class="yt-timestamp" data-t="00:23:00">[00:23:00]</a>. Measuring cost and latency also relies on traditional tools <a class="yt-timestamp" data-t="00:23:11">[00:23:11]</a>. These are crucial aspects of [[evaluating_ai_system_performance | evaluating AI system performance]].

## Building an Evaluation Framework

A robust [[evaluating_ai_system_performance | evaluation framework]] starts with a **gold standard set** <a class="yt-timestamp" data-t="00:23:31">[00:23:31]</a>. This set serves as the foundation for the entire system, so investing time in its quality is paramount <a class="yt-timestamp" data-t="00:23:52">[00:23:52]</a>. It's generally not recommended to use GenAI to create the gold standard set, as it risks building a system that replicates the GenAI's own errors <a class="yt-timestamp" data-t="00:24:00">[00:24:00]</a>. While GenAI can help create a "silver standard set," it must always be human-reviewed for accuracy <a class="yt-timestamp" data-t="00:24:16">[00:24:16]</a>. This is one of the key [[steps_to_create_effective_evaluations_for_ai_applications | steps to create effective evaluations for AI applications]].

### Evaluation Process
1.  **Input Generation**: An input from the gold standard set is fed into a prompt template and then an LLM to generate an output, which includes both the answer and its reasoning <a class="yt-timestamp" data-t="00:24:26">[00:24:26]</a>.
2.  **Judging**: The generated output is compared against the matching answer from the gold standard using a "judge prompt" <a class="yt-timestamp" data-t="00:24:41">[00:24:41]</a>. The judge generates a numerical score and, crucially, the reasoning behind that score <a class="yt-timestamp" data-t="00:24:45">[00:24:45]</a>.
3.  **Categorization and Summary**: Outputs are often categorized (categories typically included in the gold standard set) to allow for breaking down results and generating summaries of right and wrong answers for each category <a class="yt-timestamp" data-t="00:24:54">[00:24:54]</a>. This provides actionable insights for improvement, forming part of [[strategies_for_ai_evaluation_and_troubleshooting | strategies for AI evaluation and troubleshooting]] and [[evaluating_ai_systems_at_scale | evaluating AI systems at scale]].

[[Evaluations and finetuning in AI development | Evaluations and finetuning in AI development]] are deeply intertwined, with [[evaluating_ai_system_performance | evaluations]] providing the necessary feedback loop for iterative improvements. Understanding [[challenges_in_ai_agent_evaluation | challenges in AI Agent Evaluation]] is also crucial, as agents introduce additional layers of complexity requiring robust [[evaluating_ai_agents_methods_and_metrics | evaluation methods and metrics]].