---
title: Voice and multimodal AI agents
videoId: OC04sP_QgTI
---

From: [[aidotengineer]] <br/> 

While text-based agents and chatbots are common, the "cool next frontier" in AI is voice AI, which is already transforming call centers globally <a class="yt-timestamp" data-t="00:01:48">[00:01:48]</a>. Over one billion calls in call centers worldwide currently utilize voice APIs <a class="yt-timestamp" data-t="00:01:55">[00:01:55]</a>.

## Applications of Voice AI
Real-time voice APIs are enabling AI agents to revolutionize call center operations <a class="yt-timestamp" data-t="00:02:03">[00:02:03]</a>. A practical example of this is the Priceline Pennybot, a production-level travel agent that allows users to book an entire vacation hands-free, without needing to type any text <a class="yt-timestamp" data-t="00:02:13">[00:02:13]</a>.

## Multimodal Agents
Beyond text-based interactions, the focus is expanding to [[Multimodal AI and the Future of Human Interaction | multimodal agents]] <a class="yt-timestamp" data-t="00:02:26">[00:02:26]</a>. These agents combine various modalities, such as voice and text, requiring a more complex approach to evaluation <a class="yt-timestamp" data-t="00:02:29">[00:02:29]</a>.

## Evaluating Voice and Multimodal Applications
[[Future of voice applications in AI | Voice applications]] are considered among the most complex types of applications ever built <a class="yt-timestamp" data-t="00:11:54">[00:11:54]</a>. Evaluating them requires additional considerations beyond just text analysis <a class="yt-timestamp" data-t="00:11:59">[00:11:59]</a>.

Specific evaluation points for voice applications include:
*   **Audio Chunk Evaluation** It's crucial to evaluate not just the generated transcript but also the underlying audio chunks <a class="yt-timestamp" data-t="00:12:07">[00:12:07]</a>.
*   **User Sentiment** Assessing the user's sentiment from their voice <a class="yt-timestamp" data-t="00:12:30">[00:12:30]</a>.
*   **Speech-to-Text Accuracy** Checking the accuracy of the speech-to-text transcription <a class="yt-timestamp" data-t="00:12:34">[00:12:34]</a>.
*   **Tone Consistency** Ensuring the tone remains consistent throughout the conversation <a class="yt-timestamp" data-t="00:12:36">[00:12:36]</a>.
*   **Intent and Speech Quality** Defining evaluations (evals) specifically for intent, speech quality, and speech-to-text accuracy related to the audio chunks <a class="yt-timestamp" data-t="00:12:53">[00:12:53]</a>.

While evaluating [[Components of AI agents | components of AI agents]] like the router, skill, and memory is standard, voice and [[Multimodal AI and the Future of Human Interaction | multimodal agents]] necessitate these deeper, modality-specific evaluations to ensure real-world effectiveness <a class="yt-timestamp" data-t="00:02:39">[00:02:39]</a>.