---
title: Reinforcement learning in large language models
videoId: b0xlsQ_6wUQ
---

From: [[aidotengineer]] <br/> 

Reinforcement Learning (RL) plays a crucial role in advancing [[building_and_scaling_large_language_models | large language models]] (LLMs), enabling them to become smarter and more capable, especially as [[large_language_models_in_ai_agents | AI agents]] <a class="yt-timestamp" data-t="00:02:17">[00:02:17]</a>.

## Enhancing Model Performance with RL
Research into reinforcement learning has demonstrated its significant impact on model performance <a class="yt-timestamp" data-t="00:02:22">[00:02:22]</a>. Using RL can remarkably increase performance, particularly in reasoning tasks such as math and coding, showing consistent improvements <a class="yt-timestamp" data-t="00:02:26">[00:02:26]</a>.

For instance, a 32-billion parameter Qwen model saw its performance increase from approximately 65% to 80% on the AM 2024 benchmark when RL was applied <a class="yt-timestamp" data-t="00:02:40">[00:02:40]</a>. This also contributes to making models smarter through inference time scaling <a class="yt-timestamp" data-t="00:22:46">[00:22:46]</a>.

## RL in Specific Applications
RL is also utilized for fine-tuning models dedicated to specific applications:
*   **Deep Research Models** Deep research models are specifically fine-tuned using reinforcement learning to enhance their quality <a class="yt-timestamp" data-t="00:20:36">[00:20:36]</a>. However, implementing RL for this task is recognized as challenging <a class="yt-timestamp" data-t="00:20:45">[00:20:45]</a>.

## Future Trends and Challenges
The integration of reinforcement learning into [[current_and_future_trends_in_large_language_models | LLM development]] is expected to deepen.

Key future directions include:
*   **Pre-training Integration** There is potential to incorporate reinforcement learning into the pre-training phase of models, moving beyond traditional next-token prediction methods <a class="yt-timestamp" data-t="00:21:55">[00:21:55]</a>.
*   **Scaling Compute** Future scaling efforts will focus on compute resources dedicated to reinforcement learning <a class="yt-timestamp" data-t="00:22:25">[00:22:25]</a>.
*   **Long-Horizon Reasoning** A significant focus is on developing models capable of long-horizon reasoning with continuous environment feedback <a class="yt-timestamp" data-t="00:22:28">[00:22:28]</a>. This means training models that can interact with their environment, receive feedback, and continue to think and adapt, making them increasingly competitive and intelligent <a class="yt-timestamp" data-t="00:22:33">[00:22:33]</a>.

The era is shifting from merely training models to training [[large_language_models_in_ai_agents | agents]], with scaling advancements achieved not only through pre-training but also significantly through RL, particularly in interactions with the environment <a class="yt-timestamp" data-t="00:24:38">[00:24:38]</a>.