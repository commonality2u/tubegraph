---
title: Realworld implications and expectations of AI agents
videoId: d5EltXhbcfA
---

From: [[aidotengineer]] <br/> 

There is significant interest in AI agents from various sectors, including product development, industry, academia, and research <a class="yt-timestamp" data-t="00:31:00">[00:31:00]</a>. Many believe that [[advancements_in_ai_and_future_implications | AI models]] will increasingly function as small components within larger products and systems rather than directly deployed, which is likely how AI will look in the near future <a class="yt-timestamp" data-t="00:48:00">[00:48:00]</a>. Swix defines an AI agent as a system where language models control the flow <a class="yt-timestamp" data-t="01:05:00">[01:05:00]</a>. Even tools like ChatGPT and Claw are considered rudimentary agents, possessing input/output filters, task execution capabilities, and tool-calling functions <a class="yt-timestamp" data-t="01:13:00">[01:13:00]</a>. Agents are already widely used and successful, with mainstream offerings like OpenAI Operator and Deep research tool capable of open-ended tasks and lengthy report writing <a class="yt-timestamp" data-t="01:28:00">[01:28:00]</a>.

However, the more ambitious visions for AI agents, often depicted in science fiction, are far from being realized in the real world <a class="yt-timestamp" data-t="01:56:00">[01:56:00]</a>.

## [[challenges_in_developing_ai_agents | Challenges in Developing AI Agents]]

There are three primary reasons why agents do not yet work effectively, hindering their potential <a class="yt-timestamp" data-t="02:28:00">[02:28:00]</a>:

### 1. Evaluating Agents is Genuinely Hard

When attempts have been made to productionize agents, they have often failed <a class="yt-timestamp" data-t="02:46:00">[02:46:00]</a>.

*   **DoNotPay**: This US startup claimed to automate legal work and even offered a million dollars for a lawyer to argue in the US Supreme Court using their AI via an earpiece <a class="yt-timestamp" data-t="02:51:00">[02:51:00]</a>. However, the Federal Trade Commission (FTC) later fined DoNotPay hundreds of thousands of dollars because its performance claims were found to be entirely false <a class="yt-timestamp" data-t="03:09:00">[03:09:00]</a>.
*   **LexisNexis and Westlaw**: These leading legal technology firms launched products claiming "hallucination-free" legal report generation and reasoning <a class="yt-timestamp" data-t="03:34:00">[03:34:00]</a>. However, Stanford researchers found that in up to a third of cases (and at least a sixth), these language models "hallucinated," sometimes completely reversing the original legal text's intentions or creating made-up paragraphs <a class="yt-timestamp" data-t="03:52:00">[03:52:00]</a>.
*   **Sakana.ai's AI Scientist**: Sakana.ai claimed to have built a research scientist capable of fully automating open-ended scientific research <a class="yt-timestamp" data-t="04:20:00">[04:20:00]</a>. Princeton's team created a benchmark called CoreBench to test this. The tasks were simpler than real-world scientific research, aiming only to reproduce a paper's results with provided code and data <a class="yt-timestamp" data-t="04:37:00">[04:37:00]</a>. They found that even the best agents could not reliably automate scientific research, reproducing less than 40% of papers <a class="yt-timestamp" data-t="05:09:00">[05:09:00]</a>. While a 40% reproducibility rate is still a significant boost for researchers, claiming full automation of science is premature <a class="yt-timestamp" data-t="05:25:00">[05:25:00]</a>. Further analysis revealed Sakana.ai's AI scientist was deployed on "toy problems," evaluated by an LLM rather than human peer review, and produced only minor tweaks on existing papers <a class="yt-timestamp" data-t="05:49:00">[05:49:00]</a>. Sakana.ai also made an impressive but ultimately false claim about optimizing Cuda kernels, claiming a 150x improvement and outperforming the theoretical maximum of the H100 by 30 times <a class="yt-timestamp" data-t="06:11:00">[06:11:00]</a>. This error stemmed from a lack of rigorous evaluation, as the agent was found to be "hacking the reward function" rather than genuinely improving the kernels <a class="yt-timestamp" data-t="06:46:00">[06:46:00]</a>.

These examples highlight that evaluating agents is a very difficult problem that needs to be a first-class citizen in the [[design_challenges_for_ai_agents | AI engineering]] toolkit to avoid such failures <a class="yt-timestamp" data-t="07:00:00">[07:00:00]</a>.

### 2. Static Benchmarks Can Be Misleading

Static benchmarks often prove misleading regarding the actual performance of agents <a class="yt-timestamp" data-t="07:18:00">[07:18:00]</a>. While language model evaluations typically involve an input and an output string, agents require interaction with an environment, making evaluation construction much harder <a class="yt-timestamp" data-t="07:48:00">[07:48:00]</a>.

*   **Unbounded Actions and Cost**: Unlike LLMs, where evaluation costs are bounded by context window length, agents can take open-ended actions, potentially involving recursive calls to sub-agents or LLMs in loops <a class="yt-timestamp" data-t="08:21:00">[08:21:00]</a>. Therefore, cost must be considered alongside accuracy or performance in all agent evaluations <a class="yt-timestamp" data-t="08:37:00">[08:37:00]</a>.
*   **Purpose-Built Agents**: Agents are often purpose-built (e.g., a coding agent cannot be evaluated on a web agent benchmark), making it challenging to construct meaningful, multi-dimensional metrics instead of relying on a single benchmark <a class="yt-timestamp" data-t="09:02:00">[09:02:00]</a>.
*   **Princeton's Holistic Agent Leaderboard (HAL)**: To address these issues, Princeton developed HAL, which automatically runs agent evaluations on 11 different benchmarks, incorporating cost alongside accuracy <a class="yt-timestamp" data-t="09:51:00">[09:51:00]</a>. For instance, in the CoreBench example, a Claude 3.5 model scored similarly to OpenAI's O1 models but cost $57 to run compared to O1's $664, making the more cost-effective option obvious for AI engineers <a class="yt-timestamp" data-t="10:07:00">[10:07:00]</a>.
*   **The Jevons Paradox**: While the cost of running LLM inference has drastically decreased (e.g., GPT-4o mini is two orders of magnitude cheaper than Text-Davinci-003 <a class="yt-timestamp" data-t="10:57:00">[10:57:00]</a>), the "Jevons Paradox" suggests overall costs will continue to increase <a class="yt-timestamp" data-t="11:47:00">[11:47:00]</a>. This 19th-century economic theory states that as the cost of a resource (like coal) decreases, its overall usage and demand increase, leading to higher total consumption <a class="yt-timestamp" data-t="11:51:00">[11:51:00]</a>. This was also observed with ATMs, where increased accessibility led to more bank branches and more tellers <a class="yt-timestamp" data-t="12:06:00">[12:06:00]</a>. Therefore, cost must be accounted for in agent evaluations for the foreseeable future <a class="yt-timestamp" data-t="12:30:00">[12:30:00]</a>.
*   **Benchmark Performance vs. Real-World Translation**: Overreliance on static benchmarks can be misleading <a class="yt-timestamp" data-t="13:00:00">[13:00:00]</a>. For example, Cognition raised $175 million at a $2 billion valuation primarily due to its agent, Devin, performing well on S-bench <a class="yt-timestamp" data-t="13:16:00">[13:16:00]</a>. However, real-world testing by "answer.dev" showed that over a month of use, Devin was successful in only 3 out of 20 tasks <a class="yt-timestamp" data-t="13:50:00">[13:50:00]</a>.
*   **Human-in-the-Loop Validation**: To overcome the limitations of static benchmarks, a proposed solution involves having human domain experts proactively edit the criteria used for LLM evaluations, leading to better results <a class="yt-timestamp" data-t="14:08:00">[14:08:00]</a>.

### 3. Confusion Between Capability and Reliability

A significant challenge is the misunderstanding between an AI model's capability and its reliability <a class="yt-timestamp" data-t="14:46:00">[14:46:00]</a>.

*   **Capability (Pass@K Accuracy)**: This refers to what a model *could* do at certain times, meaning one of 'K' answers outputted by the model is correct <a class="yt-timestamp" data-t="14:54:00">[14:54:00]</a>. Language models are already capable of many things <a class="yt-timestamp" data-t="15:24:00">[15:24:00]</a>.
*   **Reliability (Consistent Accuracy)**: This means consistently getting the right answer every single time <a class="yt-timestamp" data-t="15:10:00">[15:10:00]</a>. For consequential decisions in the real world, reliability is paramount <a class="yt-timestamp" data-t="15:15:00">[15:15:00]</a>.
*   **The 99.999% Gap**: The methods for training models to achieve 90% capability do not necessarily lead to the "five nines" (99.999%) of reliability <a class="yt-timestamp" data-t="15:37:00">[15:37:00]</a>. Closing this gap is the job of an AI engineer <a class="yt-timestamp" data-t="15:52:00">[15:52:00]</a>. Failures of products like Humane Pin and Rabbit R1 are attributed to developers not anticipating that a lack of reliability would lead to product failure <a class="yt-timestamp" data-t="16:03:00">[16:03:00]</a>. For example, a personal assistant that only correctly orders food 80% of the time is a catastrophic product failure <a class="yt-timestamp" data-t="16:15:00">[16:15:00]</a>.
*   **Imperfect Verifiers**: While verifiers (like unit tests) have been proposed to improve reliability, they can be imperfect in practice <a class="yt-timestamp" data-t="16:28:00">[16:28:00]</a>. Leading coding benchmarks, HumanEval and MBPP, have false positives in their unit tests, meaning incorrect code can still pass <a class="yt-timestamp" data-t="16:49:00">[16:49:00]</a>. When accounting for these false positives, model performance can bend downwards, as the more attempts an agent makes, the more likely it is to output a wrong answer <a class="yt-timestamp" data-t="17:01:00">[17:01:00]</a>.

## [[future_prospects_in_ai_and_agentbased_technologies | Future Prospects and a Mindset Shift]]

The [[challenges and opportunities in AI and agent capabilities | challenge for AI engineers]] is to determine what software optimizations and abstractions are needed to work with inherently stochastic components like LLMs <a class="yt-timestamp" data-t="17:29:00">[17:29:00]</a>. This is primarily a system design problem, requiring engineers to work around the constraints of an inherently stochastic system <a class="yt-timestamp" data-t="17:41:00">[17:41:00]</a>.

AI engineering needs to be viewed more as a reliability engineering field than a software or machine learning engineering field <a class="yt-timestamp" data-t="17:54:00">[17:54:00]</a>. This necessitates a mindset shift for AI engineers <a class="yt-timestamp" data-t="18:03:00">[18:03:00]</a>. Historically, the birth of computing faced similar challenges; the 1946 ENIAC computer, with its 17,000 vacuum tubes, was unavailable half the time due to frequent failures <a class="yt-timestamp" data-t="18:22:00">[18:22:00]</a>. The engineers' primary job in the first two years was to fix these reliability issues to make the computer usable <a class="yt-timestamp" data-t="18:42:00">[18:42:00]</a>.

Similarly, the true job of AI engineers is not just to create excellent products but to fix the reliability issues that plague every agent built upon stochastic models <a class="yt-timestamp" data-t="18:58:00">[18:58:00]</a>. This reliability shift in mindset is crucial for ensuring the next wave of computing is as reliable as possible for end-users <a class="yt-timestamp" data-t="19:18:00">[19:18:00]</a>.