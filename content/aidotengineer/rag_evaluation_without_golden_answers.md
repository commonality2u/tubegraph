---
title: RAG evaluation without golden answers
videoId: 1cQlnfwmIdU
---

From: [[aidotengineer]] <br/> 

Traditional [[evaluation and improvement of RAG solutions | RAG evaluation]] often relies on "golden answers" or "golden chunks" (pre-defined correct responses or relevant text segments), which presents a significant scalability challenge <a class="yt-timestamp" data-t="00:00:22">[00:00:22]</a>. To address this, a new open-source project called **Open Rag Eval** has been developed for quick and scalable [[retrieval augmented generation RAG | RAG]] [[evaluation and improvement of RAG solutions | evaluation]] that does not require these golden datasets <a class="yt-timestamp" data-t="00:00:06">[00:00:06]</a>.

## Open Rag Eval Overview

Open Rag Eval is an open-source project designed to solve the non-scalable nature of [[retrieval augmented generation RAG | RAG]] [[evaluation and improvement of RAG solutions | evaluation]] that demands golden answers or chunks <a class="yt-timestamp" data-t="00:00:19">[00:00:19]</a>. It is a research-backed initiative, developed in collaboration with the University of Waterloo's Jimmy Lynn lab <a class="yt-timestamp" data-t="00:00:32">[00:00:32]</a>.

### Architecture and Workflow
The [[components of RAG stack | architecture]] of Open Rag Eval involves several steps to generate [[Metrics for RAG evaluation | evaluation]] files:
1.  **Queries**: The process begins with a set of user-defined queries, ranging from tens to thousands, which are crucial for the [[retrieval augmented generation RAG | RAG]] system <a class="yt-timestamp" data-t="00:00:47">[00:00:47]</a>.
2.  **[[components of RAG stack | RAG]] Connector**: A connector collects the actual information generated by the [[retrieval augmented generation RAG | RAG]] pipeline, including chunks and answers <a class="yt-timestamp" data-t="00:00:56">[00:00:56]</a>. Connectors are available for Vectara, LangChain, LlamaIndex, and more are being added <a class="yt-timestamp" data-t="00:01:04">[00:01:04]</a>. These generate the raw [[retrieval augmented generation RAG | RAG]] outputs <a class="yt-timestamp" data-t="00:01:09">[00:01:09]</a>.
3.  **[[evaluation and improvement of RAG solutions | Evaluation]] Run**: The collected outputs are then fed into the [[evaluation and improvement of RAG solutions | evaluation]] engine, which runs various [[Metrics for RAG evaluation | metrics]] grouped into evaluators <a class="yt-timestamp" data-t="00:01:16">[00:01:16]</a>.
4.  **[[Metrics for RAG evaluation | Evaluation]] Files**: The evaluators generate comprehensive [[retrieval augmented generation RAG | RAG]] [[evaluation and improvement of RAG solutions | evaluation]] files containing all necessary data to assess the [[retrieval augmented generation RAG | RAG]] pipeline's performance <a class="yt-timestamp" data-t="00:01:24">[00:01:24]</a>.

## [[Metrics for RAG evaluation | Key Metrics]] for No-Golden-Answer [[evaluation and improvement of RAG solutions | Evaluation]]

Open Rag Eval introduces several [[Metrics for RAG evaluation | metrics]] that enable [[retrieval augmented generation RAG | RAG]] [[evaluation and improvement of RAG solutions | evaluation]] without relying on golden answers <a class="yt-timestamp" data-t="00:01:36">[00:01:36]</a>.

### Umbrella (Retrieval Metric)
Umbrella is a [[Metrics for RAG evaluation | retrieval metric]] designed to assess retrieval quality without golden chunks <a class="yt-timestamp" data-t="00:01:44">[00:01:44]</a>. It scores each retrieved chunk or passage on a scale of zero to three:
*   **Zero**: The chunk has no relevance to the query <a class="yt-timestamp" data-t="00:02:16">[00:02:16]</a>.
*   **Three**: The chunk is dedicated to the query and contains the exact answer <a class="yt-timestamp" data-t="00:02:22">[00:02:22]</a>.
Research from the [[University collaboration in RAG research | University of Waterloo]] lab demonstrates that this approach correlates well with human judgment, ensuring reliable results even without golden chunks <a class="yt-timestamp" data-t="00:02:32">[00:02:32]</a>.

### AutoNuggetizer (Generation Metric)
AutoNuggetizer is a [[Metrics for RAG evaluation | generation metric]] that also operates without requiring golden answers <a class="yt-timestamp" data-t="00:02:53">[00:02:53]</a>. Its process involves three steps:
1.  **Nugget Creation**: Atomic units called "nuggets" are created from the response <a class="yt-timestamp" data-t="00:03:00">[00:03:00]</a>.
2.  **Nugget Rating**: Each nugget is assigned a "vital" or "okay" rating, and the top 20 nuggets are selected <a class="yt-timestamp" data-t="00:03:07">[00:03:07]</a>.
3.  **LLM Judge Analysis**: An LLM (Large Language Model) judge analyzes the [[retrieval augmented generation RAG | RAG]] system's response to determine if each selected nugget is fully or partially supported by the answer <a class="yt-timestamp" data-t="00:03:15">[00:03:15]</a>.

### Citation Faithfulness
This metric measures the fidelity of citations within the [[retrieval augmented generation RAG | RAG]] system's response <a class="yt-timestamp" data-t="00:03:30">[00:03:30]</a>. It assesses whether a cited passage is fully supported, partially supported, or not supported at all by the response content <a class="yt-timestamp" data-t="00:03:34">[00:03:34]</a>.

### Hallucination Detection
This [[Metrics for RAG evaluation | metric]] utilizes Vectara's Hallucination Detection model to verify if the entire generated response aligns with the retrieved content <a class="yt-timestamp" data-t="00:03:45">[00:03:45]</a>.

## [[Tools and interfaces for RAG evaluation | User Interface]]

Open Rag Eval provides a user-friendly interface at `open_evaluation.ai` to visualize the results <a class="yt-timestamp" data-t="00:03:57">[00:03:57]</a>. Users can drag and drop their [[evaluation and improvement of RAG solutions | evaluation]] files onto the platform to view a detailed UI that compares retrieval scores, different generation scores, and other relevant data across all queries <a class="yt-timestamp" data-t="00:04:08">[00:04:08]</a>.

## Benefits and Contributions

Open Rag Eval is a powerful, open-source package designed to help optimize and tune [[retrieval augmented generation RAG | RAG]] pipelines <a class="yt-timestamp" data-t="00:04:27">[00:04:27]</a>. Its open nature ensures transparency in how the [[Metrics for RAG evaluation | metrics]] work <a class="yt-timestamp" data-t="00:04:34">[00:04:34]</a>. While it includes connectors for popular [[components of RAG stack | RAG]] frameworks like Vectara, LangChain, and LlamaIndex, the project encourages contributions of issues or pull requests for additional connectors or improvements <a class="yt-timestamp" data-t="00:04:44">[00:04:44]</a>.