---
title: Metrics for RAG evaluation
videoId: 1cQlnfwmIdU
---

From: [[aidotengineer]] <br/> 

[[open_rag_eval_project_overview | Open RAG Eval]] is a new open-source project designed for quick and scalable [[evaluation_and_improvement_of_rag_solutions | RAG evaluation]] <a class="yt-timestamp" data-t="00:00:06">[00:00:06]</a>. It addresses a major challenge in [[evaluation_and_improvement_of_rag_solutions | RAG evaluation]]: the requirement for golden answers or golden chunks, which is not scalable <a class="yt-timestamp" data-t="00:00:22">[00:00:22]</a>. The project is research-backed, developed in collaboration with the [[university_collaboration_in_rag_research | University of Waterloo]]'s Jimmy Lynn lab <a class="yt-timestamp" data-t="00:00:32">[00:00:32]</a>.

## Open RAG Eval Architecture Overview
The evaluation process begins with a set of queries (e.g., 10, 100, or 1,000) important for a RAG system <a class="yt-timestamp" data-t="00:00:47">[00:00:47]</a>. A RAG connector collects information such as chunks and answers generated by a RAG pipeline <a class="yt-timestamp" data-t="00:00:54">[00:00:54]</a>. Connectors are available for Vectara, LangChain, and LlamaIndex, with more in development <a class="yt-timestamp" data-t="00:01:03">[00:01:03]</a>. These connectors produce the RAG outputs <a class="yt-timestamp" data-t="00:01:09">[00:01:09]</a>. The actual evaluation then runs various metrics, which are grouped into evaluators that generate RAG evaluation files <a class="yt-timestamp" data-t="00:01:16">[00:01:16]</a>.

## Key Metrics for [[rag_evaluation_without_golden_answers | RAG Evaluation without Golden Answers]]
[[open_rag_eval_project_overview | Open RAG Eval]] offers several metrics designed to function without requiring golden answers <a class="yt-timestamp" data-t="00:01:36">[00:01:36]</a>.

### Umbrella (Retrieval Metric)
Umbrella is a retrieval metric that assigns a score between zero and three to a chunk <a class="yt-timestamp" data-t="00:02:08">[00:02:08]</a>:
*   **Zero**: The chunk/passage has nothing to do with the query <a class="yt-timestamp" data-t="00:02:16">[00:02:16]</a>.
*   **Three**: The chunk is dedicated to the query and contains the exact answer <a class="yt-timestamp" data-t="00:02:20">[00:02:20]</a>.
Research by the [[university_collaboration_in_rag_research | University of Waterloo]]'s Jim Lynn lab indicates that this approach correlates well with human judgment, even without golden chunks <a class="yt-timestamp" data-t="00:02:32">[00:02:32]</a>.

### Auto Nuggetizer (Generation Metric)
Auto Nuggetizer is a generation metric that does not require golden answers <a class="yt-timestamp" data-t="00:02:53">[00:02:53]</a>. It involves three steps:
1.  **Nugget Creation**: Atomic units called nuggets are created <a class="yt-timestamp" data-t="00:03:00">[00:03:00]</a>.
2.  **Rating and Sorting**: Each nugget is assigned a "vital" or "okay" rating, and the top 20 nuggets are selected <a class="yt-timestamp" data-t="00:03:06">[00:03:06]</a>.
3.  **LLM Judge Analysis**: An LLM judge analyzes the RAG system's response to determine if each selected nugget is fully or partially supported by the answer <a class="yt-timestamp" data-t="00:03:15">[00:03:15]</a>.

### Citation Faithfulness
This metric measures the fidelity of citations within the response <a class="yt-timestamp" data-t="00:03:30">[00:03:30]</a>. It determines if the citation in the passage is:
*   High fidelity (fully supported) <a class="yt-timestamp" data-t="00:03:37">[00:03:37]</a>.
*   Partially supported <a class="yt-timestamp" data-t="00:03:39">[00:03:39]</a>.
*   Not supported in the response <a class="yt-timestamp" data-t="00:03:40">[00:03:40]</a>.

### Hallucination Detection
This metric uses Vectara's Hallucination Detection Model (HHM) to check if the entire response aligns with the retrieved content <a class="yt-timestamp" data-t="00:03:45">[00:03:45]</a>.

## [[tools_and_interfaces_for_rag_evaluation | User Interface]]
After running an evaluation, the resulting evaluation files can be dragged and dropped onto [[tools_and_interfaces_for_rag_evaluation | open evaluation.ai]] <a class="yt-timestamp" data-t="00:04:06">[00:04:06]</a>. This provides a user interface that displays all queries run and allows comparison of retrieval scores, different generation scores, and other metrics <a class="yt-timestamp" data-t="00:04:12">[00:04:12]</a>.

## Benefits
[[open_rag_eval_project_overview | Open RAG Eval]] is a powerful package that can help optimize and tune RAG pipelines <a class="yt-timestamp" data-t="00:04:27">[00:04:27]</a>. Being open source, it offers full transparency on how the metrics work <a class="yt-timestamp" data-t="00:04:34">[00:04:34]</a>. It includes connectors for Vectara, LangChain, and LlamaIndex, and welcomes contributions for other RAG pipeline connectors <a class="yt-timestamp" data-t="00:04:44">[00:04:44]</a>.