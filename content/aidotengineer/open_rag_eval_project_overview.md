---
title: Open rag eval project overview
videoId: 1cQlnfwmIdU
---

From: [[aidotengineer]] <br/> 

Open Rag Eval is a new [[open_sourcing_and_applications_in_realworld_tasks | open source]] project designed for quick and scalable [[evaluation_and_improvement_of_rag_solutions | RAG evaluation]] <a class="yt-timestamp" data-t="00:00:06">[00:00:06]</a>. The project aims to address a significant challenge in [[evaluation_and_improvement_of_rag_solutions | RAG evaluation]]: the requirement for golden answers or golden chunks, which is not scalable <a class="yt-timestamp" data-t="00:00:22">[00:00:22]</a>.

The project is backed by research conducted in collaboration with the [[university_collaboration_in_rag_research | University of Waterloo]], specifically the [[university_collaboration_in_rag_research | Jimmy Lynn lab]] <a class="yt-timestamp" data-t="00:00:32">[00:00:32]</a>.

## How Open Rag Eval Works

The general architecture of Open Rag Eval involves several steps to perform [[evaluation_and_improvement_of_rag_solutions | RAG evaluation]] <a class="yt-timestamp" data-t="00:00:45">[00:00:45]</a>:

1.  **Queries**: The process begins with a set of queries, which can range from 10 to 1,000, collected for the [[components_of_rag_stack | RAG system]] <a class="yt-timestamp" data-t="00:00:47">[00:00:47]</a>.
2.  **RAG Connector**: A RAG connector collects actual information, chunks, and answers generated by the [[components_of_rag_stack | RAG pipeline]] <a class="yt-timestamp" data-t="00:00:54">[00:00:54]</a>. Connectors are available for Victara, LangChain, and LlamaIndex, with more being added <a class="yt-timestamp" data-t="00:01:04">[00:01:04]</a>. These connectors generate the RAG outputs <a class="yt-timestamp" data-t="00:01:09">[00:01:09]</a>.
3.  **Evaluation**: The evaluation phase runs various [[metrics_for_rag_evaluation | metrics]], which are grouped into evaluators <a class="yt-timestamp" data-t="00:01:16">[00:01:16]</a>.
4.  **RAG Evaluation Files**: The evaluators produce RAG evaluation files containing all necessary information to assess the [[components_of_rag_stack | RAG pipeline]] <a class="yt-timestamp" data-t="00:01:24">[00:01:24]</a>.

## Key Metrics for Evaluation

Open Rag Eval utilizes specific [[metrics_for_rag_evaluation | metrics]] that enable [[rag_evaluation_without_golden_answers | RAG evaluation without golden answers]] <a class="yt-timestamp" data-t="00:01:36">[00:01:36]</a>:

*   **Umbrella (Retrieval Metric)**: This metric evaluates retrieval without requiring golden chunks <a class="yt-timestamp" data-t="00:01:44">[00:01:44]</a>. It assigns a score between 0 and 3 to a chunk, indicating its relevance to the query:
    *   **0**: Nothing to do with the query <a class="yt-timestamp" data-t="00:02:16">[00:02:16]</a>.
    *   **3**: Dedicated to the query and contains the exact answer <a class="yt-timestamp" data-t="00:02:20">[00:02:20]</a>.
    Research by the [[university_collaboration_in_rag_research | University of Waterloo]] lab shows that this approach correlates well with human judgment <a class="yt-timestamp" data-t="00:02:32">[00:02:32]</a>.

*   **Auto Nuggetizer (Generation Metric)**: This generation metric also does not require golden answers <a class="yt-timestamp" data-t="00:02:53">[00:02:53]</a>. It involves three steps:
    1.  **Nugget Creation**: Atomic units called "nuggets" are created <a class="yt-timestamp" data-t="00:03:00">[00:03:00]</a>.
    2.  **Nugget Rating**: Each nugget is assigned a "vital" or "okay" rating, and the top 20 are selected <a class="yt-timestamp" data-t="00:03:05">[00:03:05]</a>.
    3.  **LLM Judge Analysis**: An LLM judge analyzes the RAG response to determine if each selected nugget is fully or partially supported by the answer <a class="yt-timestamp" data-t="00:03:15">[00:03:15]</a>.

*   **Citation Faithfulness**: This metric measures whether citations in the response are high fidelity, meaning they are fully supported, partially supported, or not supported by the cited passage <a class="yt-timestamp" data-t="00:03:30">[00:03:30]</a>.

*   **Hallucination Detection**: This uses Victara's Hallucination Detection Model (HHM) to check if the entire response aligns with the retrieved content <a class="yt-timestamp" data-t="00:03:45">[00:03:45]</a>.

## User Interface

Open Rag Eval provides a cool [[tools_and_interfaces_for_rag_evaluation | user interface]] for visualizing evaluation results <a class="yt-timestamp" data-t="00:03:58">[00:03:58]</a>. After running an evaluation, users can drag and drop the output files onto open evaluation.ai to view a [[tools_and_interfaces_for_rag_evaluation | UI]] that displays all run queries, retrieval scores, and generation scores for comparison <a class="yt-timestamp" data-t="00:04:06">[00:04:06]</a>.

## Benefits and Contribution

Open Rag Eval is a powerful package that can help users [[evaluation_and_improvement_of_rag_solutions | optimize and tune their RAG pipeline]] <a class="yt-timestamp" data-t="00:04:29">[00:04:29]</a>. Being [[open_sourcing_and_applications_in_realworld_tasks | open source]], it offers full transparency into how the [[metrics_for_rag_evaluation | metrics]] work <a class="yt-timestamp" data-t="00:04:34">[00:04:34]</a>. The project includes connectors for Victara, LangChain, and LlamaIndex, and contributions for other [[components_of_rag_stack | RAG pipeline]] connectors are welcome <a class="yt-timestamp" data-t="00:04:44">[00:04:44]</a>.