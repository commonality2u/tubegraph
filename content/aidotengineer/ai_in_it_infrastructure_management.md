---
title: AI in IT infrastructure management
videoId: VZzUhELgYk4
---

From: [[aidotengineer]] <br/> 

DataDog, an observability and security platform for cloud applications, focuses on helping users observe and take action on their systems to build safer and more devops-friendly environments <a class="yt-timestamp" data-t="01:22:00">[01:22:00]</a>. The company has been integrating [[ai_in_enterprise_applications | AI]] into its offerings since around 2015, with features like proactive alerting, root cause analysis, impact analysis, and change tracking <a class="yt-timestamp" data-t="01:46:00">[01:46:00]</a>.

The current era marks a significant shift in [[ai_in_enterprise_applications | AI]], comparable to the microprocessor or the transition to SaaS, driven by bigger, smarter models, reasoning capabilities, and multimodal [[ai_in_enterprise_applications | AI]] <a class="yt-timestamp" data-t="02:06:00">[02:06:00]</a>. This shift is making intelligence increasingly accessible and expected, prompting DataDog to leverage these advancements and offer more to customers by evolving beyond just a devops platform to providing [[ai_in_enterprise_applications | AI]] agents that utilize the platform for them <a class="yt-timestamp" data-t="02:29:00">[02:29:00]</a>.

## DataDog's AI Agents (Bits AI)

DataDog is developing "Bits AI," an [[ai_in_enterprise_applications | AI]] assistant designed to help with devops problems <a class="yt-timestamp" data-t="01:04:00">[01:04:00]</a>. This initiative involves developing the agents, performing evaluations, and building new types of observability <a class="yt-timestamp" data-t="03:06:00">[03:06:00]</a>. Currently, DataDog is working on two key [[ai_in_enterprise_applications | AI]] agents in private beta:

### AI On-Call Engineer

The [[ai_in_enterprise_applications | AI]] On-Call Engineer is designed to respond to alerts, ideally preventing human engineers from being paged in the middle of the night <a class="yt-timestamp" data-t="03:34:00">[03:34:00]</a>.

*   **Functionality:** When an alert occurs, the agent proactively kicks off, situationally orients itself by reading runbooks and grabbing context <a class="yt-timestamp" data-t="04:04:00">[04:04:00]</a>. It then investigates by reviewing logs, metrics, and traces to understand the situation <a class="yt-timestamp" data-t="04:16:00">[04:16:00]</a>. This agent can automatically run investigations and pull summaries and information before a human engineer even gets to their computer, providing insights into why an alert occurred or a trace showed an error <a class="yt-timestamp" data-t="04:24:00">[04:24:00]</a>.
*   **Human-AI Collaboration:** A new page supports human-[[ai_in_workplaces | AI]] collaboration, allowing users to verify agent actions, learn from their processes, and build trust <a class="yt-timestamp" data-t="04:47:00">[04:47:00]</a>. Users can see the reasoning behind a hypothesis, what the agent found, and the steps it took from the runbook, similar to overseeing a junior engineer <a class="yt-timestamp" data-t="05:03:00">[05:03:00]</a>.
*   **Problem Resolution:** The agent operates by forming hypotheses about what is happening, reasoning over them, and using tools to test ideas by running queries against logs, metrics, and other data <a class="yt-timestamp" data-t="05:30:00">[05:30:00]</a>. If a root cause is found, it can suggest remediations, such as paging another team or scaling infrastructure up or down <a class="yt-timestamp" data-t="05:51:00">[05:51:00]</a>. It can integrate with existing DataDog workflows <a class="yt-timestamp" data-t="06:11:00">[06:11:00]</a>.
*   **Post-Mortem Generation:** After an incident is remediated, the [[ai_in_enterprise_applications | AI]] On-Call Engineer can write a post-mortem report summarizing what occurred, what the agent did, and what humans did, preparing it for the morning <a class="yt-timestamp" data-t="06:26:00">[06:26:00]</a>.

### AI Software Engineer

The [[ai_in_enterprise_applications | AI]] Software Engineer functions as a proactive developer or devops/software engineering agent <a class="yt-timestamp" data-t="06:55:00">[06:55:00]</a>.

*   **Functionality:** This agent observes and acts on errors, automatically analyzing them, identifying causes, and proposing solutions <a class="yt-timestamp" data-t="07:00:00">[07:00:00]</a>. Solutions can include generating code fixes and creating tests to prevent recurrence, reducing on-call incidents <a class="yt-timestamp" data-t="07:10:00">[07:10:00]</a>.
*   **Workflow Integration:** It can catch issues like recursion problems, propose fixes, and even create recursion tests <a class="yt-timestamp" data-t="07:22:00">[07:22:00]</a>. Users have the option to create a pull request in GitHub or open the diff in VS Code for editing <a class="yt-timestamp" data-t="07:30:00">[07:30:00]</a>. This workflow significantly reduces the time engineers spend manually writing and testing code <a class="yt-timestamp" data-t="07:38:00">[07:38:00]</a>.

## Lessons Learned Building AI Agents

Building these [[ai_in_enterprise_applications | AI]] agents has provided DataDog with several key learnings:

*   **Scoping Tasks for Evaluation:** It is crucial to define "jobs to be done" and clearly understand step-by-step what is desired, thinking from a human perspective first <a class="yt-timestamp" data-t="08:01:00">[08:01:00]</a>. Building vertical, task-specific agents is preferred over generalized ones <a class="yt-timestamp" data-t="08:48:00">[08:48:00]</a>. Tasks should be measurable and verifiable at each step, as demos are easy to build, but consistent verification and improvement are challenging <a class="yt-timestamp" data-t="08:52:00">[08:52:00]</a>. Domain experts should be utilized as design partners or task verifiers, not as code or rule writers, due to the stochastic nature of models <a class="yt-timestamp" data-t="09:10:00">[09:10:00]</a>.
*   **Importance of Evaluation:** Deep thought about evaluation is paramount from the start, as fuzzy, stochastic [[ai_in_enterprise_applications | AI]] systems require robust evaluation <a class="yt-timestamp" data-t="09:31:00">[09:31:00]</a>. This includes offline, online, and living evaluations, with end-to-end task measurements and appropriate instrumentation to gather human feedback <a class="yt-timestamp" data-t="09:52:00">[09:52:00]</a>.
*   **Building the Right Team:** While a few ML experts are helpful, the core team should consist of optimistic generalists who are proficient in coding and willing to experiment quickly <a class="yt-timestamp" data-t="10:11:00">[10:11:00]</a>. [[ai_in_workplaces | AI]]-augmented teammates who are excited about day-to-day [[ai_in_workplaces | AI]] use and eager to learn in a fast-changing field are essential <a class="yt-timestamp" data-t="10:38:00">[10:38:00]</a>.
*   **Evolving User Experience (UX):** The user experience for [[ai_in_enterprise_applications | AI]] agents is a constantly evolving area, and traditional UX patterns are changing <a class="yt-timestamp" data-t="11:00:00">[11:00:00]</a>. The preference is for agents that act more like human teammates rather than relying on new pages or buttons <a class="yt-timestamp" data-t="11:28:00">[11:28:00]</a>.
*   **Observability Matters:** [[importance_of_infrastructure_design_for_ai_applications | Observability]] is critical and should not be an afterthought for complex [[ai_in_workflow_automation_and_augmentation | AI workflow automation and augmentation]] <a class="yt-timestamp" data-t="11:36:00">[11:36:00]</a>. Situational awareness is necessary for debugging problems <a class="yt-timestamp" data-t="11:44:00">[11:44:00]</a>. DataDog's "LLM observability" view helps by grouping a wide variety of [[ai_in_enterprise_applications | AI]] model interactions and calls into a single pane of glass <a class="yt-timestamp" data-t="11:50:00">[11:50:00]</a>. Agent workflows, which can involve hundreds of complex multi-step calls, require specialized views like an "agent graph" to make debugging human-readable and identify errors <a class="yt-timestamp" data-t="12:26:00">[12:26:00]</a>.
*   **"Agent or Application Layer Bitter Lesson":** General methods that leverage new, off-the-shelf [[ai_in_enterprise_applications | AI]] models are ultimately the most effective <a class="yt-timestamp" data-t="13:15:00">[13:15:00]</a>. Fine-tuning specific models for particular tasks can become obsolete when new, more capable foundation models are released <a class="yt-timestamp" data-t="13:26:00">[13:26:00]</a>. It's important to be able to easily try out new models without being tied to older ones <a class="yt-timestamp" data-t="13:45:00">[13:45:00]</a>.

## Future Outlook for AI in IT Infrastructure Management

The future of [[ai_in_enterprise_applications | AI]] is expected to be dynamic and accelerating <a class="yt-timestamp" data-t="14:49:00">[14:49:00]</a>.

*   **Agents as Users:** There is a strong belief that [[ai_in_enterprise_applications | AI]] agents may surpass humans as users of SaaS products like DataDog within the next five years <a class="yt-timestamp" data-t="14:01:00">[14:01:00]</a>. This means companies should design their products not just for humans but also for agents that might use their APIs <a class="yt-timestamp" data-t="14:21:00">[14:21:00]</a>.
*   **Teams of Agents:** DataDog anticipates offering teams of DevSecOps agents for hire, capable of directly using the platform and handling tasks like on-call responsibilities <a class="yt-timestamp" data-t="14:56:00">[14:56:00]</a>.
*   **New Company Creation:** It is envisioned that small companies will be built by individuals using automated developers like Cursor or Devin to bring ideas to life, with [[ai_in_enterprise_applications | AI]] agents handling operations and security, enabling an order of magnitude more ideas to reach the real world <a class="yt-timestamp" data-t="15:24:00">[15:24:00]</a>.