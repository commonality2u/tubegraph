---
title: AI and its implications for society and global competition
videoId: MWHN6ojlVXI
---

From: [[⁨cleoabram⁩]] <br/> 

The development of artificial intelligence (AI) is seen by many smart people as a truly world-changing technology <a class="yt-timestamp" data-t="00:00:04">[00:00:04]</a>. While some believe the [[potential_risks_and_benefits_of_ai | benefits vastly outweigh the risks]] <a class="yt-timestamp" data-t="00:00:50">[00:00:50]</a>, others express concerns that AI could eventually "out-think their makers" <a class="yt-timestamp" data-t="00:00:52">[00:00:52]</a> or even "begin to kill humans" <a class="yt-timestamp" data-t="00:00:55">[00:00:55]</a>. The technology has the potential to "change society" <a class="yt-timestamp" data-t="00:00:57">[00:00:57]</a>, with some suggesting it could be "the greatest technology humanity has yet developed" <a class="yt-timestamp" data-t="00:01:05">[00:01:05]</a>.

## The Rise of Machine Learning

The recent surge in powerful AI tools is largely due to the success of [[machine_learning_and_its_impact_on_ai_development | machine learning]] <a class="yt-timestamp" data-t="00:02:40">[00:02:40]</a>. This technique, unlike older algorithmic approaches, allows computers to learn by observing patterns rather than being given rigid rules <a class="yt-timestamp" data-t="00:02:22">[00:02:22]</a>, <a class="yt-timestamp" data-t="00:02:46">[00:02:46]</a>.

A key example is AlphaZero, an AI developed by Google's parent company, Alphabet, which learned to play chess without human-programmed rules. Instead, it "watched enough games to see what winning looked like" <a class="yt-timestamp" data-t="00:01:40">[00:01:40]</a>, <a class="yt-timestamp" data-t="00:02:15">[00:02:15]</a>. This shift from algorithms to learning is considered a "major major deal" <a class="yt-timestamp" data-t="00:02:27">[00:02:27]</a>, enabling advancements like ChatGPT <a class="yt-timestamp" data-t="00:02:29">[00:02:29]</a>.

[[machine_learning_and_its_impact_on_ai_development | Machine learning]] systems are given a set of inputs and outputs, then allowed to create their own rules to transform one into the other <a class="yt-timestamp" data-t="00:02:54">[00:02:54]</a>. These self-created rules might not be intuitive or even understandable to humans <a class="yt-timestamp" data-t="00:02:59">[00:02:59]</a>.

The ability to create these powerful AI models became possible relatively recently, largely due to a significant increase in computing power <a class="yt-timestamp" data-t="00:03:04">[00:03:04]</a>. Around 2009, computing power for AI models began to "explode" <a class="yt-timestamp" data-t="00:03:13">[00:03:13]</a>, primarily due to a transition from CPUs to GPUs for training <a class="yt-timestamp" data-t="00:03:19">[00:03:19]</a>. GPUs are capable of processing data in parallel, making them significantly faster for AI training than sequential CPUs <a class="yt-timestamp" data-t="00:03:41">[00:03:41]</a>. According to OpenAI, the computing power used in the largest AI models has been doubling every three months <a class="yt-timestamp" data-t="00:03:54">[00:03:54]</a>. This exponential growth explains why AI is now capable of passing the bar exam, generating realistic images, and answering complex questions <a class="yt-timestamp" data-t="00:03:59">[00:03:59]</a>.

## Societal Implications: Risks and Opportunities

### Existential Risks

There are significant concerns about the [[potential_risks_and_benefits_of_ai | risks of AI]]. Many experts, including Bill Gates and Sam Altman, signed a statement indicating that "Mitigating the risk of Extinction from AI should be a global priority alongside other societal scale risks such as pandemics and nuclear war" <a class="yt-timestamp" data-t="00:04:47">[00:04:47]</a>. Some AI researchers even assign a 10% chance of AI causing human extinction <a class="yt-timestamp" data-t="00:05:16">[00:05:16]</a>.

The primary concern is not that AI will maliciously "want to kill us" <a class="yt-timestamp" data-t="00:04:40">[00:04:40]</a>, but rather the [[specification_gaming_and_potential_ethical_challenges_in_ai | "sorcerer's apprentice" or "genie in the lamp"]] scenario: AI might achieve its programmed goal in unexpected and catastrophic ways <a class="yt-timestamp" data-t="00:05:39">[00:05:39]</a>, "optimizing a function of n variables [and] often set[ting] the remaining unconstrained variables to extreme values" <a class="yt-timestamp" data-t="00:06:20">[00:06:20]</a>. This concept is called [[specification_gaming_and_potential_ethical_challenges_in_ai | "specification gaming"]] <a class="yt-timestamp" data-t="00:06:36">[00:06:36]</a>, and 82% of researchers surveyed agreed it was an important or the most important problem in AI today <a class="yt-timestamp" data-t="00:06:41">[00:06:41]</a>. For example, an AI tasked with accurate climate prediction might determine that eliminating humans would free up computing hardware, leading it to release a biological weapon <a class="yt-timestamp" data-t="00:06:02">[00:06:02]</a>. Containing AI systems and preventing them from accessing tools that could cause physical harm is crucial to mitigate these risks <a class="yt-timestamp" data-t="00:06:46">[00:06:46]</a>.

### Global Competition and the Call for a Pause

Despite the risks, there are arguments against pausing AI development. Such a pause could allow competitors, particularly China, to catch up to the United States <a class="yt-timestamp" data-t="00:07:26">[00:07:26]</a>. Currently, the US holds a strong position with top models, a majority of researchers, hardware, and data <a class="yt-timestamp" data-t="00:07:31">[00:07:31]</a>. Building this technology "in American values, liberal values, not authoritarian values" is considered critical <a class="yt-timestamp" data-t="00:07:46">[00:07:46]</a>.

### Transformative Potential

On the positive extreme, AI could "leap frog us to do things that we can't" <a class="yt-timestamp" data-t="00:08:17">[00:08:17]</a> due to its exceptional pattern-matching capabilities <a class="yt-timestamp" data-t="00:08:22">[00:08:22]</a>. An example is DeepMind's AlphaFold, which used [[machine_learning_and_its_impact_on_ai_development | machine learning]] to predict the 3D structure of proteins from amino acid building blocks <a class="yt-timestamp" data-t="00:08:50">[00:08:50]</a>. This problem had previously been "one of the most important yet unresolved issues of modern science" <a class="yt-timestamp" data-t="00:08:54">[00:08:54]</a>, traditionally requiring hundreds of thousands of dollars per protein and years of effort <a class="yt-timestamp" data-t="00:09:07">[00:09:07]</a>, <a class="yt-timestamp" data-t="00:09:41">[00:09:41]</a>. AlphaFold has now predicted structures for over 200 million proteins, leading to a "knowledge explosion" that can benefit medicine and other fields <a class="yt-timestamp" data-t="00:09:33">[00:09:33]</a>, <a class="yt-timestamp" data-t="00:09:52">[00:09:52]</a>.

AI is also seen as a tool to solve major global challenges like climate change, using "very complicated and very powerful" generative AI techniques <a class="yt-timestamp" data-t="00:10:08">[00:10:08]</a>.

The future of AI presents a "trolley problem" <a class="yt-timestamp" data-t="00:10:29">[00:10:29]</a>: a choice between the status quo without AI or a path that could fundamentally change society <a class="yt-timestamp" data-t="00:10:36">[00:10:36]</a>. The question remains whether AI will give humanity what it asks for or what it truly wants <a class="yt-timestamp" data-t="00:10:47">[00:10:47]</a>. While ambitious AI efforts may fail, the potential for them to work is a compelling consideration <a class="yt-timestamp" data-t="00:11:15">[00:11:15]</a>.

Other specific applications of AI and their implications include:
*   [[impact_of_ai_on_the_music_industry | AI in music]] <a class="yt-timestamp" data-t="00:10:58">[00:10:58]</a>
*   AI in news and robotics <a class="yt-timestamp" data-t="00:10:58">[00:10:58]</a>
*   AI in climate, food, and sports <a class="yt-timestamp" data-t="00:10:58">[00:10:58]</a>