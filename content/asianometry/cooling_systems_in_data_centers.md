---
title: Cooling systems in data centers
videoId: tJYSzc7YkY0
---

From: [[asianometry]] <br/> 

Data centers are essential for modern technology, but they require significant resources, especially energy and [[water_usage_in_data_centers | water]] <a class="yt-timestamp" data-t="00:00:21">[00:00:21]</a>. The connection between energy and [[water_usage_in_data_centers | water]] consumption in data centers is particularly close, largely due to the demands of their cooling systems <a class="yt-timestamp" data-t="00:00:24">[00:00:24]</a>. A data center with 15 megawatts of IT capacity can use 80-130 million gallons of [[water_usage_in_data_centers | water]] annually, comparable to three hospitals or two 18-hole golf courses <a class="yt-timestamp" data-t="00:00:02">[00:00:02]</a>, <a class="yt-timestamp" data-t="00:00:11">[00:00:11]</a>.

## Necessity of Cooling
Almost all of a data center's consumed electricity is converted into heat <a class="yt-timestamp" data-t="00:02:34">[00:02:34]</a>. Even when not at full capacity, data centers draw 60-100% of their maximum power, generating substantial heat <a class="yt-timestamp" data-t="00:02:39">[00:02:39]</a>. To prevent long-term damage, electronic equipment must be kept cool <a class="yt-timestamp" data-t="00:02:49">[00:02:49]</a>. Hard disk drives typically require temperatures around 45 degrees Celsius (113 degrees Fahrenheit), while solid-state chips can tolerate higher limits of 85 degrees Celsius <a class="yt-timestamp" data-t="00:02:54">[00:02:54]</a>, <a class="yt-timestamp" data-t="00:02:59">[00:02:59]</a>. Maintaining stable temperature and humidity is the primary function of a cooling system <a class="yt-timestamp" data-t="00:03:07">[00:03:07]</a>.

## Types of Cooling Systems
There are two main types of cooling systems: air and liquid cooling <a class="yt-timestamp" data-t="00:03:10">[00:03:10]</a>.

### Air Cooling Systems
Most data centers utilize air cooling, often as part of a raised-floor system <a class="yt-timestamp" data-t="00:03:14">[00:03:14]</a>.
*   **Raised-Floor System**: Server racks are elevated 2-4 feet above the ground, creating an underfloor area primarily for cold air <a class="yt-timestamp" data-t="00:03:20">[00:03:20]</a>.
*   **CRAC Units**: Cold air is supplied by the data center's Computer Room Air Conditioner (CRAC) <a class="yt-timestamp" data-t="00:03:29">[00:03:29]</a>.
*   **Airflow**: Air is directed into cold aisles, where it absorbs heat from servers. The resulting hot air rises and is collected for transfer out of the room via fluid heat exchangers <a class="yt-timestamp" data-t="00:03:33">[00:03:33]</a>, <a class="yt-timestamp" data-t="00:03:39">[00:03:39]</a>.
*   **Two-Fluid Loop System**: The most common system involves two fluid loops:
    1.  **Process Loop**: A fluid refrigerant (often a water and glycol mix) removes heat from the data room floor <a class="yt-timestamp" data-t="00:03:54">[00:03:54]</a>.
    2.  **Condenser Loop**: This heat is then transferred to a water-based condenser loop for final transfer to the outside <a class="yt-timestamp" data-t="00:04:02">[00:04:02]</a>. This system, similar to those in nuclear power plants, prevents contamination and offers flexibility and efficiency, albeit at a higher cost <a class="yt-timestamp" data-t="00:04:08">[00:04:08]</a>, <a class="yt-timestamp" data-t="00:04:17">[00:04:17]</a>.
*   **Cooling Towers**: A set of cooling towers cools the water in the final loop. Hot water flows down, and some evaporates, releasing energy. The cooled water is then returned for reuse <a class="yt-timestamp" data-t="00:04:23">[00:04:23]</a>, <a class="yt-timestamp" data-t="00:04:32">[00:04:32]</a>. Approximately 1% of the water evaporates for every 10 degrees Fahrenheit (5.6 degrees Celsius) of cooling, depending on ambient conditions <a class="yt-timestamp" data-t="00:04:38">[00:04:38]</a>, <a class="yt-timestamp" data-t="00:04:43">[00:04:43]</a>. Evaporated [[water_usage_in_data_centers | water]] must be replaced with "make-up" [[water_usage_in_data_centers | water]] <a class="yt-timestamp" data-t="00:04:56">[00:04:56]</a>.

### Liquid Cooling Systems
Liquid is more efficient at capturing and transferring heat than air <a class="yt-timestamp" data-t="00:12:48">[00:12:48]</a>. Liquid cooling can directly cool chips, reducing overall [[impact_of_data_centers_on_energy_consumption | energy consumption]] and improving processor performance <a class="yt-timestamp" data-t="00:12:53">[00:12:53]</a>, <a class="yt-timestamp" data-t="00:12:59">[00:12:59]</a>, <a class="yt-timestamp" data-t="00:13:03">[00:13:03]</a>. These solutions are likely to be adopted more widely, especially when [[heat_recapture_in_data_centers | heat recapture]] is also implemented <a class="yt-timestamp" data-t="00:12:43">[00:12:43]</a>.

## Factors Influencing Cooling Needs
*   **Size of Data Centers**: Larger facilities, known as "hyperscale" data centers (e.g., Google, Meta), span football fields, contain around 5,000 servers, and are 10,000 square meters large <a class="yt-timestamp" data-t="00:00:52">[00:00:52]</a>, <a class="yt-timestamp" data-t="00:01:09">[00:01:09]</a>. They are built for [[energy_efficiency_in_ai_data_centers | energy efficiency scaling]] and economics <a class="yt-timestamp" data-t="00:01:36">[00:01:36]</a>. Smaller data centers might fit in a closet <a class="yt-timestamp" data-t="00:00:52">[00:00:52]</a>.
*   **Power Usage Effectiveness (PUE)**: This metric measures a data center's [[energy_efficiency_in_ai_data_centers | energy efficiency]] <a class="yt-timestamp" data-t="00:01:43">[00:01:43]</a>. PUE is calculated by dividing the total energy delivered by the total energy going to the ICT equipment <a class="yt-timestamp" data-t="00:01:49">[00:01:49]</a>. An ideal PUE is 1.0, meaning all energy goes to IT <a class="yt-timestamp" data-t="00:01:56">[00:01:56]</a>. Larger data centers generally have lower PUEs; Google and Microsoft hyperscale centers claim PUEs of 1.1 or 1.2, while a closet data center might have a PUE of 2.5 <a class="yt-timestamp" data-t="00:02:05">[00:02:05]</a>, <a class="yt-timestamp" data-t="00:02:08">[00:02:08]</a>, <a class="yt-timestamp" data-t="00:02:12">[00:02:12]</a>. The difference is primarily due to cooling efficiency <a class="yt-timestamp" data-t="00:02:19">[00:02:19]</a>.

## [[water_usage_in_data_centers | Water Usage]]
Data centers use [[water_usage_in_data_centers | water]] directly for cooling and indirectly through [[impact_of_data_centers_on_energy_consumption | energy]] generation <a class="yt-timestamp" data-t="00:05:01">[00:05:01]</a>.
*   **Direct Consumption**: Evaporation in cooling towers consumes thousands of gallons of [[water_usage_in_data_centers | water]] <a class="yt-timestamp" data-t="00:05:56">[00:05:56]</a>. Major companies like Google used about 4.3 billion gallons of [[water_usage_in_data_centers | water]] for cooling in 2022, with 25% being reclaimed wastewater or seawater <a class="yt-timestamp" data-t="00:06:24">[00:06:24]</a>, <a class="yt-timestamp" data-t="00:06:30">[00:06:30]</a>. Digital Realty reports 36% of their [[water_usage_in_data_centers | water]] from municipal non-drinkable sources <a class="yt-timestamp" data-t="00:06:48">[00:06:48]</a>. AWS has 20 global data centers using recycled [[water_usage_in_data_centers | water]] for cooling <a class="yt-timestamp" data-t="00:07:10">[00:07:10]</a>.
*   **Indirect Consumption**: Generating thermoelectric power (coal, natural gas, nuclear) requires [[water_usage_in_data_centers | water]] for boiling and spinning turbines <a class="yt-timestamp" data-t="00:05:15">[00:05:15]</a>. Studies show that [[water_usage_in_data_centers | water]] usage from [[impact_of_data_centers_on_energy_consumption | energy]] generation can be 2-3 times larger than direct cooling consumption <a class="yt-timestamp" data-t="00:05:44">[00:05:44]</a>, <a class="yt-timestamp" data-t="00:05:48">[00:05:48]</a>. Using air conditioning systems instead of evaporative cooling towers would consume even more power, and thus indirectly, more [[water_usage_in_data_centers | water]] <a class="yt-timestamp" data-t="00:06:03">[00:06:03]</a>.
*   **Water Sources**: The majority of [[water_usage_in_data_centers | water]] directly withdrawn for cooling often comes from potable local supplies <a class="yt-timestamp" data-t="00:07:40">[00:07:40]</a>, <a class="yt-timestamp" data-t="00:07:48">[00:07:48]</a>. With 40-50% of the global population living in [[water_usage_in_data_centers | water]]-scarce areas, and data centers often located near population centers, this adds new competition for [[water_usage_in_data_centers | water]] <a class="yt-timestamp" data-t="00:07:54">[00:07:54]</a>, <a class="yt-timestamp" data-t="00:08:00">[00:08:00]</a>. Companies like Meta are funding projects to restore [[water_usage_in_data_centers | water]] in basins like the Colorado River and using long-term storage credits to avoid drawing from municipal areas <a class="yt-timestamp" data-t="00:08:43">[00:08:43]</a>, <a class="yt-timestamp" data-t="00:08:48">[00:08:48]</a>, <a class="yt-timestamp" data-t="00:08:54">[00:08:54]</a>.

## Cooling Innovations and Strategies

### [[free_cooling | Free Cooling]]
[[free_cooling | Free cooling]] utilizes natural environmental conditions to cool data centers, reducing the need for traditional HVAC systems and [[water_usage_in_data_centers | water]] evaporation <a class="yt-timestamp" data-t="00:09:09">[00:09:09]</a>, <a class="yt-timestamp" data-t="00:09:18">[00:09:18]</a>.
*   **Direct [[free_cooling | Free Cooling]]**: Involves drawing in outside air <a class="yt-timestamp" data-t="00:09:25">[00:09:25]</a>. However, outside air often requires dehumidification, air filtration, and cleaning to protect electronics from smoke, dust, and gases, which can offset cost benefits depending on location <a class="yt-timestamp" data-t="00:09:31">[00:09:31]</a>, <a class="yt-timestamp" data-t="00:09:37">[00:09:37]</a>, <a class="yt-timestamp" data-t="00:09:43">[00:09:43]</a>. Studies show average [[energy_efficiency_in_ai_data_centers | energy]] savings of 5.4-7.9% in Europe, and up to 60% in Southern Australian capital cities <a class="yt-timestamp" data-t="00:09:56">[00:09:56]</a>, <a class="yt-timestamp" data-t="00:10:00">[00:10:00]</a>, <a class="yt-timestamp" data-t="00:10:12">[00:10:12]</a>. Cold climates offer significant advantages, leading Nordic governments to attract data center investments <a class="yt-timestamp" data-t="00:10:23">[00:10:23]</a>, <a class="yt-timestamp" data-t="00:10:28">[00:10:28]</a>. Meta plans to use direct [[free_cooling | free cooling]] to cut [[water_usage_in_data_centers | water]] usage by 60% <a class="yt-timestamp" data-t="00:09:01">[00:09:01]</a>.
*   **Waterside [[free_cooling | Free Cooling]]**: Data centers near cold seas can use seawater for cooling <a class="yt-timestamp" data-t="00:10:52">[00:10:52]</a>, <a class="yt-timestamp" data-t="00:10:57">[00:10:57]</a>. Google's Hamina data center in Finland, a converted paper mill, uses existing pipes to draw fresh seawater. The two-loop cooling system ensures the seawater does not mix with the internal cooling fluid <a class="yt-timestamp" data-t="00:11:02">[00:11:02]</a>, <a class="yt-timestamp" data-t="00:11:07">[00:11:07]</a>, <a class="yt-timestamp" data-t="00:11:14">[00:11:14]</a>.

### [[heat_recapture_in_data_centers | Heat Recapture]]
Recovering and reusing the heat generated by data centers is another sustainable approach <a class="yt-timestamp" data-t="00:11:30">[00:11:30]</a>.
*   **Applications**: Recaptured heat can be used for desalinating [[water_usage_in_data_centers | water]], pre-heating [[water_usage_in_data_centers | water]] in thermoelectric plants, direct [[impact_of_data_centers_on_energy_consumption | power]] generation, or piping it to homes for general heating or hot [[water_usage_in_data_centers | water]] <a class="yt-timestamp" data-t="00:11:35">[00:11:35]</a>, <a class="yt-timestamp" data-t="00:11:39">[00:11:39]</a>, <a class="yt-timestamp" data-t="00:11:44">[00:11:44]</a>.
*   **Feasibility**: Heat from air-cooled data centers can be hot enough (35-45 degrees Celsius) <a class="yt-timestamp" data-t="00:11:50">[00:11:50]</a>, <a class="yt-timestamp" data-t="00:11:53">[00:11:53]</a>. Space and [[water_usage_in_data_centers | water]] heating is the single largest end-use of [[impact_of_data_centers_on_energy_consumption | energy]] in homes, accounting for about 6% of America's total [[impact_of_data_centers_on_energy_consumption | energy consumption]] <a class="yt-timestamp" data-t="00:11:58">[00:11:58]</a>, <a class="yt-timestamp" data-t="00:12:03">[00:12:03]</a>.
*   **Challenges**: The main issues are the inability to efficiently move heat over long distances, requiring demand sources (households) to be relatively close to the data center <a class="yt-timestamp" data-t="00:12:08">[00:12:08]</a>, <a class="yt-timestamp" data-t="00:12:13">[00:12:13]</a>. Additionally, many cities lack the necessary district heating infrastructure of insulated pipes <a class="yt-timestamp" data-t="00:12:19">[00:12:19]</a>, <a class="yt-timestamp" data-t="00:12:24">[00:12:24]</a>. Nordic countries have made significant progress in this area <a class="yt-timestamp" data-t="00:12:34">[00:12:34]</a>.

### Optimizing Temperature Ranges
The American Society of Heating, Refrigerating and Air-Conditioning Engineers (ASHRAE) recommends a data center temperature range of 15 to 32 degrees Celsius <a class="yt-timestamp" data-t="00:13:13">[00:13:13]</a>, <a class="yt-timestamp" data-t="00:13:16">[00:13:16]</a>, <a class="yt-timestamp" data-t="00:13:21">[00:13:21]</a>. While some operators cool even more, raising a data center's temperature by just 1-2 degrees can yield significant financial benefits <a class="yt-timestamp" data-t="00:13:26">[00:13:26]</a>, <a class="yt-timestamp" data-t="00:13:31">[00:13:31]</a>, <a class="yt-timestamp" data-t="00:13:36">[00:13:36]</a>. Google tests have shown data centers can operate at higher temperatures <a class="yt-timestamp" data-t="00:13:47">[00:13:47]</a>.

## [[ai_boom_and_data_center_demands | AI Boom]] and Future Demands
The ongoing [[ai_boom_and_data_center_demands | AI boom]] is rapidly increasing data center [[impact_of_data_centers_on_energy_consumption | energy]] and [[water_usage_in_data_centers | water]] consumption <a class="yt-timestamp" data-t="00:13:59">[00:13:59]</a>, <a class="yt-timestamp" data-t="00:14:03">[00:14:03]</a>. Microsoft's 2022 [[water_usage_in_data_centers | water]] consumption jumped 34% from 2021, likely due to ChatGPT and other generative AI products <a class="yt-timestamp" data-t="00:14:07">[00:14:07]</a>, <a class="yt-timestamp" data-t="00:14:13">[00:14:13]</a>, <a class="yt-timestamp" data-t="00:14:21">[00:14:21]</a>. Requests for individual data center demands are increasing from 30 megawatts to 60-90 megawatts or greater, with campuses requiring up to several gigawatts <a class="yt-timestamp" data-t="00:14:34">[00:14:34]</a>, <a class="yt-timestamp" data-t="00:14:38">[00:14:38]</a>, <a class="yt-timestamp" data-t="00:14:43">[00:14:43]</a>, <a class="yt-timestamp" data-t="00:14:48">[00:14:48]</a>, <a class="yt-timestamp" data-t="00:14:54">[00:14:54]</a>.

AI is projected to propel data center's share of global [[impact_of_data_centers_on_energy_consumption | energy]] generation to 4.5% by 2030, implying a similar growth in [[water_usage_in_data_centers | water]] consumption <a class="yt-timestamp" data-t="00:15:03">[00:15:03]</a>, <a class="yt-timestamp" data-t="00:15:09">[00:15:09]</a>, <a class="yt-timestamp" data-t="00:15:17">[00:15:17]</a>. Nvidia's increasingly power-hungry GPU products will further escalate [[impact_of_data_centers_on_energy_consumption | energy]] and [[water_usage_in_data_centers | water]] demands for training and deploying AI models <a class="yt-timestamp" data-t="00:15:22">[00:15:22]</a>, <a class="yt-timestamp" data-t="00:15:26">[00:15:26]</a>, <a class="yt-timestamp" data-t="00:15:30">[00:15:30]</a>, <a class="yt-timestamp" data-t="00:15:33">[00:15:33]</a>, <a class="yt-timestamp" data-t="00:15:35">[00:15:35]</a>.

## Conclusion
The future of compute and AI will require significantly more [[impact_of_data_centers_on_energy_consumption | electricity]] and [[water_usage_in_data_centers | water]] <a class="yt-timestamp" data-t="00:15:39">[00:15:39]</a>, <a class="yt-timestamp" data-t="00:15:43">[00:15:43]</a>. This demand spans from the [[semiconductor_water_usage_and_challenges | semiconductor]] fabrication stage (e.g., TSMC's growing [[impact_of_data_centers_on_energy_consumption | energy]] consumption in Taiwan <a class="yt-timestamp" data-t="00:15:47">[00:15:47]</a>, <a class="yt-timestamp" data-t="00:15:50">[00:15:50]</a>, <a class="yt-timestamp" data-t="00:15:56">[00:15:56]</a>) to the operation of data centers themselves, which generate heat requiring further [[impact_of_data_centers_on_energy_consumption | energy]] for removal <a class="yt-timestamp" data-t="00:16:03">[00:16:03]</a>, <a class="yt-timestamp" data-t="00:16:07">[00:16:07]</a>.

To address these challenges, future data centers will need to rapidly adopt a combination of [[free_cooling | free cooling]], [[heat_recapture_in_data_centers | waste heat recovery]], and renewable [[impact_of_data_centers_on_energy_consumption | energy]] sources like solar <a class="yt-timestamp" data-t="00:16:12">[00:16:12]</a>, <a class="yt-timestamp" data-t="00:16:16">[00:16:16]</a>. This transition is crucial for both sustainability and financial viability <a class="yt-timestamp" data-t="00:16:22">[00:16:22]</a>.