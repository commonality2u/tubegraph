---
title: Compute in memory technology
videoId: 5tmGKTNW8DQ
---

From: [[asianometry]] <br/> 

"Compute in memory" (CIM) refers to a class of technologies aiming to integrate processing elements directly within or very near memory components <a class="yt-timestamp" data-t="00:07:30">[00:07:30]</a>. This approach seeks to alleviate the "memory wall" problem, where the constant shuttling of data between processing units (CPUs or GPUs) and separate memory banks becomes a bottleneck <a class="yt-timestamp" data-t="00:02:54">[00:02:54]</a>. Other names for this concept include processing-in-memory, computational RAM, near-data computing, memory-centric computing, and in-memory computation <a class="yt-timestamp" data-t="00:07:42">[00:07:42]</a>.

## The Memory Wall Problem

The rapid growth of deep learning models since 2012 has led to an explosion in parameter counts, with models growing hundreds of thousands of times <a class="yt-timestamp" data-t="00:00:02">[00:00:02]</a>. For example, OpenAI's DALL-E 2 has 3.5 billion parameters, Google's Imagen 4.6 billion, and GPT-3 175 billion <a class="yt-timestamp" data-t="00:00:10">[00:00:10]</a>. Google has even pre-trained a model with 1 trillion parameters <a class="yt-timestamp" data-t="00:00:24">[00:00:24]</a>. These increasingly large models strain the ability of existing hardware, with many [[memory_limitations_in_deep_learning | limitations tied back to memory]] and its usage <a class="yt-timestamp" data-t="00:00:28">[00:00:28]</a>.

Most modern computers use a [[von_neumann_architecture_vs_brain_computing | Von Neumann architecture]], storing both instructions and data in the same memory bank <a class="yt-timestamp" data-t="00:01:00">[00:01:00]</a>. While this architecture has enabled powerful software, it differs significantly from the human brain, which tightly integrates compute with memory and input/output communication <a class="yt-timestamp" data-t="00:01:28">[00:01:28]</a>. Conventional computers separate high-precision compute from memory and communication, leading to significant consequences for memory <a class="yt-timestamp" data-t="00:01:40">[00:01:40]</a>.

Despite efforts by the AI hardware industry to scale memory and processing unit performance (e.g., Nvidia V100, A100, H100 GPUs with increasing memory capacities) <a class="yt-timestamp" data-t="00:01:55">[00:01:55]</a>, hardware performance is not keeping pace with model growth, especially in terms of memory <a class="yt-timestamp" data-t="00:02:15">[00:02:15]</a>. Leading-edge models can require hundreds of gigabytes of memory <a class="yt-timestamp" data-t="00:02:23">[00:02:23]</a>, with a trillion-parameter model estimated to require 320 A100 GPUs (each with 80GB) <a class="yt-timestamp" data-t="00:02:30">[00:02:30]</a>. This mismatch means processing units waste cycles waiting for data to move in and out of memory and for read/write operations to complete, a bottleneck known as the "memory wall" or "memory capacity bottleneck" <a class="yt-timestamp" data-t="00:02:39">[00:02:39]</a>.

### Practical and Technological Limits
Adding more memory faces practical and [[technological_advancements_in_semiconductor_manufacturing | technological limits]], including issues with connections and wiring <a class="yt-timestamp" data-t="00:03:07">[00:03:07]</a>.

*   **Energy Consumption**: A significant [[energy_efficiency_in_ai_hardware | energy limitation]] is associated with shuttling data between the chip and memory due to electrical connection losses <a class="yt-timestamp" data-t="00:03:22">[00:03:22]</a>. Accessing off-chip memory uses 200 times more energy than a floating-point operation <a class="yt-timestamp" data-t="00:03:32">[00:03:32]</a>. For instance, 80% of a Google TPU's energy usage comes from electrical connections, not logic units <a class="yt-timestamp" data-t="00:03:39">[00:03:39]</a>. DRAM alone can account for 40% of total system power <a class="yt-timestamp" data-t="00:03:47">[00:03:47]</a>, making storage and memory significant factors in data center profitability <a class="yt-timestamp" data-t="00:03:54">[00:03:54]</a>.
*   **Capital Costs**: The upfront cost of AI hardware is substantial. A trillion-parameter model requiring 320 A100 GPUs (at $32,000 each) would cost approximately $10 million for the hardware alone <a class="yt-timestamp" data-t="00:04:16">[00:04:16]</a>. This does not include the energy costs for running inference, which makes up 90% of a model's total cost <a class="yt-timestamp" data-t="00:04:44">[00:04:44]</a>.

Historically, the adoption of Dynamic Random Access Memory (DRAM) in the 1960s and 70s was driven by its low latency and cheap manufacturing <a class="yt-timestamp" data-t="00:05:01">[00:05:01]</a>. However, after 1980, compute scaling (focused on transistor density) far outpaced memory scaling, which had to optimize for capacity, bandwidth, and latency simultaneously <a class="yt-timestamp" data-t="00:05:26">[00:05:26]</a>. Over 20 years, memory capacity improved 128x, bandwidth 20x, but latency only 30% <a class="yt-timestamp" data-t="00:05:51">[00:05:51]</a>. Shrinking DRAM cells beyond a certain size also leads to worse performance, reliability, security, latency, and energy inefficiency due to increased leakage and vulnerability to noise <a class="yt-timestamp" data-t="00:06:02">[00:06:02]</a>.

## Historical Exploration
The idea of embedding compute capabilities within memory dates back decades:
*   **1960s**: Professor Harold Stone of Stanford University explored the concept of logic and memory, noting that while microprocessor transistor counts grew rapidly, communication with memory was limited by pin count <a class="yt-timestamp" data-t="00:09:03">[00:09:03]</a>. He proposed moving computation into memory caches <a class="yt-timestamp" data-t="00:09:25">[00:09:25]</a>.
*   **1990s**:
    *   **1995**: Terraces produced what is considered the first processor-in-memory chip, integrating a single-bit logical unit with a 4-bit memory <a class="yt-timestamp" data-t="00:09:33">[00:09:33]</a>.
    *   **1997**: UC Berkeley professors, including David Patterson, launched the IRAM project with the goal of putting a microprocessor and DRAM on the same chip <a class="yt-timestamp" data-t="00:09:53">[00:09:53]</a>.

These early proposals did not gain widespread adoption due to practical manufacturing reasons <a class="yt-timestamp" data-t="00:10:11">[00:10:11]</a>. Logic and memory fabrication have opposing goals: logic emphasizes speed and performance, while memory prioritizes high density, low cost, and low leakage <a class="yt-timestamp" data-t="00:10:16">[00:10:16]</a>. DRAM designs are highly regular with parallel wires, whereas logic designs are far more complex <a class="yt-timestamp" data-t="00:10:38">[00:10:38]</a>. Logic processes use 7-12 or more metal layers for complexity, while DRAM processes use 3-4 <a class="yt-timestamp" data-t="00:10:57">[00:10:57]</a>. Attempting to combine them results in significant drawbacks, such as logic circuits being 80% larger and 22% worse performing on a DRAM process, or embedded DRAM (eDRAM) being less reliable, using more power, and taking up to 10 times more space on a logic process <a class="yt-timestamp" data-t="00:11:07">[00:11:07]</a>.

## Modern Approaches to Compute in Memory
The industry has developed workarounds to address the manufacturing challenges, broadly categorized into three levels: device, circuit, and system <a class="yt-timestamp" data-t="00:11:34">[00:11:34]</a>.

### Device Level
This approach involves using new types of memory hardware beyond conventional DRAM and SRAM <a class="yt-timestamp" data-t="00:11:47">[00:11:47]</a>. Notable examples include:
*   **Resistive Random Access Memory (RRAM or ReRAM)**: Stores information by changing the electrical resistance of a material. This structure allows RRAM to compute logical functions directly within memory cells <a class="yt-timestamp" data-t="00:12:19">[00:12:19]</a>. RRAM is considered one of the most promising emerging memory technologies due to its compatibility with silicon CMOS, but substantial hurdles remain before commercialization <a class="yt-timestamp" data-t="00:12:42">[00:12:42]</a>.
*   **Spin-Transfer Torque Magnetoresistive Random Access Memory (STT-MRAM)** <a class="yt-timestamp" data-t="00:12:00">[00:12:00]</a>.

### Circuit Level
This involves modifying peripheral circuits to perform calculations directly inside SRAM or DRAM memory arrays, often referred to as "in-situ computing" <a class="yt-timestamp" data-t="00:12:57">[00:12:57]</a>.
*   **Ambit**: An in-memory accelerator proposed by researchers from Microsoft, Nvidia, Intel, ETH Zurich, and Carnegie Mellon <a class="yt-timestamp" data-t="00:13:21">[00:13:21]</a>. It leverages DRAM's internal bandwidth by activating three rows at a time (two for inputs, one for output) to implement AND/OR logic functions <a class="yt-timestamp" data-t="00:13:42">[00:13:42]</a>. While conceptually attractive, it takes multiple cycles to perform basic logic, and more complex logic remains challenging <a class="yt-timestamp" data-t="00:14:02">[00:14:02]</a>.

These device and circuit-level approaches still face challenges, with their performance generally falling short of [[ai_inference_versus_training_hardware | Von Neumann GPU ASIC]]-centric systems, leading to a "jack of all trades, master of none" situation <a class="yt-timestamp" data-t="00:14:16">[00:14:16]</a>.

### System Level
The industry is increasingly moving towards implementing compute in memory at a system level, integrating discrete processing units and memory in very close proximity <a class="yt-timestamp" data-t="00:14:39">[00:14:39]</a>.
*   **Advanced Packaging Technologies**: This is enabled by new [[chip_design_process_and_techniques | packaging technologies]] like 2.5D or 3D memory die stacking, where multiple DRAM memory dies are stacked on top of a CPU die <a class="yt-timestamp" data-t="00:14:53">[00:14:53]</a>. Connections are made using thousands of "through-silicon vias" (TSVs), providing immense internal bandwidth <a class="yt-timestamp" data-t="00:15:04">[00:15:04]</a>.
*   **AMD 3D V-Cache**: AMD is working on this concept with its 3D V-Cache, based on TSMC's 3D stacking technology, to add more memory cache to processors <a class="yt-timestamp" data-t="00:15:15">[00:15:15]</a>. This technology can be extended to add hundreds of gigabytes of memory to an AI ASIC, integrating world-class memory and logic dies closer than ever without being on the same die <a class="yt-timestamp" data-t="00:15:28">[00:15:28]</a>.

## Benefits and Future Outlook
Compute in memory is well-suited for deep learning, which involves massive matrix calculations <a class="yt-timestamp" data-t="00:08:20">[00:08:20]</a>. An ideal CIM chip can execute multiply-accumulate (MAC) operations directly inside the memory chip <a class="yt-timestamp" data-t="00:08:41">[00:08:41]</a>. This is particularly beneficial for running inference at the edge (outside data centers), where energy, size, or heat restrictions apply <a class="yt-timestamp" data-t="00:08:50">[00:08:50]</a>. Cutting up to 80% of a neural network's energy usage would be a significant advantage <a class="yt-timestamp" data-t="00:08:58">[00:08:58]</a>.

While current Nvidia A100 and H100 AI GPUs are formidable competitors <a class="yt-timestamp" data-t="00:15:56">[00:15:56]</a>, the slowing pace of leading-edge [[technological_advancements_in_semiconductor_manufacturing | semiconductor technology]] scaling necessitates new approaches to achieve more powerful and robust AI hardware <a class="yt-timestamp" data-t="00:16:04">[00:16:04]</a>. As bigger models generally perform better and current models still have room for improvement, they may need to grow larger <a class="yt-timestamp" data-t="00:16:14">[00:16:14]</a>. Unless new systems and hardware like compute in memory can overcome existing [[memory_limitations_in_deep_learning | memory limits]], deep learning might fall short of its full potential <a class="yt-timestamp" data-t="00:16:28">[00:16:28]</a>.