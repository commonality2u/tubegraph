---
title: Role of AI accelerators in neural network training and inference
videoId: L0948yq2Hqk
---

From: [[asianometry]] <br/> 

[[ai_accelerator_hardware_market_and_its_growth | AI accelerators]] are specialized hardware designed for running specific [[ai_and_ai_chip_boom | AI algorithms]] <a class="yt-timestamp" data-t="00:00:13">[00:00:13]</a>. While [[nvidia_and_ai_chip_competition | Nvidia's GPU evolution]] helped kick off the [[neural_network_revolution_and_role_of_gpus | neural network revolution]] <a class="yt-timestamp" data-t="00:00:02">[00:00:02]</a>, GPUs were not specifically designed for [[neural_network_revolution_and_role_of_gpus | neural network]] algorithms <a class="yt-timestamp" data-t="00:00:07">[00:00:07]</a>. This led companies to develop custom hardware for this purpose <a class="yt-timestamp" data-t="00:00:11">[00:00:11]</a>.

The [[ai_accelerator_hardware_market_and_its_growth | AI accelerator hardware market]] is estimated to be worth over $35 billion <a class="yt-timestamp" data-t="00:00:20">[00:00:20]</a>, with venture capitalists investing nearly $2 billion in [[ai_and_ai_chip_boom | AI chip]] startups in 2021 <a class="yt-timestamp" data-t="00:00:24">[00:00:24]</a>. TSMC views [[ai_accelerator_hardware_market_and_its_growth | AI accelerator hardware]] as a top secular driver for their future revenue <a class="yt-timestamp" data-t="00:00:32">[00:00:32]</a>.

## Evolution and Early Concepts
In 2011, a paper presented at the CVPR conference by New York University and Yale University discussed a scalable hardware architecture for big and deep [[neural_network_revolution_and_role_of_gpus | neural networks]] <a class="yt-timestamp" data-t="00:00:58">[00:00:58]</a>. At this time, GPUs were widely accepted as the de facto standard hardware for running and testing [[ai_and_ai_chip_boom | AI algorithms]] <a class="yt-timestamp" data-t="00:01:12">[00:01:12]</a>. However, the paper highlighted that custom hardware could significantly improve performance and [[energy_efficiency_in_ai_data_centers | power consumption]] by two orders of magnitude <a class="yt-timestamp" data-t="00:01:19">[00:01:19]</a>.

This paper proposed an architecture that processed each step in a [[neural_network_revolution_and_role_of_gpus | neural network]] in parallel, applying it to a computer vision algorithm <a class="yt-timestamp" data-t="00:01:42">[00:01:42]</a>. They implemented this on a Z-Link's Vertex 6 FPGA, a type of chip allowing custom configurations, and observed substantial improvements <a class="yt-timestamp" data-t="00:01:51">[00:01:51]</a>. A hypothetical 45-nanometer chip design also presented in the paper outperformed competitors significantly <a class="yt-timestamp" data-t="00:02:08">[00:02:08]</a>.

## Understanding Neural Networks and AI Accelerator Roles
At its core, a [[neural_network_revolution_and_role_of_gpus | neural network]] is a complex mathematical function that uses simple processing elements to model intricate relationships between many inputs <a class="yt-timestamp" data-t="00:02:30">[00:02:30]</a>. [[neural_network_revolution_and_role_of_gpus | Neural networks]] are represented with matrices <a class="yt-timestamp" data-t="00:02:43">[00:02:43]</a>. Processing involves turning input data (like an image) into a matrix, then multiplying this input matrix with weight matrices through various layers, and applying an activation function to the results <a class="yt-timestamp" data-t="00:02:44">[00:02:44]</a>.

[[ai_accelerator_hardware_market_and_its_growth | AI accelerators]] primarily serve two roles:
1.  **Training Phase**: Using large datasets to determine the optimal weight values for the network's layers <a class="yt-timestamp" data-t="00:03:11">[00:03:11]</a>.
2.  **Inference Stage**: Utilizing the trained [[neural_network_revolution_and_role_of_gpus | neural network]] function to infer a result from new input data <a class="yt-timestamp" data-t="00:03:21">[00:03:21]</a>. This is when the network is used to identify objects in a photo, for example <a class="yt-timestamp" data-t="00:03:35">[00:03:35]</a>.

[[ai_accelerator_hardware_market_and_its_growth | Neural network accelerators]] surpass generalized hardware like CPUs and GPUs because they are specifically tuned to handle matrix multiplication and convolution operations <a class="yt-timestamp" data-t="00:03:43">[00:03:43]</a>, which account for approximately 90% of the computational work in [[neural_network_revolution_and_role_of_gpus | neural networks]] <a class="yt-timestamp" data-t="00:03:51">[00:03:51]</a>. Given that some networks, as surveyed by [[impact_of_ai_chip_innovations_by_companies_like_google_and_amazon | Google]] in 2016, had up to 100 million weights <a class="yt-timestamp" data-t="00:03:56">[00:03:56]</a>, custom hardware becomes a logical necessity <a class="yt-timestamp" data-t="00:04:07">[00:04:07]</a>.

## Pioneers in AI Accelerator Development
A small community of [[ai_and_ai_chip_boom | AI hardware]] projects emerged following the 2011 paper <a class="yt-timestamp" data-t="00:04:12">[00:04:12]</a>.
*   **Dianna (Chinese Academy of Sciences)**: Launched in 2015, Dianna is a hardware piece tuned for computer vision-specific [[neural_network_revolution_and_role_of_gpus | neural networks]] <a class="yt-timestamp" data-t="00:04:24">[00:04:24]</a>. Their 65-nanometer accelerator consumed 400 times less [[energy_efficiency_in_ai_data_centers | energy]] than a GPU and was compact enough for embedding alongside commercial image sensors <a class="yt-timestamp" data-t="00:04:38">[00:04:38]</a>.
*   **IRIS (MIT/DARPA)**: The "flexible accelerator for mobile devices running [[neural_network_revolution_and_role_of_gpus | neural networks]]" was an MIT project focused on running inferences on images using pre-trained models <a class="yt-timestamp" data-t="00:04:57">[00:04:57]</a>. It achieved 10 times greater [[energy_efficiency_in_ai_data_centers | energy efficiency]] compared to contemporary GPUs <a class="yt-timestamp" data-t="00:05:11">[00:05:11]</a>.
*   **Teradeep (NNX processor)**: Founded by members of the original 2011 CVPR team, Teradeep aimed to enable deep learning capabilities in older webcams and hardware with their NNX processor <a class="yt-timestamp" data-t="00:05:17">[00:05:17]</a>.

These early projects primarily focused on inferring results from pre-trained models <a class="yt-timestamp" data-t="00:05:34">[00:05:34]</a>.

## Classification of AI Accelerators: Training vs. Inference, Edge vs. Server
The [[ai_accelerator_hardware_market_and_its_growth | AI hardware]] industry classifies accelerators along two main axes:
1.  **Training vs. Inference**: As discussed, whether the chip is used for training the model or for running the trained model <a class="yt-timestamp" data-t="00:05:52">[00:05:52]</a>.
2.  **Environment of Operation (Edge vs. Server)**:
    *   **Edge [[ai_accelerator_hardware_market_and_its_growth | AI chips]]**: Placed directly into devices like smartphones, cars, IoT devices, or wearables <a class="yt-timestamp" data-t="00:06:11">[00:06:11]</a>. Designers must consider power and size constraints <a class="yt-timestamp" data-t="00:06:18">[00:06:18]</a>.
    *   **Server Chips**: Used in data centers, where the primary concern is the cost-performance ratio <a class="yt-timestamp" data-t="00:06:24">[00:06:24]</a>. Power consumption is a substantial portion of the total cost of ownership, making [[energy_efficiency_in_ai_data_centers | energy efficiency]] crucial <a class="yt-timestamp" data-t="00:06:29">[00:06:29]</a>. These are typically higher-end products <a class="yt-timestamp" data-t="00:06:37">[00:06:37]</a>.

## Impact of Major Tech Companies: Google's TPU and Beyond
The [[ai_accelerator_hardware_market_and_its_growth | AI accelerator]] market gained wide prominence after [[impact_of_ai_chip_innovations_by_companies_like_google_and_amazon | Google]]'s announcement in 2016 <a class="yt-timestamp" data-t="00:07:01">[00:07:01]</a>.

### Google's Tensor Processing Unit (TPU)
[[impact_of_ai_chip_innovations_by_companies_like_google_and_amazon | Google]] began seriously developing its own chips in 2013, recognizing the immense computational demand of training and deploying [[neural_network_revolution_and_role_of_gpus | neural networks]] on their servers <a class="yt-timestamp" data-t="00:07:14">[00:07:14]</a>. This led to the Tensor Processing Unit (TPU) <a class="yt-timestamp" data-t="00:07:28">[00:07:28]</a>. The first TPUs, quickly designed and fabricated with a 28-nanometer process, entered [[impact_of_ai_chip_innovations_by_companies_like_google_and_amazon | Google's]] data centers in 2015 and were publicly unveiled in May 2016 <a class="yt-timestamp" data-t="00:07:32">[00:07:32]</a>.

TPUs are Application Specific Integrated Circuits (ASICs) <a class="yt-timestamp" data-t="00:07:45">[00:07:45]</a>, primarily specialized for matrix multiplication <a class="yt-timestamp" data-t="00:07:51">[00:07:51]</a>. At its heart is a Matrix Multiply Unit (MMU) containing 65,536 multiply-accumulator (MAC) circuits arranged in a 256x256 array <a class="yt-timestamp" data-t="00:07:55">[00:07:55]</a>. These MAC units simply multiply two numbers and add them to an accumulation sum <a class="yt-timestamp" data-t="00:08:09">[00:08:09]</a>. The MMU processes both input data for inference and the millions of weights loaded from memory <a class="yt-timestamp" data-t="00:08:19">[00:08:19]</a>.

Compared to contemporary server chips like the 18-core Intel Haswell CPU and [[nvidia_and_ai_chip_competition | Nvidia]] Kepler K80 GPU, the TPU has significantly more MAC units and on-chip memory for intermediate results <a class="yt-timestamp" data-t="00:08:43">[00:08:43]</a>. While CPUs and GPUs handle generalized tasks, the TPU's specialized design means [[impact_of_ai_chip_innovations_by_companies_like_google_and_amazon | Google's]] first-generation TPU could run inference 50 to 30 times faster and with 30 to 80 times better [[energy_efficiency_in_ai_data_centers | energy efficiency]] <a class="yt-timestamp" data-t="00:09:00">[00:09:00]</a>. Recent TPU iterations are capable of trillions of floating-point operations per second <a class="yt-timestamp" data-t="00:09:16">[00:09:16]</a>.

[[impact_of_ai_chip_innovations_by_companies_like_google_and_amazon | Google]]'s investment in TPUs saves them millions by reducing their reliance on CPUs and GPUs for data centers <a class="yt-timestamp" data-t="00:09:43">[00:09:43]</a>. Their control over their own machine learning software framework, TensorFlow, allows for precise customization of hardware to software <a class="yt-timestamp" data-t="00:09:55">[00:09:55]</a>. This end-to-end system enables features like object searches across years of uploaded [[impact_of_ai_chip_innovations_by_companies_like_google_and_amazon | Google]] Photos <a class="yt-timestamp" data-t="00:10:06">[00:10:06]</a>. Although [[impact_of_ai_chip_innovations_by_companies_like_google_and_amazon | Google]] doesn't sell TPUs, their computing power can be rented via [[impact_of_ai_chip_innovations_by_companies_like_google_and_amazon | Google]] Cloud Service <a class="yt-timestamp" data-t="00:10:16">[00:10:16]</a>. [[impact_of_ai_chip_innovations_by_companies_like_google_and_amazon | Google]] essentially pioneered the server [[ai_accelerator_hardware_market_and_its_growth | AI ASIC industry]] <a class="yt-timestamp" data-t="00:10:27">[00:10:27]</a>.

### Other Server-Side AI Accelerators
Other companies have quickly followed [[impact_of_ai_chip_innovations_by_companies_like_google_and_amazon | Google's]] lead:
*   **[[impact_of_ai_chip_innovations_by_companies_like_google_and_amazon | Amazon]] Trainium**: As the largest cloud hyperscaler with AWS, [[impact_of_ai_chip_innovations_by_companies_like_google_and_amazon | Amazon]] announced their Trainium devices, specialized [[ai_accelerator_hardware_market_and_its_growth | AI training accelerators]], in December 2020 <a class="yt-timestamp" data-t="00:10:33">[00:10:33]</a>.
*   **Habana (Intel)**: Founded in 2016 with a data center product for training machine learning models, Habana was acquired by Intel in December 2019 for $2 billion <a class="yt-timestamp" data-t="00:11:04">[00:11:04]</a>.
*   **[[nvidia_and_ai_chip_competition | Nvidia]] Tesla V100**: [[nvidia_and_ai_chip_competition | Nvidia]] also offers its own custom [[ai_and_ai_chip_boom | AI hardware]], like the Tesla V100, which is an [[ai_accelerator_hardware_market_and_its_growth | AI processor]] with over 20 billion transistors and 5,120 cores <a class="yt-timestamp" data-t="00:11:20">[00:11:20]</a>. [[nvidia_and_ai_chip_competition | Nvidia's]] strong software competitive advantage makes them a formidable player <a class="yt-timestamp" data-t="00:11:30">[00:11:30]</a>.

This intense competition from tech giants has made the server [[ai_accelerator_hardware_market_and_its_growth | AI market]] challenging for startups <a class="yt-timestamp" data-t="00:11:37">[00:11:37]</a>.

## Edge AI Chips and Smartphone Integration
Half of the [[ai_and_ai_chip_boom | AI chip]] market is in specialized [[ai_accelerator_hardware_market_and_its_growth | AI chips]] for mobile phones <a class="yt-timestamp" data-t="00:12:05">[00:12:05]</a>. These chips often handle the increasingly computation-heavy image processing operations in smartphones, like transforming raw image data into a finished photo <a class="yt-timestamp" data-t="00:12:12">[00:12:12]</a>, a process that mirrors a [[role_of_ai_in_smartphone_photography | phone processor's]] steps <a class="yt-timestamp" data-t="00:01:32">[00:01:32]</a>.

Mobile CPU processors are also expanding their chips with on-board [[neural_network_revolution_and_role_of_gpus | neural networking]] hardware to capture this market <a class="yt-timestamp" data-t="00:12:19">[00:12:19]</a>.
*   **Apple's Neural Engine**: In 2017, the iPhone X A11 Bionic chip from Apple incorporated a dedicated [[neural_network_revolution_and_role_of_gpus | neural network]] hardware called the Neural Engine <a class="yt-timestamp" data-t="00:12:29">[00:12:29]</a>. Its first version had two cores and could perform 600 billion operations per second <a class="yt-timestamp" data-t="00:12:39">[00:12:39]</a>. Apple's control over its software stack facilitates ideal integration <a class="yt-timestamp" data-t="00:12:46">[00:12:46]</a>. Initially used for Face ID and Animojis, Apple later opened up the second version of the Neural Engine to developers via its Core ML API <a class="yt-timestamp" data-t="00:12:52">[00:12:52]</a>.
*   **MediaTek**: Taiwanese fabless chip maker MediaTek is also adding dedicated [[ai_and_ai_chip_boom | AI functionality]] to its chips, such as in their Dimensity 5G mobile processors <a class="yt-timestamp" data-t="00:13:02">[00:13:02]</a>.

Opportunities for inference on the edge still exist in areas like IoT devices and autonomous driving <a class="yt-timestamp" data-t="00:13:14">[00:13:14]</a>.

## The Future of AI Accelerators
Despite specialized architectures, [[ai_accelerator_hardware_market_and_its_growth | AI accelerators]] can still take many hours to train a single machine learning model for production <a class="yt-timestamp" data-t="00:13:35">[00:13:35]</a>.

### Beyond Von Neumann Architecture
Traditional data center servers use a Von Neumann architecture, separating the CPU (processor) and memory <a class="yt-timestamp" data-t="00:13:49">[00:13:49]</a>. This separation slows down [[neural_network_revolution_and_role_of_gpus | neural network]] processing, which is highly parallel <a class="yt-timestamp" data-t="00:14:05">[00:14:05]</a>. One suggestion for future [[ai_accelerator_hardware_market_and_its_growth | AI chips]] is to implement them using silicon photonics <a class="yt-timestamp" data-t="00:14:42">[00:14:42]</a>. Silicon photonics uses light instead of electricity to send signals, allowing for faster speeds and no additional heating concerns <a class="yt-timestamp" data-t="00:14:21">[00:14:21]</a>.

### Comparison to Bitcoin Mining Hardware
The [[comparison_of_ai_accelerator_industry_with_bitcoin_mining | AI accelerator hardware industry]] can be compared to the [[comparison_of_ai_accelerator_industry_with_bitcoin_mining | Bitcoin mining hardware industry]] <a class="yt-timestamp" data-t="00:14:44">[00:14:44]</a>. Both started with CPUs, then GPUs, and eventually transitioned to increasingly powerful ASICs <a class="yt-timestamp" data-t="00:14:50">[00:14:50]</a>. For Bitcoin miners, the next step has typically been to move to more advanced nodes <a class="yt-timestamp" data-t="00:14:58">[00:14:58]</a>, which benefits large chip companies like [[nvidia_and_ai_chip_competition | Nvidia]] due to their access to resources <a class="yt-timestamp" data-t="00:15:08">[00:15:08]</a>.

However, future [[ai_accelerator_hardware_market_and_its_growth | AI accelerators]] may not necessarily follow the same path of relying solely on advanced nodes <a class="yt-timestamp" data-t="00:15:05">[00:15:05]</a>. The more interesting approaches will likely come from companies finding new ways to achieve similar inference or training results without requiring an advanced node, possibly through innovations like silicon photonics or advanced parallelism <a class="yt-timestamp" data-t="00:15:16">[00:15:16]</a>.