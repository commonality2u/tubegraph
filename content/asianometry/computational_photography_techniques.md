---
title: Computational photography techniques
videoId: yY8OFp0-UZw
---

From: [[asianometry]] <br/> 

Modern smartphones, such as the iPhone 13 Pro, feature highly advanced cameras that are often a key selling point for upgrades <a class="yt-timestamp" data-t="00:00:04">[00:00:04]</a>. These devices are capable of taking images comparable to standalone cameras <a class="yt-timestamp" data-t="00:00:30">[00:00:30]</a>. Major smartphone manufacturers invest significantly in this area, with Apple employing 800 people on the iPhone's camera in 2015 <a class="yt-timestamp" data-t="00:00:35">[00:00:35]</a>, and Xiaomi announcing thousands of hires for their phone cameras in 2021 <a class="yt-timestamp" data-t="00:00:43">[00:00:43]</a>. This impressive capability is largely due to sophisticated computer and semiconductor engineering <a class="yt-timestamp" data-t="00:00:57">[00:00:57]</a>.

## [[traditional_vs_digital_cameras | Traditional vs Digital Cameras]] and Basic Structure
Unlike traditional analog cameras that expose photographic film to light <a class="yt-timestamp" data-t="00:01:31">[00:01:31]</a>, today's smartphone cameras operate fundamentally differently <a class="yt-timestamp" data-t="00:01:41">[00:01:41]</a>. A basic [[digital_photography_disruption | digital camera]] module consists of a lens placed on top of an image sensor chip <a class="yt-timestamp" data-t="00:01:47">[00:01:47]</a>. Additional components like sensors and actuators may be added to enhance performance <a class="yt-timestamp" data-t="00:01:54">[00:01:54]</a>.

### [[challenges_in_smartphone_camera_engineering | Challenges in Smartphone Camera Engineering]]
Smartphones face unique [[challenges_in_smartphone_camera_engineering | engineering challenges]] due to their compact size compared to larger [[digital_photography_disruption | digital photography equipment]] like DSLRs <a class="yt-timestamp" data-t="00:02:06">[00:02:06]</a>.
*   **Size Constraints**: Camera modules are typically 7 to 10 millimeters thick <a class="yt-timestamp" data-t="00:02:20">[00:02:20]</a>, necessitating small sensors (around 5mm x 4mm) <a class="yt-timestamp" data-t="00:02:30">[00:02:30]</a>. Larger sensors, like those in DSLRs (up to 36mm x 24mm), are preferred as they capture more light, reducing motion blur, noise, and improving dynamic range <a class="yt-timestamp" data-t="00:02:37">[00:02:37]</a>.
*   **Limited Optics**: Smartphone optics are smaller and less adjustable than DSLR lenses <a class="yt-timestamp" data-t="00:02:55">[00:02:55]</a>.
    *   **Fixed and Limited Aperture**: A smaller aperture means less light reaching the sensor <a class="yt-timestamp" data-t="00:03:09">[00:03:09]</a>.
    *   **Limited Zoom Function**: Traditional optical zoom, which involves physically moving the lens, is difficult to implement in thin smartphones <a class="yt-timestamp" data-t="00:03:24">[00:03:24]</a>.

### Evolution of Image Sensors
The concept of charge-coupled devices (CCDs) was first proposed by William Boyle and George Smith at Bell Labs in 1969 <a class="yt-timestamp" data-t="00:03:58">[00:03:58]</a>, earning them a Nobel Prize in Physics in 2009 <a class="yt-timestamp" data-t="00:04:10">[00:04:10]</a>. While CCDs initially dominated mobile imaging, complementary metal-oxide-semiconductor (CMOS) sensors emerged as a more promising alternative <a class="yt-timestamp" data-t="00:04:21">[00:04:21]</a>. CMOS sensors consume less power, can be manufactured by conventional foundries like TSMC, and are less expensive <a class="yt-timestamp" data-t="00:04:30">[00:04:30]</a>, leading to their dominance in the imaging industry, particularly in smartphones <a class="yt-timestamp" data-t="00:04:41">[00:04:41]</a>. The imaging chip business was estimated to be $20 billion in 2020, dominated by Samsung, Sony, and Omnivision <a class="yt-timestamp" data-t="00:04:53">[00:04:53]</a>.

Despite advancements, physical constraints limit how far image sensor technology can progress <a class="yt-timestamp" data-t="00:05:05">[00:05:05]</a>. This is where the powerful computing capabilities of modern smartphones come into play <a class="yt-timestamp" data-t="00:03:44">[00:03:44]</a>, driving the field of computational photography <a class="yt-timestamp" data-t="00:05:18">[00:05:18]</a>.

## The Image Processing Pipeline
When a photo is taken with a smartphone, it goes through an elaborate image processing pipeline <a class="yt-timestamp" data-t="00:05:32">[00:05:32]</a>. Each manufacturer has its unique pipeline, modifying parameters and algorithms to differentiate their camera performance <a class="yt-timestamp" data-t="00:05:41">[00:05:41]</a>.

1.  **Photodiodes and Color Filters**: The camera image sensor is a 2D grid of photodiodes, which convert photons into electrical charges <a class="yt-timestamp" data-t="00:05:59">[00:05:59]</a>. These charges don't inherently correspond to color, so each photodiode has a color filter laid on top <a class="yt-timestamp" data-t="00:06:09">[00:06:09]</a>. Smartphones often use dedicated image processing hardware to map these charges to the correct colors <a class="yt-timestamp" data-t="00:06:17">[00:06:17]</a>. The filter arrangement is known as a Bayer filter, named after Bryce Bayer of Kodak who proposed it in 1975 <a class="yt-timestamp" data-t="00:06:30">[00:06:30]</a>. Modern manufacturers group color filters into multi-cell pixel clusters (macro pixels) for enhanced light sensitivity <a class="yt-timestamp" data-t="00:06:38">[00:06:38]</a>.
2.  **Demosaicing Algorithms**: The raw output, called the Bayer pattern image, appears distorted or like a mosaic <a class="yt-timestamp" data-t="00:06:49">[00:06:49]</a> because each photodiode captures only one color <a class="yt-timestamp" data-t="00:07:00">[00:07:00]</a>. The image processor uses "demosaicing algorithms" to fill in these gaps and reconstruct the scene's actual color <a class="yt-timestamp" data-t="00:07:04">[00:07:04]</a>. The simplest algorithms infer missing RGB values from adjacent pixels <a class="yt-timestamp" data-t="00:07:17">[00:07:17]</a>. This process means about two-thirds of the image data is computer-generated, mimicking the human vision system where the brain compares signals from different cone types to create color vision <a class="yt-timestamp" data-t="00:07:32">[00:07:32]</a>.
3.  **White Balance Correction**: The image is then processed for white balance, correcting colors to appear as if lit by a neutral white light <a class="yt-timestamp" data-t="00:08:06">[00:08:06]</a>. This involves algorithms that estimate the scene's illumination and how the sensor's color filter responds to it, applying an "illumination value" to the pixels' RGB values <a class="yt-timestamp" data-t="00:08:31">[00:08:31]</a>.
4.  **Proprietary Color Manipulation**: Image processors may manipulate colors in proprietary ways, either based on user settings (e.g., "vivid" mode) or pre-programmed by the vendor to create a distinct look (e.g., Samsung photos vs. iPhones) <a class="yt-timestamp" data-t="00:08:43">[00:08:43]</a>.
5.  **Denoising Algorithms**: Another algorithm is applied to reduce image noise â€“ artifacts not present in the original scene <a class="yt-timestamp" data-t="00:09:09">[00:09:09]</a>. Balancing noise reduction is critical; too much can make an image look overly smooth or fake <a class="yt-timestamp" data-t="00:09:19">[00:09:19]</a>.
6.  **Resizing and Saving**: Finally, the image processor resizes the data, adjusts RGB values for screen display, and saves it in formats like JPEG, PNG, or HEIC <a class="yt-timestamp" data-t="00:09:39">[00:09:39]</a>.

## Porting Optical Features to Smartphones with Computation

### Zoom
Optical zoom, which involves physically moving lenses, is difficult in thin smartphones <a class="yt-timestamp" data-t="00:10:02">[00:10:02]</a>. Attempts by companies like Samsung, Nokia, and Asus resulted in bulky camera bumps with disappointing performance <a class="yt-timestamp" data-t="00:10:17">[00:10:17]</a>.
*   **Digital Zoom**: Most smartphones offer digital zoom, which crops the original image and upscales the remainder <a class="yt-timestamp" data-t="00:10:30">[00:10:30]</a>. This often weakens image resolution <a class="yt-timestamp" data-t="00:10:40">[00:10:40]</a>. Algorithms are used to try and enhance missing details, though a simple approach of filling in details using nearby pixel data hasn't been sufficient <a class="yt-timestamp" data-t="00:10:46">[00:10:46]</a>.
*   **Multiple Rear Cameras**: The prevalent approach is to include multiple rear cameras, such as a wide camera and a telephoto camera <a class="yt-timestamp" data-t="00:11:00">[00:11:00]</a>. These "dual aperture zoom cameras" were introduced by Corephotonics in 2014 and adopted by leading phone makers like Apple and Xiaomi <a class="yt-timestamp" data-t="00:11:12">[00:11:12]</a>, allowing users to swap lenses for a zoom effect <a class="yt-timestamp" data-t="00:11:26">[00:11:26]</a>.
*   **Folded Zoom**: Some high-end phones from Oppo, Samsung, and Huawei use a "folded zoom" design <a class="yt-timestamp" data-t="00:11:30">[00:11:30]</a>. This involves a 45-degree mirror to bend light sideways, enabling zoom without increasing camera thickness <a class="yt-timestamp" data-t="00:11:35">[00:11:35]</a>.

### Low Light and Night Photography
[[smartphone_camera_advancements | Smartphone camera advancements]] have significantly pushed the boundaries of computational photography, especially in low light and nighttime performance <a class="yt-timestamp" data-t="00:11:56">[00:11:56]</a>. Unlike DSLRs with larger pixels and adjustable apertures that can capture enough light with tripods <a class="yt-timestamp" data-t="00:12:27">[00:12:27]</a>, smartphones leverage computational techniques.
*   **Burst Processing / Image Stacking**: Smartphone cameras capture and merge many image frames <a class="yt-timestamp" data-t="00:12:45">[00:12:45]</a>. This technique, rooted in astrophotography, involves "image stacking" to reduce noise and produce better images <a class="yt-timestamp" data-t="00:12:51">[00:12:51]</a>. The camera continuously captures frames, and when the shutter is pressed, it selects a key frame and merges others to create a single high-quality image <a class="yt-timestamp" data-t="00:13:00">[00:13:00]</a>. Challenges include reliably aligning image sequences to avoid distortion <a class="yt-timestamp" data-t="00:13:15">[00:13:15]</a>.
*   **Google Pixel's Night Sight**: The 2018 Google Pixel phone pioneered this breakthrough with its Night Sight feature <a class="yt-timestamp" data-t="00:13:27">[00:13:27]</a>. Camera designers put effort into accommodating jittery hands and moving objects that could cause blur, yielding impressive results <a class="yt-timestamp" data-t="00:13:33">[00:13:33]</a>. Burst photography is now a general tool for denoising, increasing resolution, and high dynamic range (HDR) compression <a class="yt-timestamp" data-t="00:13:48">[00:13:48]</a>.

## [[role_of_ai_in_smartphone_photography | Role of AI in Smartphone Photography]]
Recent advancements in onboard AI processor technology have enabled cameras to achieve new heights in imaging performance <a class="yt-timestamp" data-t="00:13:56">[00:13:56]</a>.

*   **AI for White Balance Correction**: Researchers collected photos, had professional photographers manually white balance them, and then fed this data into a machine learning model to create more effective color constancy algorithms <a class="yt-timestamp" data-t="00:14:06">[00:14:06]</a>. This method is particularly effective in low-light conditions and was a prominent feature in Google Pixel's Night Sight <a class="yt-timestamp" data-t="00:14:21">[00:14:21]</a>.
*   **AI for Sharpening Digitally Zoomed Images**: Building on simple algorithms for sharpening digital zoom blur, machine learning models are trained with high and low-resolution imagery to properly sharpen and enhance blurry edges <a class="yt-timestamp" data-t="00:14:31">[00:14:31]</a>. NVIDIA applies similar concepts to upscale video game assets using Deep Learning Super Sampling (DLSS) <a class="yt-timestamp" data-t="00:14:48">[00:14:48]</a>.
*   **AI for Bokeh (Portrait Mode)**: Traditional smartphone cameras struggle to achieve shallow depth of field (bokeh) where a subject is in focus and the background is blurred <a class="yt-timestamp" data-t="00:15:02">[00:15:02]</a>. Computational photography allows mobile phones to generate synthetic bokeh <a class="yt-timestamp" data-t="00:15:14">[00:15:14]</a>. Modern cameras use a second camera or a dedicated depth sensor to determine subject distance, then introduce depth blur to simulate the effect <a class="yt-timestamp" data-t="00:15:19">[00:15:19]</a>. This is the basis of iPhone's Portrait Mode, which uses AI to recognize people or animals and blur the rest of the scene <a class="yt-timestamp" data-t="00:15:30">[00:15:30]</a>.

## Conclusion
Modern [[smartphone_camera_advancements | smartphone camera advancements]] heavily rely on computer processing and image manipulation to create digital photos <a class="yt-timestamp" data-t="00:15:55">[00:15:55]</a>. Whether through simple mathematical operations or complex machine learning models, the image data saved and uploaded is significantly "doctored" from what the sensor actually "sees" <a class="yt-timestamp" data-t="00:16:06">[00:16:06]</a>. This transformation means that while the captured scene is real, the smartphone image of it has become increasingly less so over time <a class="yt-timestamp" data-t="00:16:44">[00:16:44]</a>, driven by the desire for visually appealing results <a class="yt-timestamp" data-t="00:16:56">[00:16:56]</a>.