---
title: Memory wall problem in AI hardware
videoId: 5tmGKTNW8DQ
---

From: [[asianometry]] <br/> 

Since the deep learning explosion began in 2012, the largest models in the industry have expanded hundreds of thousands of times <a class="yt-timestamp" data-t="00:00:02">[00:00:02]</a>. For example, OpenAI's DALL-E 2 has 3.5 billion parameters, Google's Imagen has 4.6 billion, and GPT-3 has 175 billion <a class="yt-timestamp" data-t="00:00:10">[00:00:10]</a>. Google recently pre-trained a model with 1 trillion parameters <a class="yt-timestamp" data-t="00:00:25">[00:00:25]</a>.

These increasingly large models put a significant strain on the ability of current hardware to accommodate them <a class="yt-timestamp" data-t="00:00:28">[00:00:28]</a>. Many of these limitations relate to memory and its usage <a class="yt-timestamp" data-t="00:00:34">[00:00:34]</a>. This challenge is known as the "memory wall" problem or a memory capacity bottleneck <a class="yt-timestamp" data-t="00:02:54">[00:02:54]</a>.

## Von Neumann Architecture Limitations

Virtually every modern computer operates on a [[von_neumann_architecture_limitations | Von Neumann architecture]], which stores both instructions and data in the same memory bank <a class="yt-timestamp" data-t="00:01:00">[00:01:00]</a>. At its core, processing units like CPUs or GPUs access memory to execute instructions and process data <a class="yt-timestamp" data-t="00:01:09">[00:01:09]</a>. While this architecture has been highly successful, enabling powerful software, it differs significantly from how the human brain functions <a class="yt-timestamp" data-t="00:01:21">[00:01:21]</a>.

The brain tightly integrates its relatively low-precision compute ability with memory and input/output communication <a class="yt-timestamp" data-t="00:01:31">[00:01:31]</a>. In contrast, computers use high-precision arithmetic (e.g., 32 or 64-bit floating point) but separate computation from memory and communication <a class="yt-timestamp" data-t="00:01:40">[00:01:40]</a>. This separation has significant consequences, especially for memory <a class="yt-timestamp" data-t="00:01:51">[00:01:51]</a>.

## The Memory Wall Problem

The [[ai_and_ai_chip_boom | AI hardware]] industry is rapidly scaling up memory and processing unit performance <a class="yt-timestamp" data-t="00:01:55">[00:01:55]</a>. For instance, [[Nvidia and AI chip competition | Nvidia's]] V100 GPU in 2017 offered 32 gigabytes, while today's top-tier [[Nvidia and AI chip competition | Nvidia]] data center GPUs, A100 and H100, boast 80 gigabytes of memory <a class="yt-timestamp" data-t="00:02:01">[00:02:01]</a>. However, hardware performance is not keeping pace with the rapid growth of [[scaling_laws_in_ai | AI models]], particularly concerning memory <a class="yt-timestamp" data-t="00:02:15">[00:02:15]</a>.

Memory allocations for leading-edge models can easily exceed hundreds of gigabytes <a class="yt-timestamp" data-t="00:02:23">[00:02:23]</a>. A trillion-parameter model, even with the latest parallelization techniques, is estimated to require 320 A100 GPUs, each with 80 gigabytes of memory <a class="yt-timestamp" data-t="00:02:28">[00:02:28]</a>. This mismatch between processing and capacity means that a processing unit wastes multiple cycles waiting for data to travel to and from memory, and for memory to perform read/write operations <a class="yt-timestamp" data-t="00:02:39">[00:02:39]</a>.

### Practical and Technological Limits

Simply adding more memory to GPUs is not a straightforward solution due to practical and technological limits <a class="yt-timestamp" data-t="00:03:00">[00:03:00]</a>. Issues include connections and wiring, similar to how widening a highway doesn't entirely solve traffic <a class="yt-timestamp" data-t="00:03:10">[00:03:10]</a>.

Furthermore, significant [[energy_efficiency_in_ai_data_centers | energy limitations]] are associated with shuttling data between the chip and memory <a class="yt-timestamp" data-t="00:03:19">[00:03:19]</a>. Electrical connections incur losses, costing energy <a class="yt-timestamp" data-t="00:03:26">[00:03:26]</a>. Accessing off-chip memory uses 200 times more energy than a floating-point operation <a class="yt-timestamp" data-t="00:03:30">[00:03:30]</a>. Eighty percent of the Google TPU's energy usage stems from its electrical connections, not its computational units <a class="yt-timestamp" data-t="00:03:39">[00:03:39]</a>. In some GPU and CPU systems, DRAM alone accounts for 40% of total system power <a class="yt-timestamp" data-t="00:03:47">[00:03:47]</a>. Given that energy constitutes 40% of a data center's operating costs, storage and memory are significant factors in their ongoing profitability <a class="yt-timestamp" data-t="00:03:54">[00:03:54]</a>.

Beyond operating costs, there are substantial upfront capital costs. A trillion-parameter model requiring 320 A100 GPUs, each costing $32,000 MSRP, would amount to $10 million for hardware alone <a class="yt-timestamp" data-t="00:04:16">[00:04:16]</a>. A hundred-trillion-parameter model might need over 6,000 such GPUs <a class="yt-timestamp" data-t="00:04:29">[00:04:29]</a>. This figure does not include the energy costs for running inference, which constitutes 90% of a model's total cost <a class="yt-timestamp" data-t="00:04:44">[00:04:44]</a>. This situation risks restricting the benefits of advanced [[ai_and_ai_chip_boom | AI]] to only the wealthiest tech giants or governments <a class="yt-timestamp" data-t="00:04:48">[00:04:48]</a>.

### Historical Context of Memory Scaling

Many shortcomings are rooted in historical and technological limits <a class="yt-timestamp" data-t="00:04:55">[00:04:55]</a>. In the 1960s and 70s, the industry adopted Dynamic Random Access Memory (DRAM) as the basis for computers due to its relatively low latency and cheap bulk manufacturing <a class="yt-timestamp" data-t="00:04:59">[00:04:59]</a>.

While this worked for a time, with the memory industry valued at $37 billion in 1995 compared to microprocessors at $20 billion <a class="yt-timestamp" data-t="00:05:16">[00:05:16]</a>, after 1980, compute scaling far outpaced memory scaling <a class="yt-timestamp" data-t="00:05:26">[00:05:26]</a>. The CPU/GPU industries primarily optimized for transistor density <a class="yt-timestamp" data-t="00:05:32">[00:05:32]</a>. The memory industry, however, had to scale DRAM capacity, bandwidth, and latency simultaneously <a class="yt-timestamp" data-t="00:05:40">[00:05:40]</a>. This led to latency improvements lagging significantly: over 20 years, memory capacity improved 128 times and bandwidth 20 times, but latency only 30 percent <a class="yt-timestamp" data-t="00:05:51">[00:05:51]</a>.

A second challenge arose when shrinking DRAM cells beyond a certain size led to worse performance, reliability, security, and energy inefficiency <a class="yt-timestamp" data-t="00:06:02">[00:06:02]</a>. A DRAM cell stores a bit of data as a charge in a capacitor, accessed by a transistor <a class="yt-timestamp" data-t="00:06:16">[00:06:16]</a>. As cells scale down to nanoscale, the capacitor and transistor become leakier and more vulnerable to electrical noise, also opening new security vulnerabilities <a class="yt-timestamp" data-t="00:06:31">[00:06:31]</a>. These fundamental technical limitations make engineering solutions extremely difficult <a class="yt-timestamp" data-t="00:06:47">[00:06:47]</a>.

## Proposed Solutions: Compute in Memory

The [[technological_innovations_and_challenges_in_supercomputer_design | memory wall problem]] opens the door to radical ideas that could offer a 10x improvement over current paradigms <a class="yt-timestamp" data-t="00:07:00">[00:07:00]</a>. One such idea is to alleviate or even eliminate the [[von_neumann_architecture_limitations | Von Neumann bottleneck]] and memory wall problems by making memory perform computations itself <a class="yt-timestamp" data-t="00:07:21">[00:07:21]</a>.

[[Compute in memory innovations | Compute in memory]] refers to a Random Access Memory (RAM) with processing elements integrated, either very near each other or on the same die <a class="yt-timestamp" data-t="00:07:30">[00:07:30]</a>. Other terms for this concept include processing-in-memory, computational RAM, near-data computing, and memory-centric computing <a class="yt-timestamp" data-t="00:07:42">[00:07:42]</a>. While the name can also refer to concepts expanding on the SRAM cache idea, the focus here is on bringing processing ability to the computer's main memory itself <a class="yt-timestamp" data-t="00:08:03">[00:08:03]</a>.

This idea is particularly well-suited for deep learning because running a neural network model primarily involves calculating massive matrices, which are essentially numerous multiply-and-accumulate (MAC) operations <a class="yt-timestamp" data-t="00:08:20">[00:08:20]</a>. The arithmetic is simple, but its sheer volume makes it problematic <a class="yt-timestamp" data-t="00:08:35">[00:08:35]</a>. An ideal [[compute_in_memory_innovations | compute in memory]] chip could execute MAC operations directly within the memory chip <a class="yt-timestamp" data-t="00:08:41">[00:08:41]</a>. This would be especially beneficial for running inference on the edge, outside the data center, where energy, size, or heat restrictions are critical, potentially cutting a neural network's energy usage by up to 80 percent <a class="yt-timestamp" data-t="00:08:48">[00:08:48]</a>.

### Historical Attempts

The concept of [[compute_in_memory_innovations | compute in memory]] is decades old, dating back to the 1960s <a class="yt-timestamp" data-t="00:09:03">[00:09:03]</a>. Professor Harold Stone of Stanford University notably explored the idea of logic in memory, observing that while microprocessor transistors grew fast, processor communication with memory was limited by pin count <a class="yt-timestamp" data-t="00:09:06">[00:09:06]</a>. He proposed moving part of the computation into memory caches <a class="yt-timestamp" data-t="00:09:25">[00:09:25]</a>.

The 1990s saw further exploration. In 1995, Terrasys produced what might be considered the first processor-in-memory chip: a 4-bit memory with an integrated single-bit logical unit <a class="yt-timestamp" data-t="00:09:30">[00:09:30]</a>. In 1997, UC Berkeley professors, including David Patterson, created the IRAM project to put a microprocessor and DRAM on the same chip <a class="yt-timestamp" data-t="00:09:53">[00:09:53]</a>.

However, none of these proposals caught on due to practical reasons <a class="yt-timestamp" data-t="00:10:09">[00:10:09]</a>:
*   **Manufacturing Difficulty**: Memory and logic are challenging to manufacture together due to opposing goals <a class="yt-timestamp" data-t="00:10:14">[00:10:14]</a>. Logic transistors prioritize speed and performance, while memory transistors demand high density, low cost, and low leakage <a class="yt-timestamp" data-t="00:10:22">[00:10:22]</a>.
*   **Design Differences**: DRAM designs are very regular with many parallel wires, while logic designs are far more complex <a class="yt-timestamp" data-t="00:10:38">[00:10:38]</a>.
*   **Metal Layers**: Logic processes use significantly more metal layers (7 to 12+) for complexity compared to contemporary DRAM processes (3 to 4) <a class="yt-timestamp" data-t="00:10:46">[00:10:46]</a>. Trying to make logic with a DRAM process results in 80% larger, 22% worse logic circuits <a class="yt-timestamp" data-t="00:11:07">[00:11:07]</a>. Conversely, making DRAM with a logic process (embedded DRAM or eDRAM) leads to cells that use more power, take 10 times more space, and are less reliable <a class="yt-timestamp" data-t="00:11:17">[00:11:17]</a>.

### Current Approaches to Compute in Memory

The industry has considered these manufacturing shortcomings and developed workarounds, leading to [[compute_in_memory_innovations | compute in memory]] proposals on three levels: device, circuit, and system <a class="yt-timestamp" data-t="00:11:34">[00:11:34]</a>.

1.  **Device Level**: This approach leverages new types of memory hardware beyond conventional DRAM and SRAM <a class="yt-timestamp" data-t="00:11:47">[00:11:47]</a>.
    *   **Resistive Random Access Memory (ReRAM or RRAM)**: This emerging technology stores information by changing the electrical resistance of a material, allowing it to compute logical functions directly within memory cells <a class="yt-timestamp" data-t="00:12:08">[00:12:08]</a>. ReRAM is one of the closest to commercialization due to its compatibility with silicon CMOS, though substantial hurdles remain <a class="yt-timestamp" data-t="00:12:42">[00:12:42]</a>.
    *   **Spin-Transfer Torque Magnetoresistive RAM (STT-MRAM)** <a class="yt-timestamp" data-t="00:12:00">[00:12:00]</a>.

2.  **Circuit Level**: This involves modifying peripheral circuits to perform calculations directly within SRAM or DRAM memory arrays, often called "in-situ computing" <a class="yt-timestamp" data-t="00:12:57">[00:12:57]</a>.
    *   **Ambit**: Proposed by researchers from Microsoft, [[Nvidia and AI chip competition | Nvidia]], Intel, ETH Zurich, and Carnegie Mellon, Ambit is an in-memory accelerator <a class="yt-timestamp" data-t="00:13:21">[00:13:21]</a>. It activates three rows of DRAM cells at a time (two for inputs, one for output) to implement AND/OR logic functions <a class="yt-timestamp" data-t="00:13:33">[00:13:33]</a>. This concept utilizes the memory's internal bandwidth for calculations <a class="yt-timestamp" data-t="00:13:53">[00:13:53]</a>. However, it takes multiple cycles and more complex logic (like XOR) remains challenging <a class="yt-timestamp" data-t="00:14:02">[00:14:02]</a>.

The downside of device and circuit level approaches is that their performance still falls short of current [[von_neumann_architecture_limitations | Von Neumann]] GPU ASIC-centric systems <a class="yt-timestamp" data-t="00:14:16">[00:14:16]</a>, leading to a "jack of all trades, master of none" situation <a class="yt-timestamp" data-t="00:14:33">[00:14:33]</a>.

3.  **System Level**: The industry is largely moving towards implementing [[compute_in_memory_innovations | compute in memory]] at a system level, integrating discrete processing units and memory at a very close level <a class="yt-timestamp" data-t="00:14:39">[00:14:39]</a>.
    *   This is enabled by new packaging technologies like 2.5D or 3D memory die stacking, where DRAM memory dies are stacked on top of a CPU die <a class="yt-timestamp" data-t="00:14:53">[00:14:53]</a>.
    *   Memories are connected to the CPU using thousands of channels called Through Silicon Vias (TSVs), providing immense internal bandwidth <a class="yt-timestamp" data-t="00:15:04">[00:15:04]</a>.
    *   AMD's 3D V-Cache, based on TSMC's 3D stacking technology, is an example, used to add more memory cache to processors <a class="yt-timestamp" data-t="00:15:15">[00:15:15]</a>.
    *   The future could see similarly advanced packaging technologies adding hundreds of gigabytes of memory to an [[role_of_ai_accelerators_in_neural_network_training and inference | AI ASIC]], integrating world-class memory and logic dies closer than ever before without placing them on the same die <a class="yt-timestamp" data-t="00:15:28">[00:15:28]</a>.

## Challenges and Future Outlook

While ideas are plentiful, execution that performs well enough to replace existing market solutions is difficult <a class="yt-timestamp" data-t="00:15:45">[00:15:45]</a>. Current [[Nvidia and AI chip competition | Nvidia]] A100 and H100 [[role_of_ai_accelerators_in_neural_network_training and inference | AI GPUs]] are formidable competitors <a class="yt-timestamp" data-t="00:15:56">[00:15:56]</a>.

However, with leading-edge semiconductor technology slowing down, new methods are needed to leapfrog towards more powerful and robust hardware for running [[ai_and_ai_chip_boom | AI]] <a class="yt-timestamp" data-t="00:16:04">[00:16:04]</a>. Generally, bigger models perform better, and today's top-performing natural language processing and computer vision models still have room for improvement, likely requiring them to grow even larger <a class="yt-timestamp" data-t="00:16:14">[00:16:14]</a>. Unless new systems and hardware can overcome these aforementioned memory limits, deep learning might fall short of its ambitious expectations <a class="yt-timestamp" data-t="00:16:28">[00:16:28]</a>.