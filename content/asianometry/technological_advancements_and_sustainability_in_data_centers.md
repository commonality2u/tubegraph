---
title: Technological advancements and sustainability in data centers
videoId: tJYSzc7YkY0
---

From: [[asianometry]] <br/> 

Modern data centers, essential for the ongoing AI boom, are significant consumers of both energy and water resources <a class="yt-timestamp" data-t="00:00:16">[00:00:16]</a>. A single data center with 15 megawatts of IT capacity is estimated to use 80-130 million gallons of water annually, comparable to three hospitals or two 18-hole golf courses <a class="yt-timestamp" data-t="00:00:02">[00:00:02]</a>. The massive water footprint of these facilities is closely tied to their energy demands <a class="yt-timestamp" data-t="00:00:24">[00:00:24]</a>.

## Data Center Classification and Efficiency
Data centers vary widely in size and function, from small closet-sized setups to "hyperscale" facilities spanning entire football fields <a class="yt-timestamp" data-t="00:00:49">[00:00:49]</a>. Hyperscale facilities typically house around 5,000 servers and measure 10,000 square meters or more <a class="yt-timestamp" data-t="00:01:06">[00:01:06]</a>. These larger facilities are often built by global operators like Google or Meta to support applications like Gmail or Facebook <a class="yt-timestamp" data-t="00:01:14">[00:01:14]</a>. Their large size is driven by [[energy_efficiency_in_data_centers | energy efficiency]] scaling and economics <a class="yt-timestamp" data-t="00:01:36">[00:01:36]</a>.

[[energy_efficiency_in_data_centers | Energy efficiency in data centers]] is measured using Power Usage Effectiveness (PUE), calculated by dividing the total energy delivered by the energy consumed by ICT equipment <a class="yt-timestamp" data-t="00:01:43">[00:01:43]</a>. A PUE of 1.0 represents 100% efficiency, meaning all energy goes to ICT <a class="yt-timestamp" data-t="00:01:56">[00:01:56]</a>. Larger data centers tend to have lower PUEs; hyperscale facilities from Google and Microsoft claim PUEs of 1.1 to 1.2, while a small closet data center might have a PUE of 2.5 <a class="yt-timestamp" data-t="00:02:05">[00:02:05]</a>. This difference is largely due to more efficient [[cooling_systems_in_data_centers | cooling systems]] in larger facilities <a class="yt-timestamp" data-t="00:02:19">[00:02:19]</a>.

## Cooling Systems and [[water_consumption_in_data_centers | Water Consumption]]
Almost all electricity consumed by a data center converts to heat <a class="yt-timestamp" data-t="00:02:34">[00:02:34]</a>. Electronic equipment, such as [[innovations_and_challenges_in_hard_disk_drive_technology | hard disk drives]] and solid-state chips, must be kept cool (around 45°C or 85°C, respectively) to prevent damage <a class="yt-timestamp" data-t="00:02:49">[00:02:49]</a>. [[cooling_systems_in_data_centers | Cooling systems]] maintain stable temperature and humidity <a class="yt-timestamp" data-t="00:03:07">[00:03:07]</a>.

The two main types of [[cooling_systems_in_data_centers | cooling systems]] are air and liquid <a class="yt-timestamp" data-t="00:03:10">[00:03:10]</a>. Most data centers use air cooling in a raised-floor system where cold air from Computer Room Air Conditioners (CRACs) is directed into cold aisles to absorb heat from servers <a class="yt-timestamp" data-t="00:03:14">[00:03:14]</a>. Hot air is then re-collected and sent to fluid heat exchangers <a class="yt-timestamp" data-t="00:03:39">[00:03:39]</a>. A common system uses two fluid loops: a process loop (often water and glycol) that takes heat from the data room, and a water-based condenser loop that transfers heat outdoors <a class="yt-timestamp" data-t="00:03:48">[00:03:48]</a>.

Cooling towers are used to cool the water in the final loop by evaporating some of it, releasing energy as steam <a class="yt-timestamp" data-t="00:04:23">[00:04:23]</a>. Approximately 1% of water evaporates for every 10 degrees Fahrenheit (5.6°C) of cooling, necessitating constant replacement with "make-up" water <a class="yt-timestamp" data-t="00:04:38">[00:04:38]</a>.

### Indirect Water Usage: Energy Generation
Beyond direct cooling, data centers incur significant indirect [[water_consumption_in_data_centers | water consumption]] through their energy demands <a class="yt-timestamp" data-t="00:05:01">[00:05:01]</a>. In 2022, data centers accounted for 1% to 1.3% of global electricity consumption <a class="yt-timestamp" data-t="00:05:07">[00:05:07]</a>. Water is withdrawn to generate power from thermoelectric sources like coal, natural gas, and nuclear, which boil water to spin turbines <a class="yt-timestamp" data-t="00:05:15">[00:05:15]</a>. In 2021, 73% of US power came from such means <a class="yt-timestamp" data-t="00:05:27">[00:05:27]</a>. Water usage from energy generation can be two to three times greater than that directly consumed by [[cooling_systems_in_data_centers | cooling systems]] <a class="yt-timestamp" data-t="00:05:44">[00:05:44]</a>.

## Company Initiatives and Challenges
Major tech companies are making efforts to mitigate their [[impact_of_data_centers_on_water_and_energy_resources | impact on water and energy resources]].
*   **Google:** Used approximately 4.3 billion gallons of water for cooling in 2022, with 25% being reclaimed wastewater or seawater <a class="yt-timestamp" data-t="00:06:24">[00:06:24]</a>.
*   **Digital Realty:** A real estate investment trust specializing in data centers, reported 36% of its water comes from municipal non-drinkable sources in 2022. Roughly half of their 2022 water consumption was in water-stressed areas <a class="yt-timestamp" data-t="00:06:36">[00:06:36]</a>.
*   **AWS:** In 2022, 20 of their global data centers used recycled water for cooling (16 in Virginia, 2 in California, 2 in Singapore) <a class="yt-timestamp" data-t="00:07:05">[00:07:05]</a>.
*   **Microsoft:** Committed to becoming water positive, sponsoring projects that replenish more water than they consume and providing water access to a million people <a class="yt-timestamp" data-t="00:07:25">[00:07:25]</a>.
*   **Meta:** In Arizona, a state experiencing drought conditions and high reliance on the Colorado River, Meta plans to fund water restoration projects in the Colorado River and Salt River basins. They will use long-term storage credits for water sourcing and leverage direct free cooling to cut water usage by 60% <a class="yt-timestamp" data-t="00:08:08">[00:08:08]</a>.

A significant concern is that the majority of water directly withdrawn for cooling by these companies is often potable water from local supplies <a class="yt-timestamp" data-t="00:07:40">[00:07:40]</a>. With 40-50% of the global population living in water-scarce areas, and many data centers needing to be near population centers, this creates competing demands for water <a class="yt-timestamp" data-t="00:07:54">[00:07:54]</a>.

## Advancements in [[cooling_systems_in_data_centers | Cooling Systems]] for Sustainability

### Free Cooling
"Free cooling" involves using natural environmental conditions to cool data centers, reducing the need for traditional HVAC systems <a class="yt-timestamp" data-t="00:09:18">[00:09:18]</a>.
*   **Direct Free Cooling:** Involves drawing in outside air. This requires filtration and dehumidification due to pollutants, which can partially offset cost benefits depending on location <a class="yt-timestamp" data-t="00:09:25">[00:09:25]</a>. Studies in Europe show average energy savings of 5.4-7.9%, while in southern Australian cities, savings can reach up to 60% <a class="yt-timestamp" data-t="00:09:56">[00:10:04]</a>. Cold climates, like the Nordics, offer significant advantages, attracting data center investments <a class="yt-timestamp" data-t="00:10:23">[00:10:23]</a>.
*   **Waterside Free Cooling:** Data centers near cold water bodies can use that water for cooling <a class="yt-timestamp" data-t="00:10:52">[00:10:52]</a>. Google's Hamina data center in Finland, a converted paper mill, uses fresh seawater through existing pipes. The two-loop cooling system ensures the seawater doesn't mix with the data center's internal coolant <a class="yt-timestamp" data-t="00:11:02">[00:11:02]</a>. Microsoft's Project Natick, which submerged a data center underwater, also demonstrated the viability of this concept <a class="yt-timestamp" data-t="00:10:34">[00:10:34]</a>.

### Heat Recapture
[[energy_efficiency_in_data_centers | Heat recapture]] involves recovering waste heat from data centers for other uses, such as desalinating water, pre-heating water in thermoelectric plants, direct power generation, or district heating for homes <a class="yt-timestamp" data-t="00:11:30">[00:11:30]</a>. Heat from air-cooled data centers can be captured at 35-45°C, which is suitable for space and water heating <a class="yt-timestamp" data-t="00:11:50">[00:11:50]</a>. Challenges include the limited distance heat can be efficiently moved and the lack of existing district heating infrastructure in many cities <a class="yt-timestamp" data-t="00:12:08">[00:12:08]</a>. Nordic countries have made significant progress in this area <a class="yt-timestamp" data-t="00:12:34">[00:12:34]</a>. Implementing [[energy_efficiency_in_data_centers | heat recapture]] often encourages the adoption of liquid [[cooling_systems_in_data_centers | cooling systems]], which are more efficient at capturing and transferring heat than air <a class="yt-timestamp" data-t="00:12:43">[00:12:43]</a>.

### Optimized Temperature Ranges
The American Society of Heating, Refrigerating and Air-Conditioning Engineers (ASHRAE) recommends a data center temperature range of 15 to 32°C <a class="yt-timestamp" data-t="00:13:13">[00:13:13]</a>. Raising a data center's temperature by even 1-2 degrees can yield significant financial benefits <a class="yt-timestamp" data-t="00:13:31">[00:13:31]</a>. Google's tests have shown their data centers can operate at higher temperatures, though research on optimal temperatures for component performance, especially [[innovations_and_challenges_in_hard_disk_drive_technology | hard drives]], is sometimes contradictory <a class="yt-timestamp" data-t="00:13:40">[00:13:40]</a>.

## The Impact of the AI Boom
The current AI boom is rapidly increasing the demand for massive data centers, leading to escalating energy and [[water_consumption_in_data_centers | water consumption]] <a class="yt-timestamp" data-t="00:13:59">[00:13:59]</a>. Microsoft's 2022 [[water_consumption_in_data_centers | water consumption]] jumped 34% from 2021, likely driven by products like ChatGPT <a class="yt-timestamp" data-t="00:14:07">[00:14:07]</a>. Utility companies are seeing individual data center requests for 60-90 megawatts or greater, and large campuses requiring hundreds of megawatts to several gigawatts <a class="yt-timestamp" data-t="00:14:26">[00:14:26]</a>. Estimates suggest that data centers' share of global energy generation could reach 4.5% by 2030, with a corresponding increase in [[water_consumption_in_data_centers | water consumption]] <a class="yt-timestamp" data-t="00:15:03">[00:15:03]</a>. New, more power-hungry GPUs like Nvidia's B100 will further intensify this trend <a class="yt-timestamp" data-t="00:15:22">[00:15:22]</a>.

The [[impact_of_data_centers_on_water_and_energy_resources | impact of data centers on water and energy resources]] begins at the [[technological_advancements_in_semiconductor_manufacturing | semiconductor manufacturing]] stage; TSMC alone is projected to consume 12.5% of Taiwan's energy by 2025 <a class="yt-timestamp" data-t="00:15:47">[00:15:47]</a>. After manufacturing, chips require even more energy and water for operation and cooling in data centers <a class="yt-timestamp" data-t="00:16:03">[00:16:03]</a>.

To address these growing demands, future data centers will need to rapidly adopt a combination of free-cooling, waste heat recovery, and [[technological_advancements_in_solar_energy | renewable energy]] sources like [[technological_advancements_in_solar_energy | solar power]] <a class="yt-timestamp" data-t="00:16:12">[00:16:12]</a>. This transition is critical for both environmental sustainability and economic viability <a class="yt-timestamp" data-t="00:16:22">[00:16:22]</a>.