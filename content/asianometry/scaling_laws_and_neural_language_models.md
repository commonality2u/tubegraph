---
title: Scaling laws and neural language models
videoId: J-BvkmNtgAM
---

From: [[asianometry]] <br/> 

The concept of "scaling laws" is a fundamental idea driving significant investment in the AI industry <a class="yt-timestamp" data-t="00:02:24">[00:02:24]</a>. These laws were introduced in a 2020 paper by OpenAI titled "Scaling laws for neural language models" <a class="yt-timestamp" data-t="00:02:27">[00:02:27]</a>.

## Core Principle
The basic premise of scaling laws is that combining more data and compute power leads to better results, specifically less loss in model performance <a class="yt-timestamp" data-t="00:02:36">[00:02:36]</a>. This idea was central to OpenAI's success in developing the GPT-series of models <a class="yt-timestamp" data-t="00:02:42">[00:02:42]</a>.

Ilya Sutskever, a co-founder of OpenAI, emphasized this point in a November appearance on the No Priors podcast:
> I was very fortunate in that I was able to realize that the reason neural networks of the time weren't good is because they are too small. So like if you tried to solve a vision task with a neural network with a thousand neurons, what can it do? It can't do anything <a class="yt-timestamp" data-t="00:02:56">[00:02:56]</a>. It doesn't matter how good your learning is and anything else <a class="yt-timestamp" data-t="00:03:10">[00:03:10]</a>. But if you have a much larger network then it can do something unprecedented <a class="yt-timestamp" data-t="00:03:13">[00:03:13]</a>.

This principle has been demonstrated by the progression of OpenAI's models: GPT-3 was large, GPT-4 was even larger and performed significantly better, and future versions like GPT-5 are expected to be even bigger <a class="yt-timestamp" data-t="00:03:18">[00:03:18]</a>. Currently, there are no indications that the scaling laws have ceased to hold true <a class="yt-timestamp" data-t="00:03:25">[00:03:25]</a>.

## Parallels with Moore's Law
The "scaling laws" for AI models draw interesting parallels with the semiconductor industry's "Moore's Law" <a class="yt-timestamp" data-t="00:03:34">[00:03:34]</a>. While the two laws describe different phenomena, they could have similar impacts on their respective industries <a class="yt-timestamp" data-t="00:03:36">[00:03:36]</a>. In the 1980s and 1990s, Moore's Law served as a rallying cry, setting the pace for the entire semiconductor industry <a class="yt-timestamp" data-t="00:03:45">[00:03:45]</a>. There is a possibility that the scaling laws could have a similar effect on the [[the_ai_and_ai_chip_boom_landscape | AI industry]], providing a simple and understandable drive for R&D roadmaps for years to come <a class="yt-timestamp" data-t="00:03:56">[00:03:56]</a>.

## Arguments and Challenges
Despite their apparent success, arguments against the scaling laws exist. One common concern is that all existing data across the internet has largely been utilized <a class="yt-timestamp" data-t="00:04:09">[00:04:09]</a>. However, similar to how the semiconductor industry overcame physics problems ahead of Moore's Law through new engineering solutions (e.g., High-K Metal Gate, FinFET, DUV Lithography) <a class="yt-timestamp" data-t="00:04:22">[00:04:22]</a>, ways around data limitations could be found with sufficient investment <a class="yt-timestamp" data-t="00:04:18">[00:04:18]</a>.

A bigger question is whether the necessary financial investment will continue to drive this scaling <a class="yt-timestamp" data-t="00:04:35">[00:04:35]</a>. This relates to the broader question of whether the current AI boom is financially sustainable, requiring substantial consumer demand for services built on these models to justify the "trillions" in investment <a class="yt-timestamp" data-t="00:09:09">[00:09:09]</a>.

For a deeper technical dive into the [[challenges_in_scaling_ai_hardware | technical issues behind scaling]], Dwarkesh Patel's post "Will scaling work?" is recommended <a class="yt-timestamp" data-t="00:04:41">[00:04:41]</a>.