---
title: Scaling Laws in AI
videoId: J-BvkmNtgAM
---

From: [[asianometry]] <br/> 

The concept of "scaling laws" is a driving force behind significant investments in the AI industry <a class="yt-timestamp" data-t="02:24:00">[02:24:00]</a>. This concept originated from a 2020 paper by OpenAI titled "Scaling laws for neural language models" <a class="yt-timestamp" data-t="02:27:00">[02:27:00]</a>. Essentially, it posits that combining more data and compute leads to improved results, characterized by less loss <a class="yt-timestamp" data-t="02:36:00">[02:36:00]</a>.

## Impact on AI Development

This idea was fundamental to OpenAI's success in developing the GPT-series models <a class="yt-timestamp" data-t="02:42:00">[02:42:00]</a>. Ilya Sutskever, a co-founder of OpenAI, noted that early neural networks were too small to be effective <a class="yt-timestamp" data-t="02:56:00">[02:56:00]</a>. He realized that a much larger network could achieve unprecedented results <a class="yt-timestamp" data-t="03:13:00">[03:13:00]</a>.

The progression of GPT models illustrates this:
*   GPT-3 was large <a class="yt-timestamp" data-t="03:18:00">[03:18:00]</a>.
*   GPT-4 was even bigger and performed significantly better <a class="yt-timestamp" data-t="03:19:00">[03:19:00]</a>.
*   GPT-5, when released, is expected to be larger still <a class="yt-timestamp" data-t="03:22:00">[03:22:00]</a>.

Currently, there are no indications that the scaling laws have ceased to apply <a class="yt-timestamp" data-t="03:25:00">[03:25:00]</a>.

## Comparison to Moore's Law

The "scaling laws" share interesting parallels with the semiconductor industry's Moore's Law <a class="yt-timestamp" data-t="03:34:00">[03:34:00]</a>. While the two "laws" are not identical in their statements, they may have similar effects on their respective industries <a class="yt-timestamp" data-t="03:39:00">[03:39:00]</a>.

In the 1980s and 1990s, Moore's Law served as a rallying cry for the semiconductor industry, dictating the pace for companies globally <a class="yt-timestamp" data-t="03:45:00">[03:45:00]</a>. Similarly, the scaling laws could have a comparable impact on the [[ai_and_ai_chip_boom | AI industry]], providing a clear, understandable framework to guide research and development roadmaps for years to come <a class="yt-timestamp" data-t="03:56:00">[03:56:00]</a>.

## Challenges and Solutions

Despite their influence, there are arguments against the scaling laws. For instance, some argue that existing data across the internet has largely been exhausted <a class="yt-timestamp" data-t="04:09:00">[04:09:00]</a>. However, potential solutions exist if sufficient funding is available <a class="yt-timestamp" data-t="04:18:00">[04:18:00]</a>. Historically, the semiconductor industry faced physical limitations ahead of Moore's Law, but new engineering solutions, such as High-K Metal Gate and FinFET, were developed <a class="yt-timestamp" data-t="04:22:00">[04:22:00]</a>.

The larger question remains whether the necessary investment can be sustained to continue driving this growth <a class="yt-timestamp" data-t="04:35:00">[04:35:00]</a>. For a deeper technical review of scaling issues, Dwarkesh Patel's "Will scaling work?" is recommended <a class="yt-timestamp" data-t="04:41:00">[04:41:00]</a>.

The semiconductor industry is generally conservative, with many veterans who have witnessed numerous boom-and-bust cycles <a class="yt-timestamp" data-t="02:08:00">[02:08:00]</a>. Sam Altman is not the first to propose a use case that he believes will revolutionize the world <a class="yt-timestamp" data-t="02:17:00">[02:17:00]</a>.

## Investment Implications

The discussion around massive investments in AI, such as the potential trillion-dollar venture associated with Sam Altman and OpenAI, is closely tied to the implications of scaling laws <a class="yt-timestamp" data-t="00:23:00">[00:23:00]</a>. This figure, though initially questioned as a potential negotiating tactic, represents the sum total of investments needed across the entire ecosystem, including real estate, power for data centers, and chip manufacturing over several years <a class="yt-timestamp" data-t="01:07:00">[01:07:00]</a>.

Even when spread out over five years and diluted with other expenses, a trillion-dollar investment would still represent a significant increase in capital expenditure, akin to a "COVID-like step function upwards" <a class="yt-timestamp" data-t="01:55:00">[01:55:00]</a>. This highlights the unprecedented financial commitment that the pursuit of larger, more capable [[deep_learning_model_scalability | deep learning models]] might require.