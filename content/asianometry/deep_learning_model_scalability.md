---
title: Deep learning model scalability
videoId: 5tmGKTNW8DQ
---

From: [[asianometry]] <br/> 

The field of deep learning has witnessed an explosive growth since 2012, with the industry's largest models expanding hundreds of thousands of times in size <a class="yt-timestamp" data-t="00:00:02">[00:00:02]</a>, <a class="yt-timestamp" data-t="00:00:06">[00:00:06]</a>. This rapid expansion places significant strain on existing hardware, particularly regarding memory, leading to what is known as the [[memory_wall_problem_in_ai_hardware | memory wall problem]] <a class="yt-timestamp" data-t="00:00:28">[00:00:28]</a>, <a class="yt-timestamp" data-t="00:00:34">[00:00:34]</a>, <a class="yt-timestamp" data-t="00:00:40">[00:00:40]</a>.

## The Growth of Deep Learning Models

Modern deep learning models are characterized by an ever-increasing number of parameters:
*   OpenAI's DALL-E 2 has 3.5 billion parameters <a class="yt-timestamp" data-t="00:00:10">[00:00:10]</a>.
*   Google's Imagen has 4.6 billion parameters <a class="yt-timestamp" data-t="00:00:13">[00:00:13]</a>.
*   GPT-3 features 175 billion parameters <a class="yt-timestamp" data-t="00:00:16">[00:00:16]</a>.
*   Google recently pre-trained a model with 1 trillion parameters <a class="yt-timestamp" data-t="00:00:26">[00:00:26]</a>.

[[scaling_laws_in_ai | Increasingly larger models]] are expected to continue to emerge <a class="yt-timestamp" data-t="00:00:21">[00:00:21]</a>, <a class="yt-timestamp" data-t="00:00:24">[00:00:24]</a>, which demands new hardware solutions to accommodate them <a class="yt-timestamp" data-t="00:00:31">[00:00:31]</a>.

## The [[memory_wall_problem_in_ai_hardware | Memory Wall Problem]]

The fundamental limitation for scaling deep learning models is often tied to memory <a class="yt-timestamp" data-t="00:00:34">[00:00:34]</a>.

### Von Neumann Architecture Limitations
Virtually every modern computer operates on a Von Neumann architecture, storing both instructions and data in the same memory bank <a class="yt-timestamp" data-t="00:01:00">[00:01:00]</a>, <a class="yt-timestamp" data-t="00:01:04">[00:01:04]</a>. While this architecture has been crucial for software development, it differs significantly from the human brain <a class="yt-timestamp" data-t="00:01:21">[00:01:21]</a>, <a class="yt-timestamp" data-t="00:01:28">[00:01:28]</a>. The brain tightly integrates compute with memory and I/O, whereas computers separate high-precision computation from memory and communication <a class="yt-timestamp" data-t="00:01:33">[00:01:33]</a>, <a class="yt-timestamp" data-t="00:01:40">[00:01:40]</a>, <a class="yt-timestamp" data-t="00:01:48">[00:01:48]</a>. This separation has significant consequences for memory performance <a class="yt-timestamp" data-t="00:01:51">[00:01:51]</a>, <a class="yt-timestamp" data-t="00:01:53">[00:01:53]</a>.

### Hardware Performance Lag
Despite efforts by the [[Nvidia and AI chip competition | AI hardware industry]] to scale memory and processing unit performance <a class="yt-timestamp" data-t="00:01:55">[00:01:55]</a>, it has not kept pace with the rapid growth of deep learning models, especially concerning memory <a class="yt-timestamp" data-t="00:02:15">[00:02:15]</a>, <a class="yt-timestamp" data-t="00:02:17">[00:02:17]</a>.
*   [[Nvidia and AI chip competition | Nvidia's]] V100 [[role_of_ai_accelerators_in_neural_network_training_and_inference | GPU]] (2017) offered 32 GB of memory <a class="yt-timestamp" data-t="00:02:02">[00:02:02]</a>, <a class="yt-timestamp" data-t="00:02:08">[00:02:08]</a>.
*   Current top-tier [[Nvidia and AI chip competition | Nvidia Data Center GPUs]], the A100 and H100, boast 80 GB of memory <a class="yt-timestamp" data-t="00:02:10">[00:02:10]</a>, <a class="yt-timestamp" data-t="00:02:13">[00:02:13]</a>.
*   Leading-edge models can easily require hundreds of gigabytes of memory <a class="yt-timestamp" data-t="00:02:23">[00:02:23]</a>, <a class="yt-timestamp" data-t="00:02:28">[00:02:28]</a>.
*   A trillion-parameter model is estimated to require 320 A100 [[role_of_ai_accelerators_in_neural_network_training_and_inference | GPUs]], each with 80 GB of memory <a class="yt-timestamp" data-t="00:02:30">[00:02:30]</a>, <a class="yt-timestamp" data-t="00:02:33">[00:02:33]</a>, <a class="yt-timestamp" data-t="00:02:37">[00:02:37]</a>.

This disparity in processing and capacity means [[role_of_ai_accelerators_in_neural_network_training_and_inference | processing units]] waste cycles waiting for data to travel to and from memory and for memory operations to complete <a class="yt-timestamp" data-t="00:02:39">[00:02:39]</a>, <a class="yt-timestamp" data-t="00:02:41">[00:02:41]</a>, <a class="yt-timestamp" data-t="00:02:44">[00:02:44]</a>, <a class="yt-timestamp" data-t="00:02:46">[00:02:46]</a>. This is the core of the [[memory_wall_problem_in_ai_hardware | memory wall]] or memory capacity bottleneck <a class="yt-timestamp" data-t="00:02:54">[00:02:54]</a>, <a class="yt-timestamp" data-t="00:02:57">[00:02:57]</a>, <a class="yt-timestamp" data-t="00:02:59">[00:02:59]</a>.

### Energy and Capital Costs
Adding more memory to [[role_of_ai_accelerators_in_neural_network_training_and_inference | GPUs]] faces practical and technological limits <a class="yt-timestamp" data-t="00:03:07">[00:03:07]</a>, <a class="yt-timestamp" data-t="00:03:08">[00:03:08]</a>.
*   **[[energy_efficiency_in_ai_data_centers | Energy Limitations]]**: Shuttling data between the chip and memory is energy-intensive due to electrical connection losses <a class="yt-timestamp" data-t="00:03:22">[00:03:22]</a>, <a class="yt-timestamp" data-t="00:03:24">[00:03:24]</a>, <a class="yt-timestamp" data-t="00:03:26">[00:03:26]</a>, <a class="yt-timestamp" data-t="00:03:28">[00:03:28]</a>. Accessing off-chip memory uses 200 times more energy than a floating-point operation <a class="yt-timestamp" data-t="00:03:32">[00:03:32]</a>, <a class="yt-timestamp" data-t="00:03:35">[00:03:35]</a>.
    *   80% of Google's TPU [[energy_efficiency_in_ai_data_centers | energy usage]] comes from electrical connections, not computational units <a class="yt-timestamp" data-t="00:03:40">[00:03:40]</a>, <a class="yt-timestamp" data-t="00:03:41">[00:03:41]</a>, <a class="yt-timestamp" data-t="00:03:43">[00:03:43]</a>, <a class="yt-timestamp" data-t="00:03:45">[00:03:45]</a>.
    *   DRAM alone can account for 40% of total system power in some GPU/CPU systems <a class="yt-timestamp" data-t="00:03:47">[00:03:47]</a>, <a class="yt-timestamp" data-t="00:03:50">[00:03:50]</a>, <a class="yt-timestamp" data-t="00:03:53">[00:03:53]</a>.
    *   [[ai_boom_and_data_center_demands | Energy]] constitutes 40% of a [[ai_boom_and_data_center_demands | data center's operating costs]], making storage and memory a significant factor in profitability <a class="yt-timestamp" data-t="00:03:54">[00:03:54]</a>, <a class="yt-timestamp" data-t="00:03:57">[00:03:57]</a>, <a class="yt-timestamp" data-t="00:04:00">[00:04:00]</a>, <a class="yt-timestamp" data-t="00:04:02">[00:04:02]</a>.
*   **Upfront Capital Costs**: A trillion-parameter model needing 320 A100 [[role_of_ai_accelerators_in_neural_network_training_and_inference | GPUs]] (at $32,000 MSRP each) would cost $10 million just for hardware <a class="yt-timestamp" data-t="00:04:16">[00:04:16]</a>, <a class="yt-timestamp" data-t="00:04:17">[00:04:17]</a>, <a class="yt-timestamp" data-t="00:04:20">[00:04:20]</a>, <a class="yt-timestamp" data-t="00:04:23">[00:04:23]</a>, <a class="yt-timestamp" data-t="00:04:25">[00:04:25]</a>, <a class="yt-timestamp" data-t="00:04:29">[00:04:29]</a>. A 100-trillion parameter model might require over 6,000 such [[role_of_ai_accelerators_in_neural_network_training_and_inference | GPUs]] <a class="yt-timestamp" data-t="00:04:31">[00:04:31]</a>, <a class="yt-timestamp" data-t="00:04:34">[00:04:34]</a>. These costs do not include the [[energy_efficiency_in_ai_data_centers | energy costs]] of running inference, which account for 90% of a model's total costs <a class="yt-timestamp" data-t="00:04:36">[00:04:36]</a>, <a class="yt-timestamp" data-t="00:04:38">[00:04:38]</a>, <a class="yt-timestamp" data-t="00:04:40">[00:04:40]</a>, <a class="yt-timestamp" data-t="00:04:42">[00:04:42]</a>, <a class="yt-timestamp" data-t="00:04:44">[00:04:44]</a>, <a class="yt-timestamp" data-t="00:04:46">[00:04:46]</a>. This risks limiting the benefits of advanced AI to a select few <a class="yt-timestamp" data-t="00:04:48">[00:04:48]</a>, <a class="yt-timestamp" data-t="00:04:50">[00:04:50]</a>, <a class="yt-timestamp" data-t="00:04:52">[00:04:52]</a>.

### Historical and Technological Limits
The reliance on Dynamic Random Access Memory (DRAM) since the 1960s and 70s forms the basis of computers due to its low latency and cheap manufacturing <a class="yt-timestamp" data-t="00:04:55">[00:04:55]</a>, <a class="yt-timestamp" data-t="00:04:57">[00:04:57]</a>, <a class="yt-timestamp" data-t="00:05:01">[00:05:01]</a>, <a class="yt-timestamp" data-t="00:05:02">[00:05:02]</a>, <a class="yt-timestamp" data-t="00:05:05">[00:05:05]</a>.
*   After 1980, compute scaling outpaced memory scaling <a class="yt-timestamp" data-t="00:05:26">[00:05:26]</a>, <a class="yt-timestamp" data-t="00:05:28">[00:05:28]</a>.
*   CPU/[[role_of_ai_accelerators_in_neural_network_training_and_inference | GPU]] industries optimized for transistor density <a class="yt-timestamp" data-t="00:05:32">[00:05:32]</a>, <a class="yt-timestamp" data-t="00:05:34">[00:05:34]</a>, <a class="yt-timestamp" data-t="00:05:36">[00:05:36]</a>.
*   The memory industry must scale capacity, bandwidth, *and* latency simultaneously <a class="yt-timestamp" data-t="00:05:40">[00:05:40]</a>, <a class="yt-timestamp" data-t="00:05:41">[00:05:41]</a>, <a class="yt-timestamp" data-t="00:05:44">[00:05:44]</a>. As a result, latency has lagged significantly:
    *   Memory capacity: 128x improvement in 20 years <a class="yt-timestamp" data-t="00:05:51">[00:05:51]</a>, <a class="yt-timestamp" data-t="00:05:54">[00:05:54]</a>.
    *   Bandwidth: 20x improvement <a class="yt-timestamp" data-t="00:05:54">[00:05:54]</a>, <a class="yt-timestamp" data-t="00:05:57">[00:05:57]</a>.
    *   Latency: Only 30% improvement <a class="yt-timestamp" data-t="00:05:57">[00:05:57]</a>, <a class="yt-timestamp" data-t="00:06:00">[00:06:00]</a>.
*   Shrinking DRAM cells beyond a certain point leads to worse performance, less reliability, reduced security, and energy inefficiency <a class="yt-timestamp" data-t="00:06:02">[00:06:02]</a>, <a class="yt-timestamp" data-t="00:06:05">[00:06:05]</a>, <a class="yt-timestamp" data-t="00:06:07">[00:06:07]</a>, <a class="yt-timestamp" data-t="00:06:09">[00:06:09]</a>, <a class="yt-timestamp" data-t="00:06:12">[00:06:12]</a>. This is because DRAM cells store data as a charge in a capacitor, which becomes leakier and more vulnerable at nanoscale sizes <a class="yt-timestamp" data-t="00:06:16">[00:06:16]</a>, <a class="yt-timestamp" data-t="00:06:19">[00:06:19]</a>, <a class="yt-timestamp" data-t="00:06:21">[00:06:21]</a>, <a class="yt-timestamp" data-t="00:06:31">[00:06:31]</a>, <a class="yt-timestamp" data-t="00:06:33">[00:06:33]</a>, <a class="yt-timestamp" data-t="00:06:36">[00:06:36]</a>, <a class="yt-timestamp" data-t="00:06:38">[00:06:38]</a>. These are fundamental problems, making engineering solutions difficult <a class="yt-timestamp" data-t="00:06:48">[00:06:48]</a>, <a class="yt-timestamp" data-t="00:06:49">[00:06:49]</a>, <a class="yt-timestamp" data-t="00:06:52">[00:06:52]</a>.

## Solutions: [[compute_in_memory_innovations | Compute In Memory]]

The challenges of the [[memory_wall_problem_in_ai_hardware | memory wall]] are driving exploration of radical ideas that could offer 10x improvements <a class="yt-timestamp" data-t="00:07:00">[00:07:00]</a>, <a class="yt-timestamp" data-t="00:07:03">[00:07:03]</a>, <a class="yt-timestamp" data-t="00:07:05">[00:07:05]</a>, <a class="yt-timestamp" data-t="00:07:08">[00:07:08]</a>. One prominent idea is [[compute_in_memory_innovations | compute in memory]], which aims to alleviate or eliminate the Von Neumann bottleneck by performing computations directly within memory <a class="yt-timestamp" data-t="00:07:21">[00:07:21]</a>, <a class="yt-timestamp" data-t="00:07:22">[00:07:22]</a>, <a class="yt-timestamp" data-t="00:07:25">[00:07:25]</a>, <a class="yt-timestamp" data-t="00:07:27">[00:07:27]</a>.

### What is [[compute_in_memory_innovations | Compute In Memory]]?
[[compute_in_memory_innovations | Compute in memory]] refers to integrating processing elements directly within a random access memory (RAM), either very near or on the same die <a class="yt-timestamp" data-t="00:07:30">[00:07:30]</a>, <a class="yt-timestamp" data-t="00:07:33">[00:07:33]</a>, <a class="yt-timestamp" data-t="00:07:35">[00:07:35]</a>, <a class="yt-timestamp" data-t="00:07:37">[00:07:37]</a>, <a class="yt-timestamp" data-t="00:07:39">[00:07:39]</a>. Other names for this concept include processing-in-memory, computational RAM, near-data computing, memory-centric computing, and in-memory computation <a class="yt-timestamp" data-t="00:07:42">[00:07:42]</a>, <a class="yt-timestamp" data-t="00:07:43">[00:07:43]</a>, <a class="yt-timestamp" data-t="00:07:46">[00:07:46]</a>, <a class="yt-timestamp" data-t="00:07:48">[00:07:48]</a>, <a class="yt-timestamp" data-t="00:07:50">[00:07:50]</a>, <a class="yt-timestamp" data-t="00:07:51">[00:07:51]</a>.

This idea is particularly well-suited for deep learning, where running [[neural_network_revolution_and_role_of_gpus | neural network models]] involves massive matrix calculations, primarily multiply-accumulate (MAC) operations <a class="yt-timestamp" data-t="00:08:20">[00:08:20]</a>, <a class="yt-timestamp" data-t="00:08:22">[00:08:22]</a>, <a class="yt-timestamp" data-t="00:08:24">[00:08:24]</a>, <a class="yt-timestamp" data-t="00:08:27">[00:08:27]</a>, <a class="yt-timestamp" data-t="00:08:30">[00:08:30]</a>, <a class="yt-timestamp" data-t="00:08:32">[00:08:32]</a>. The arithmetic is simple, but the sheer volume makes the memory bottleneck critical <a class="yt-timestamp" data-t="00:08:35">[00:08:35]</a>, <a class="yt-timestamp" data-t="00:08:37">[00:08:37]</a>, <a class="yt-timestamp" data-t="00:08:39">[00:08:39]</a>. An ideal [[compute_in_memory_innovations | compute in memory]] chip could execute MAC operations directly within the memory chip <a class="yt-timestamp" data-t="00:08:41">[00:08:41]</a>, <a class="yt-timestamp" data-t="00:08:44">[00:08:44]</a>, <a class="yt-timestamp" data-t="00:08:47">[00:08:47]</a>. This is especially beneficial for edge inference, where [[energy_efficiency_in_ai_data_centers | energy]], size, or heat restrictions are significant, potentially cutting [[neural_network_revolution_and_role_of_gpus | neural network]] [[energy_efficiency_in_ai_data_centers | energy usage]] by up to 80% <a class="yt-timestamp" data-t="00:08:48">[00:08:48]</a>, <a class="yt-timestamp" data-t="00:08:50">[00:08:50]</a>, <a class="yt-timestamp" data-t="00:08:52">[00:08:52]</a>, <a class="yt-timestamp" data-t="00:08:55">[00:08:55]</a>, <a class="yt-timestamp" data-t="00:08:58">[00:08:58]</a>, <a class="yt-timestamp" data-t="00:09:00">[00:09:00]</a>.

### History of [[compute_in_memory_innovations | Compute In Memory]]
The concept of [[compute_in_memory_innovations | compute in memory]] dates back to the 1960s <a class="yt-timestamp" data-t="00:09:03">[00:09:03]</a>, <a class="yt-timestamp" data-t="00:09:05">[00:09:05]</a>.
*   **Harold Stone (1960s)**: Professor Harold Stone of Stanford University first explored integrating logic and memory, noting that while transistor count grew rapidly, processor communication with memory was limited by pin count <a class="yt-timestamp" data-t="00:09:06">[00:09:06]</a>, <a class="yt-timestamp" data-t="00:09:09">[00:09:09]</a>, <a class="yt-timestamp" data-t="00:09:11">[00:09:11]</a>, <a class="yt-timestamp" data-t="00:09:14">[00:09:14]</a>, <a class="yt-timestamp" data-t="00:09:16">[00:09:16]</a>, <a class="yt-timestamp" data-t="00:09:18">[00:09:18]</a>, <a class="yt-timestamp" data-t="00:09:20">[00:09:20]</a>, <a class="yt-timestamp" data-t="00:09:22">[00:09:22]</a>. He proposed moving computation into memory caches <a class="yt-timestamp" data-t="00:09:25">[00:09:25]</a>, <a class="yt-timestamp" data-t="00:09:27">[00:09:27]</a>.
*   **Terraces (1995)**: Produced what could be considered the first processor-in-memory chip, a 4-bit memory with an integrated single-bit logical unit capable of simple logic operations <a class="yt-timestamp" data-t="00:09:30">[00:09:30]</a>, <a class="yt-timestamp" data-t="00:09:33">[00:09:33]</a>, <a class="yt-timestamp" data-t="00:09:36">[00:09:36]</a>, <a class="yt-timestamp" data-t="00:09:38">[00:09:38]</a>, <a class="yt-timestamp" data-t="00:09:40">[00:09:40]</a>, <a class="yt-timestamp" data-t="00:09:42">[00:09:42]</a>, <a class="yt-timestamp" data-t="00:09:44">[00:09:44]</a>, <a class="yt-timestamp" data-t="00:09:47">[00:09:47]</a>, <a class="yt-timestamp" data-t="00:09:50">[00:09:50]</a>, <a class="yt-timestamp" data-t="00:09:52">[00:09:52]</a>.
*   **UC Berkeley IRAM Project (1997)**: Professors including David Patterson aimed to put a microprocessor and DRAM on the same chip <a class="yt-timestamp" data-t="00:09:53">[00:09:53]</a>, <a class="yt-timestamp" data-t="00:09:56">[00:09:56]</a>, <a class="yt-timestamp" data-t="00:09:58">[00:09:58]</a>, <a class="yt-timestamp" data-t="00:10:01">[00:10:01]</a>, <a class="yt-timestamp" data-t="00:10:03">[00:10:03]</a>, <a class="yt-timestamp" data-t="00:10:06">[00:10:06]</a>.
However, none of these early proposals gained traction due to practical manufacturing challenges <a class="yt-timestamp" data-t="00:10:07">[00:10:07]</a>, <a class="yt-timestamp" data-t="00:10:09">[00:10:09]</a>, <a class="yt-timestamp" data-t="00:10:11">[00:10:11]</a>, <a class="yt-timestamp" data-t="00:10:14">[00:10:14]</a>.

### Manufacturing Challenges
Integrating memory and logic on the same chip is difficult due to their opposing fabrication goals <a class="yt-timestamp" data-t="00:10:16">[00:10:16]</a>, <a class="yt-timestamp" data-t="00:10:17">[00:10:17]</a>, <a class="yt-timestamp" data-t="00:10:19">[00:10:19]</a>, <a class="yt-timestamp" data-t="00:10:22">[00:10:22]</a>.
*   Logic transistors prioritize speed and performance <a class="yt-timestamp" data-t="00:10:22">[00:10:22]</a>, <a class="yt-timestamp" data-t="00:10:25">[00:10:25]</a>.
*   Memory transistors prioritize high density, low cost, and low leakage <a class="yt-timestamp" data-t="00:10:25">[00:10:25]</a>, <a class="yt-timestamp" data-t="00:10:27">[00:10:27]</a>, <a class="yt-timestamp" data-t="00:10:29">[00:10:29]</a>, <a class="yt-timestamp" data-t="00:10:33">[00:10:33]</a>.
DRAM designs are regular with many parallel wires, while logic designs are more complex <a class="yt-timestamp" data-t="00:10:38">[00:10:38]</a>, <a class="yt-timestamp" data-t="00:10:40">[00:10:40]</a>, <a class="yt-timestamp" data-t="00:10:43">[00:10:43]</a>.
*   Contemporary DRAM processes use 3-4 metal layers <a class="yt-timestamp" data-t="00:10:57">[00:10:57]</a>, <a class="yt-timestamp" data-t="00:10:59">[00:10:59]</a>.
*   Contemporary logic processes use 7-12+ metal layers <a class="yt-timestamp" data-t="00:11:01">[00:11:01]</a>, <a class="yt-timestamp" data-t="00:11:03">[00:11:03]</a>, <a class="yt-timestamp" data-t="00:11:06">[00:11:06]</a>.
Using a DRAM process for logic results in 80% bigger and 22% worse performing logic circuits <a class="yt-timestamp" data-t="00:11:07">[00:11:07]</a>, <a class="yt-timestamp" data-t="00:11:10">[00:11:10]</a>, <a class="yt-timestamp" data-t="00:11:12">[00:11:12]</a>, <a class="yt-timestamp" data-t="00:11:14">[00:11:14]</a>, <a class="yt-timestamp" data-t="00:11:17">[00:11:17]</a>. Conversely, using a logic process for DRAM creates embedded DRAM (eDRAM) cells that use significantly more power, take 10x more space, and are less reliable <a class="yt-timestamp" data-t="00:11:20">[00:11:20]</a>, <a class="yt-timestamp" data-t="00:11:23">[00:11:23]</a>, <a class="yt-timestamp" data-t="00:11:26">[00:11:26]</a>, <a class="yt-timestamp" data-t="00:11:29">[00:11:29]</a>, <a class="yt-timestamp" data-t="00:11:32">[00:11:32]</a>.

### Levels of [[compute_in_memory_innovations | Compute In Memory]] Implementation
The industry is developing workarounds for these manufacturing shortcomings, proposing [[compute_in_memory_innovations | compute in memory]] at three levels: device, circuit, and system <a class="yt-timestamp" data-t="00:11:34">[00:11:34]</a>, <a class="yt-timestamp" data-t="00:11:36">[00:11:36]</a>, <a class="yt-timestamp" data-t="00:11:38">[00:11:38]</a>, <a class="yt-timestamp" data-t="00:11:40">[00:11:40]</a>, <a class="yt-timestamp" data-t="00:11:42">[00:11:42]</a>, <a class="yt-timestamp" data-t="00:11:45">[00:11:45]</a>, <a class="yt-timestamp" data-t="00:11:47">[00:11:47]</a>.

#### Device Level
This level focuses on new types of memory hardware beyond conventional DRAM and SRAM <a class="yt-timestamp" data-t="00:11:47">[00:11:47]</a>, <a class="yt-timestamp" data-t="00:11:49">[00:11:49]</a>, <a class="yt-timestamp" data-t="00:11:51">[00:11:51]</a>.
*   **Resistive Random Access Memory (ReRAM or RRAM)**: Stores information by changing the electrical resistance of a material <a class="yt-timestamp" data-t="00:11:53">[00:11:53]</a>, <a class="yt-timestamp" data-t="00:11:56">[00:11:56]</a>, <a class="yt-timestamp" data-t="00:12:00">[00:12:00]</a>, <a class="yt-timestamp" data-t="00:12:10">[00:12:10]</a>, <a class="yt-timestamp" data-t="00:12:12">[00:12:12]</a>, <a class="yt-timestamp" data-t="00:12:14">[00:12:14]</a>, <a class="yt-timestamp" data-t="00:12:17">[00:12:17]</a>, <a class="yt-timestamp" data-t="00:12:19">[00:12:19]</a>, <a class="yt-timestamp" data-t="00:12:22">[00:12:22]</a>, <a class="yt-timestamp" data-t="00:12:24">[00:12:24]</a>. This structure allows ReRAM to compute logical functions directly within memory cells <a class="yt-timestamp" data-t="00:12:26">[00:12:26]</a>, <a class="yt-timestamp" data-t="00:12:29">[00:12:29]</a>, <a class="yt-timestamp" data-t="00:12:32">[00:12:32]</a>, <a class="yt-timestamp" data-t="00:12:34">[00:12:34]</a>, <a class="yt-timestamp" data-t="00:12:37">[00:12:37]</a>. ReRAM is one of the most promising emerging memory technologies due to its compatibility with silicon CMOS, though substantial hurdles remain for commercialization <a class="yt-timestamp" data-t="00:12:42">[00:12:42]</a>, <a class="yt-timestamp" data-t="00:12:44">[00:12:44]</a>, <a class="yt-timestamp" data-t="00:12:46">[00:12:46]</a>, <a class="yt-timestamp" data-t="00:12:47">[00:12:47]</a>, <a class="yt-timestamp" data-t="00:12:50">[00:12:50]</a>, <a class="yt-timestamp" data-t="00:12:52">[00:12:52]</a>, <a class="yt-timestamp" data-t="00:12:55">[00:12:55]</a>.
*   Spin-Transfer Torque Magnetoresistive Random Access Memory (STT-MRAM) <a class="yt-timestamp" data-t="00:12:00">[00:12:00]</a>, <a class="yt-timestamp" data-t="00:12:03">[00:12:03]</a>, <a class="yt-timestamp" data-t="00:12:08">[00:12:08]</a>.

#### Circuit Level
This involves modifying peripheral circuits to perform calculations directly within SRAM or DRAM memory arrays (in-situ computing) <a class="yt-timestamp" data-t="00:12:57">[00:12:57]</a>, <a class="yt-timestamp" data-t="00:12:59">[00:12:59]</a>, <a class="yt-timestamp" data-t="00:13:01">[00:13:01]</a>, <a class="yt-timestamp" data-t="00:13:03">[00:13:03]</a>, <a class="yt-timestamp" data-t="00:13:06">[00:13:06]</a>, <a class="yt-timestamp" data-t="00:13:09">[00:13:09]</a>, <a class="yt-timestamp" data-t="00:13:12">[00:13:12]</a>.
*   **Ambit**: An in-memory accelerator proposed by researchers from Microsoft, [[Nvidia and AI chip competition | Nvidia]], Intel, ETH Zurich, and Carnegie Mellon <a class="yt-timestamp" data-t="00:13:21">[00:13:21]</a>, <a class="yt-timestamp" data-t="00:13:24">[00:13:24]</a>, <a class="yt-timestamp" data-t="00:13:26">[00:13:26]</a>, <a class="yt-timestamp" data-t="00:13:30">[00:13:30]</a>. It leverages DRAM sub-arrays by activating three rows simultaneously to implement AND/OR logic functions (two for inputs, one for output) <a class="yt-timestamp" data-t="00:13:33">[00:13:33]</a>, <a class="yt-timestamp" data-t="00:13:35">[00:13:35]</a>, <a class="yt-timestamp" data-t="00:13:38">[00:13:38]</a>, <a class="yt-timestamp" data-t="00:13:40">[00:13:40]</a>, <a class="yt-timestamp" data-t="00:13:42">[00:13:42]</a>, <a class="yt-timestamp" data-t="00:13:45">[00:13:45]</a>, <a class="yt-timestamp" data-t="00:13:47">[00:13:47]</a>, <a class="yt-timestamp" data-t="00:13:50">[00:13:50]</a>. While conceptually attractive for utilizing internal memory bandwidth <a class="yt-timestamp" data-t="00:13:53">[00:13:53]</a>, <a class="yt-timestamp" data-t="00:13:55">[00:13:55]</a>, <a class="yt-timestamp" data-t="00:13:57">[00:13:57]</a>, <a class="yt-timestamp" data-t="00:13:59">[00:13:59]</a>, Ambit takes multiple cycles for basic logic and struggles with more complex implementations like XNOR <a class="yt-timestamp" data-t="00:14:02">[00:14:02]</a>, <a class="yt-timestamp" data-t="00:14:05">[00:14:05]</a>, <a class="yt-timestamp" data-t="00:14:08">[00:14:08]</a>, <a class="yt-timestamp" data-t="00:14:12">[00:14:12]</a>, <a class="yt-timestamp" data-t="00:14:14">[00:14:14]</a>.

The main drawback of device and circuit level [[compute_in_memory_innovations | compute in memory]] approaches is that their performance still falls short of current Von Neumann [[role_of_ai_accelerators_in_neural_network_training_and_inference | GPU ASIC-centric systems]] <a class="yt-timestamp" data-t="00:14:16">[00:14:16]</a>, <a class="yt-timestamp" data-t="00:14:19">[00:14:19]</a>, <a class="yt-timestamp" data-t="00:14:21">[00:14:21]</a>, <a class="yt-timestamp" data-t="00:14:23">[00:14:23]</a>, <a class="yt-timestamp" data-t="00:14:25">[00:14:25]</a>, <a class="yt-timestamp" data-t="00:14:29">[00:14:29]</a>. Putting memory and logic together creates a "jack of all trades, master of none" situation <a class="yt-timestamp" data-t="00:14:33">[00:14:33]</a>, <a class="yt-timestamp" data-t="00:14:36">[00:14:36]</a>, <a class="yt-timestamp" data-t="00:14:38">[00:14:38]</a>.

#### System Level
This middle ground involves integrating discrete processing units and memory at a very close level, enabled by new packaging technologies like 2.5D or 3D memory die stacking <a class="yt-timestamp" data-t="00:14:39">[00:14:39]</a>, <a class="yt-timestamp" data-t="00:14:41">[00:14:41]</a>, <a class="yt-timestamp" data-t="00:14:43">[00:14:43]</a>, <a class="yt-timestamp" data-t="00:14:45">[00:14:45]</a>, <a class="yt-timestamp" data-t="00:14:47">[00:14:47]</a>, <a class="yt-timestamp" data-t="00:14:50">[00:14:50]</a>, <a class="yt-timestamp" data-t="00:14:53">[00:14:53]</a>, <a class="yt-timestamp" data-t="00:14:55">[00:14:55]</a>.
*   DRAM memory dies can be stacked on top of a CPU die <a class="yt-timestamp" data-t="00:14:59">[00:14:59]</a>, <a class="yt-timestamp" data-t="00:15:01">[00:15:01]</a>.
*   Connections are made via thousands of channels called Through-Silicon Vias (TSVs), providing immense internal bandwidth <a class="yt-timestamp" data-t="00:15:04">[00:15:04]</a>, <a class="yt-timestamp" data-t="00:15:06">[00:15:06]</a>, <a class="yt-timestamp" data-t="00:15:09">[00:15:09]</a>, <a class="yt-timestamp" data-t="00:15:12">[00:15:12]</a>, <a class="yt-timestamp" data-t="00:15:15">[00:15:15]</a>.
*   **AMD's 3D V-Cache**: Based on TSMC's 3D stacking packaging technology, this adds more memory cache to processors <a class="yt-timestamp" data-t="00:15:17">[00:15:17]</a>, <a class="yt-timestamp" data-t="00:15:19">[00:15:19]</a>, <a class="yt-timestamp" data-t="00:15:22">[00:15:22]</a>, <a class="yt-timestamp" data-t="00:15:25">[00:15:25]</a>, <a class="yt-timestamp" data-t="00:15:28">[00:15:28]</a>.
This approach allows for the integration of world-class memory and logic dies closer than ever before without needing to place them on the same die <a class="yt-timestamp" data-t="00:15:30">[00:15:30]</a>, <a class="yt-timestamp" data-t="00:15:31">[00:15:31]</a>, <a class="yt-timestamp" data-t="00:15:33">[00:15:33]</a>, <a class="yt-timestamp" data-t="00:15:36">[00:15:36]</a>, <a class="yt-timestamp" data-t="00:15:38">[00:15:38]</a>, <a class="yt-timestamp" data-t="00:15:41">[00:15:41]</a>, <a class="yt-timestamp" data-t="00:15:43">[00:15:43]</a>.

## Outlook

While innovative ideas are abundant, the challenge lies in executing them to outperform existing solutions like [[Nvidia and AI chip competition | Nvidia's]] A100 and H100 [[role_of_ai_accelerators_in_neural_network_training_and_inference | AI GPUs]], which are formidable competitors <a class="yt-timestamp" data-t="00:15:45">[00:15:45]</a>, <a class="yt-timestamp" data-t="00:15:48">[00:15:48]</a>, <a class="yt-timestamp" data-t="00:15:51">[00:15:51]</a>, <a class="yt-timestamp" data-t="00:15:53">[00:15:53]</a>, <a class="yt-timestamp" data-t="00:15:56">[00:15:56]</a>, <a class="yt-timestamp" data-t="00:15:58">[00:15:58]</a>, <a class="yt-timestamp" data-t="00:16:02">[00:16:02]</a>.

With the slowdown in leading-edge [[technological_innovations_and_challenges_in_supercomputer_design | semiconductor technology]], new methods are essential to create more powerful and robust hardware for AI <a class="yt-timestamp" data-t="00:16:04">[00:16:04]</a>, <a class="yt-timestamp" data-t="00:16:06">[00:16:06]</a>, <a class="yt-timestamp" data-t="00:16:08">[00:16:08]</a>, <a class="yt-timestamp" data-t="00:16:10">[00:16:10]</a>, <a class="yt-timestamp" data-t="00:16:12">[00:16:12]</a>.
<br>
> [!info] The necessity of overcoming memory limitations
> Generally speaking, bigger models perform better <a class="yt-timestamp" data-t="00:16:14">[00:16:14]</a>, <a class="yt-timestamp" data-t="00:16:17">[00:16:17]</a>. As today's best natural language processing and computer vision models still have room for improvement, they may need to grow even larger <a class="yt-timestamp" data-t="00:16:19">[00:16:19]</a>, <a class="yt-timestamp" data-t="00:16:21">[00:16:21]</a>, <a class="yt-timestamp" data-t="00:16:23">[00:16:23]</a>, <a class="yt-timestamp" data-t="00:16:26">[00:16:26]</a>. If new systems and hardware are not developed to overcome these limits, deep learning may fall short of its full potential <a class="yt-timestamp" data-t="00:16:28">[00:16:28]</a>, <a class="yt-timestamp" data-t="00:16:31">[00:16:31]</a>, <a class="yt-timestamp" data-t="00:16:33">[00:16:33]</a>, <a class="yt-timestamp" data-t="00:16:34">[00:16:34]</a>, <a class="yt-timestamp" data-t="00:16:36">[00:16:36]</a>.