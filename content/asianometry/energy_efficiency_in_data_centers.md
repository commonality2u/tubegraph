---
title: Energy efficiency in data centers
videoId: tJYSzc7YkY0
---

From: [[asianometry]] <br/> 

Data centers, particularly those fueling the current AI boom, are significant consumers of both energy and water, with these two resources being very closely tied together <a class="yt-timestamp" data-t="00:00:24">[00:00:24]</a>. A data center with 15 megawatts of IT capacity can use an estimated 80-130 million gallons of water annually <a class="yt-timestamp" data-t="00:00:06">[00:00:06]</a>, equivalent to the water consumption of three hospitals or two 18-hole golf courses <a class="yt-timestamp" data-t="00:00:11">[00:00:11]</a>. This article focuses on how [[technological_advancements_and_sustainability_in_data_centers|energy efficiency]] is measured and pursued within these facilities.

## Data Center Classification and Efficiency Metrics
Data centers vary widely in size and function, from small closet-sized setups to massive facilities spanning entire football fields <a class="yt-timestamp" data-t="00:00:52">[00:00:52]</a>. They are typically classified by floor space, the number of servers, or their power consumption <a class="yt-timestamp" data-t="00:01:01">[00:01:01]</a>. The largest facilities are termed "hyperscale," typically housing around 5,000 servers and covering 10,000 square meters <a class="yt-timestamp" data-t="00:01:09">[00:01:09]</a>.

A key metric for measuring a data center's [[energy_efficiency_in_ai_hardware|energy efficiency]] is Power Usage Effectiveness (PUE) <a class="yt-timestamp" data-t="00:01:43">[00:01:43]</a>. PUE is calculated by dividing the total energy delivered to the data center by the total energy consumed by the Information and Communications Technology (ICT) equipment <a class="yt-timestamp" data-t="00:01:49">[00:01:49]</a>. An ideal PUE is 1.0, meaning 100% of the energy is used by the ICT equipment <a class="yt-timestamp" data-t="00:01:58">[00:01:58]</a>.

Generally, larger data centers tend to have lower PUE values due to [[energy_efficiency_in_ai_hardware|energy efficiency scaling]] and economics <a class="yt-timestamp" data-t="00:01:36">[00:01:36]</a>. Hyperscale data centers from companies like Google and Microsoft report PUEs of 1.1 or 1.2, while a smaller "closet" data center might have a PUE of around 2.5 <a class="yt-timestamp" data-t="00:02:12">[00:02:12]</a>. The primary reason for this difference is the efficiency of their [[cooling_systems_in_data_centers|cooling systems]] <a class="yt-timestamp" data-t="00:02:19">[00:02:19]</a>.

## [[cooling_systems_in_data_centers|Cooling Systems]] and Their Energy Impact
Almost all electricity consumed by a data center is converted into heat <a class="yt-timestamp" data-t="00:02:34">[00:02:34]</a>. Even when not at full capacity, data centers still withdraw 60-100% of their maximum power, generating substantial heat <a class="yt-timestamp" data-t="00:02:42">[00:02:42]</a>. To prevent damage, electronic equipment must be kept cool, with hard disk drives needing to be around 45 degrees Celsius (113 degrees Fahrenheit) and solid-state chips up to 85 degrees Celsius <a class="yt-timestamp" data-t="00:02:59">[00:02:59]</a>.

The [[cooling_systems_in_data_centers|cooling system]] maintains stable temperature and humidity <a class="yt-timestamp" data-t="00:03:07">[00:03:07]</a>. Most data centers use air cooling in a raised-floor system, where cold air is supplied by a Computer Room Air Conditioner (CRAC) and circulates through aisles to cool servers <a class="yt-timestamp" data-t="00:03:14">[00:03:14]</a>. The hot air is then collected and sent to fluid heat exchangers for transfer outside <a class="yt-timestamp" data-t="00:03:39">[00:03:39]</a>.

A common system uses two fluid loops: a process loop (often water and glycol) to remove heat from the data room, and a water-based condenser loop for final heat transfer to the outside <a class="yt-timestamp" data-t="00:03:54">[00:03:54]</a>. Cooling towers are used to cool the water in the final loop, where some water evaporates, releasing energy <a class="yt-timestamp" data-t="00:04:23">[00:04:23]</a>. Approximately 1% of water evaporates for every 10 degrees Fahrenheit of cooling <a class="yt-timestamp" data-t="00:04:43">[00:04:43]</a>, necessitating a constant supply of make-up water <a class="yt-timestamp" data-t="00:04:56">[00:04:56]</a>.

## Indirect Water Consumption through Energy Generation
Beyond direct [[cooling_systems_in_data_centers|cooling systems]], data centers indirectly consume a vast amount of water through electricity generation <a class="yt-timestamp" data-t="00:05:01">[00:05:01]</a>. In 2022, data centers accounted for 1-1.3% of global electricity consumption <a class="yt-timestamp" data-t="00:05:07">[00:05:07]</a>. Thermoelectric power generation (coal, natural gas, nuclear) requires water to boil and spin turbines <a class="yt-timestamp" data-t="00:05:21">[00:05:21]</a>. A 2021 study showed 73% of US power came from such means <a class="yt-timestamp" data-t="00:05:27">[00:05:27]</a>. Water usage for energy generation can be 2-3 times larger than water directly consumed by [[cooling_systems_in_data_centers|cooling systems]] <a class="yt-timestamp" data-t="00:05:48">[00:05:48]</a>. While evaporative cooling towers consume thousands of gallons directly, foregoing them for air conditioning systems leads to even higher indirect water usage due to increased power consumption <a class="yt-timestamp" data-t="00:06:03">[00:06:03]</a>.

## Strategies for Improved Energy Efficiency

### Renewable Energy
Companies like Apple are working to power their data centers with more renewable energy or offset their energy use through programs <a class="yt-timestamp" data-t="00:06:09">[00:06:09]</a>. However, most data centers currently draw power from their local grid <a class="yt-timestamp" data-t="00:06:17">[00:06:17]</a>.

### Free Cooling
"Free cooling" involves using the natural environment to cool data centers, minimizing reliance on mechanical HVAC systems or excessive water evaporation <a class="yt-timestamp" data-t="00:09:18">[00:09:18]</a>.
*   **Direct Free Cooling**: This involves bringing outside air directly into the data center <a class="yt-timestamp" data-t="00:09:25">[00:09:25]</a>. However, this air often requires dehumidification, filtration, and cleaning to protect electronics <a class="yt-timestamp" data-t="00:09:37">[00:09:37]</a>. While energy savings depend on location, studies in Europe show average savings of 5.4-7.9% <a class="yt-timestamp" data-t="00:10:04">[00:10:04]</a>, and up to 60% in some Australian cities <a class="yt-timestamp" data-t="00:10:17">[00:10:17]</a>. This potential drives Nordic governments to attract data center investments due to their cold climates <a class="yt-timestamp" data-t="00:10:28">[00:10:28]</a>.
*   **Waterside Free Cooling**: Data centers near cold seas can use seawater for cooling <a class="yt-timestamp" data-t="00:10:57">[00:10:57]</a>. Google's Hamina data center in Finland, a converted paper mill, uses existing pipes to draw fresh seawater. Thanks to a two-loop system, the seawater never mixes with the internal cooling fluid, and after use, it is mixed with more seawater to cool down before being returned to the sea <a class="yt-timestamp" data-t="00:11:14">[00:11:14]</a>.

### Heat Recapture
Another strategy is to recover and reuse the heat generated by data centers <a class="yt-timestamp" data-t="00:11:30">[00:11:30]</a>. This recaptured heat can be used for purposes such as desalinating water, pre-heating water in thermoelectric plants, direct power generation, or piping it to homes for space or water heating <a class="yt-timestamp" data-t="00:11:44">[00:11:44]</a>. Captured heat from air-cooled data centers can be warm enough (35-45 degrees Celsius) for household heating, which accounts for about 6% of America's total energy consumption <a class="yt-timestamp" data-t="00:12:03">[00:12:03]</a>.

The main challenge is that heat cannot be efficiently moved over long distances, requiring demand sources (households) to be relatively close to the data center <a class="yt-timestamp" data-t="00:12:13">[00:12:13]</a>. Infrastructure, such as district heating networks, is also a barrier in many cities <a class="yt-timestamp" data-t="00:12:24">[00:12:24]</a>. Data centers adopting heat recapture are also likely to employ liquid cooling solutions, as liquid is much more efficient at capturing and transferring heat than air <a class="yt-timestamp" data-t="00:12:48">[00:12:48]</a>, reducing overall energy consumption and improving processor performance <a class="yt-timestamp" data-t="00:12:53">[00:12:53]</a>.

### Optimizing Temperature Ranges
The American Society of Heating, Refrigerating and Air-Conditioning Engineers (ASHRAE) recommends a data center temperature range of 15 to 32 degrees Celsius <a class="yt-timestamp" data-t="00:13:21">[00:13:21]</a>. While some operators cool their facilities even more, assuming better electronics performance, raising a data center's temperature by just 1-2 degrees can yield significant financial benefits <a class="yt-timestamp" data-t="00:13:36">[00:13:36]</a>. Google has conducted tests showing their data centers can operate at higher temperatures <a class="yt-timestamp" data-t="00:13:47">[00:13:47]</a>.

## The Impact of the AI Boom
The ongoing AI boom is rapidly escalating energy demands for data centers <a class="yt-timestamp" data-t="00:14:03">[00:14:03]</a>. Microsoft's 2022 sustainability report indicated a 34% increase in water consumption from 2021 to 2022, largely attributed to ChatGPT and other generative AI products <a class="yt-timestamp" data-t="00:14:21">[00:14:21]</a>. Utility providers are seeing individual data center requests for power ranging from 60-90 megawatts, with some large campuses requiring up to several gigawatts <a class="yt-timestamp" data-t="00:14:54">[00:14:54]</a>.

Analysts predict that AI will propel data center share to 4.5% of global energy generation by 2030 <a class="yt-timestamp" data-t="00:15:09">[00:15:09]</a>. This significant increase in power consumption implies a correlated rise in [[water_consumption_in_data_centers|water consumption]] <a class="yt-timestamp" data-t="00:15:17">[00:15:17]</a>. Nvidia's increasingly power-hungry GPU products, like the B100, will further contribute to higher power and [[water_consumption_in_data_centers|water consumption]] for training leading-edge AI models and large-scale deployments <a class="yt-timestamp" data-t="00:15:30">[00:15:30]</a>, <a class="yt-timestamp" data-t="00:15:35">[00:15:35]</a>.

## Conclusion
The future of computing and AI will require substantially more electricity and [[water_consumption_in_data_centers|water]] <a class="yt-timestamp" data-t="00:15:43">[00:15:43]</a>. This demand begins at the [[semiconductor_industry_water_usage|semiconductor fabrication]] stage; for example, TSMC in Taiwan is projected to consume 12.5% of Taiwan's total energy by 2025 <a class="yt-timestamp" data-t="00:15:56">[00:15:56]</a>. Once chips are in data centers, their operation generates significant heat, requiring even more energy for cooling <a class="yt-timestamp" data-t="00:16:07">[00:16:07]</a>.

To address these growing demands, future data centers must rapidly adopt a combination of [[cooling_systems_in_data_centers|free-cooling]], waste heat recovery, and renewable energy sources like solar <a class="yt-timestamp" data-t="00:16:16">[00:16:16]</a>. This transition is critical for both [[technological_advancements_and_sustainability_in_data_centers|sustainability]] and economic viability <a class="yt-timestamp" data-t="00:16:22">[00:16:22]</a>.