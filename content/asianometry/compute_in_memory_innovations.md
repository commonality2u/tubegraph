---
title: Compute in memory innovations
videoId: 5tmGKTNW8DQ
---

From: [[asianometry]] <br/> 

Since the deep learning explosion began in 2012, the industry's largest models have grown hundreds of thousands of times <a class="yt-timestamp" data-t="00:00:02">[00:00:02]</a>. Modern models like OpenAI's DALL-E 2 have 3.5 billion parameters, Google's Imagen has 4.6 billion, and GPT-3 has 175 billion <a class="yt-timestamp" data-t="00:00:10">[00:00:10]</a>. Google recently pre-trained a model with 1 trillion parameters <a class="yt-timestamp" data-t="00:00:25">[00:00:25]</a>. These increasingly large models strain hardware capabilities, with many limitations tied to memory usage <a class="yt-timestamp" data-t="00:00:28">[00:00:28]</a>. This has led to exploring [[memory_wall_problem_in_ai_hardware | Deep learning's memory wall problem]] and memory-centric paradigms to solve it <a class="yt-timestamp" data-t="00:00:38">[00:00:38]</a>.

## The Memory Wall Problem in AI Hardware

Virtually every modern computer runs on a [[Von Neumann architecture limitations | Von Neumann architecture]], which stores both instructions and data on the same memory bank <a class="yt-timestamp" data-t="00:01:00">[00:01:00]</a>. Processing units (CPUs or GPUs) access memory to execute instructions and process data <a class="yt-timestamp" data-t="00:01:14">[00:01:14]</a>. While effective for software, this architecture differs from the human brain, which tightly integrates compute with memory and I/O communication <a class="yt-timestamp" data-t="00:01:28">[00:01:28]</a>. Computers separate compute from memory and communication, leading to consequences, especially for memory <a class="yt-timestamp" data-t="00:01:47">[00:01:47]</a>.

The AI hardware industry is rapidly scaling memory and processing unit performance <a class="yt-timestamp" data-t="00:01:55">[00:01:55]</a>. For instance, [[Nvidias innovation with general processing unit concept | Nvidia's]] V100 GPU had a 32 GB offering in 2017, and today's top-of-the-line [[Nvidias innovation with general processing unit concept | Nvidia]] Data Center GPUs (A100 and H100) support 80 GB of memory <a class="yt-timestamp" data-t="00:02:02">[00:02:02]</a>. Despite this, hardware performance, particularly memory, struggles to keep pace with model growth <a class="yt-timestamp" data-t="00:02:15">[00:02:15]</a>.

### Capacity and Performance Bottlenecks
Memory allocations for leading-edge models can easily exceed hundreds of gigabytes <a class="yt-timestamp" data-t="00:02:23">[00:02:23]</a>. A trillion-parameter model is estimated to require 320 A100 GPUs, each with 80 GB of memory <a class="yt-timestamp" data-t="00:02:30">[00:02:30]</a>. This disparity means processing units waste cycles waiting for data to travel to and from memory and for read/write operations <a class="yt-timestamp" data-t="00:02:39">[00:02:39]</a>. This limitation is known as the [[memory_wall_problem_in_ai_hardware | memory wall]] or memory capacity bottleneck <a class="yt-timestamp" data-t="00:02:54">[00:02:54]</a>.

### Practical and Technological Limits to Memory Expansion
Adding more memory faces practical and technological limits, including connections and wiring <a class="yt-timestamp" data-t="00:03:07">[00:03:07]</a>.

*   **Energy Limitations**: Shuttling data between the chip and memory is energy-intensive due to electrical connection losses <a class="yt-timestamp" data-t="00:03:19">[00:03:19]</a>. Accessing off-chip memory uses 200 times more energy than a floating-point operation <a class="yt-timestamp" data-t="00:03:32">[00:03:32]</a>. Eighty percent of the Google TPU's energy usage comes from its electrical connections, not its logic computational units <a class="yt-timestamp" data-t="00:03:40">[00:03:40]</a>. DRAM alone accounts for 40% of total system power in some GPU and CPU systems <a class="yt-timestamp" data-t="00:03:47">[00:03:47]</a>. Energy comprises 40% of a data center's operating costs, making storage and memory a significant factor in profitability <a class="yt-timestamp" data-t="00:03:54">[00:03:54]</a>.
*   **Capital Costs**: Purchasing AI hardware involves substantial upfront costs <a class="yt-timestamp" data-t="00:04:07">[00:04:07]</a>. A trillion-parameter model requiring 320 A100 GPUs (each costing $32,000 MSRP) would be $10 million <a class="yt-timestamp" data-t="00:04:16">[00:04:16]</a>. A 100-trillion parameter model might need over 6,000 such GPUs <a class="yt-timestamp" data-t="00:04:29">[00:04:29]</a>. This does not include energy costs for inference, which account for 90% of a model's total costs <a class="yt-timestamp" data-t="00:04:44">[00:04:44]</a>. These costs risk restricting advanced AI benefits to only the wealthiest tech giants or governments <a class="yt-timestamp" data-t="00:04:48">[00:04:48]</a>.

### Historical and Technological Limits

Much of the shortcomings relate to historical and technological limits. The industry adopted Dynamic Random Access Memory (DRAM) in the 1960s and 70s as the basis for computers due to its relatively low latency and cheap bulk manufacturing <a class="yt-timestamp" data-t="00:04:55">[00:04:55]</a>. While the memory industry was valued at $37 billion by 1995, compute scaling outpaced memory scaling after 1980 <a class="yt-timestamp" data-t="00:05:16">[00:05:16]</a>. CPU/GPU industries optimized for transistor density, while the memory industry had to scale DRAM capacity, bandwidth, and latency simultaneously <a class="yt-timestamp" data-t="00:05:32">[00:05:32]</a>. Over 20 years, memory capacity improved 128 times, bandwidth 20 times, but latency only 30% <a class="yt-timestamp" data-t="00:05:51">[00:05:51]</a>.

Shrinking DRAM cells beyond a certain size results in worse performance, reliability, security, latency, and energy efficiency <a class="yt-timestamp" data-t="00:06:02">[00:06:02]</a>. A DRAM cell stores data as a charge in a capacitor, accessed via a transistor <a class="yt-timestamp" data-t="00:06:16">[00:06:16]</a>. Scaling down to nanoscale makes the capacitor and transistor leakier and more vulnerable to electrical noise, opening new security vulnerabilities <a class="yt-timestamp" data-t="00:06:31">[00:06:31]</a>. These fundamental technical limitations are extremely difficult to engineer around, with only small solutions expected from grinding out improvements <a class="yt-timestamp" data-t="00:06:47">[00:06:47]</a>.

## Compute in Memory: A Radical Idea

The challenges open the door to radical ideas that could offer a 10x improvement over the current paradigm <a class="yt-timestamp" data-t="00:07:00">[00:07:00]</a>. One such idea is to alleviate or eliminate the [[Von Neumann architecture limitations | Von Neumann bottleneck]] and [[memory_wall_problem_in_ai_hardware | memory wall problems]] by making memory perform computations itself <a class="yt-timestamp" data-t="00:07:21">[00:07:21]</a>.

"Compute in memory" refers to random access memory with integrated processing elements, either very near each other or on the same die <a class="yt-timestamp" data-t="00:07:30">[00:07:30]</a>. Other names include processing in memory, computational RAM, near-data computing, memory-centric computing, and in-memory computation <a class="yt-timestamp" data-t="00:07:42">[00:07:42]</a>. While concepts like SRAM for cache on-chip with the CPU exist, the focus is on bringing processing ability to the computer's main memory <a class="yt-timestamp" data-t="00:08:03">[00:08:03]</a>.

This idea is well-suited for deep learning, as neural network models primarily involve calculating massive matrices (multiply and accumulate, or MAC, operations) <a class="yt-timestamp" data-t="00:08:20">[00:08:20]</a>. The arithmetic is simple, but the sheer volume of operations is the problem <a class="yt-timestamp" data-t="00:08:35">[00:08:35]</a>. An ideal compute in memory chip could execute MAC operations directly within the memory chip <a class="yt-timestamp" data-t="00:08:41">[00:08:41]</a>. This is particularly beneficial for running inference on the edge (outside data centers), where energy, size, or heat restrictions apply, as it could cut a neural network's energy usage by up to 80% <a class="yt-timestamp" data-t="00:08:48">[00:08:48]</a>.

### Historical Explorations

The concept of compute in memory dates back to the 1960s <a class="yt-timestamp" data-t="00:09:03">[00:09:03]</a>.

*   **Harold Stone (1960s)**: Professor Harold Stone of Stanford University notably explored the idea of logic and memory integration. He observed that while microprocessor transistor counts grew rapidly, processor communication with memory was limited by pin count <a class="yt-timestamp" data-t="00:09:06">[00:09:06]</a>. He proposed moving part of the computation into memory caches <a class="yt-timestamp" data-t="00:09:24">[00:09:24]</a>.
*   **Terraces (1995)**: Terraces produced what was likely the first processor in memory chip: a standard 4-bit memory with an integrated single-bit logical unit <a class="yt-timestamp" data-t="00:09:33">[00:09:33]</a>. This arithmetic logic unit could take data, apply simple logic, and write it back to memory <a class="yt-timestamp" data-t="00:09:44">[00:09:44]</a>.
*   **UC Berkeley IRAM Project (1997)**: Professors at UC Berkeley, including [[Advancements in semiconductor technology | David Patterson]] (inventor of RISC), created the IRAM project with the goal of putting a microprocessor and DRAM on the same chip <a class="yt-timestamp" data-t="00:09:53">[00:09:53]</a>.

Despite these proposals, none gained widespread adoption due to practical reasons <a class="yt-timestamp" data-t="00:10:07">[00:10:07]</a>.

### Challenges in Manufacturing

Manufacturing memory and logic together is difficult because their fabrication processes have opposing goals <a class="yt-timestamp" data-t="00:10:14">[00:10:14]</a>. Logic transistors prioritize speed and performance, while memory transistors aim for high density, low cost, and low leakage <a class="yt-timestamp" data-t="00:10:22">[00:10:22]</a>. DRAM designs are highly regular with many parallel wires, whereas logic designs are more complex <a class="yt-timestamp" data-t="00:10:38">[00:10:38]</a>.

*   **Metal Layers**: Circuit elements use metal layers for connections; more layers allow greater complexity but increase current leakage and reduce reliability <a class="yt-timestamp" data-t="00:10:46">[00:10:46]</a>. Contemporary DRAM processes use 3-4 metal layers, while logic processes use 7-12 or more <a class="yt-timestamp" data-t="00:10:57">[00:10:57]</a>.
    *   Making logic circuits with a DRAM process would result in circuits 80% bigger and performing 22% worse <a class="yt-timestamp" data-t="00:11:07">[00:11:07]</a>.
    *   Making DRAM cells with a logic process (embedded DRAM or eDRAM) would result in cells that use significantly more power, take up to 10 times more space, and are less reliable <a class="yt-timestamp" data-t="00:11:19">[00:11:19]</a>.

## Workarounds and Approaches

The industry has considered these manufacturing shortcomings and developed various workarounds, often categorized into device, circuit, and system levels <a class="yt-timestamp" data-t="00:11:34">[00:11:34]</a>.

### Device Level
This level focuses on new types of memory hardware beyond conventional DRAM and SRAM <a class="yt-timestamp" data-t="00:11:47">[00:11:47]</a>. Notable examples include Resistive Random Access Memory (ReRAM) and Spin Transfer Torque Magnetoresistive Random Access Memory (STT-MRAM) <a class="yt-timestamp" data-t="00:11:53">[00:11:53]</a>.

*   **ReRAM**: One of the more promising emerging memory technologies <a class="yt-timestamp" data-t="00:12:08">[00:12:08]</a>. Unlike conventional RAM that stores information as a charge in a capacitor, ReRAM stores information by changing the electrical resistance of a material <a class="yt-timestamp" data-t="00:12:12">[00:12:12]</a>. This structure allows ReRAM to compute logical functions directly within memory cells <a class="yt-timestamp" data-t="00:12:26">[00:12:26]</a>. ReRAM is close to commercialization due to its compatibility with silicon CMOS, but substantial hurdles remain <a class="yt-timestamp" data-t="00:12:42">[00:12:42]</a>.

### Circuit Level
This involves modifying peripheral circuits to perform calculations directly inside SRAM or DRAM memory arrays, known as "in situ computing" <a class="yt-timestamp" data-t="00:12:57">[00:12:57]</a>. These methods are clever but require intimate knowledge of memory operation and can be difficult to understand <a class="yt-timestamp" data-t="00:13:12">[00:13:12]</a>.

*   **Ambit**: An in-memory accelerator proposed by researchers from Microsoft, [[Nvidias innovation with general processing unit concept | Nvidia]], Intel, ETH Zurich, and Carnegie Mellon <a class="yt-timestamp" data-t="00:13:21">[00:13:21]</a>. It leverages DRAM memory sub-arrays by activating three rows at a time (two for inputs, one for output) to implement AND/OR logic functions <a class="yt-timestamp" data-t="00:13:33">[00:13:33]</a>. This concept utilizes the memory's internal bandwidth for calculations <a class="yt-timestamp" data-t="00:13:53">[00:13:53]</a>. However, Ambit performs basic logic operations over multiple cycles, and more complex logic (like XNOR) remains challenging <a class="yt-timestamp" data-t="00:14:02">[00:14:02]</a>.

A significant downside of device and circuit-level compute in memory approaches is that their performance still falls short of current [[Von Neumann architecture limitations | Von Neumann]] GPU/ASIC-centric systems <a class="yt-timestamp" data-t="00:14:16">[00:14:16]</a>. Putting memory and logic together often results in a "jack of all trades, master of none" situation, echoing challenges from the 1990s <a class="yt-timestamp" data-t="00:14:33">[00:14:33]</a>.

### System Level
The industry appears to be moving towards implementing compute in memory at a system level, integrating discrete processing units and memory at a very close level <a class="yt-timestamp" data-t="00:14:39">[00:14:39]</a>. This is possible due to new packaging technologies such as 2.5D or 3D memory die stacking, where DRAM memory dies are stacked on top of a CPU die <a class="yt-timestamp" data-t="00:14:53">[00:14:53]</a>. These memories connect to the CPU via thousands of channels called Through Silicon Vias (TSVs), providing immense internal bandwidth <a class="yt-timestamp" data-t="00:15:04">[00:15:04]</a>.

*   **AMD 3D V-Cache**: AMD is working on a similar concept with their 3D V-Cache, based on TSMC's 3D stacking packaging technology, used to add more memory cache to their processor chips <a class="yt-timestamp" data-t="00:15:15">[00:15:15]</a>.
*   **AI ASICs**: Future [[technological_innovations_and_challenges_in_supercomputer_design | advanced packaging technologies]] could add hundreds of gigabytes of memory to an AI ASIC, integrating world-class memory and logic dies closer than ever without placing them on the same die <a class="yt-timestamp" data-t="00:15:28">[00:15:28]</a>.

## Conclusion

While laboratory ideas are abundant, successful execution requires performance competitive with existing market solutions, like the formidable [[Nvidias innovation with general processing unit concept | Nvidia]] A100 and H100 AI GPUs <a class="yt-timestamp" data-t="00:15:45">[00:15:45]</a>. However, with [[technological_innovations_and_challenges_in_supercomputer_design | leading-edge semiconductor technology slowing]], new methods are necessary to advance hardware for AI <a class="yt-timestamp" data-t="00:16:04">[00:16:04]</a>. Generally, bigger models perform better, and today's best natural language processing and computer vision models may need to grow further to improve <a class="yt-timestamp" data-t="00:16:14">[00:16:14]</a>. Without developing new systems and hardware to overcome these memory limitations, deep learning might fall short of its "Great Expectations" <a class="yt-timestamp" data-t="00:16:28">[00:16:28]</a>.