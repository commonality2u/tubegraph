---
title: Energy efficiency in AI hardware
videoId: 5tmGKTNW8DQ
---

From: [[asianometry]] <br/> 

The growth of deep learning models since 2012 has been exponential, with leading models increasing in size by hundreds of thousands of times. For example, OpenAI's DALL-E 2 has 3.5 billion parameters, Google's Imagen has 4.6 billion, and GPT-3 has 175 billion parameters <a class="yt-timestamp" data-t="00:00:10">[00:00:10]</a>. Google has even pre-trained a model with 1 trillion parameters <a class="yt-timestamp" data-t="00:00:26">[00:00:26]</a>. These increasingly large models severely strain the capabilities of current hardware, particularly concerning memory <a class="yt-timestamp" data-t="00:00:28">[00:00:28]</a>. This challenge is known as the "memory wall problem" or "memory capacity bottleneck" <a class="yt-timestamp" data-t="00:00:40">[00:00:40]</a>, <a class="yt-timestamp" data-t="00:02:54">[00:02:54]</a>.

## The Von Neumann Bottleneck and Memory Wall
Virtually all modern computers operate on a Von Neumann architecture, which means both instructions and data are stored in the same memory bank <a class="yt-timestamp" data-t="00:01:00">[00:01:00]</a>. Processing units, such as CPUs or [[evolution_of_ai_hardware_and_gpus | GPUs]], access this shared memory to execute instructions and process data <a class="yt-timestamp" data-t="00:01:12">[00:01:12]</a>. While this architecture has been fundamental to software development, it differs significantly from how a human brain operates <a class="yt-timestamp" data-t="00:01:21">[00:01:21]</a>. The brain tightly integrates compute with memory and input/output, using relatively low precision <a class="yt-timestamp" data-t="00:01:31">[00:01:31]</a>. In contrast, computers use high-precision floating-point arithmetic but separate compute from memory and communication <a class="yt-timestamp" data-t="00:01:40">[00:01:40]</a>. This separation leads to significant consequences, especially for memory <a class="yt-timestamp" data-t="00:01:51">[00:01:51]</a>.

Despite efforts by the [[the_ai_and_ai_chip_boom_landscape | AI hardware]] industry to scale up memory and processing unit performance, hardware improvements are not keeping pace with the rapid growth of AI models <a class="yt-timestamp" data-t="00:01:55">[00:01:55]</a>. For example, Nvidia's V100 [[evolution_of_ai_hardware_and_GPUs | GPU]] from 2017 had a 32GB offering, while current top-tier Nvidia A100 and H100 data center [[evolution_of_ai_hardware_and_GPUs | GPUs]] feature 80GB of memory <a class="yt-timestamp" data-t="00:02:02">[00:02:02]</a>, <a class="yt-timestamp" data-t="00:02:08">[00:02:08]</a>. However, leading-edge models can easily require hundreds of gigabytes of memory <a class="yt-timestamp" data-t="00:02:23">[00:02:23]</a>. A trillion-parameter model is estimated to need 320 A100 [[evolution_of_ai_hardware_and_GPUs | GPUs]], each with 80GB of memory <a class="yt-timestamp" data-t="00:02:30">[00:02:30]</a>. This disparity in processing power and memory capacity causes processing units to waste cycles waiting for data to travel to and from memory and for memory operations to complete <a class="yt-timestamp" data-t="00:02:39">[00:02:39]</a>.

## Energy and Financial Limitations
Simply adding more memory to [[evolution_of_ai_hardware_and_GPUs | GPUs]] is not a viable solution due to practical and technological limits, including issues with connections and wiring <a class="yt-timestamp" data-t="00:03:00">[00:03:00]</a>.

### Energy Consumption
A significant limitation is the energy consumed when shuttling data between the chip and memory, as these electrical connections incur losses <a class="yt-timestamp" data-t="00:03:19">[00:03:19]</a>. Accessing off-chip memory uses 200 times more energy than a floating-point operation <a class="yt-timestamp" data-t="00:03:32">[00:03:32]</a>. For instance, 80% of a Google TPU's energy usage comes from its electrical connections, not its computational units <a class="yt-timestamp" data-t="00:03:39">[00:03:39]</a>. In some modern [[evolution_of_ai_hardware_and_GPUs | GPU]] and [[evolution_of_ai_hardware_and_GPUs | CPU]] systems, DRAM alone accounts for 40% of the total system power <a class="yt-timestamp" data-t="00:03:47">[00:03:47]</a>.

### Financial Sustainability
[[energy_efficiency_in_data_centers | Energy makes up 40% of a data center's operating costs]] <a class="yt-timestamp" data-t="00:03:54">[00:03:54]</a>. Therefore, storage and memory are significant factors in a data center's ongoing profitability <a class="yt-timestamp" data-t="00:04:00">[00:04:00]</a>. In addition to operational energy costs, there are substantial upfront capital costs for purchasing [[the_ai_and_ai_chip_boom_landscape | AI hardware]] <a class="yt-timestamp" data-t="00:04:07">[00:04:07]</a>. A trillion-parameter model requiring 320 A100 [[evolution_of_ai_hardware_and_GPUs | GPUs]] (at $32,000 each) would cost approximately $10 million for the hardware alone <a class="yt-timestamp" data-t="00:04:16">[00:04:16]</a>. A 100 trillion-parameter model could require over 6,000 such [[evolution_of_ai_hardware_and_GPUs | GPUs]] <a class="yt-timestamp" data-t="00:04:29">[00:04:29]</a>. These costs do not even include the energy expenses for running inference, which constitutes 90% of a model's total cost <a class="yt-timestamp" data-t="00:04:36">[00:04:36]</a>. This situation risks restricting the benefits of advanced [[the_ai_and_ai_chip_boom_landscape | AI]] to only the wealthiest tech giants or governments <a class="yt-timestamp" data-t="00:04:48">[00:04:48]</a>.

## Historical Context and Scaling Limits
The industry's reliance on Dynamic Random Access Memory (DRAM) since the 1960s and 70s was largely due to its low latency and cheap bulk manufacturing <a class="yt-timestamp" data-t="00:04:55">[00:04:55]</a>. While this worked well for decades, by 1980, compute scaling began to far outpace memory scaling <a class="yt-timestamp" data-t="00:05:26">[00:05:26]</a>. [[evolution_of_ai_hardware_and_GPUs | CPU]] and [[evolution_of_ai_hardware_and_GPUs | GPU]] industries primarily optimized for transistor density <a class="yt-timestamp" data-t="00:05:32">[00:05:32]</a>. In contrast, the memory industry had to balance capacity, bandwidth, and latency simultaneously <a class="yt-timestamp" data-t="00:05:40">[00:05:40]</a>. Over the past 20 years, memory capacity improved 128 times, and bandwidth 20 times, but latency only improved by 30% <a class="yt-timestamp" data-t="00:05:51">[00:05:51]</a>.

Furthermore, shrinking DRAM cells beyond a certain size leads to diminishing returns, resulting in worse performance, less reliability, reduced security, and increased energy inefficiency <a class="yt-timestamp" data-t="00:06:02">[00:06:02]</a>. A DRAM cell stores a bit of data as a charge in a capacitor, accessed by a transistor <a class="yt-timestamp" data-t="00:06:16">[00:06:16]</a>. When scaled down to nanoscale sizes, the capacitor and transistor become leakier and more vulnerable to electrical noise and security vulnerabilities <a class="yt-timestamp" data-t="00:06:31">[00:06:31]</a>. These fundamental technical limitations make it extremely difficult to engineer around <a class="yt-timestamp" data-t="00:06:47">[00:06:47]</a>.

## Solutions: Compute in Memory
The limitations of conventional hardware open the door for radical new ideas, potentially offering 10x improvements <a class="yt-timestamp" data-t="00:07:00">[00:07:00]</a>. One such idea is [[silicon_photonics_in_ai_chip_development | Silicon photonics]], which uses light for energy-efficient data transfer <a class="yt-timestamp" data-t="00:07:09">[00:07:09]</a>. Another, more central solution, is to alleviate or eliminate the Von Neumann bottleneck and memory wall by making memory perform computations itself <a class="yt-timestamp" data-t="00:07:21">[00:07:21]</a>.

### What is Compute in Memory?
"Compute in memory" (also known as processing in memory, computational RAM, near-data computing, or memory-centric computing) refers to integrating processing elements directly within random access memory <a class="yt-timestamp" data-t="00:07:30">[00:07:30]</a>. This means the processing units and memory are very close or even on the same die <a class="yt-timestamp" data-t="00:07:37">[00:07:37]</a>.

This concept is well-suited for deep learning, where neural networks involve massive matrix calculations <a class="yt-timestamp" data-t="00:08:20">[00:08:20]</a>. The arithmetic itself is relatively simple, but the sheer volume of operations is the challenge <a class="yt-timestamp" data-t="00:08:35">[00:08:35]</a>. An ideal compute in memory chip could execute multiply and accumulate (MAC) operations directly inside the memory chip <a class="yt-timestamp" data-t="00:08:41">[00:08:41]</a>.

#### Benefits for AI Inference
[[ai_inference_versus_training_hardware | This approach is particularly helpful for running AI inference]] on edge devices outside data centers, where energy, size, or heat restrictions are common <a class="yt-timestamp" data-t="00:08:48">[00:08:48]</a>. Being able to cut up to 80% of a neural network's energy usage would be a significant advantage <a class="yt-timestamp" data-t="00:08:55">[00:08:55]</a>.

### Historical Attempts and Manufacturing Challenges
The idea of compute in memory dates back to the 1960s with Professor Harold Stone's exploration of logic and memory integration <a class="yt-timestamp" data-t="00:09:03">[00:09:03]</a>. The 1990s saw further developments, including Terraces' 1995 processor-in-memory chip and UC Berkeley's IRAM project (1997) to put a microprocessor and DRAM on the same chip <a class="yt-timestamp" data-t="00:09:30">[00:09:30]</a>, <a class="yt-timestamp" data-t="00:09:53">[00:09:53]</a>.

However, these early attempts didn't gain traction due to manufacturing challenges <a class="yt-timestamp" data-t="00:10:09">[00:10:09]</a>. Memory and logic have opposing manufacturing goals: logic transistors prioritize speed and performance, while memory transistors aim for high density, low cost, and low leakage <a class="yt-timestamp" data-t="00:10:14">[00:10:14]</a>. DRAM designs are highly regular with many parallel wires, whereas logic designs are more complex <a class="yt-timestamp" data-t="00:10:38">[00:10:38]</a>. For example, contemporary DRAM processes use 3-4 metal layers, while logic processes use 7-12 or more <a class="yt-timestamp" data-t="00:10:57">[00:10:57]</a>. Trying to make logic circuits with a DRAM process would make them 80% bigger and 22% worse performing, while making DRAM cells with a logic process (embedded DRAM) would result in cells that use significantly more power, take 10 times more space, and are less reliable <a class="yt-timestamp" data-t="00:11:07">[00:11:07]</a>.

### Modern Workarounds and Approaches
The industry is now considering various workarounds across device, circuit, and system levels:

*   **Device Level:** This involves new memory hardware types beyond conventional DRAM and SRAM, such as Resistive Random Access Memory (ReRAM) and Spin Transfer Torque Magnetoresistive Random Access Memory (STT-MRAM) <a class="yt-timestamp" data-t="00:11:47">[00:11:47]</a>. ReRAM, which stores information by changing electrical resistance, is promising due to its compatibility with silicon CMOS, allowing logical functions directly within memory cells <a class="yt-timestamp" data-t="00:12:08">[00:12:08]</a>. However, substantial hurdles remain before commercialization <a class="yt-timestamp" data-t="00:12:50">[00:12:50]</a>.
*   **Circuit Level:** This approach modifies peripheral circuits to perform calculations directly within SRAM or DRAM memory arrays (in-situ computing) <a class="yt-timestamp" data-t="00:12:57">[00:12:57]</a>. An example is Ambit, an in-memory accelerator that activates three rows of DRAM cells simultaneously to implement AND/OR logic functions <a class="yt-timestamp" data-t="00:13:21">[00:13:21]</a>. While conceptually attractive for utilizing memory's internal bandwidth, these methods can take multiple cycles and face challenges with more complex logic implementations <a class="yt-timestamp" data-t="00:13:53">[00:13:53]</a>.
*   **System Level:** This is the current middle ground, integrating discrete processing units and memory at a very close level <a class="yt-timestamp" data-t="00:14:39">[00:14:39]</a>. This is enabled by new packaging technologies like 2.5D or 3D memory die stacking, where DRAM memory dies are stacked on top of a [[evolution_of_ai_hardware_and_GPUs | CPU]] die and connected via thousands of channels called through-silicon vias (TSVs) <a class="yt-timestamp" data-t="00:14:53">[00:14:53]</a>. This provides immense internal bandwidth <a class="yt-timestamp" data-t="00:15:12">[00:15:12]</a>. AMD's 3D V-Cache, based on TSMC's 3D stacking, is an example, adding more memory cache to processors <a class="yt-timestamp" data-t="00:15:16">[00:15:16]</a>. This technology promises to integrate world-class memory and logic dies closer than ever without placing them on the same die <a class="yt-timestamp" data-t="00:15:36">[00:15:36]</a>.

## The Future of AI Hardware
While innovative ideas are plentiful, the [[challenges_in_scaling_ai_hardware | challenge lies in executing them]] to outperform existing market solutions like the formidable [[nvidia_and_competition_in_the_ai_chip_market | Nvidia]] A100 and H100 [[evolution_of_ai_hardware_and_GPUs | AI GPUs]] <a class="yt-timestamp" data-t="00:15:45">[00:15:45]</a>. However, with leading-edge semiconductor technology slowing down, new methods are crucial to develop more powerful and robust hardware for [[the_ai_and_ai_chip_boom_landscape | AI]] <a class="yt-timestamp" data-t="00:16:04">[00:16:04]</a>. Bigger models generally perform better, and current [[the_ai_and_ai_chip_boom_landscape | AI]] models will likely need to grow even larger to improve further <a class="yt-timestamp" data-t="00:16:14">[00:16:14]</a>. Without advancements in systems and [[custom_ai_hardware_design_for_neural_networks | hardware design]] to overcome current memory and energy limits, deep learning may not fulfill its full potential <a class="yt-timestamp" data-t="00:16:28">[00:16:28]</a>.